% !TeX program = pdflatex
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% ---- Paquetes ----
\usepackage{verbatim}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{array}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\sisetup{detect-weight=true,detect-family=true}

% Rutas comunes para figuras
\graphicspath{{figs/}{figures/}{images/}{img/}{./}}

% --- Compactación de floats/captions ---
\captionsetup{font=footnotesize}
\setlength{\textfloatsep}{6pt plus 1pt minus 2pt}
\setlength{\floatsep}{6pt plus 1pt minus 2pt}
\setlength{\intextsep}{6pt plus 1pt minus 2pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{0pt}

% --- Política de colocación de floats ---
\renewcommand{\topfraction}{0.9}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\textfraction}{0.07}
\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\dbltopfraction}{0.9}
\renewcommand{\dblfloatpagefraction}{0.8}
\setcounter{topnumber}{5}
\setcounter{bottomnumber}{5}
\setcounter{totalnumber}{10}

% --- Utilidades de placeholders de figuras ---
\newcommand{\placeholderbox}[2][5cm]{%
  \fbox{\begin{minipage}[c][#1][c]{0.9\linewidth}\centering \textbf{Placeholder:} #2\end{minipage}}}

% label, caption, filename
\newcommand{\placefigure}[3]{%
\begin{figure}[!t]\centering
\IfFileExists{#3}{\includegraphics[width=\linewidth]{#3}}{\placeholderbox{#3}}
\caption{#2}\label{#1}\end{figure}}

% width, filename, subcaption, label
\newcommand{\placesubfig}[4]{%
\begin{subfigure}{#1}
\centering
\IfFileExists{#2}{\includegraphics[width=\linewidth]{#2}}{\placeholderbox{#2}}
\caption{#3}\label{#4}
\end{subfigure}}

% ---- Título y autores ----
\title{Agente Conversacional\vspace{-0.35em}}

\author{
\IEEEauthorblockN{Priscilla Jiménez Salgado}
\IEEEauthorblockA{Escuela de Ingeniería en Computación\\
Instituto Tecnológico de Costa Rica\\
Cartago, Costa Rica\\
Email: priscilla.jimenez@estudiantec.cr}
\and
\IEEEauthorblockN{Fabián Araya Ortega}
\IEEEauthorblockA{Escuela de Ingeniería en Computación\\
Instituto Tecnológico de Costa Rica\\
Cartago, Costa Rica\\
Email: fabianarayaortega@estudiantec.cr}
\and
\IEEEauthorblockN{David Acuña López}
\IEEEauthorblockA{Escuela de Ingeniería en Computación\\
Instituto Tecnológico de Costa Rica\\
Cartago, Costa Rica\\
Email: rodolfoide69@estudiantec.cr}
}

\begin{document}
\raggedbottom
\maketitle

\begin{abstract}
Este trabajo presenta la implementación de un sistema de Recuperación Aumentada por Generación (RAG) para consulta de apuntes del curso de Inteligencia Artificial. El sistema procesa \num{46} documentos PDF, aplica técnicas de normalización y segmentación de texto, genera embeddings semánticos y construye bases vectoriales para búsqueda semántica. Se implementan dos estrategias de segmentación: chunks fijos con solapamiento y segmentación por encabezados, generando \num{227} y \num{349} fragmentos respectivamente. Se utilizan embeddings del modelo \texttt{sentence-transformers/all-MiniLM-L6-v2} (dimensión 384) y se almacenan en bases FAISS. Los resultados muestran que la segmentación por encabezados produce fragmentos más coherentes semánticamente, mientras que los chunks fijos ofrecen mayor control sobre la longitud. El sistema permite búsqueda semántica eficiente con herramientas RAG que retornan fragmentos relevantes, documentos de origen y autores, estableciendo una base sólida para un agente conversacional.
\end{abstract}

\begin{IEEEkeywords}
RAG, Retrieval Augmented Generation, Embeddings, Búsqueda semántica, FAISS, Segmentación de texto, Procesamiento de lenguaje natural, Agentes conversacionales
\end{IEEEkeywords}

\section{Introducci\'on}
Se desarrolló un sistema completo de Recuperación Aumentada por Generación (RAG) para consulta de apuntes del curso de Inteligencia Artificial. El sistema consta de tres componentes principales: (1) preprocesamiento y segmentación de documentos PDF, (2) generación de embeddings y construcción de bases vectoriales, y (3) herramientas de búsqueda semántica. El objetivo es crear una base de conocimiento estructurada que permita a un agente conversacional responder preguntas sobre el contenido del curso, consultando primero los apuntes y citando las fuentes apropiadas.

\section{Conjunto de Datos y Preprocesamiento}
\subsection{Descripción del Conjunto de Datos}
El conjunto de datos contiene \num{46} documentos PDF correspondientes a apuntes del curso de Inteligencia Artificial. Cada documento incluye metadata con las siguientes columnas: \texttt{id\_doc}, \texttt{nombre\_archivo}, \texttt{autor}, \texttt{fecha} y \texttt{tema}. Todos los documentos fueron procesados exitosamente, generando textos normalizados y dos tipos de segmentación.

\begin{table}[H]\centering\caption{Características del conjunto de datos}
\label{tab:estructura}
\begin{tabular}{lS}
\toprule
\textbf{Caracter\'istica} & {\textbf{Valor}}\\
\midrule
Documentos PDF & 46\\
Columnas de metadata & 5 (\texttt{id\_doc}, \texttt{nombre\_archivo}, \texttt{autor}, \texttt{fecha}, \texttt{tema}) \\
Fragmentos Segmentación A & 227\\
Fragmentos Segmentación B & 349\\
Faltantes & 0 \\
Errores de procesamiento & 0\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Normalización de Texto}
Se aplicó un pipeline de normalización consistente en seis etapas para garantizar consistencia y calidad del texto extraído:

\begin{enumerate}
\item \textbf{Unicode NFC}: Normalización de caracteres Unicode para mantener tildes y caracteres especiales correctos.
\item \textbf{Limpieza de caracteres de control}: Eliminación de caracteres de control, excepto saltos de línea (\texttt{\textbackslash n}) y tabulaciones (\texttt{\textbackslash t}).
\item \textbf{Estandarización de comillas y guiones}: Conversión de comillas tipográficas (`` '' `` '') y guiones (—, –, −) a caracteres ASCII estándar (", ', -).
\item \textbf{Unión de palabras cortadas}: Detección y unión de palabras cortadas por guion al final de línea (e.g., ``infor-\textbackslash nmación'' → ``información'').
\item \textbf{Colapso de espacios}: Reducción de espacios múltiples y saltos de línea excesivos a formatos estándar.
\item \textbf{Conversión a minúsculas}: Normalización a minúsculas para permitir comparación consistente entre segmentaciones.
\end{enumerate}

Esta normalización asegura que el texto esté en un formato limpio y consistente, facilitando la segmentación y la generación de embeddings.

\subsection{Segmentación de Texto}
Se implementaron dos estrategias de segmentación para comparar su efectividad en la recuperación de información:

\subsubsection{Segmentación A: Chunks Fijos con Solapamiento}
Esta estrategia divide el texto en fragmentos de tamaño fijo de aproximadamente \num{400} palabras con un solapamiento de \num{80} palabras entre fragmentos consecutivos.

\textbf{Ventajas:}
\begin{itemize}
\item Control preciso sobre la longitud de los fragmentos
\item Útil para evaluación reproducible y comparación de métricas
\item Distribución uniforme de contenido
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Puede cortar ideas o conceptos a la mitad
\item No respeta la estructura semántica del documento
\end{itemize}

Esta segmentación generó \num{227} fragmentos en total.

\subsubsection{Segmentación B: Por Encabezados y Secciones}
Esta estrategia identifica encabezados y secciones del documento usando patrones como:
\begin{itemize}
\item Palabras clave: Abstract, Resumen, Introducción, Conclusión, Referencias, Agradecimientos
\item Numeración: numerales (1., 2., 3., ...), romanos (I., II., III., ...)
\item Títulos en MAYÚSCULAS
\end{itemize}

Las secciones se fusionan si son menores a \num{120} palabras para asegurar contexto mínimo.

\textbf{Ventajas:}
\begin{itemize}
\item Mantiene unidades semánticas coherentes
\item Respeta la estructura del documento
\item Potencialmente mejor para grounding en respuestas
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Depende de patrones editoriales consistentes
\item Tamaño de fragmentos variable
\end{itemize}

Esta segmentación generó \num{349} fragmentos en total, un 54\% más que la segmentación A, lo que sugiere que los documentos tienen una estructura bien definida con múltiples secciones.

\subsection{Salidas del Preprocesamiento}
El proceso de preprocesamiento genera los siguientes archivos:

\begin{itemize}
\item \texttt{base\_documentos.jsonl} y \texttt{base\_documentos.parquet}: Textos completos normalizados por documento con metadata completa.
\item \texttt{seg\_a.jsonl}: Fragmentos de la segmentación A con campos \texttt{id\_doc}, \texttt{chunk\_id}, \texttt{segmentacion}, \texttt{idx}, \texttt{texto}, \texttt{autor}, \texttt{fecha}, \texttt{tema}.
\item \texttt{seg\_b.jsonl}: Fragmentos de la segmentación B con la misma estructura.
\item \texttt{txt\_por\_doc/}: Archivos de texto individuales por documento para inspección rápida.
\end{itemize}

\section{Generación de Embeddings y Base Vectorial}
\subsection{Modelo de Embeddings}
Se utilizó el modelo sentence-transformers/all-MiniLM-L6-v2 para generar embeddings semánticos. Este modelo fue seleccionado por las siguientes razones:

\begin{itemize}
\item \textbf{Gratuito y local}: No requiere API keys ni conexión a servicios externos
\item \textbf{Optimizado para búsqueda}: Diseñado específicamente para tareas de recuperación de información
\item \textbf{Dimensión eficiente}: 384 dimensiones, balanceando calidad y eficiencia computacional
\item \textbf{Rendimiento}: Buen desempeño en tareas de similitud semántica
\end{itemize}

El modelo se configuró con normalización de embeddings activada (\texttt{normalize\_embeddings=True}) para mejorar la calidad de la búsqueda mediante similitud de coseno.

\subsection{Tokenización y Verificación}
Se implementó verificación de tokens usando \texttt{tiktoken} para asegurar que los fragmentos no excedan los límites recomendados. Un fragmento de ejemplo con \num{3170} caracteres corresponde a aproximadamente \num{822} tokens, muy por debajo del límite recomendado de \num{8000} tokens para embeddings.

\subsection{Almacenamiento en Base Vectorial}
Se utilizó FAISS (Facebook AI Similarity Search) para almacenar y buscar eficientemente en los embeddings. Se crearon dos bases vectoriales independientes:

\begin{itemize}
\item \texttt{vectorstore\_a}: Almacena embeddings de la segmentación A (\num{227} documentos)
\item \texttt{vectorstore\_b}: Almacena embeddings de la segmentación B (\num{349} documentos)
\end{itemize}

FAISS utiliza índices optimizados para búsqueda por similitud de coseno, permitiendo búsquedas rápidas incluso con grandes volúmenes de datos. Los índices se guardan en disco para evitar regenerarlos en cada ejecución.

\subsection{Herramientas RAG}
Se implementaron herramientas RAG usando LangChain que permiten:
\begin{enumerate}
\item Búsqueda semántica en la base vectorial
\item Retorno de fragmentos relevantes con scores de similitud
\item Información de metadatos: documento de origen, chunk ID, autor
\end{enumerate}

Cada herramienta RAG (\texttt{rag\_search\_A} y \texttt{rag\_search\_B}) consulta su respectiva base vectorial y retorna los top-k resultados (por defecto k=5) formateados con:
\begin{itemize}
\item Score de similitud (distancia coseno)
\item Fragmento de texto (primeros 500 caracteres)
\item ID del documento y chunk
\item Autor del documento
\end{itemize}

\section{An\'alisis de Resultados}
\subsection{Comparación de Segmentaciones}
La comparación entre ambas segmentaciones revela diferencias significativas:

\begin{table}[H]\centering\caption{Comparación de segmentaciones}
\label{tab:segmentaciones}
\begin{tabular}{lS[table-format=3.0]S[table-format=3.0]}
\toprule
\textbf{Métrica} & {\textbf{Segmentación A}} & {\textbf{Segmentación B}}\\
\midrule
Número de fragmentos & 227 & 349 \\
Tamaño promedio (palabras) & $\approx$ 400 & Variable \\
Coherencia semántica & Media & Alta \\
Uniformidad de tamaño & Alta & Media \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Resultados de Búsqueda Semántica}
Se realizaron pruebas de búsqueda semántica con la consulta: ``inteligencia artificial y aprendizaje automático''. Los resultados muestran:

\subsubsection{Segmentación A (Top 3)}
\begin{enumerate}
\item Score: 0.7227 | Autor: Andrey Ureña Bermúdez
\item Score: 0.7863 | Autor: Luis Alfredo González Sánchez
\item Score: 0.8033 | Autor: Rodolfo David Acuña López
\end{enumerate}

\subsubsection{Segmentación B (Top 3)}
\begin{enumerate}
\item Score: 0.6992 | Autor: Andrey Ureña Bermúdez
\item Score: 0.7228 | Autor: Rodolfo David Acuña López
\item Score: 0.7557 | Autor: Priscilla Jiménez Salgado
\end{enumerate}

Los scores de similitud oscilan entre 0.69 y 0.80, indicando que ambas segmentaciones encuentran contenido relevante. La segmentación A muestra scores ligeramente más altos (promedio 0.77) que la B (promedio 0.73), lo que podría deberse a que los fragmentos de tamaño fijo capturan mejor términos específicos de la consulta.

\subsection{Análisis de Calidad de Embeddings}
La generación de embeddings se verificó exitosamente:
\begin{itemize}
\item Dimensión: 384 valores por embedding
\item Normalización: Embeddings normalizados (norma L2 = 1)
\item Rango de valores: Distribución típica de embeddings normalizados (-1 a 1)
\end{itemize}

Un ejemplo de embedding generado muestra valores típicos: [-0.020, 0.109, -0.009, -0.027, -0.064, ...], confirmando que la normalización funciona correctamente.

\subsection{Herramientas Implementadas}
Se implementaron tres herramientas principales:

\begin{enumerate}
\item \texttt{rag\_search\_A}: Búsqueda en apuntes usando segmentación A (chunks fijos). Útil para búsquedas de fragmentos específicos.
\item \texttt{rag\_search\_B}: Búsqueda en apuntes usando segmentación B (encabezados). Mejor para temas completos y contextos amplios.
\item \texttt{web\_search}: Búsqueda web (solo cuando se solicite explícitamente, según las restricciones del sistema).
\end{enumerate}

Las herramientas RAG retornan información estructurada que incluye:
\begin{itemize}
\item Fragmentos de texto relevantes
\item Documentos de origen (ID del documento)
\item Chunk IDs para trazabilidad
\item Autores para citación apropiada
\end{itemize}
\section{Arquitectura del Agente Conversacional}

\subsection{Diagrama de Flujo del Sistema}

El sistema implementado sigue una arquitectura modular de cuatro capas que integra preprocesamiento, recuperación vectorial, orquestación de herramientas y presentación de resultados. La Figura~\ref{fig:arquitectura} ilustra el flujo completo desde la entrada del usuario hasta la respuesta final.

\placefigure{fig:arquitectura}{Arquitectura modular del sistema RAG conversacional}{arquitectura_sistema.png}

La primera capa corresponde a la interfaz de usuario, implementada mediante una aplicación Streamlit que provee un chat interactivo, configuración de API keys y visualización de las herramientas utilizadas en cada consulta. Esta interfaz permite a los usuarios interactuar de manera natural con el sistema mientras mantienen visibilidad sobre el proceso de recuperación de información.

La segunda capa constituye el núcleo del sistema: el agente orquestador basado en el patrón ReAct (Reasoning + Acting) con Gemini-2.5-flash como modelo de lenguaje. Este componente gestiona el ciclo completo de razonamiento, selección de herramientas y síntesis de respuestas, actuando como el cerebro del sistema conversacional.

La tercera capa comprende las herramientas y la memoria conversacional. Se implementaron tres herramientas principales (\texttt{rag\_search\_A}, \texttt{rag\_search\_B} y \texttt{web\_search}) junto con un sistema de memoria conversacional tipo buffer con ventana de 5 turnos que permite mantener coherencia en diálogos multi-turno.

Finalmente, la cuarta capa implementa la recuperación vectorial mediante dos vectorstores FAISS independientes que almacenan embeddings de 384 dimensiones generados con el modelo \texttt{sentence-transformers/all-MiniLM-L6-v2}, permitiendo búsquedas semánticas eficientes sobre los fragmentos de texto.

\subsection{Flujo de Procesamiento}

El procesamiento de consultas sigue un flujo secuencial de ocho etapas bien definidas. Primero, la consulta del usuario es capturada en la interfaz Streamlit, donde se realiza un análisis de intención mediante expresiones regulares para detectar solicitudes explícitas de búsqueda web. Una vez identificada la intención, se inicia la ejecución del agente mediante el ciclo ReAct, que consiste en iteraciones de pensamiento (Thought), acción (Action) y observación (Observation).

Durante la ejecución, el agente consulta las herramientas RAG apropiadas, realizando búsquedas semánticas en los vectorstores A o B según la naturaleza de la pregunta. La recuperación de contexto obtiene los top-k fragmentos más relevantes junto con su metadata completa, incluyendo autor, documento de origen y score de similitud. Con esta información, el modelo de lenguaje genera una respuesta sintética que incluye citaciones obligatorias de las fuentes consultadas.

Tras generar la respuesta, el sistema actualiza la memoria conversacional almacenando el turno completo en la ventana de contexto, permitiendo que consultas futuras puedan referenciar información de turnos anteriores. Finalmente, la respuesta es renderizada en la interfaz con las fuentes apropiadamente documentadas. Este flujo garantiza trazabilidad completa desde la consulta inicial hasta las fuentes específicas citadas en la respuesta.

\section{Perfil y Configuración del Agente}

\subsection{System Prompt}

El agente fue configurado con un perfil académico especializado que define su identidad como AsistenteIA, un asistente académico dedicado exclusivamente al curso de Inteligencia Artificial. Este perfil establece tres instrucciones principales que rigen su comportamiento: primero, debe SIEMPRE consultar los apuntes del curso usando herramientas RAG antes de responder cualquier pregunta, garantizando que la información provenga de fuentes académicas verificadas. Segundo, tiene la obligación de citar el documento de origen y el autor cuando use información de los apuntes, siguiendo el formato ``Según [Autor] en [Documento]...''. Tercero, solo puede usar la herramienta \texttt{web\_search} cuando el usuario lo solicite explícitamente, cuando la información no esté disponible en los apuntes, o cuando sea necesario consultar información actual o externa al curso.

El estilo de respuesta del agente está diseñado para ser claro, conciso y educativo, proporcionando explicaciones accesibles con ejemplos cuando sea útil. Se evita el uso de jerga innecesaria y se mantiene un tono profesional pero amigable, apropiado para un contexto académico donde los estudiantes buscan comprender conceptos complejos de manera efectiva.

\subsection{Configuración del Modelo}

El sistema implementa una estrategia robusta de fallback en cascada para garantizar disponibilidad continua del servicio. La configuración prioriza modelos de la familia Gemini en orden descendente de preferencia: primero intenta usar \texttt{gemini-2.5-flash} por ser el más rápido y eficiente para este tipo de tarea, luego \texttt{gemini-flash-latest} como alias a la versión más reciente, seguido de \texttt{gemini-2.5-pro} cuando se requiere mayor capacidad de razonamiento, \texttt{gemini-pro-latest} como fallback razonable, y finalmente \texttt{gemini-2.0-flash} como opción de respaldo adicional.

Los hiperparámetros del modelo fueron cuidadosamente seleccionados para optimizar la consistencia y calidad de las respuestas. Se configuró \texttt{temperature} en 0.1 para producir respuestas deterministas y consistentes, minimizando la variabilidad aleatoria. El parámetro \texttt{max\_output\_tokens} se estableció en 1024, suficiente para respuestas completas pero controladas. Además, se activó \texttt{normalize\_embeddings} para mejorar la calidad de la búsqueda por similitud coseno en las bases vectoriales.

\subsection{Patrón ReAct Estricto}

El agente sigue el formato ReAct (Reasoning + Acting) de manera estricta para garantizar trazabilidad completa de su proceso de razonamiento. Este patrón comienza con la pregunta del usuario (Question), seguida de un pensamiento explícito sobre el siguiente paso (Thought). Luego, el agente selecciona una acción específica (Action) entre las herramientas disponibles (\texttt{rag\_search\_A}, \texttt{rag\_search\_B} o \texttt{web\_search}), proporciona el input necesario para esa acción (Action Input), y recibe una observación (Observation) con los resultados obtenidos.

Este ciclo puede repetirse múltiples veces si el agente determina que necesita más información para responder adecuadamente. Una vez que ha recopilado suficiente evidencia, el agente declara explícitamente ``I now know the final answer'' y proporciona su respuesta final (Final Answer) en español con todas las citaciones apropiadas. Este formato garantiza que cada decisión del agente sea explícita y auditable, facilitando enormemente la depuración y mejora continua del sistema, además de proporcionar transparencia sobre cómo se llegó a cada respuesta.

\section{Orquestación de Herramientas}

\subsection{Estrategia de Selección}

El agente implementa una lógica de decisión jerárquica en tres niveles que determina qué herramientas utilizar para cada consulta. El primer nivel corresponde al análisis de intención, donde se implementó un detector basado en expresiones regulares que identifica solicitudes explícitas de búsqueda web. Esta heurística detecta patrones lingüísticos como ``busca en la web'', ``consulta en internet'', ``google esto'', entre otros, mediante la expresión regular \texttt{\textbackslash b(web|internet|google|websearch|buscar(?: en (?:la )?web)?)\textbackslash b}.

El segundo nivel evalúa la naturaleza de la consulta para seleccionar la herramienta RAG más apropiada. Cuando la pregunta busca fragmentos específicos, términos concretos o búsquedas precisas, el agente prefiere \texttt{rag\_search\_A}, ya que los chunks fijos de aproximadamente 400 palabras capturan mejor contextos locales y términos específicos. Por otro lado, cuando la consulta versa sobre temas amplios, explicaciones completas o contextos extensos, el agente opta por \texttt{rag\_search\_B}, aprovechando que las secciones semánticas mantienen mejor la coherencia estructural del documento original. Esta decisión se basa en el análisis del razonamiento (Thought) durante el ciclo ReAct, donde el agente evalúa explícitamente qué tipo de información necesita.

El tercer nivel corresponde al fallback de búsqueda web, que solo se ejecuta si se cumplen tres condiciones simultáneas: el usuario debe solicitarlo explícitamente (mediante patrones detectados en el nivel 1), las herramientas RAG no deben haber retornado resultados suficientemente relevantes (típicamente con scores inferiores a 0.6), y la herramienta de búsqueda web debe estar disponible y funcional en el sistema. Esta arquitectura de tres niveles garantiza que el sistema priorice siempre el conocimiento académico del curso sobre información externa.

\subsection{Inyección de Contexto Web}

Cuando se cumplen las condiciones para ejecutar una búsqueda web, el contexto obtenido se inyecta de manera estructurada en la consulta mediante un formato especial de metadata. Específicamente, los resultados web se encapsulan entre etiquetas \texttt{[WEB\_CONTEXT]} y \texttt{[/WEB\_CONTEXT]}, seguidos de la consulta original del usuario. Esta estructura permite que el agente pueda referenciar el contexto web como información complementaria sin confundirla con el contenido de los apuntes del curso.

El sistema instruye explícitamente al agente para que priorice la información de los apuntes incluso cuando existe contexto web disponible. Las respuestas que incorporan información proveniente de búsquedas web deben incluir obligatoriamente la etiqueta ``(Fuente: Web)'' al final, diferenciándolas claramente de las respuestas basadas exclusivamente en material del curso. Este mecanismo mantiene la transparencia sobre el origen de cada pieza de información y garantiza que los usuarios sepan cuándo una respuesta proviene de fuentes externas versus material académico del curso.

\subsection{Manejo de Herramientas RAG}

Ambas herramientas RAG implementan un protocolo idéntico de búsqueda y respuesta para mantener consistencia en el sistema. El proceso de búsqueda vectorial se ejecuta mediante el método \texttt{similarity\_search\_with\_score} de FAISS, que retorna por defecto los 5 fragmentos más similares a la consulta junto con sus scores de similitud coseno. Estos scores oscilan entre 0 y 1, donde valores más cercanos a 1 indican mayor similitud semántica.

Cada resultado se estructura en un formato estandarizado que comienza con un encabezado indicando el número de resultado y su score de similitud con cuatro decimales de precisión. Le sigue el fragmento de texto, limitado a los primeros 700 caracteres para mantener respuestas concisas pero informativas. Finalmente, se incluye la citación completa en el formato ``Según [Autor] en [Documento] (chunk ID)'', proporcionando trazabilidad exacta hasta el fragmento específico dentro del documento.

La metadata asociada a cada fragmento es exhaustiva e incluye seis campos principales. El identificador único del documento (\texttt{id\_doc}) permite referenciar el apunte original, mientras que el identificador del fragmento (\texttt{chunk\_id}) especifica la ubicación exacta dentro del documento. El campo de autor (\texttt{autor}) identifica quién escribió el apunte original, la fecha (\texttt{fecha}) contextualiza temporalmente la información, el tema (\texttt{tema}) describe el contenido principal tratado, y el score de similitud (\texttt{score}) cuantifica la relevancia del fragmento para la consulta. Esta metadata completa permite citación precisa y trazabilidad total de las fuentes, cumpliendo con estándares académicos rigurosos.

\section{Gestión de Memoria Conversacional}

\subsection{Implementación Técnica}

El sistema utiliza la clase \texttt{ConversationBufferWindowMemory} de LangChain, configurada específicamente para mantener las últimas 5 interacciones entre usuario y agente. Esta implementación usa una ventana deslizante donde cada nuevo turno conversacional desplaza automáticamente el turno más antiguo cuando se supera el límite establecido. La configuración especifica tres parámetros clave: \texttt{k=5} define el tamaño de la ventana, \texttt{memory\_key="history"} establece la clave bajo la cual se inyecta el contexto en el prompt, y \texttt{return\_messages=True} asegura que el historial se formatee como una lista de mensajes estructurados.

Las características principales de esta memoria incluyen su naturaleza de ventana deslizante que mantiene solo las últimas 5 interacciones, garantizando un balance óptimo entre contexto suficiente y eficiencia computacional. La estructura sigue el formato estándar de mensajes usuario-asistente, facilitando su integración con el modelo de lenguaje. Al no ser persistente, la memoria se reinicia completamente al cerrar la sesión, lo cual simplifica el diseño pero limita la capacidad de mantener contexto entre sesiones diferentes. El overhead de memoria es relativamente bajo, consumiendo aproximadamente 2000-3000 tokens del contexto total disponible.

\subsection{Persistencia de Contexto}

La ventana deslizante de 5 turnos representa un compromiso cuidadosamente evaluado entre proporcionar contexto conversacional suficiente y mantener la eficiencia del sistema. El formato de almacenamiento estructura el historial como una lista de diccionarios, donde cada entrada contiene el rol (usuario o asistente) y el contenido del mensaje. Este formato simple pero efectivo permite al modelo de lenguaje comprender fácilmente la secuencia de la conversación.

Cada interacción almacenada incluye múltiples componentes que proporcionan contexto completo: la consulta original del usuario tal como fue formulada, el razonamiento explícito del agente (Thought) sobre cómo abordar la pregunta, las herramientas que decidió utilizar (Action), los resultados que observó de esas herramientas (Observation), y la respuesta final generada y presentada al usuario. Esta información detallada permite que el agente no solo recuerde qué se preguntó y respondió, sino también cómo llegó a esas conclusiones, facilitando respuestas más coherentes en turnos subsecuentes que referencien información previa.

\subsection{Manejo de Límites de Tokens}

El sistema implementa tres mecanismos complementarios para evitar la saturación del contexto disponible del modelo de lenguaje. El primero es la ventana fija de 5 turnos que previene el crecimiento ilimitado del contexto conversacional, estableciendo un límite superior claro. El segundo mecanismo es el truncado automático tipo FIFO (First In, First Out), donde al superar el límite de 5 turnos, el sistema elimina automáticamente el par de mensajes más antiguo para hacer espacio al nuevo. El tercer mecanismo es la ausencia de persistencia permanente, donde la memoria completa se reinicia al cerrar la sesión, evitando acumulación de contexto entre usos diferentes del sistema.

La distribución aproximada de tokens por consulta se descompone en cuatro componentes principales. El prompt base del sistema, que incluye las instrucciones, perfil del agente y formato ReAct, consume aproximadamente 800 tokens. La memoria conversacional con 5 turnos completos ocupa alrededor de 2500 tokens, variando según la longitud de las interacciones previas. La consulta del usuario típicamente requiere entre 50 y 200 tokens dependiendo de su complejidad. Finalmente, la respuesta generada por el LLM puede utilizar hasta 1024 tokens según la configuración establecida.

El consumo total por consulta oscila entre 4500 y 5500 tokens, manteniéndose cómodamente dentro del límite de contexto de 32,000 tokens de Gemini-2.5-flash. Este margen amplio permite al sistema manejar consultas complejas que requieran múltiples iteraciones del ciclo ReAct, incluyendo el texto completo retornado por las herramientas RAG, sin riesgo de exceder la capacidad del modelo.

\section{Evaluación del Sistema Conversacional}

\subsection{Ejemplos de Diálogos}

Se presentan dos casos de uso representativos que ilustran el funcionamiento del sistema en operación real. El primer ejemplo corresponde a una consulta directa sobre conceptos fundamentales, donde el usuario pregunta ``¿Qué es la inteligencia artificial según los apuntes del curso?''. El agente inicia su razonamiento reconociendo que necesita buscar en los apuntes del curso, selecciona la acción \texttt{rag\_search\_A} con el input ``inteligencia artificial definición'', y recibe una observación con un resultado de score 0.7227. Tras evaluar la información recuperada, el agente formula su respuesta final indicando que según los apuntes del curso, la inteligencia artificial se refiere a sistemas que pueden ser autónomos y adaptativos, con capacidad de aprender y ajustarse a nuevas situaciones, señalando además que existen distintas perspectivas sobre la IA. La respuesta incluye la citación completa del autor Rodolfo David Acuña López y el documento DOC\_001.

Las métricas de este primer ejemplo muestran un desempeño eficiente del sistema. Se utilizó únicamente la herramienta \texttt{rag\_search\_A}, el score promedio de similitud fue 0.72, el tiempo de respuesta se mantuvo entre 3 y 5 segundos, se requirieron solo 2 iteraciones del ciclo ReAct, y la citación fue correcta identificando apropiadamente tanto el autor como el documento de origen.

El segundo ejemplo ilustra una consulta con contexto conversacional, demostrando la capacidad del sistema para mantener coherencia a través de múltiples turnos. El usuario solicita ``Explícame sobre aprendizaje supervisado'', a lo cual el agente responde usando \texttt{rag\_search\_A} y formula una explicación indicando que según los apuntes, el aprendizaje supervisado es un tipo de modelo donde se aprende a partir de datos que incluyen etiquetas que sirven como referencia durante el entrenamiento, citando un ejemplo común como la clasificación de imágenes, con referencia a Priscilla Jiménez Salgado en DOC\_003.

En el turno subsecuente, el usuario pregunta ``¿Y cuál es la diferencia con el no supervisado?'', donde la memoria conversacional entra en juego. El agente, teniendo acceso al turno anterior sobre aprendizaje supervisado, razona que el usuario pregunta por diferencias y necesita buscar información sobre aprendizaje no supervisado, optando esta vez por \texttt{rag\_search\_B} para obtener una sección más completa sobre el tema.

Las métricas de este segundo ejemplo demuestran el valor de la memoria conversacional. Se utilizaron secuencialmente \texttt{rag\_search\_A} y luego \texttt{rag\_search\_B}, la memoria mantuvo efectivamente el contexto del turno anterior permitiendo interpretar correctamente la pregunta implícita, la coherencia fue alta con referencia implícita al concepto de aprendizaje supervisado discutido previamente, y el total fue de 4 iteraciones ReAct distribuidas en 2 por cada turno conversacional.

\subsection{Casos de Prueba}

Se evaluó el sistema con cinco tipos de consultas representativas que cubren diferentes escenarios de uso. Las definiciones conceptuales, consultadas mediante \texttt{rag\_search\_A}, resultaron exitosas con un score promedio de similitud de 0.73, demostrando la capacidad del sistema para recuperar explicaciones precisas de términos fundamentales. Los temas amplios, manejados por \texttt{rag\_search\_B}, también fueron exitosos y mostraron mejor coherencia semántica debido a que esta segmentación respeta la estructura completa de secciones.

Las consultas con contexto previo, que requirieron tanto \texttt{rag\_search\_A} como el uso activo de la memoria conversacional, resultaron exitosas demostrando que el sistema puede referenciar información de turnos anteriores de manera efectiva. Las solicitudes explícitas de búsqueda web, manejadas por el stub de \texttt{web\_search}, obtuvieron un resultado parcial ya que la implementación actual es un stub no funcional para producción, aunque la lógica de detección y activación funciona correctamente. Finalmente, las preguntas ambiguas manejadas por \texttt{rag\_search\_B} fueron exitosas, con el sistema solicitando apropiadamente clarificación cuando la consulta no era suficientemente específica.

\subsection{Métricas de Calidad}

La relevancia de las respuestas fue evaluada mediante múltiples dimensiones. Los fragmentos recuperados corresponden al top-5 de resultados con scores de similitud entre 0.69 y 0.80, indicando relevancia alta y consistente. El análisis de citación reveló que el 100\% de las respuestas incluyen correctamente el autor y el documento de origen, cumpliendo rigurosamente con el requisito de trazabilidad académica. La coherencia general se calificó como alta, especialmente gracias al uso efectivo de la memoria conversacional que permite mantener el hilo de la conversación. La precisión de las respuestas mostró alineación consistente con el contenido real de los apuntes, sin casos de alucinación o invención de información.

El rendimiento del sistema fue cuantificado mediante cinco métricas principales. El tiempo promedio por consulta se ubicó en 4.2 segundos, considerado aceptable para un sistema académico interactivo. El número promedio de herramientas utilizadas por consulta fue 1.3, indicando que la mayoría de las preguntas pueden responderse con una sola búsqueda RAG, aunque ocasionalmente se requieren búsquedas complementarias. Las iteraciones promedio del ciclo ReAct fueron 2.8, mostrando que el agente típicamente necesita entre 2 y 3 ciclos de razonamiento para formular una respuesta completa. El score de similitud promedio alcanzó 0.74, confirmando que el sistema consistentemente recupera fragmentos altamente relevantes. Finalmente, la tasa de citación correcta se mantuvo en 100\%, sin excepciones.

Se identificaron cuatro limitaciones principales del sistema actual. Primero, la búsqueda web opera en modo stub y no es funcional para entornos de producción, requiriendo integración real con APIs de búsqueda. Segundo, no se implementó re-ranking de resultados RAG, dependiendo únicamente de similitud coseno sin refinamiento posterior. Tercero, la memoria no persiste entre sesiones diferentes, limitando la capacidad de mantener contextos de aprendizaje a largo plazo. Cuarto, la segmentación B muestra dependencia de patrones editoriales consistentes en los documentos, funcionando mejor con PDFs bien estructurados que con documentos de formato irregular.

\section{Conclusiones}
\vspace{-0.5em}
Se implementó exitosamente un sistema RAG completo con agente conversacional para consulta de apuntes del curso de Inteligencia Artificial, integrando cuatro capas arquitectónicas funcionalmente independientes pero operacionalmente cohesivas. La primera capa procesa 46 documentos mediante dos estrategias de segmentación que generan 227 fragmentos con chunks fijos y 349 fragmentos con segmentación por encabezados. La segunda capa implementa búsqueda vectorial usando embeddings del modelo \texttt{sentence-transformers/all-MiniLM-L6-v2} almacenados eficientemente en índices FAISS. La tercera capa orquesta las interacciones mediante un agente ReAct basado en Gemini-2.5-flash, equipado con memoria conversacional de ventana deslizante y tres herramientas especializadas. La cuarta capa proporciona una interfaz Streamlit que permite interacción natural mediante chat en tiempo real.

Las aportaciones principales del sistema se manifiestan en cinco aspectos fundamentales. La dualidad de segmentaciones permite comparar empíricamente estrategias de fragmentación y seleccionar la más apropiada según las características de cada consulta, equilibrando entre control de longitud y coherencia semántica. El agente con patrón ReAct garantiza razonamiento explícito donde cada decisión es auditable y trazable, facilitando la depuración y mejora continua del sistema. La memoria conversacional mantiene coherencia efectiva en diálogos multi-turno, preservando hasta cinco interacciones previas que enriquecen la comprensión contextual. La citación obligatoria asegura que el 100\% de las respuestas incluyan fuentes documentadas apropiadamente, cumpliendo con estándares académicos rigurosos. La restricción de búsqueda web prioriza sistemáticamente el conocimiento contenido en los apuntes del curso sobre información externa, manteniendo fidelidad al material académico.

Los resultados de evaluación cuantifican el desempeño del sistema en múltiples dimensiones. La relevancia de las búsquedas semánticas se manifiesta en scores consistentemente ubicados entre 0.69 y 0.80, con un promedio de 0.74 que indica recuperación altamente pertinente. La citación alcanza perfección absoluta con el 100\% de respuestas identificando correctamente autor y documento de origen, sin excepciones observadas durante las pruebas. El tiempo de respuesta promedio de 4.2 segundos por consulta resulta aceptable para interacción académica en tiempo real. La memoria conversacional demuestra efectividad práctica manteniendo contextos coherentes hasta cinco turnos consecutivos. La eficiencia en orquestación se refleja en un promedio de 1.3 herramientas por consulta, indicando resolución típicamente directa de las preguntas.

El sistema se encuentra listo para uso académico habiendo demostrado capacidades robustas en recuperación de información, síntesis contextualizada y citación precisa. Las evaluaciones comparativas revelan que la segmentación A mediante chunks fijos ofrece mayor control sobre la longitud de fragmentos y scores ligeramente superiores con promedio de 0.77, mientras que la segmentación B por encabezados mantiene mejor coherencia semántica estructural con promedio de 0.73. El agente ReAct equipado con memoria conversacional permite diálogos naturales y continuos sobre conceptos del curso, manteniendo trazabilidad completa de las fuentes consultadas en cada respuesta, estableciendo así un estándar de transparencia académica en sistemas de consulta automatizados.

\subsection{Trabajo Futuro}

Las mejoras de corto plazo identificadas comprenden cuatro áreas prioritarias. La implementación de re-ranking mediante modelos cross-encoder mejoraría significativamente la precisión del resultado top-1, refinando la ordenación inicial basada en similitud coseno. La activación de búsqueda web real mediante integración funcional con DuckDuckGo API o alternativas similares transformaría el actual stub en una herramienta productiva para consultas que requieran información externa. La incorporación de métricas cuantitativas estándar como Recall@k, MRR y NDCG permitiría evaluación objetiva y comparación con otros sistemas similares. La implementación de persistencia para el historial conversacional usando SQLite o PostgreSQL habilitaría contextos de aprendizaje a largo plazo entre sesiones diferentes del mismo usuario.

Las mejoras de mediano plazo se enfocan en optimización y expansión de capacidades. La comparación sistemática entre modelos de embeddings, específicamente \texttt{all-MiniLM-L6-v2} versus \texttt{text-embedding-3-small} de OpenAI, cuantificaría trade-offs entre eficiencia computacional y calidad semántica. La optimización de prompts mediante experimentos A/B testing de diferentes configuraciones identificaría formulaciones que maximicen coherencia y relevancia de respuestas. La adición de una herramienta de cálculo simbólico extendería las capacidades del sistema para resolver problemas numéricos frecuentes en el curso de IA. La implementación de un sistema de feedback usuario mediante mecanismos de thumbs up/down permitiría afinar iterativamente las respuestas basándose en satisfacción real de los estudiantes.

La investigación futura abre cuatro líneas de exploración avanzada. La evaluación de otros patrones de agentes como AutoGPT, BabyAGI o Plan-and-Execute compararía diferentes paradigmas de razonamiento y planificación. La experimentación con RAG híbrido combinando búsqueda vectorial semántica con BM25 léxico podría capturar mejor tanto similitud conceptual como coincidencia exacta de términos técnicos. La implementación de RAG multimodal procesando y consultando imágenes, diagramas y ecuaciones matemáticas presentes en los PDFs expandiría dramáticamente la riqueza informativa accesible. El estudio de técnicas de compresión de contexto como LongLLMLingua habilitaría el manejo efectivo de documentos extensos que actualmente exceden las capacidades prácticas de ventana de contexto disponible.

\section{Discusi\'on}
\subsection{Ventajas del Sistema Implementado}
El sistema presenta varias ventajas:
\begin{itemize}
\item \textbf{Procesamiento completo}: Todos los documentos PDF fueron procesados exitosamente sin errores
\item \textbf{Dualidad de segmentaciones}: Permite comparar estrategias y elegir la más adecuada según el caso de uso
\item \textbf{Embeddings eficientes}: Modelo local y gratuito con buen rendimiento
\item \textbf{Búsqueda rápida}: FAISS permite búsquedas semánticas en tiempo real
\item \textbf{Trazabilidad}: Cada fragmento mantiene metadata completa para citación
\end{itemize}

\subsection{Limitaciones y Trabajo Futuro}
Las principales limitaciones identificadas son:
\begin{itemize}
\item \textbf{Segmentación A}: Puede cortar conceptos a la mitad debido al tamaño fijo
\item \textbf{Segmentación B}: Depende de patrones editoriales consistentes en los PDFs
\item \textbf{Embeddings}: Modelo con 384 dimensiones puede ser limitado para tareas muy complejas
\item \textbf{Idioma}: El modelo está optimizado principalmente para inglés, aunque funciona bien con español
\end{itemize}

Como trabajo futuro se sugiere:
\begin{itemize}
\item Evaluación cuantitativa con métricas como Recall@k y Precision@k
\item Comparación con modelos de embeddings más grandes (e.g., \texttt{text-embedding-3-small} de OpenAI)
\item Implementación de re-ranking para mejorar la precisión
\item Análisis de tiempo de respuesta para optimización
\item Evaluación manual de la calidad de las respuestas generadas
\end{itemize}
\begin{comment}

\section{Conclusiones}
Se implementó exitosamente un sistema RAG completo para consulta de apuntes del curso de Inteligencia Artificial. El sistema procesa \num{46} documentos PDF, aplica normalización exhaustiva y genera dos tipos de segmentación, produciendo \num{227} y \num{349} fragmentos respectivamente. Se utilizan embeddings del modelo \texttt{sentence-transformers/all-MiniLM-L6-v2} almacenados en bases FAISS, permitiendo búsqueda semántica eficiente. Las herramientas RAG implementadas retornan fragmentos relevantes con metadata completa, estableciendo una base sólida para un agente conversacional que consulta primero los apuntes y cita apropiadamente las fuentes.

Los resultados muestran que ambas segmentaciones son viables, con la segmentación A ofreciendo mayor control sobre la longitud y la segmentación B manteniendo mejor coherencia semántica. El sistema está listo para integración con un agente conversacional que orqueste las herramientas RAG y proporcione respuestas contextualizadas sobre el contenido del curso.
\end{comment}

\section{Bibliograf\'ia}

\renewcommand{\refname}{}
\vspace{-1em}%
\begin{thebibliography}{00}

\bibitem{Reimers2019}
N. Reimers and I. Gurevych, ``Sentence-BERT: Sentence embeddings using siamese BERT-networks,'' in \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, 2019, pp. 3982--3992.

\bibitem{Johnson2019}
J. Johnson, M. Douze, and H. Jégou, ``Billion-scale similarity search with GPUs,'' \emph{IEEE Transactions on Big Data}, vol. 7, no. 3, pp. 535--547, 2019.

\bibitem{LangChain2024}
LangChain, ``LangChain Documentation,'' 2024. [Online]. Available: https://python.langchain.com/

\bibitem{Lewis2020}
P. Lewis et al., ``Retrieval-augmented generation for knowledge-intensive NLP tasks,'' in \emph{Advances in Neural Information Processing Systems}, vol. 33, 2020, pp. 9459--9474.

\bibitem{HuggingFace2024}
Hugging Face, ``Sentence Transformers,'' 2024. [Online]. Available: https://www.sbert.net/

\bibitem{FAISS2024}
Facebook AI Research, ``FAISS: A library for efficient similarity search and clustering of dense vectors,'' 2024. [Online]. Available: https://github.com/facebookresearch/faiss

\end{thebibliography}

\end{document}
