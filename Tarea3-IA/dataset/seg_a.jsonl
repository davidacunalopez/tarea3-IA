{"id_doc": "DOC_033", "segmentacion": "A", "chunk_id": "DOC_033_A_000", "idx": 0, "autor": "María José Chacón Rodríguez", "fecha": "2025-10-07", "tema": "Conceptos avanzados de redes neuronales convolucionales (CNN), con énfasis en arquitecturas modernas como AlexNet, VGG y ResNet, aplicadas al procesamiento de imágenes.", "texto": "apuntes ia clase 7/10 gianmarco oporta pe'rez ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica san jose', costa rica gooporta@estudiantec.cr abstract-el presente documento recopila los apuntes de la mu'ltiples agentes que colaboran o compiten entre s'ı clase del 7 de octubre, correspondientes al curso de inteligencia y con su entorno para alcanzar objetivos comunes o artificial. se abordan los lineamientos del proyecto 1, centrado individuales. en redes neuronales convolucionales aplicadas al reconocimiento de voz mediante espectrogramas, as'ı como los fundamentos teo'ricos de las redes convolucionales y su aplicacio'n en tareas ii. descripcio'ndelproyecto de clasificacio'n y segmentacio'n de ima'genes. el primer proyecto tiene como objetivo aplicar redes neuindex terms-inteligencia artificial, redes neuronales conronales convolucionales (cnn) en la tarea de reconocimiento volucionales, clasificacio'n de audio, pytorch, espectrogramas de voz a partir de espectrogramas. su propo'sito es desarrollar un modelo de clasificacio'n multiclase capaz de identificar i. resultadosdelquiz5 distintos tipos de sonidos, tales como vocalizaciones humanas se realizo' el quiz 5 durante el inicio de la sesio'n, la cual y animales, instrumentos musicales o ruidos ambientales. contiene las siguientes preguntas: para lograrlo, se utiliza un conjunto de datos pu'blico orientado a la clasificacio'n de audio, en el cual cada muestra - pregunta:describaque' esunaredtotalmenteconectada. corresponde a una grabacio'n corta etiquetada con su clase r/ es una red neuronal en la cual cada neurona esta' correspondiente.antesdelentrenamiento,lassen˜alesacu'sticas conectada con todas las neuronas de la capa siguiente, son transformadas en representaciones visuales denominadas de principio a fin. espectrogramas, las cuales se emplean como entrada a las - pregunta: mencione tres funciones de activacio'n no redes convolucionales. lineales. r/ relu, sigmoide y tanh. elproyectorequierelaconstruccio'nmanualdedosmodelos - pregunta: describa los cuatro componentes principales empleando pytorch sin librer'ıas de alto nivel: de un agente llm. - modelo a: variante cla'sica de lenet-5 adaptada al r/ reconocimiento de audio mediante espectrogramas. - perfil:definelaidentidadopersonalidaddelagente, - modelo b: arquitectura alternativa fundamentada en determinando su tono, estilo de comunicacio'n y literatura acade'mica o en un disen˜o propio justificado comportamiento general. teo'ricamente. - memoria: permite conservar informacio'n relevante se deben generar dos versiones del conjunto de datos: una de interacciones anteriores o resultados previos, fa- con los audios transformados a ima'genes (base) y otra con cilitando la continuidad y el contexto en tareas versionesaumentadasmediantete'cnicasdedataaugmentation extensas. orientadasaldominiodelaudio.esteprocesobuscaincremen- - herramientas:correspondenarecursosofunciones tar la robustez del modelo y su capacidad de generalizacio'n. externas que el agente puede invocar para realizar durante la fase de entrenamiento se construyen cuoperacionesespec'ıficasoaccederainformacio'nadi- atro combinaciones"}
{"id_doc": "DOC_033", "segmentacion": "A", "chunk_id": "DOC_033_A_001", "idx": 1, "autor": "María José Chacón Rodríguez", "fecha": "2025-10-07", "tema": "Conceptos avanzados de redes neuronales convolucionales (CNN), con énfasis en arquitecturas modernas como AlexNet, VGG y ResNet, aplicadas al procesamiento de imágenes.", "texto": "disen˜o propio justificado comportamiento general. teo'ricamente. - memoria: permite conservar informacio'n relevante se deben generar dos versiones del conjunto de datos: una de interacciones anteriores o resultados previos, fa- con los audios transformados a ima'genes (base) y otra con cilitando la continuidad y el contexto en tareas versionesaumentadasmediantete'cnicasdedataaugmentation extensas. orientadasaldominiodelaudio.esteprocesobuscaincremen- - herramientas:correspondenarecursosofunciones tar la robustez del modelo y su capacidad de generalizacio'n. externas que el agente puede invocar para realizar durante la fase de entrenamiento se construyen cuoperacionesespec'ıficasoaccederainformacio'nadi- atro combinaciones principales: modelo a/base, modelo cional. a/aumentado,modelob/baseymodelob/aumentado.cada - planificacio'n o razonamiento: consiste en la ca- modelo se entrena con diferentes hiperpara'metros, evaluando pacidad del agente para interpretar las instrucciones sudesempen˜oconme'tricascomoprecisio'n,pe'rdida,f1-score del usuario, elaborar estrategias y seleccionar la y matriz de confusio'n. la herramienta weights & biases se accio'n ma's apropiada segu'n el objetivo planteado. utiliza para monitorear y visualizar los resultados durante el - pregunta:describaladiferenciaentresistemasdeagente entrenamiento. u'nico y sistemas multiagentes. finalmente, los modelos seleccionados se comparan para r/ un agente u'nico percibe su entorno, toma decisiones determinar el de mejor rendimiento general. el informe debe y ejecuta acciones de manera independiente, mientras presentarse en formato ieee, acompan˜ado del co'digo fuente que los sistemas multiagentes esta'n conformados por y el cuaderno en jupyter notebook. iii. aspectospra'cticosdelproyecto el desarrollo debe realizarse en pytorch, construyendo manualmente cada capa de la red. es necesario registrar todas las me'tricas relevantes utilizadas en clases anteriores, incluyendo la pe'rdida, precisio'n y f1-score. dado que los espectrogramas generados pueden ser pesados, se recomienda reducir su resolucio'n a 224 p'ıxeles por lado. la fecha tentativa de entrega fue establecida para el jueves 30deoctubre.seesperaquelosmodelosimplementadossean fig.1. redesconvolucionales completamente reproducibles y que incluyan mecanismos de control de aleatoriedad y registro de resultados. c. aplicaciones comunes iv. fundamentosderedesneuronales convolucionales lasredesconvolucionalesseaplicanampliamenteendiversas tareas de visio'n artificial, entre las que destacan: las redes neuronales convolucionales (cnn, por sus siglas en ingle's) representan una evolucio'n de las redes neuronales - clasificacio'n de ima'genes. tradicionales orientadas al procesamiento de datos estruc- - segmentacio'n de objetos. turados espacialmente, como ima'genes o espectrogramas. a - segmentacio'n de instancias. diferencia de las redes completamente conectadas, las cnn - procesamiento general de ima'genes. aprendenpatronesespacialesyjera'rquicosdemaneraeficiente. estas arquitecturas han demostrado una gran eficacia en hasta este punto del curso, se ha trabajado con redes problemas de reconocimiento visual, deteccio'n de patrones y que reciben un vector de caracter'ısticas como entrada, lo procesamiento de sen˜ales en el dominio de la visio'n. transforman mediante capas ocultas y"}
{"id_doc": "DOC_033", "segmentacion": "A", "chunk_id": "DOC_033_A_002", "idx": 2, "autor": "María José Chacón Rodríguez", "fecha": "2025-10-07", "tema": "Conceptos avanzados de redes neuronales convolucionales (CNN), con énfasis en arquitecturas modernas como AlexNet, VGG y ResNet, aplicadas al procesamiento de imágenes.", "texto": "datos estruc- - segmentacio'n de objetos. turados espacialmente, como ima'genes o espectrogramas. a - segmentacio'n de instancias. diferencia de las redes completamente conectadas, las cnn - procesamiento general de ima'genes. aprendenpatronesespacialesyjera'rquicosdemaneraeficiente. estas arquitecturas han demostrado una gran eficacia en hasta este punto del curso, se ha trabajado con redes problemas de reconocimiento visual, deteccio'n de patrones y que reciben un vector de caracter'ısticas como entrada, lo procesamiento de sen˜ales en el dominio de la visio'n. transforman mediante capas ocultas y producen una salida. references sin embargo, este enfoque no considera la estructura espacial de los datos, lo cual puede generar errores al mover, rotar o [1] s.pacheco,\"convolutionalneuralnetworks\"presentacio'n, escalar los objetos dentro de una imagen. a. ejemplo: dataset cifar-10 el conjunto de datos cifar-10 se utiliza frecuentemente para experimentacio'n en visio'n por computadora. contiene ima'genes a color de taman˜o 32×32×3, representando diez clases diferentes. aunquecadaimagenesrelativamentepequen˜a,unaversio'n de mayor resolucio'n, por ejemplo 200×200×3, aumentar'ıa dra'sticamente el nu'mero de para'metros de entrada, lo que dificultar'ıa la escalabilidad del modelo. b. estructura de una red convolucional en una red convolucional, las neuronas se organizan en tres dimensiones: ancho, alto y profundidad. cada neurona esta' conectada u'nicamente a una pequen˜a regio'n de la capa anterior, en lugar de estar completamente conectada, lo cual reduce la complejidad y mejora la eficiencia computacional. las capas principales que componen una cnn son: - capa convolucional: calcula las salidas de las neuronas conectadas a regiones locales de la capa anterior. - capa de agrupamiento (pooling): reduce el taman˜o espacial de las representaciones, manteniendo las caracter'ısticas ma's relevantes. - capa completamente conectada (fully connected): transforma la representacio'n final en probabilidades de pertenencia a una clase espec'ıfica. a medida que las ima'genes avanzan a trave's de las capas convolucionales y de agrupamiento, se reduce su taman˜o espacial, pero aumenta la abstraccio'n de las caracter'ısticas aprendidas."}
{"id_doc": "DOC_032", "segmentacion": "A", "chunk_id": "DOC_032_A_000", "idx": 0, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "redes neuronales convolucionales y backpropagation apuntesdeclases rodolfo david acun˜a lo'pez escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica rodolfoide69@estudiantec.cr abstract-en este documento podra' encontrar informacio'n a. respuesta del quiz sobre la semana 10 de clases de ia, donde se comparten las se realizo' el quiz 5 donde se establecieron las respuestas respuestas del quiz 5, se comentan detalles sobre el primer de este. las preguntas con sus respuestas son las siguientes: proyecto, un pequen˜o resumen de la clase anterior sobre back propagationysehablasobreuntemanuevodondepodemosver pregunta: describa que' es una red totalmente conectada temas como convnet o arquitectura de red convolucional. (fully connected) respuesta: es un tipo de red neuronal en index terms-redes neuronales convolucionales, backprop- la que cada neurona esta' conectada con todas las neuronas de agation, cnn, reconocimiento de patrones, procesamiento de la capa siguiente. ima'genes, deep learning pregunta: mencione 3 funciones de activacio'n no-lineales. respuesta: relu, sigmoide y tanh i. introduccio'n pregunta: describa los 4 componentes principales de un agente llm. respuesta: las redes neuronales han revolucionado el campo de la in- - perfil: puede tener su propia personalidad teligenciaartificial,especialmenteentareasdereconocimiento - memoria: permite que el agente recuerde informacio'n depatronesyprocesamientodesen˜ales.enestedocumentose pasada o resultados previos para mantener contexto en abordan dos conceptos fundamentales: el algoritmo de back- tareas largas propagation, que permite el entrenamiento eficiente de redes - herramientas: son funciones externas que el agente neuronales profundas, y las redes neuronales convolucionales puede usar para ejecutar acciones (cnn),quehandemostradoserparticularmenteefectivaspara - planificacio'n o razonamiento: decide que' hacer, interel procesamiento de ima'genes y sen˜ales. pretandolasinstruccionesdelusuarioyeligiendolamejor el backpropagation es un algoritmo de optimizacio'n que accio'n calcula los gradientes de la funcio'n de pe'rdida con respecto pregunta: describa la diferencia entre sistemas de agente a los para'metros de la red, permitiendo su ajuste mediante u'nico y sistemas multiagentes. respuesta: un agente u'nico descenso de gradiente. por otro lado, las cnn introducen percibe su entorno, toma decisiones y actu'a por s'ı mismo, conceptos como convolucio'n y pooling que aprovechan la mientrasquelossistemasmultiagentessonvariosagentesque estructura espacial de los datos, reduciendo significativamente interactu'an entre s'ı y con el entorno. el nu'mero de para'metros necesarios comparado con redes b. explicacio'n del proyecto totalmente conectadas. paraesteproyectonecesitamosaplicarredesneuronalespara este documento se estructura de la siguiente manera: el reconocimiento de voz a partir de espectrogramas, es decir, primero se presentan aspectos administrativos del curso inreconocimiento de patrones en voz donde utilizaremos una cluyendo respuestas del quiz y detalles del proyecto, luego se"}
{"id_doc": "DOC_032", "segmentacion": "A", "chunk_id": "DOC_032_A_001", "idx": 1, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "como convolucio'n y pooling que aprovechan la mientrasquelossistemasmultiagentessonvariosagentesque estructura espacial de los datos, reduciendo significativamente interactu'an entre s'ı y con el entorno. el nu'mero de para'metros necesarios comparado con redes b. explicacio'n del proyecto totalmente conectadas. paraesteproyectonecesitamosaplicarredesneuronalespara este documento se estructura de la siguiente manera: el reconocimiento de voz a partir de espectrogramas, es decir, primero se presentan aspectos administrativos del curso inreconocimiento de patrones en voz donde utilizaremos una cluyendo respuestas del quiz y detalles del proyecto, luego se arquitectura que se llama redes neuronales convolucionales revisa el algoritmo de backpropagation con sus fundamentos (cnn) la cual nos sirve para el procesamiento de ima'genes. matema'ticos, y finalmente se introduce el concepto de redes elretodeesteproyectoesanalizarunaserietemporalenun convolucionales con la arquitectura alexnet como ejemplo. audiodondeanalizaremoslasen˜alquevienealsegundodonde estemos procesando, por ejemplo, si estamos procesando un ii. aspectosadministrativos 5t, hay que procesar un 5t+1, 5t+2, y as'ı sucesivamente. se podr'ıan resolver con redes recurrentes. debido a que no hubo noticias previas a la clase, se inicio' otra forma de hacer esto es convertir esa voz en espectrocon una breve explicacio'n sobre el primer proyecto de redes gramas, la cual es un diagrama de tiempo y las frecuencias neuronales. queproducelasen˜aldeaudio.conestoseproduceunpatro'n. una herramienta mencionada por el profesor es weights and biases, la cual es una herramienta de seguimiento y visualizacio'n de experimentos de machine learning donde nosotrosejecutamosunentrenamientoyvemosentiemporeal desde cualquier computador co'mo se esta' comportando un modelo. la ventaja es que podemos ver el comportamiento por lo que podemos detenerlo si no vemos buenos resultados. el procesamiento de ima'genes puede ser algo pesado por lo que debemos reducir el taman˜o de estas a un taman˜o de 224x224. esto debido a que computacionalmente se vuelve costoso. fig.1. backandforwardpropagation. nosepuedenutilizarlibrer'ıascomoresnetquesirvenpara abstraer la definicio'n de capas ma's alla' de torch.nn. tenemos solo una neurona. cada una de esas neuronas esta'n el profesor nos recomienda buscar una herramienta en compuestas por una funcio'n no lineal que tiene como entrada redes neuronales que nos pueda hacer toda la arquitectura una funcio'n lineal. para optimizar los pesos en esa funcio'n del modelo. incluso esta se puede hacer en pytorch, queda debemos hacer derivadas parciales. al final esa derivada va a nuestra disposicio'n. a ser el activador de la capa anterior por lo que no necesito tenemos dos modelos: conocerco'mofuecomputadacadacapaanterior.solonecesito - lenet-5 cla'sico: este es la arquitectura ma's ba'sica elresultadoyyaconesopuedocalcularladerivadaqueyovoy (como el profesor lo menciona) para el procesamiento a necesitar. si yo ocupo calcular la"}
{"id_doc": "DOC_032", "segmentacion": "A", "chunk_id": "DOC_032_A_002", "idx": 2, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "nos pueda hacer toda la arquitectura una funcio'n lineal. para optimizar los pesos en esa funcio'n del modelo. incluso esta se puede hacer en pytorch, queda debemos hacer derivadas parciales. al final esa derivada va a nuestra disposicio'n. a ser el activador de la capa anterior por lo que no necesito tenemos dos modelos: conocerco'mofuecomputadacadacapaanterior.solonecesito - lenet-5 cla'sico: este es la arquitectura ma's ba'sica elresultadoyyaconesopuedocalcularladerivadaqueyovoy (como el profesor lo menciona) para el procesamiento a necesitar. si yo ocupo calcular la derivada parcial, respecto de ima'genes el cual fue creado por yann lecun. a la funcio'n de pe'rdida con mi para'metro w, lo que tengo que - arquitectura alternativa: esta esta' basada en literatura hacer es aplicar la regla de la cadena para llegar al para'metro la cual implementa cualquier arquitectura distinta. demifuncio'n.hayca'lculosquesiempresevanarepetirporlo podemosescogerdiferentesespectrogramascomoporejemque podremos guardar esos ca'lculos para evitar recalcularlos plo, el data augmentation el cual trata de aumentar los datos de nuevo para cada uno de los para'metros. deentrenamientoparamejorarlageneralizacio'ndemimodelo con la finalidad de obtener mejores patrones. ∂l ∂zl ∂al∂l i = i, en el paper specaugment, que sale en la bibliograf'ıa del ∂wl ∂wl ∂zl ∂al proyecto, propone 3 tipos de te'cnicas: ∂l ∂zl∂al∂l i = i. - time masking: donde tomo una frecuencia del 1 al 1.5 ∂bl ∂bl ∂zl ∂al donde hago una ma'scara para cancelar el ruido esto nos da como resultado un vector gradiente, la cual - time warping: para estirar o encoger podemos ver en la fig. 2, que tiene el ca'lculo de todos los - frequencymasking:queaplicama'scarassimilarespero gradientes por todos los para'metros en la red. en el eje de la frecuencia, lo que simula la pe'rdida o interferencia de ciertas bandas del espectro de audio el entrenamiento se debe realizar varias veces por lo que se debe dejar todo montado y conectado la herramienta de weights and biases. esto porque el entrenamiento con ima'genespuedeserpesado,requieredegpu.elprofesormenciona que podemos usar google colab pero que es limitado, por lo que debemos aprovechar los recursos. la extensio'n de la documentacio'n debe ser de ma'ximo 10 pa'ginas. los apuntes anteriores son los ma's relevantes sobre la explicacio'n. fig.2. vectorgradiente. c. repaso de back propagation si tenemos mu'ltiples neuronas, tenemos que utilizar super'ındices o sub'ındices, podemos ver un ejemplo en la fig. este nos permite determinar cua'nto aporta cada peso al 3. el primero me indica la capa en la que me encuentro, en error total de la red, ajustando los"}
{"id_doc": "DOC_032", "segmentacion": "A", "chunk_id": "DOC_032_A_003", "idx": 3, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "lo que debemos aprovechar los recursos. la extensio'n de la documentacio'n debe ser de ma'ximo 10 pa'ginas. los apuntes anteriores son los ma's relevantes sobre la explicacio'n. fig.2. vectorgradiente. c. repaso de back propagation si tenemos mu'ltiples neuronas, tenemos que utilizar super'ındices o sub'ındices, podemos ver un ejemplo en la fig. este nos permite determinar cua'nto aporta cada peso al 3. el primero me indica la capa en la que me encuentro, en error total de la red, ajustando los para'metros en la direccio'n este caso el l. el segundo me indica cua'l neurona es para contraria al proceso de propagacio'n hacia adelante, as'ı como identificar cada una de ellas. los pesos esta'n asociados a las lo podemos observar en la fig. 1. capasmevanaindicarhaciado'ndevoyydedo'ndeprovengo. vamosaverlasoperacionescomografosdondevanaserun podemos tener dos funciones, las cuales son: tipo de operaciones donde vamos a ponerle un sobrenombre. - preactivacio'n el sobrenombre nos puede ayudar con las derivadas parciales. cuando tratamos de optimizar un grafo, contamos con dos z(l) =b(l)+ n (cid:88)l-1 w(l) a(l-1) etapas. la de salida la cual le llamamos activacio'n l donde j j j,k k k=1 de error mayor. ba'sicamente las neuronas se desconectan de susentradasoriginalesyrecibenotrasparalascualesnofueron entrenadas. hayundatasetquesellamacifar-10elcualson10clases con taman˜o pequen˜o 32x32 pero son a color, con 3 canales. por tanto, si tuviera que hacer una red neuronal para conectar cada p'ıxel a una neurona, tendre' una entrada de 3072 pesos. con esto entramos a un problema de dimensionalidad. esto se ve bien pero las ima'genes son pequen˜as, ¿que' pasa si se vuelven ma's grandes? para resolver este problema, entra fig.3. grafodimensional. en juego el convnet, donde vamos a tener 3 dimensiones, donde vamos a tener filtros. cada filtro se encargara' de reconocer patrones en una imagen. esos filtros pueden ser - activacio'n (cid:16) (cid:17) reconocimientos de l'ıneas verticales, horizontales, diagonales, a(l) =g z(l) j j entreotras,queseespecializanenextraerinformacio'ndeesas la funcio'n de activacio'n se aplica a toda la capa. toda la ima'genes. capasecomputaconsigmoide.tenemosfuncionesdepe'rdida, donde podemos tener una pe'rdida total dada por: l = (cid:88) nl (cid:0) a(l)-y (cid:1)2 i j j j=1 el resultado de la derivada de pe'rdida con respecto a la activacio'n es la siguiente: fig.4. redtotalmenteconectadavsconvnet mi salida se va a reducir, es decir, si ten'ıa 224x224, mi ∂ ∂ l a i l = (cid:0) (al 1 -y 1 )2+(al 2 -y 2 )2+---+(al n -y n )2(cid:1)′ salidapuedeser212x212,ysieran3canales,puedequeahora j tengan 64 canales. esa cantidad de canales van a representar ∂l"}
{"id_doc": "DOC_032", "segmentacion": "A", "chunk_id": "DOC_032_A_004", "idx": 4, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "tener una pe'rdida total dada por: l = (cid:88) nl (cid:0) a(l)-y (cid:1)2 i j j j=1 el resultado de la derivada de pe'rdida con respecto a la activacio'n es la siguiente: fig.4. redtotalmenteconectadavsconvnet mi salida se va a reducir, es decir, si ten'ıa 224x224, mi ∂ ∂ l a i l = (cid:0) (al 1 -y 1 )2+(al 2 -y 2 )2+---+(al n -y n )2(cid:1)′ salidapuedeser212x212,ysieran3canales,puedequeahora j tengan 64 canales. esa cantidad de canales van a representar ∂l i =2 (cid:0) al -y (cid:1) la cantidad de filtros que yo tuve que calcular. esos filtros ∂al j j pueden representar colores, l'ıneas, nu'meros, entre otros. j la arquitectura esta' compuesta por 3 etapas: el resultado de la derivada de activacio'n con respecto a la de reactivacio'n es la siguiente: a. convolutional layer enestasetomaelfiltro,sedeslizaporlaimagenparahacer ∂a( j l) =g (cid:16) z(l) (cid:17) (cid:0) 1-g (cid:16) z(l) (cid:17)(cid:1) el ca'lculo de los features. tenemos como entrada el largo, ∂z(l) j j anchoycanalesquevoyaprocesar.estacomputalasalidade j neurona que se encuentran conectadas a las regiones locales. en la siguiente figura yo puedo hacer varios ca'lculos de porlotanto,sisequiereusarunacantidaddexfiltros,lasalida derivadas, donde a m'ı no me interesa co'mo llego' el valor de esta va a ser el ancho, largo y x. despue's de los filtros se z ya que al final es un valor que me llego' a la funcio'n, donde aplica una funcio'n de activacio'n. esos filtros se calculan por se aplicaron varias derivadas para llegar a un valor. al final, cada canal. cada neurona que compute, no me interesa co'mo me llego' la informacio'n desde la funcio'n de pe'rdida ya que a partir de b. pooling layer cierto punto. con el valor resultante, puedo sacar la derivada estaseencargadereducirlasdimensionesenlargoyancho, con respecto a x y con respecto a y almacenadas, y tener las en otras palabras, reducir la imagen. esta aplica la operacio'n derivadas desde mi funcio'n de pe'rdida con mis entradas. en de downsampling a lo largo de dimensiones espaciales como resumen, la propagacio'n hacia adelante es la capa de entrada elanchoylargo.sisuentradaesde32x32x12,susalidapuede por la de salida y la propagacio'n hacia atra's es desde la capa llegar a ser de 16x16x16. este no tiene para'metros y no me de salida hacia la de entrada donde se calcula la gradiente del afecta la profundidad. error con respecto a los pesos de cada capa. c. fully-connected iii. convnet en esta es ma's fa'cil reducir la informacio'n de una imagen hasta"}
{"id_doc": "DOC_032", "segmentacion": "A", "chunk_id": "DOC_032_A_005", "idx": 5, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "de dimensiones espaciales como resumen, la propagacio'n hacia adelante es la capa de entrada elanchoylargo.sisuentradaesde32x32x12,susalidapuede por la de salida y la propagacio'n hacia atra's es desde la capa llegar a ser de 16x16x16. este no tiene para'metros y no me de salida hacia la de entrada donde se calcula la gradiente del afecta la profundidad. error con respecto a los pesos de cada capa. c. fully-connected iii. convnet en esta es ma's fa'cil reducir la informacio'n de una imagen hasta ahora hemos trabajado con una red fully connected a un vector ma's pequen˜o que el que yo ten'ıa en la entrada, con entradas y salidas. el problema es que en el modelo entonces, a partir de ese momento hago mi clasificador. en anterior usado para mnist, nos puede dar varios errores otras palabras, se calcula la probabilidad de que pertenezca como si muevo las ima'genes de un centro, y las neuronas que a una clase y as'ı convertir una imagen de p'ıxeles a una se activaban estaban fijas, entonces podemos tener un margen probabilidad de pertenecer a una clase. d. alexnet esta es una arquitectura que salio' para la revolucio'n de convoluciones y el deep learning en todos sus aspectos. fig.5. arquitecturaalexnet. en la fig. 5 podemos apreciar el proceso. los cubitos de adentro representan el taman˜o de los filtros. de esta manera podemos tratar un problema de clasificacio'n en ima'genes. lo primero que se hace es una convolucio'n donde extraemos ciertas caracter'ısticas. para reducir las ima'genes se realiza un pooling, donde se pierden p'ıxeles. al final llegamos a un resumen de la imagen anterior. solo quedar'ıa hacer una fully connected. fig.6. reduccio'ndeimagen."}
{"id_doc": "DOC_034", "segmentacion": "A", "chunk_id": "DOC_034_A_000", "idx": 0, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "apuntes de clase #2 luis felipe calderón pérez escuela de ingeniería en computación tecnológico de costa rica cartago, costa rica 2021048663 9-10-2025 resumen-este documento presenta los apuntes de la decima semana del curso de inteligencia artificial. se realizó un repaso de las redes neuronales convolucionales (cnn), partiendo de las limitacionesdelasredesfullyconnectedylanecesidaddeextraer información espacial de las imágenes. se estudió el funcionamiento de la estructura de las capas convolucionales, pooling y fully-connected y feature map. finalmente, se discutieron los figura1. arquitecturacnnvsnn[3] principios para diseñar arquitecturas convolucionales eficientes, considerando tamaños de filtro, stride, padding y reducción de mismas estarán conectadas a pequeñas regiones de la capa parámetros. anterior, y esto reduce el tamaño de la imagen a un vector. index terms-cnn, capas convolucionales, transfer learning, en cada cara del cubo de las neuronas se tienen n filtros arquitecturas convolucionales. de tamaño acorde a las imágenes de entrada y obtenemos un i. breverepasodelaclaseanterior featuremap,queeselresultadodeaplicarelfiltroalaimagen i-a. redes convolucionales anterior. recibimosuninputdecaracterísticas,elcualtransformamos i-b. arquitectura de cnn con una serie de capas ocultas. requiere de 3 capas principales se había visto hasta el momento las redes fully connected. 1. convolutional layer teníamos el problema de que aprendemos una secuencia de computa el filtro contra una imagen. píxeles, lo que nos lleva a errores si movemos los objetos de recibe de entrada el ancho, largo y canales. lugar o si aplicamos rotación a los objetos. lo correcto sería tiene n filtros que extraen características de las imásacar la información de la imagen para tomar una decisión. genes. también se habló del dataset de cifar-10, donde hay aplica una capa de activación imágenes con 3 canales de 32x32. y nos damos cuenta de tiene como parámetros (wx)n+b que no es escalable, ya que se tendrían 120,000 parámetros que ajustar únicamente en la entrada. 2. pooling layer por ello se buscan métodos más eficientes; entonces lle- reduce de la imagen en ancho y largo. gamos a las convnet, en donde las neuronas se organizan se encarga de aplicar el downsampling a lo largo del en 3 dimensiones: largo, ancho y profundidad (canales). las ancho y largo. no tiene parámetros. se introduce periódicamente en medio de capas convolucionales. lo más usual es usar max pooling. fórmula dimensionalidad - entrada wxhxd. - k, tamaño de kernel. - s, stride. figura3. funcionamientokernelcapadeconvolución[3] - d, mantiene la profundidad. w 2 = w- s k +1 ii-a. filtro o kernel 3. fully-connected es una matriz bidimensional de"}
{"id_doc": "DOC_034", "segmentacion": "A", "chunk_id": "DOC_034_A_001", "idx": 1, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "se organizan se encarga de aplicar el downsampling a lo largo del en 3 dimensiones: largo, ancho y profundidad (canales). las ancho y largo. no tiene parámetros. se introduce periódicamente en medio de capas convolucionales. lo más usual es usar max pooling. fórmula dimensionalidad - entrada wxhxd. - k, tamaño de kernel. - s, stride. figura3. funcionamientokernelcapadeconvolución[3] - d, mantiene la profundidad. w 2 = w- s k +1 ii-a. filtro o kernel 3. fully-connected es una matriz bidimensional de números, que transforma esta parte clasifica, ya que calcula la probabilidad de una imagen en el momento en que deslice ese filtro, produpertenecer a una clase; transformando una imagen de ciendo una imagen como salida. para aplicar los filtros se va píxeles a probabilidad de pertenecer a una clase. a tener algo similar a un caso donde una imagen de entrada las cnns nos permiten resolver tiene mucho ruido y se le aplica un gaussian kernel (tiene clasificación de imágenes. una campana de gauss) y da una imagen resultante como si segmentación de objetos. tuvieraunblur.dependiendodelfiltrousado,obtenemosotras segmentación de instancias. formas como resultado e influimos en el procesamiento. procesamiento de imágenes. ii-b. local receptive fields ii. capadeconvolución una neurona esta conectada a un campo en específico del tiene varias características, están compuestas por varios input. esto es muy eficiente, ya que podría haber filtros que filtros (w), va a tener un comportamiento local, lo deslizamos extraigan líneas verticales, otros horizontales y así. y va a seguir extrayendo las mismas features alrededor de la ii-b1. campo receptivo: es un filtro de nxn, donde cada imagen. esto permite que los pesos se ajusten para que sirvan neurona estará enfocada en un solo campo receptivo. en una posición como en otra. ii-b2. stride: es la forma clásica de deslizar el filtro, según una cantidad de pasos que realiza el filtro sobre la imagen durante la convolución. ii-b3. padding: técnica para agregar pixeles alrededor del borde de la imagen, permitiendo controlar el tamaño de salida de la convolución. se recomienda que el padding se llene con 0, ya que si se usa 1 podría generar ruido o mala figura2. arquitecturaalexnet[2] data. y su fórmula es k-1, k = tamaño del filtro. 2 ii-b4. cálculo de dimensiones: papers la cantidad de convoluciones y relu se usa la fórmula m, cantidad de pixeles en fila/columna. de 3≥n≥0. k, tamaño del kernel. iii-a. ¿que arquitectura preferimos? p, tamaño del"}
{"id_doc": "DOC_034", "segmentacion": "A", "chunk_id": "DOC_034_A_002", "idx": 2, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "de la imagen, permitiendo controlar el tamaño de salida de la convolución. se recomienda que el padding se llene con 0, ya que si se usa 1 podría generar ruido o mala figura2. arquitecturaalexnet[2] data. y su fórmula es k-1, k = tamaño del filtro. 2 ii-b4. cálculo de dimensiones: papers la cantidad de convoluciones y relu se usa la fórmula m, cantidad de pixeles en fila/columna. de 3≥n≥0. k, tamaño del kernel. iii-a. ¿que arquitectura preferimos? p, tamaño del padding. se prefiere a las arquitecturas con convoluciones pequeñas, s, cantidad de pasos. yaquelasconvolucionesgrandesnosllevanaquelasneuronas m-k+2p +1= dimension resultante s se computen de forma lineal y que la cantidad de pesos sea ii-c. pesos mayor. si se tuviera una imagen de 224x224x3, con un tamaño iii-b. algunas \"reglas\" de kernel 11, stride de 4 y padding de 0, y le aplicamos la el tamaño de la imagen debería ser divisible por 2. fórmula anterior da 55. y con la arquitectura de alexnet 2 las convoluciones deben usar campos receptivos pequeobtenemos que aprendimos 96 de profundidad. ños 3x3, con un stride de 1. ii-d. pesos compartidos para pooling layer es común max pooling de f=2, s=3. si ya tenemos un filtro que extrae cierta caracteristica, y s, cantidad de pasos. sirve para una posición; también sirve para otra posición. por iii-c. menciones finales lo que, vamos a usar el mismo filtro para toda la imagen. al final se mencionan arquitecturas similares a lenet para ii-e. transfer learning tomarencuentaparaelproyecto,talescomo,alexnet,afnet, semencionaqueenelpaperdealexnetdespuésdeaplicar googlenet (reduce parámetros), vgg16 y resnet. su arquitectura y lo referente a ella. y se dan cuenta que en nota: embedding, información distribuida en espacio veclas primeras capas hay figuras o información similar. por lo torial que retorna mi nn. que se introduce el término de transfer learning, que consiste referencias en pasar el peso de las primeras capas a otra red, para ahorrar [1] \"resnet, alexnet, vggnet, inception: understanding various architiempo de entrenamiento. tectures of convolutional networks,\" cv-tricks.com, aug. 01, 2022. https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/ iii. arquitecturasconvolucionales [2] r.r.abril,\"redesconvolucionales,\"lamáquinaoráculo,jul.2025, se componen de convolutional layer, pooling layer y [online]. available: https://lamaquinaoraculo.com/deep-learning/redesneuronales-convolucionales/ de dense layer. se deben tomar desiciones sobre nuestra [3] s. a. p. portuguez, \"apuntes de la clase de inteligencia artificial,\" arquitectura, por ejemplo, si la convolución reduce el input cartago,costarica,agosto2025,clasedel9deoctubredel2025. debo decidir si hago o no el pooling. y estas desiciones determinan el comportamiento del tamaño de la imagen, pero si la imagen es"}
{"id_doc": "DOC_034", "segmentacion": "A", "chunk_id": "DOC_034_A_003", "idx": 3, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "entrenamiento. tectures of convolutional networks,\" cv-tricks.com, aug. 01, 2022. https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/ iii. arquitecturasconvolucionales [2] r.r.abril,\"redesconvolucionales,\"lamáquinaoráculo,jul.2025, se componen de convolutional layer, pooling layer y [online]. available: https://lamaquinaoraculo.com/deep-learning/redesneuronales-convolucionales/ de dense layer. se deben tomar desiciones sobre nuestra [3] s. a. p. portuguez, \"apuntes de la clase de inteligencia artificial,\" arquitectura, por ejemplo, si la convolución reduce el input cartago,costarica,agosto2025,clasedel9deoctubredel2025. debo decidir si hago o no el pooling. y estas desiciones determinan el comportamiento del tamaño de la imagen, pero si la imagen es muy reducida, le llega poca información a la fully connected. se introdujó el término de stack, que es, input → [[conv → relu]∗n → pool?]∗m → [fc → relu] ∗ k → fc,m ≥ 0,k ≥ 0, y se menciona que en"}
{"id_doc": "DOC_038", "segmentacion": "A", "chunk_id": "DOC_038_A_000", "idx": 0, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "resumen sobre autoencoders, segmentación y rags: conceptos y arquitecturas andrés sánchez rojas escuela de ingeniería en computación instituto tecnológico de costa rica 16/10/2025 resumen-estossonlosapuntesdelasegundaclasedesemana patrones relevantes y reconstruir las entradas con alta fidelidad. 11 del curso de ia. los autoencoders son modelos de aprendizaje estaversatilidadlosconvierteenherramientasvaliosastantoen no supervisado que aprenden a reconstruir sus entradas a través aplicaciones de seguridad como en procesamiento de imágenes deunarepresentacióncomprimida(espaciolatente).enestedocuy señales. mento se resumen conceptos clave: arquitectura encoder/decoder, variational autoencoders (vae) y la reparametrización, funciones de pérdida típicas, y aplicaciones como detección de anomalías ii-b. encoder y decoder y denoising. también se introduce la segmentación de imagen y la arquitectura u-net, y se discuten conceptos relacionados el encoder es un conjunto de bloques convolucionales que con rags (retrieval augmented generation), agentes basados en extraen la información más relevante de la entrada y descartan llm,tokenizaciónyembeddings.elobjetivoesofrecerunavisión compacta y legible para un lector que busca una introducción loquenoaporta,comprimiendolosdatosatravésdeun\"cuello técnica y aplicada. de botella\" para eliminar ruido y características innecesarias; index terms-autoencoders, variational autoencoders, u-net, la salida de ese proceso es el vector o espacio latente, una segmentación de imagen, rag, tokenización, embeddings representacióndebajadimensionalidadqueconservalosrasgos útilesparadiferenciarpatrones.eldecodereslapartequetoma i. introducción el espacio latente y reconstruye la señal o imagen original, este documento sintetiza los principios y aplicaciones expandiendo la información comprimida para producir una prácticas de los autoencoders, describiendo su entrenamiento salida lo más fiel posible a la entrada; su objetivo es invertir sin etiquetas por reconstrucción, sus usos en reducción de dila codificación del encoder y permitir tareas como denoising, mensionalidad, compresión, detección de anomalías, denoising upscaling o detección de anomalías mediante la comparación y superresolución, y la extensión hacia variantes relevantes entre entrada y reconstrucción. comolosvariationalautoencoders;ademáspresentalaconexión con tareas de visión por computador (p. ej., segmentación y ii-c. aplicaciones arquitecturas tipo u-net) y la extensión a representaciones para texto mediante tokenización y embeddings, así como su entre las aplicaciones prácticas destacan: papel en sistemas más amplios como rags y agentes basados en llm. el texto ofrece una guía práctica con definiciones, detección de anomalías y fraude . fórmulas y recomendaciones operativas para implementar eliminación de ruido (denoising). experimentos en imágenes y texto. aumento de resolución (upscaling). reconocimiento facial y compresión de imágenes. ii. autoencoders ii-a. definición y propósito iii. variationalautoencoder losautoencoderssonunaarquitecturanovedosaenelámbito del aprendizaje automático que se caracteriza por comparar son una variante probabilística de los autoencoders que sus salidas con las mismas entradas, lo que permite entrenarlos generan una representación latente continua"}
{"id_doc": "DOC_038", "segmentacion": "A", "chunk_id": "DOC_038_A_001", "idx": 1, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "llm. el texto ofrece una guía práctica con definiciones, detección de anomalías y fraude . fórmulas y recomendaciones operativas para implementar eliminación de ruido (denoising). experimentos en imágenes y texto. aumento de resolución (upscaling). reconocimiento facial y compresión de imágenes. ii. autoencoders ii-a. definición y propósito iii. variationalautoencoder losautoencoderssonunaarquitecturanovedosaenelámbito del aprendizaje automático que se caracteriza por comparar son una variante probabilística de los autoencoders que sus salidas con las mismas entradas, lo que permite entrenarlos generan una representación latente continua modelada como sin necesidad de etiquetas, clasificándolos como modelos no una distribución. en lugar de devolver un único vector latente supervisados. su principal utilidad radica en la reducción de determinista, el encoder estima parámetros de una distribución: dimensionalidad,ofreciendorepresentacionesmáspotentesque la media µ(x) y la log-varianza logσ2(x). técnicas clásicas como el análisis de componentes principales (pca).estacapacidaddecompresiónyreconstrucciónloshace iii-a. reparametrización especialmente útiles en tareas como la detección de anomalías, la identificación de transacciones fraudulentas, la eliminación la reparametrización permite que la aleatoriedad se aísle de ruido en datos, el aumento de resolución (upscaling) y el re- en una variable independiente, de forma que los gradientes conocimientofacial.enesencia,losautoencodersaprendenuna puedan fluir hacia los parámetros que predicen la media y la codificación eficiente de los datos, lo que les permite capturar varianza. iii-b. funciones de pérdida vi. tokenizaciónyembeddings la pérdida de un variational autoencoder combina dos vi-a. tokenización términos: la tokenización convierte texto en secuencias de identifica1. reconstruction loss: mide la discrepancia entre la dores. estrategias comunes de tokenización son: por palabra, entrada y la reconstrucción producida por el decoder. por subword, por caracter, por bytes. cada estrategia tiene 2. kl divergence: compara qué tanto se parecen dos trade-offs en cobertura, eficiencia y manejo de formas raras. distribuciones. vi-b. embeddings la pérdida total habitual es la suma: reconstruction loss + kl los embeddings son vectores densos que representan tokens divergence. osecuenciasenunespaciodondelaproximidadindicasimilitud semántica. al agregar embeddings de tokens (por ejemplo iii-c. espacio latente y generación mediante promedio o modelos que producen representaciones un beneficio importante de los variational autoencoders es de secuencia) se obtienen vectores de frases/consultas útiles que el espacio latente resultante es continuo: puntos cercanos para búsquedas semánticas y recuperación en rags, y como en el espacio latente generan observaciones similares, lo entrada para razonamiento en agentes. que permite interpolación y generación de nuevas muestras vii. conclusiones mediante muestreo. los puntos vistos en esta clase y resumidos en este docuiv. segmentacióndeimagen mento ofrecen una síntesis compacta de"}
{"id_doc": "DOC_038", "segmentacion": "A", "chunk_id": "DOC_038_A_002", "idx": 2, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "que producen representaciones un beneficio importante de los variational autoencoders es de secuencia) se obtienen vectores de frases/consultas útiles que el espacio latente resultante es continuo: puntos cercanos para búsquedas semánticas y recuperación en rags, y como en el espacio latente generan observaciones similares, lo entrada para razonamiento en agentes. que permite interpolación y generación de nuevas muestras vii. conclusiones mediante muestreo. los puntos vistos en esta clase y resumidos en este docuiv. segmentacióndeimagen mento ofrecen una síntesis compacta de conceptos relevantes en autoencoders, variational autoencoders, segmentación de lasegmentaciónconsisteenlocalizaryetiquetarpíxelesque imágenes con arquitecturas como u-net, y rags y agentes pertenecenaobjetosdeinterésdentrodeunaimagen.devuelve basados en llms. estos avances han permitido la expansión un mapa donde cada píxel tiene una etiqueta, siendo útil en del uso de la inteligencia artificial en ambientes en los que aplicaciones que requieren alta resolución espacial como el antesnosehubieraconsideradoútil.sinembargo,debemosser análisis médico o el conteo de células. responsablesaldecidirquétareasrealmenterequierenunagente o pueden usar un sistema más ligero de machine learning. iv-a. arquitectura u-net u-net es una arquitectura en forma de ü\"similar a un autoencoder pero con skip connections entre las capas de encoder y decoder. estas conexiones permiten conservar información durante el upsampling, mejorando significativamente la precisión de los mapas de segmentación. u-net ha demostrado ser especialmente útil en tareas médicas como la identificación de células cancerígenas. v. ragsyagentes v-a. rag: retrieval augmented generation los rags combinan recuperaciónde documentos relevantes con generación de lenguaje. el flujo general es: 1. convertir la consulta y fragmentos de texto en embeddings. 2. buscar textos relevantes en una base de conocimiento mediante medidas de similitud en el espacio de embeddings. 3. pasar los fragmentos recuperados como contexto a un modelo de lenguaje para generar respuestas más fundamentadas. v-b. agentes basados en llm los agentes usan un llm como núcleo de decisión para orquestarpasos(consultarfuentes,ejecutarapis,leermemoria). un agente integra recuperación, gestión del contexto y conectores a herramientas externas para resolver tareas complejas de forma autónoma."}
{"id_doc": "DOC_035", "segmentacion": "A", "chunk_id": "DOC_035_A_000", "idx": 0, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "apuntes ia clase 14/10/2025 juan jime'nez valverde escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica juand0908@estudiantec.cr abstract-estedocumentoresumelosconceptosclavevistosen c. stride, padding y ca'lculo de dimensiones laclasedeinteligenciaartificialsobreredesneuronalesconvolucionales(cnn)yautoencoders.seabordantemasfundamentales - stride: indica cua'ntos pasos da el filtro al desplazarse comolosfiltros,camposreceptivos,stride,paddingylascapasde sobre la imagen. un stride mayor reduce el taman˜o de la pooling, as'ı como las arquitecturas cla'sicas de cnn, entre ellas salida. lenet,alexnet,zfnet,googlenet,vgg16,resnetydensenet. - padding:agregap'ıxeles(usualmenteceros)alrededorde adema's, se presentan consideraciones pra'cticas para el ana'lisis la imagen de entrada para controlar el taman˜o de salida de modelos de aprendizaje profundo, como la visualizacio'n de activaciones, los embeddings de caracter'ısticas y la estructura y preservar dimensiones. el padding sime'trico t'ıpico se de los autoencoders, incluyendo sus aplicaciones en reduccio'n calcula como: de dimensionalidad, deteccio'n de anomal'ıas y procesamiento (k-1) de ima'genes. el objetivo es ofrecer una visio'n general clara y p= 2 concisa,u'tiltantoparaelestudioteo'ricocomoparalaaplicacio'n pra'ctica. donde k es el taman˜o del kernel. index terms-redes neuronales convolucionales, autoencoders, visualizacio'n de activaciones, embeddings, pooling, ar- el taman˜o de salida se calcula con: quitecturas de aprendizaje profundo (m-k+2p) dimensio'n de salida= +1 i. introduccio'n s las redes neuronales convolucionales (cnn) se han condonde: m es el taman˜o de la entrada, k el taman˜o del kernel, vertido en un pilar fundamental de la visio'n por computadora p el padding aplicado y s el stride. moderna, ya que permiten extraer automa'ticamente caracter'ısticas jera'rquicas a partir de ima'genes. comprender sus d. pesos y arquitectura de alexnet mecanismosinternos,incluyendolosfiltros,loscamposreceptivos,elstride,elpaddingylasoperacionesdepooling,resulta en redes convolucionales se utilizan pesos compartidos, esencial para disen˜ar arquitecturas eficientes e interpretar el lo que reduce dra'sticamente el nu'mero de para'metros, ya que comportamiento de los modelos. el mismo conjunto de filtros se aplica en todas las posiciones por su parte, los autoencoders complementan el uso de las espaciales.enlaarquitecturadealexnet,porejemplo,seemcnn al aprender representaciones compactas de los datos sin plean96filtrosenlaprimeracapaconvolucional,permitiendo requeriretiquetas,loqueloshaceidealesparatareasdeaprenextraer mu'ltiples caracter'ısticas visuales de forma eficiente. dizaje no supervisado como la reduccio'n de dimensionalidad, la deteccio'n de anomal'ıas y la reconstruccio'n de ima'genes. e. pooling layer este documento sintetiza los principales conceptos y arquitecturas revisados en clase, sirviendo como material de despue's de las convoluciones, se aplica la capa de pooling, referencia para la comprensio'n y aplicacio'n pra'ctica de estos que resume la informacio'n espacial (alto y ancho) sin alterar modelos en inteligencia artificial. la cantidad de canales. existen dos tipos principales: ii. repasodelaclaseanterior - max pooling: conserva el valor ma'ximo de cada regio'n. a. filtros o kernels - average pooling:"}
{"id_doc": "DOC_035", "segmentacion": "A", "chunk_id": "DOC_035_A_001", "idx": 1, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "de ima'genes. e. pooling layer este documento sintetiza los principales conceptos y arquitecturas revisados en clase, sirviendo como material de despue's de las convoluciones, se aplica la capa de pooling, referencia para la comprensio'n y aplicacio'n pra'ctica de estos que resume la informacio'n espacial (alto y ancho) sin alterar modelos en inteligencia artificial. la cantidad de canales. existen dos tipos principales: ii. repasodelaclaseanterior - max pooling: conserva el valor ma'ximo de cada regio'n. a. filtros o kernels - average pooling: calcula el promedio de los valores en la regio'n. sonmatrices(porejemplo,de3×3o5×5)quesedeslizan sobre la imagen para aplicar convoluciones. el gaussian dada una entrada de taman˜o w ×h ×d, el pooling reduce kernel se utiliza para suavizar la imagen y eliminar ruido. w y h, manteniendo d. b. campo receptivo f. fully-connected layer es la regio'n local de la imagen a la que una neurona esta' conectada. por ejemplo, para una entrada de 32×32×3 y un finalmente, las caracter'ısticas extra'ıdas se transforman en campo receptivo de 5×5, cada neurona tendra' 5×5×3=75 un u'nico vector, conectando todas las neuronas entre s'ı. esta pesos. capa permite realizar la clasificacio'n final del modelo. g. arquitecturas convolucionales una red convolucional combina secuencias de convolucio'n →activacio'n(relu)→pooling.estepatro'nserepitevarias veces para extraer informacio'n progresivamente ma's abstracta de la imagen. generalmente, se prefieren filtros pequen˜os (como 3 × 3) para capturar detalles locales de forma ma's eficiente. el convolutional stack se forma al aplicar mu'ltiples capas de convolucio'n consecutivas. por ejemplo, en una imagen de 5×5,unfiltro3×3puededesplazarseparagenerarunasalida de 3×3. regla pra'ctica: las dimensiones de las ima'genes deben ser divisibles entre 2, lo cual facilita la reduccio'n progresiva mediante pooling. h. principales arquitecturas 1) lenet: disen˜ada por yann lecun et al. (1998), fue una fig.1. representacio'ndeembeddingsmediantet-sne. de las primeras redes convolucionales exitosas. cuenta con 5 capas: dos convolucionales, dos de pooling y una totalmente conectada [1]. su estructura sirvio' de base para las redes profundas. los features aprendidos por las capas internas modernas. suelen ser dif'ıciles de entender por los humanos, lo que 2) alexnet: propuesta por krizhevsky, sutskever y hinton complica saber que' esta' \"viendo\" realmente el modelo. (2012), marco' un hito en la visio'n por computadora. procesa 2) visualizacio'n y ana'lisis de activaciones: una forma de ima'genes de 224×224 con filtros grandes (11×11, 5×5, entender mejor el funcionamiento interno es observar: 3×3) y cinco capas convolucionales. popularizo' el uso de relu, dropout y la utilizacio'n de mu'ltiples gpus para el - visualizacio'n"}
{"id_doc": "DOC_035", "segmentacion": "A", "chunk_id": "DOC_035_A_002", "idx": 2, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "ser dif'ıciles de entender por los humanos, lo que 2) alexnet: propuesta por krizhevsky, sutskever y hinton complica saber que' esta' \"viendo\" realmente el modelo. (2012), marco' un hito en la visio'n por computadora. procesa 2) visualizacio'n y ana'lisis de activaciones: una forma de ima'genes de 224×224 con filtros grandes (11×11, 5×5, entender mejor el funcionamiento interno es observar: 3×3) y cinco capas convolucionales. popularizo' el uso de relu, dropout y la utilizacio'n de mu'ltiples gpus para el - visualizacio'n de activaciones: muestra que' regiones de la imagen activan ciertas neuronas. entrenamiento [2]. 3) zfnet: basada en alexnet, reduce la profundidad y - visualizacio'n de filtros: permite observar los pesos de los kernels. en las primeras capas, estos muestran taman˜o de los filtros para analizar co'mo afectan las capas patronesreconocibles(bordes,colores,texturas),mientras a la representacio'n interna. sirvio' como experimento para que en capas profundas se vuelven ma's abstractos. visualizar activaciones intermedias y optimizar arquitecturas. 4) googlenet (inception): reduciendo los ma's de 60 mil- estosme'todosayudanadetectarsielmodeloesta' aprendiendo lonesdepara'metrosdealexnetaunos4millones,googlenet caracter'ısticas relevantes o solo ruido. introdujo los mo'dulos inception [3]. cada mo'dulo combina 3) embeddings y reduccio'n de dimensionalidad: las reconvoluciones de diferentes taman˜os (1 × 1, 3 × 3, 5 × 5) des pueden transformar ima'genes en representaciones vectorijunto con max pooling, concatenando sus resultados. al final, ales llamadas embeddings. estas representaciones condensan la salida (7×7×1024) se aplana y se pasa a un average la informacio'n relevante de una imagen, permitiendo separar pooling de 1×1×1024. clases en el espacio de caracter'ısticas. al reducir la dimen5) vgg16: caracterizada por su simplicidad, utiliza sionalidad (manteniendo las distancias relativas), podemos u'nicamente filtrosde 3×3 yaumenta la profundidad hasta16 visualizar las relaciones entre clases. capas.estaarquitecturademostro' queaumentarlaprofundidad una te'cnica comu'n para ello es t-sne, que proyecta estos mejora el rendimiento si se mantienen filtros pequen˜os y vectores a dos dimensiones preservando la estructura del consistentes. espacio original (fig. 1). 6) resnet: introduce las conexiones residuales, que per- 4) mapasdeactivacio'n: adema'sdelasvisualizacionesde miten el paso de informacio'n entre capas no adyacentes. esto filtros, es posible generar mapas de activacio'n o heatmaps evita la degradacio'n del gradiente en redes muy profundas y que muestran que' regiones espec'ıficas de la imagen influyen mejora la capacidad de entrenamiento. ma's en la decisio'n del modelo. estas te'cnicas son u'tiles, 7) densenet: conecta cada capa con todas las anteriores, por ejemplo, en aplicaciones me'dicas para resaltar fracturas favoreciendo la reutilizacio'n de caracter'ısticas y reduciendo o anomal'ıas en radiograf'ıas."}
{"id_doc": "DOC_035", "segmentacion": "A", "chunk_id": "DOC_035_A_003", "idx": 3, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "el paso de informacio'n entre capas no adyacentes. esto filtros, es posible generar mapas de activacio'n o heatmaps evita la degradacio'n del gradiente en redes muy profundas y que muestran que' regiones espec'ıficas de la imagen influyen mejora la capacidad de entrenamiento. ma's en la decisio'n del modelo. estas te'cnicas son u'tiles, 7) densenet: conecta cada capa con todas las anteriores, por ejemplo, en aplicaciones me'dicas para resaltar fracturas favoreciendo la reutilizacio'n de caracter'ısticas y reduciendo o anomal'ıas en radiograf'ıas. la cantidad de para'metros necesarios. este enfoque mejora la b. autoencoders eficiencia y el flujo de informacio'n a lo largo de la red. aunqueutilizanarquitecturassimilaresalasredesconvoluiii. materiadeclase cionales, los autoencoders trabajan sin etiquetas expl'ıcitas, a. problemas en las redes neuronales convolucionales porloqueseconsideranme'todosnosupervisados.suobjetivo 1) explicabilidad del modelo: uno de los principales de- es reconstruir la entrada original, aprendiendo una represaf'ıos actuales es la falta de interpretabilidad en las redes sentacio'n interna comprimida. iv. partesdelautoencoder los autoencoders se componen de tres partes principales: el encoder, el cuello de botella y el decoder. cada una cumple una funcio'n espec'ıfica en el proceso de codificacio'n y reconstruccio'n de los datos. a. encoder el encoder esta' formado por un conjunto de bloques convolucionales seguidos de mo'dulos de pooling. su funcio'n principal es extraer las caracter'ısticas ma's relevantes de la imagendeentradaycomprimirlainformacio'n.laexpectativa del encoder es aprender informacio'n importante de la entrada mediante un proceso de downsampling, reduciendo la dimensionalidad y conservando los rasgos esenciales. b. cuello de botella el cuello de botella constituye la parte ma's importante y fig.2. estructuraba'sicadeunautoencoder. pequen˜a del modelo. representa la informacio'n comprimida en un espacio latente, donde se encuentran codificadas las caracter'ısticas ma's significativas. esta capa restringe el flujo el proceso consta de tres partes, como se muestra en la de informacio'n proveniente del encoder al decoder, limitando fig. 2: la cantidad de datos que pueden ser reconstruidos. 1) encoder: reduce la imagen a un vector compacto. c. decoder 2) espacio latente: contiene la representacio'n esencial o el decoder esta' compuesto por una serie de convoluciones codificada de la entrada. que realizan upsampling para reconstruir la imagen original a 3) decoder: reconstruye la imagen original a partir del partir del vector latente. en pytorch, esta tarea suele implevector latente. mentarse mediante capas convtranspose2d. el objetivo 1) tareas comunes de un autoencoder: del decoder es generar una salida lo ma's fiel posible a la - reduccio'n de dimensionalidad: genera una repre- entrada original. sentacio'n ma's"}
{"id_doc": "DOC_035", "segmentacion": "A", "chunk_id": "DOC_035_A_004", "idx": 4, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "representacio'n esencial o el decoder esta' compuesto por una serie de convoluciones codificada de la entrada. que realizan upsampling para reconstruir la imagen original a 3) decoder: reconstruye la imagen original a partir del partir del vector latente. en pytorch, esta tarea suele implevector latente. mentarse mediante capas convtranspose2d. el objetivo 1) tareas comunes de un autoencoder: del decoder es generar una salida lo ma's fiel posible a la - reduccio'n de dimensionalidad: genera una repre- entrada original. sentacio'n ma's compacta y poderosa que pca, conserd. hiperpara'metros a considerar vando la informacio'n esencial. el desempen˜o del autoencoder depende en gran medida de - deteccio'n de anomal'ıas: se entrena para reconstruir los hiperpara'metros seleccionados, entre los que destacan: datos de una tarea tomando en cuenta u'nicamente ejemplos positivos o normales. por ejemplo: - taman˜o de la codificacio'n (vector latente): determina el nivel de compresio'n de los datos. un taman˜o menor - transferencias bancarias correctas. implica mayor compresio'n, pero puede perderse infor- - audio o ima'genes de alta fidelidad sin defectos. macio'n relevante. el modelo aprende la representacio'n latente de estos ca- - nu'mero de capas: define la profundidad del encoder sosy,alpresentarleejemplosano'malos,sureconstruccio'n y del decoder. un nu'mero mayor de capas genera un falla, evidenciando la anomal'ıa. modelo ma's complejo y con mayor capacidad de repre- - procesamiento de ima'genes (fig. 3): permite tareas sentacio'n, mientras que un nu'mero menor lo hace ma's como compresio'n, eliminacio'n de ruido o incluso super ra'pido pero menos preciso. resolucio'n, es decir, generar ima'genes de alta resolucio'n a partir de versiones borrosas o pequen˜as. v. conclusiones durante la clase se destacaron los componentes esenciales estos principios sientan las bases de los algoritmos generay las arquitecturas principales de las redes neuronales contivosmodernos,dondeelmodeloaprendeareconstruirocrear volucionales, explicando co'mo las capas, filtros y operaciones contenido visual de forma auto'noma. de pooling trabajan en conjunto para extraer informacio'n relevante de las ima'genes. la visualizacio'n de activaciones y embeddings permite comprender mejor el funcionamiento interno de los modelos profundos, mejorando su interpretabilidad y facilitando el diagno'stico de su desempen˜o. asimismo, los autoencoders se presentaron como herramientas potentes dentro del aprendizaje no supervisado, capacesdecomprimirinformacio'n,detectaranomal'ıasymejorar fig.3. procesamientodeima'genes la calidad de las ima'genes mediante su reconstruccio'n. dominar estos conceptos proporciona una base teo'rica y pra'cticaso'lidaparaeldisen˜o,ana'lisisyaplicacio'nefectivade modelos de aprendizaje profundo en distintos contextos. references [1] y.lecun,l.bottou,y.bengio,andp.haffner,\"gradient-basedlearning appliedtodocumentrecognition,\"proceedingsoftheieee,vol.86,no. 11,pp.2278-2324,1998. [2] a. krizhevsky, i. sutskever, and g. hinton, \"imagenet classification withdeepconvolutionalneuralnetworks,\"advancesinneuralinformationprocessingsystems(nips),pp.1097-1105,2012. [3] c.szegedyetal.,\"goingdeeperwithconvolutions,\"proceedingsofthe ieeeconferenceoncomputervisionandpatternrecognition(cvpr), pp.1-9,2015."}
{"id_doc": "DOC_035", "segmentacion": "A", "chunk_id": "DOC_035_A_005", "idx": 5, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "interno de los modelos profundos, mejorando su interpretabilidad y facilitando el diagno'stico de su desempen˜o. asimismo, los autoencoders se presentaron como herramientas potentes dentro del aprendizaje no supervisado, capacesdecomprimirinformacio'n,detectaranomal'ıasymejorar fig.3. procesamientodeima'genes la calidad de las ima'genes mediante su reconstruccio'n. dominar estos conceptos proporciona una base teo'rica y pra'cticaso'lidaparaeldisen˜o,ana'lisisyaplicacio'nefectivade modelos de aprendizaje profundo en distintos contextos. references [1] y.lecun,l.bottou,y.bengio,andp.haffner,\"gradient-basedlearning appliedtodocumentrecognition,\"proceedingsoftheieee,vol.86,no. 11,pp.2278-2324,1998. [2] a. krizhevsky, i. sutskever, and g. hinton, \"imagenet classification withdeepconvolutionalneuralnetworks,\"advancesinneuralinformationprocessingsystems(nips),pp.1097-1105,2012. [3] c.szegedyetal.,\"goingdeeperwithconvolutions,\"proceedingsofthe ieeeconferenceoncomputervisionandpatternrecognition(cvpr), pp.1-9,2015."}
{"id_doc": "DOC_036", "segmentacion": "A", "chunk_id": "DOC_036_A_000", "idx": 0, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "inteligencia artificial apuntes semana 11, clase #1 luis fernando benavides villegas instituto tecnolo'gico de costa rica cartago, costa rica lubenavides@estudiantec.cr abstract-este documento recopila los apuntes de la clase del de aplicacio'n del filtro, por lo que reduce el taman˜o del mapa martes 14 de octubre de 2025 para el curso de inteligencia ar- de salida. tificial. se repasan conceptos como los fundamentos de las redes 2) padding: agrega p'ıxeles de relleno alrededor de la neuronales convolucionales (cnn), abarcando el uso de filtros, imagendeentradaparacontrolareltaman˜odelmapadesalida stride,paddingypoolingparalaextraccio'ndecaracter'ısticasen ima'genes.seanalizanarquitecturasrepresentativascomolenet, y preservar las dimensiones espaciales. un padding sime'trico alexnet, googlenet, vgg, resnet y densenet, destacando su evita que la convolucio'n reduzca el taman˜o de la imagen: evolucio'n y aportes al aprendizaje profundo. adema's, se introk-1 ducenlosconceptosdeembeddingsyvisualizacio'ndeactivaciones p= , para interpretar el comportamiento de los modelos, junto con el 2 estudio de los autoencoders y su aplicacio'n en reconstruccio'n de donde k es el taman˜o del filtro. ima'genes,reduccio'ndedimensionalidad,deteccio'ndeanomal'ıas y generacio'n de datos. 3) dimensio'ndesalida: dadaunaimagendeentradayun index terms-inteligencia artificial, redes neuronales con- kernel, el taman˜o de salida se calcula como: volucionales, pooling, embeddings, visualizacio'n, autoencoder, (m+2p -k) deep learning. +1, s i. repasodelaclaseanterior donde m es el taman˜o de la entrada, k el taman˜o del filtro, p el padding y s el stride. a. convoluciones y filtros c. comparticio'n de pesos una convolucio'n consiste en aplicar un filtro (kernel) sobre una imagen para extraer informacio'n relevante. el filtro es en una red convolucional, los mismos pesos que se caluna matriz de nu'meros que se entrena junto con la red. al cularon para una regio'n espec'ıfica se reutilizan en todas las desplazarseporlaimagen,calculaunvalorporcadaposicio'n, dema's posiciones donde el filtro se deslice. por ejemplo, generandounanuevaimagenllamadamapadecaracter'ısticas si un filtro aprende a detectar l'ıneas verticales, esa misma (feature map o activation map). configuracio'n de pesos servira' para reconocerlas sin importar 1) filtrogaussiano: produceunaimagenconunefectode en que' parte de la imagen aparezcan. de esta manera, se desenfoque (blur), eliminando el ruido y dejando solo la parte reducesignificativamentelacantidaddepara'metrosaentrenar del contorno. y mejora la eficiencia del modelo. 2) redes neuronales: en redes convencionales, todos los en arquitecturas como alexnet, permite que las primeras p'ıxeles estan conectados a todas las neuronas de la siguiente capas aprendan caracter'ısticas generales como bordes y colcapa. en las convoluciones, solo una porcio'n de los p'ıxeles ores, mientras que las capas ma's profundas combinan esa esta' conectada, observando solo una parte espec'ıfica de la informacio'n para reconocer formas y objetos ma's complejos. imagen."}
{"id_doc": "DOC_036", "segmentacion": "A", "chunk_id": "DOC_036_A_001", "idx": 1, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "parte reducesignificativamentelacantidaddepara'metrosaentrenar del contorno. y mejora la eficiencia del modelo. 2) redes neuronales: en redes convencionales, todos los en arquitecturas como alexnet, permite que las primeras p'ıxeles estan conectados a todas las neuronas de la siguiente capas aprendan caracter'ısticas generales como bordes y colcapa. en las convoluciones, solo una porcio'n de los p'ıxeles ores, mientras que las capas ma's profundas combinan esa esta' conectada, observando solo una parte espec'ıfica de la informacio'n para reconocer formas y objetos ma's complejos. imagen. d. capa de pooling 3) campo receptivo: es la regio'n de la imagen que una neurona observa para generar su salida. depende tanto del despue'sdeaplicarlasconvoluciones,seutilizaunacapade taman˜o de la entrada como del filtro aplicado. por ejemplo, si pooling para reducir el taman˜o espacial de la imagen y manlaimagendeentradaesde32×32×3,lareddebeprocesarlos tener solo la informacio'n ma's relevante. esta operacio'n toma tres canales de color. si el filtro tiene taman˜o 5×5, entonces bloqueslocalesyrealizaunaoperacio'nestad'ısticasobreellos, el campo receptivo resultante sera' un cubo de 5×5×3, es como el ma'ximo o el promedio, para resumir su contenido. decir, todas las neuronas que intervienen en esa regio'n. estos - max pooling: selecciona el valor ma'ximo de cada campos extraen caracter'ısticas necesarias para el clasificador. bloque. es el me'todo ma's utilizado. - average pooling: calcula el promedio de los valores de b. para'metros de la convolucio'n cada bloque. 1) stride: define cua'nto se deslizan los filtros sobre la - l2 pooling: aplica una norma cuadra'tica sobre los valimagendeentrada.unstridemayorprovocamenosposiciones ores. el pooling reduce el ancho y el alto de la imagen, pero c) zfnet: creada en base a alexnet, ajusta el taman˜o conserva la cantidad de canales, por lo que con entrada de de los filtros y la profundidad para estudiar co'mo cada capa taman˜o w ×h×d, el pooling reduce w y h, manteniendo transforma la informacio'n. introdujo te'cnicas para visualizar d.estoevitaqueelmodelocrezcaencantidaddepara'metros activacionesintermedias,ayudandoacomprenderydepurarel y mantiene la informacio'n esencial para las siguientes capas. comportamiento interno de las cnn. d) googlenet (inception): presentada por google en e. capa fully-connected 2014, redujo de 60 a 4 millones de para'metros mediante los tras las etapas de convolucio'n y pooling, la red produce mo'dulos inception, que combinan convoluciones de distintos un vector que resume las caracter'ısticas ma's relevantes de la taman˜os (1×1, 3×3, 5×5) y max pooling en paralelo. en imagen.estevectorseconectaaunaovariascapastotalmente la etapa final, un average pooling global transforma el tensor conectadas. cada neurona de estas capas esta' conectada con de 7×7×1024 en un vector 1×1×1024, reemplazando"}
{"id_doc": "DOC_036", "segmentacion": "A", "chunk_id": "DOC_036_A_002", "idx": 2, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "google en e. capa fully-connected 2014, redujo de 60 a 4 millones de para'metros mediante los tras las etapas de convolucio'n y pooling, la red produce mo'dulos inception, que combinan convoluciones de distintos un vector que resume las caracter'ısticas ma's relevantes de la taman˜os (1×1, 3×3, 5×5) y max pooling en paralelo. en imagen.estevectorseconectaaunaovariascapastotalmente la etapa final, un average pooling global transforma el tensor conectadas. cada neurona de estas capas esta' conectada con de 7×7×1024 en un vector 1×1×1024, reemplazando las todas las salidas anteriores, permitiendo combinar las carac- capas densas y mejorando la eficiencia [2]. ter'ısticas extra'ıdas para realizar la clasificacio'n final. e) vgg16: simplifica el disen˜o utilizando solo filtros el perceptro'n multicapa (mlp) se encarga de transformar pequen˜os de 3×3 y bloques repetidos de convolucio'n y pooleste vector en una prediccio'n, como la probabilidad de perte- ing. aumenta la profundidad hasta 16 o 19 capas, mostrando nencia a una clase espec'ıfica. que ma's capas con filtros simples mejoran el rendimiento general. f. arquitecturas convolucionales f) resnet: introduce las conexiones residuales, que per1) estructura general: una arquitectura convolucional se miten que la informacio'n fluya entre capas no consecutivas. compone de bloques repetidos de: estas conexiones evitan el desvanecimiento del gradiente y convolucio'n→activacio'n→pooling posibilitanentrenarredesextremadamenteprofundasdeforma estable. estosbloquesserepitenvariasvecesparaextraerinformacio'n g) densenet: conectacadacapacontodaslasanteriores progresivamentema'sabstracta.posteriormente,elresultadose dentro de un bloque, promoviendo la reutilizacio'n de caracaplana(flatten)yseconectaaunaoma'scapasfullyconnected ter'ısticas y reduciendo la redundancia. esta estructura densa para la clasificacio'n. mejoralapropagacio'ndelgradiente,optimizalaeficienciadel serecomiendaelusodefiltrospequen˜os(porejemplo,3×3 modelo y mantiene un nu'mero reducido de para'metros. o 5×5) ya que permiten: - reducir la cantidad de para'metros a aprender. ii. problemasconlasredesneuronales - capturar relaciones no lineales al encadenar mu'ltiples convolucionales capas. a pesar de su alto desempen˜o, las redes convolucionales se filtros grandes (7×7 o ma's) capturan ma's informacio'n en comportan como una \"caja negra\", ya que resulta dif'ıcil comuna sola capa, pero aumentan excesivamente el nu'mero de prender que' tipo de informacio'n esta'n utilizando para tomar para'metros y reducen la no linealidad. sus decisiones. las representaciones internas que generan son 2) reglas pra'cticas: altamente abstractas, lo que plantea un reto importante de - es preferible que las dimensiones de las ima'genes sean interpretabilidad. divisibles entre 2 para facilitar las reducciones con max uno de los principales desaf'ıos actuales es entender que' pooling. es lo que realmente afecta a la red durante el proceso de - en general, se utiliza stride de 2 y padding de 1 para clasificacio'n.lascapasinternasaprendencaracter'ısticascommantener dimensiones manejables. plejas que no siempre son"}
{"id_doc": "DOC_036", "segmentacion": "A", "chunk_id": "DOC_036_A_003", "idx": 3, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "representaciones internas que generan son 2) reglas pra'cticas: altamente abstractas, lo que plantea un reto importante de - es preferible que las dimensiones de las ima'genes sean interpretabilidad. divisibles entre 2 para facilitar las reducciones con max uno de los principales desaf'ıos actuales es entender que' pooling. es lo que realmente afecta a la red durante el proceso de - en general, se utiliza stride de 2 y padding de 1 para clasificacio'n.lascapasinternasaprendencaracter'ısticascommantener dimensiones manejables. plejas que no siempre son comprensibles para los humanos. - el pooling de 2 × 2 es el ma's comu'n, reduciendo la este problema de explicabilidad motiva el uso de te'cnicas imagen a la mitad en cada dimensio'n. de visualizacio'n que permitan analizar la respuesta de las 3) principales arquitecturas: neuronas ante diferentes est'ımulos visuales. a) lenet-5: propuesta por yann lecun en 1998, fue a. visualizacio'n y ana'lisis de activaciones una de las primeras redes convolucionales aplicadas al reconocimiento de d'ıgitos escritos a mano [1]. su estructura una forma pra'ctica de estudiar el comportamiento interno incluye dos capas convolucionales, dos de pooling y una de las cnn es observar los feature maps generados por cada totalmente conectada, estableciendo la base para las redes capa. modernas de visio'n por computadora. en estos mapas se puede identificar que' regiones de la b) alexnet: desarrollada por krizhevsky, sutskever y imagen activan ciertas neuronas y, por lo tanto, cua'les son hinton en el 2012, marco' el inicio del deep learning mod- los elementos visuales que el modelo considera relevantes. erno. procesa ima'genes de 224 × 224 con filtros grandes en las primeras capas, las activaciones suelen asemejarse (11×11, 5×5, 3×3), emplea activaciones relu, dropout todav'ıaalaimagenoriginal,peroconformeseprofundiza,las y entrenamiento distribuido en mu'ltiples gpus, logrando un representacionessevuelvencadavezma'sabstractasydif'ıciles salto significativo en precisio'n sobre el conjunto imagenet. de interpretar. fig.2. diagramadelfuncionamientodeunautoencoder proporcionandotransparenciaalprocesodedecisio'ndelmodelo. fig.1. representacio'ndeclasescont-sne. iii. autoencoders es una red neuronal disen˜ada para aprender una repreadema's, la inspeccio'n de los filtros aprendidos ayuda a sentacio'n comprimida de sus datos de entrada. a diferencia verificar que' patrones esta' capturando la red. los filtros de las redes supervisadas, no necesita etiquetas externas, ya inicialestiendenamostrartexturas,bordesocolores,mientras que su objetivo es reconstruir la entrada en la salida. durante que los u'ltimos codifican composiciones ma's complejas. este el entrenamiento, el modelo aprende a capturar los patrones tipo de ana'lisis permite detectar si la red esta' aprendiendo ma's relevantes de los datos, filtrando el ruido y conservando caracter'ısticas significativas o simplemente ruido"}
{"id_doc": "DOC_036", "segmentacion": "A", "chunk_id": "DOC_036_A_004", "idx": 4, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "comprimida de sus datos de entrada. a diferencia verificar que' patrones esta' capturando la red. los filtros de las redes supervisadas, no necesita etiquetas externas, ya inicialestiendenamostrartexturas,bordesocolores,mientras que su objetivo es reconstruir la entrada en la salida. durante que los u'ltimos codifican composiciones ma's complejas. este el entrenamiento, el modelo aprende a capturar los patrones tipo de ana'lisis permite detectar si la red esta' aprendiendo ma's relevantes de los datos, filtrando el ruido y conservando caracter'ısticas significativas o simplemente ruido del conjunto la informacio'n esencial. de entrenamiento. la estructura ba'sica se compone de tres partes (fig. 2): b. reduccio'n de dimensionalidad encoder→espacio latente→decoder para comprender las redes, se puede hacer el estudio de el aprendizaje del autoencoder consiste en minimizar el sus embeddings. al final de la red, cada imagen puede error de reconstruccio'n entre la entrada original y la salida representarse mediante un vector nume'rico que resume su reconstruida. aunque no haya etiquetas externas, el entreinformacio'n sema'ntica. ima'genes similares quedan pro'ximas namiento es parcialmente supervisado, ya que la salida se entre s'ı en este espacio vectorial, mientras que las de distintas compara directamente con la entrada. clases se separan claramente. estas representaciones pueden visualizarse mediante algoritmos de reduccio'n de dimension- a. encoder alidad, como: consiste en una serie de bloques convolucionales seguidos - t-sne: proyecta los vectores a dos o tres dimensiones, de operaciones de pooling, con el objetivo de extraer las preservando la estructura de las distancias originales, caracter'ısticas ma's relevantes de la entrada y comprimir la como se puede ver en la fig. 1. informacio'n a trave's de un proceso de reduccio'n espacial o - pca:alternativama'ssimple,aunquemenosefectivapara downsampling. cada bloque convolucional aprende distintos relaciones no lineales. niveles de representacio'n, pasando de detalles simples como bordes y texturas a rasgos ma's abstractos. de esta manera, el cuando la separacio'n entre clases es clara en el espacio encoder transforma los datos originales en una versio'n ma's reducido, se considera que el modelo ha aprendido una repcompacta, conservando u'nicamente la informacio'n esencial resentacio'n adecuada. por el contrario, si las clases aparecen para la reconstruccio'n posterior. mezcladas, indica que la red no ha logrado distinguir correctamente las caracter'ısticas de cada una. b. espacio latente o cuello de botella en esta etapa se almacena la informacio'n esencial en c. mapas de activacio'n un vector de baja dimensionalidad, conocido como espacio se pueden generar heatmaps o mapas de activacio'n que latente. este vector contiene"}
{"id_doc": "DOC_036", "segmentacion": "A", "chunk_id": "DOC_036_A_005", "idx": 5, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "el modelo ha aprendido una repcompacta, conservando u'nicamente la informacio'n esencial resentacio'n adecuada. por el contrario, si las clases aparecen para la reconstruccio'n posterior. mezcladas, indica que la red no ha logrado distinguir correctamente las caracter'ısticas de cada una. b. espacio latente o cuello de botella en esta etapa se almacena la informacio'n esencial en c. mapas de activacio'n un vector de baja dimensionalidad, conocido como espacio se pueden generar heatmaps o mapas de activacio'n que latente. este vector contiene la codificacio'n interna de la destacanlaszonasespec'ıficasdeunaimagenqueinfluyenma's entrada, capturando u'nicamente los rasgos ma's significativos. enladecisio'ndelmodelo.estosmapassonu'tilesparaverificar debidoasutaman˜olimitado,restringeelflujodeinformacio'n silaredesta' enfoca'ndoseenlasregionescorrectasdelobjeto. hacia el decoder, lo que obliga al modelo a conservar solo por ejemplo, estos me'todos permiten justificar predicciones, lo ma's relevante para lograr una reconstruccio'n efectiva. en como localizar una fractura o anomal'ıa en una radiograf'ıa, este espacio, muestras similares tienden a ubicarse pro'ximas iv. conclusiones las redes convolucionales permiten extraer automa'ticamente caracter'ısticas jera'rquicas de las ima'genes, impulsando el desarrollo de arquitecturas cada vez ma's profundas y eficientes. a pesar de su potencia, siguen siendo poco interpretables, por lo que se recurre a te'cnicas de visualizacio'n y ana'lisis de activaciones. finalmente, los autoencoders ampl'ıan estos conceptos al aprendizaje no fig.3. ejemplodesuper-resolucio'nconautoencoder. supervisado, permitiendo la compresio'n, reconstruccio'n y generacio'n de datos a partir de representaciones latentes. entre s'ı, formando agrupamientos que reflejan la estructura referencias sema'ntica de los datos. [1] y.lecun,l.bottou,y.bengio,andp.haffner,\"gradient-basedlearning c. decoder y reconstruccio'n appliedtodocumentrecognition,\"proceedingsoftheieee,vol.86,no. 11,pp.2278-2324,1998. a partir del vector del espacio latente, utiliza capas de [2] c.szegedyetal.,\"goingdeeperwithconvolutions,\"proceedingsofthe upsampling o convoluciones transpuestas para expandir pro- ieeeconferenceoncomputervisionandpatternrecognition(cvpr), pp.1-9,2015. gresivamente la representacio'n comprimida hasta recuperar la forma original. durante este proceso, el modelo aprende a reconstruir los detalles perdidos, generando una salida que se asemeje lo ma's posible a la entrada inicial. d. aplicaciones de los autoencoders - reduccio'n de dimensionalidad: obtener representaciones ma's compactas que las de pca. - deteccio'n de anomal'ıas: los ejemplos normales se reconstruyen bien, mientras que los at'ıpicos muestran un errordereconstruccio'nelevado.sepuedefijarunumbral para decidir cua'ndo un dato es ano'malo. - eliminacio'nderuido:aprenderareconstruirunaimagen limpia a partir de una ruidosa. - edicio'n y generacio'n de ima'genes: al modificar el vector latente se pueden crear variantes o nuevas ima'genes, por ejemplo, para comprimirlas. - super-resolucio'n: generar versiones de alta resolucio'n a partir de ima'genes pequen˜as (fig. 3). e. hiperpara'metros relevantes - taman˜o del vector latente: define la cantidad de informacio'n que el modelo puede retener en el espacio comprimido. un vector ma's pequen˜o produce un"}
{"id_doc": "DOC_036", "segmentacion": "A", "chunk_id": "DOC_036_A_006", "idx": 6, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "errordereconstruccio'nelevado.sepuedefijarunumbral para decidir cua'ndo un dato es ano'malo. - eliminacio'nderuido:aprenderareconstruirunaimagen limpia a partir de una ruidosa. - edicio'n y generacio'n de ima'genes: al modificar el vector latente se pueden crear variantes o nuevas ima'genes, por ejemplo, para comprimirlas. - super-resolucio'n: generar versiones de alta resolucio'n a partir de ima'genes pequen˜as (fig. 3). e. hiperpara'metros relevantes - taman˜o del vector latente: define la cantidad de informacio'n que el modelo puede retener en el espacio comprimido. un vector ma's pequen˜o produce un modelo ma'seficienteenco'mputo,peroconmenorcapacidadpara capturardetallesdelaimagen.encambio,unvectorma's grande permite representar ma's caracter'ısticas, aunque incrementaelcostodeentrenamientoydeprocesamiento. - nu'mero de capas: tanto el encoder como el decoder puedenvariarenprofundidad.unmayornu'merodecapas permite modelar relaciones ma's complejas, pero tambie'n hace el entrenamiento ma's pesado y sensible al ajuste de para'metros. - funcio'n de pe'rdida: para tareas de reconstruccio'n de ima'genes se utiliza comu'nmente el mean squared error (mse). esta funcio'n compara cada p'ıxel de la imagen original con el de la reconstruccio'n, midiendo su diferencia. un error cercano a cero indica que el modelo ha logrado reproducir correctamente la entrada."}
{"id_doc": "DOC_037", "segmentacion": "A", "chunk_id": "DOC_037_A_000", "idx": 0, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "apuntes semana 11 clase #1 14/10/2025 alex steven naranjo mas'ıs instituto tecnolo'gico de costa rica cartago, costa rica email: alnaranjo@estudiantec.cr resumen-este documento recopila los apuntes de la clase b. para'metrosdelaconvolucio'n:stride,paddingytaman˜o del martes 14 de octubre de 2025 para el curso de inteligencia de salida artificial.seabordaronlosfundamentosdelasredesneuronales para una entrada 1d de longitud m, kernel k, padding p y convolucionales (cnn), explicando el funcionamiento de los filtros, el campo receptivo, el stride, el padding y las capas stride s, la salida es: de pooling para la extraccio'n de caracter'ısticas en ima'genes. (cid:22) (cid:23) m+2p-k adema's, se estudiaron arquitecturas cla'sicas como lenet, alex- out= +1. s net,googlenet/inception,vgg16,resnetydensenet.finalmente, se introdujeron los conceptos de embeddings, visualizacio'n de en 2d se aplica por dimensio'n (alto y ancho). el padding activaciones y autoencoders, analizando sus aplicaciones en resime'trico t'ıpico para \"conservacio'n de taman˜o\" con s=1 es duccio'ndedimensionalidad,deteccio'ndeanomal'ıas,eliminacio'n p = k-1 (si k es impar). el stride > 1 reduce la resolucio'n de ruido y super-resolucio'n, junto con consideraciones pra'cticas 2 de entrenamiento y seleccio'n de hiperpara'metros. espacial. index terms-redes neuronales convolucionales, pooling, c. pesos compartidos y eficiencia embeddings, visualizacio'n, autoencoder, deep learning la comparticio'n de pesos aplica el mismo kernel en todas i. introduccio'n lasposicionesespaciales,reduciendopara'metrosfrenteacapas densas. en primeras capas, la red aprende bordes y texturas; lasredesneuronalesconvolucionales(cnn)sonunpilar en capas profundas, patrones sema'nticos ma's abstractos. en la visio'n por computadora moderna, pues permiten extraer d. capa de pooling automa'ticamente caracter'ısticas jera'rquicas de las ima'genes. comprendersuscomponentescomofiltros,camposreceptivos, reduce la resolucio'n espacial conservando canales: stride,paddingypoolingesesencialparadisen˜ararquitecturas max pooling: retiene el valor ma'ximo de cada ventana. eficientes. por su parte, los autoencoders complementan este average pooling: promedia los valores. aprendizaje al representar la informacio'n de forma comprimi- regla pra'ctica: pooling 2×2 con stride 2 para reduccio'n a la da, sin necesidad de etiquetas externas, y habilitan tareas de mitad. mantiene d =c y reduce h,w. in aprendizaje no supervisado/semisupervisado. e. activaciones, normalizacio'n y regularizacio'n ii. fundamentosderedesneuronales activacio'n: relu es esta'ndar en cnn modernas (evita convolucionales(cnns) saturacio'n y acelera entrenamiento). tanh/sigmoid pueden usarse en salidas espec'ıficas. a. filtros (kernels) y campos receptivos batch normalization (bn): estabiliza la distribucio'n un filtro 2d de taman˜o k×k se desliza sobre la imagen (o de activaciones, permite mayores tasas de aprendizaje y mapa de activacio'n) para producir un feature map. para una acelera la convergencia. entrada rgb h ×w ×c y c filtros, cada filtro tiene regularizacio'n: dropout (t'ıpico en capas densas), l2 in out taman˜o k×k×c y produce"}
{"id_doc": "DOC_037", "segmentacion": "A", "chunk_id": "DOC_037_A_001", "idx": 1, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "convolucionales(cnns) saturacio'n y acelera entrenamiento). tanh/sigmoid pueden usarse en salidas espec'ıficas. a. filtros (kernels) y campos receptivos batch normalization (bn): estabiliza la distribucio'n un filtro 2d de taman˜o k×k se desliza sobre la imagen (o de activaciones, permite mayores tasas de aprendizaje y mapa de activacio'n) para producir un feature map. para una acelera la convergencia. entrada rgb h ×w ×c y c filtros, cada filtro tiene regularizacio'n: dropout (t'ıpico en capas densas), l2 in out taman˜o k×k×c y produce un canal en la salida. (weightdecay)ydataaugmentationreducensobreajuste. in filtro gaussiano: suaviza la imagen (blur) y reduce f. capa fully-connected (mlp) y clasificacio'n ruido; resalta contornos al combinarse con operadores tras extraer mapas de activacio'n, se aplica flatten (o global de gradiente. averagepooling)ycapasdensasparaclasificacio'n.enproblecampo receptivo (rf): regio'n de la entrada que \"ve\" mas multi-clase se usa softmax y pe'rdida de entrop'ıa cruzada. una neurona de una capa dada. aumenta con la profundidad. si encadenamos capas con kernel k y stride s , iii. arquitecturasconvolucionales i i el rf efectivo crece de forma acumulativa. a. lenet-5 para'metros y costo: el nu'mero de para'metros en una capa pionera (lecun, 1998) para d'ıgitos manuscritos (mnist). conv es k2 - c - c + c (sesgo). la complejidad dos bloques conv+pooling y capas densas. introdujo la viabiin out out computacional se aproxima por h -w -k2-c -c . lidad pra'ctica de cnns. out out in out b. alexnet (2012) krizhevskyetal.popularizanrelu,dropout,entrenamiento en mu'ltiples gpus y kernels grandes (11×11, 5×5, 3×3) en entradas 224×224. disparo' la adopcio'n de deep learning a gran escala. c. zfnet y visualizacio'n intermedia ajusta taman˜os de kernel/stride y estudia feature maps internos para entender que' aprende cada capa, motivando pra'cticas de disen˜o y depuracio'n. d. googlenet / inception mo'dulos con ramas paralelas (1×1, 3×3, 5×5 + max pooling);reducepara'metros(de∼60ma∼4m)usandocuellos 1×1 y global average pooling al final. e. vgg-16 filosof'ıa de simplicidad: solo 3×3 + profundidad (16/19 capas).apesardemuchospara'metros,esunbaselinedida'ctico muy usado. figura1. representacio'ndeembeddingsmediantet-sne. f. resnet (redes residuales) skip connections (y = f(x)+x) permiten entrenar redes muy profundas mitigando vanishing gradient. bloques basic/bottleneck se apilan eficientemente. g. densenet conexiones densas \"todas con todas\" dentro del bloque; fomenta reutilizacio'n de caracter'ısticas, mejora el flujo de gradiente y reduce para'metros a igual rendimiento. iv. explicabilidaddelmodeloyembeddings a. visualizacio'n de activaciones y filtros observar feature maps muestra que' regiones activan cafigura2. estructuraba'sicadeunautoencoder. da neurona. en capas iniciales, activaciones recuerdan bordes/colores; en capas profundas, part'ıculas sema'nticas ma's complejas. v. autoencoders(codificadoresautoma'ticos) b. embeddings y"}
{"id_doc": "DOC_037", "segmentacion": "A", "chunk_id": "DOC_037_A_002", "idx": 2, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "skip connections (y = f(x)+x) permiten entrenar redes muy profundas mitigando vanishing gradient. bloques basic/bottleneck se apilan eficientemente. g. densenet conexiones densas \"todas con todas\" dentro del bloque; fomenta reutilizacio'n de caracter'ısticas, mejora el flujo de gradiente y reduce para'metros a igual rendimiento. iv. explicabilidaddelmodeloyembeddings a. visualizacio'n de activaciones y filtros observar feature maps muestra que' regiones activan cafigura2. estructuraba'sicadeunautoencoder. da neurona. en capas iniciales, activaciones recuerdan bordes/colores; en capas profundas, part'ıculas sema'nticas ma's complejas. v. autoencoders(codificadoresautoma'ticos) b. embeddings y reduccio'n de dimensionalidad a. estructura general y objetivo losembeddingssonvectoresenrdquecapturansema'ntica. vectores de clases similares tienden a agruparse en el espacio encoder→espacio latente→decoder latente. aprenden a reconstruir la entrada. aunque la sen˜al de entret-sne:proyeccio'nnolineala2d/3dpreservandovecinnamientoesauto-supervisada(salida=entrada),seconsideran darios locales. t'ıpicamenteme'todosnosupervisadospornorequeriretiquetas pca: proyeccio'n lineal; u'til como baseline o preproceexternas. samiento. b. componentes y variantes c. mapas de activacio'n (heatmaps) encoder: reduce espacialidad y comprime informacio'n heatmaps sen˜alan zonas que ma's influyen en la prediccio'n (conv + downsampling). (u'til en aplicaciones me'dicas/industriales para justificar deci- latente: vector/tensor compacto; su taman˜o controla capacisiones). dad vs. compresio'n. b. optimizacio'n y regularizacio'n optimizadores: sgd+momentum (control fino), adam (ra'pida convergencia). lr scheduling: step/cosine/plateau. regularizacio'n: l2 (weight decay), dropout (sobre todo en densas), early stopping. c. reglas pra'cticas de arquitectura dimensiones divisibles entre 2 para facilitar pooling. preferir kernels pequen˜os (3 × 3 / 5 × 5) y apilar figura3. ejemploconceptualdesuper-resolucio'nconautoencoder. profundidad para mayor no linealidad. usarglobalaveragepoolingantesdedensasparareducir para'metros. decoder: reconstruye con upsampling o convoluciones transinsertar bn despue's de conv y antes de relu para puestas. estabilidad. variantes: denoising (entrenar con entrada ruidosa y salida limpia),sparse(regularizalatente),under/overcomplete.(no- d. notas de implementacio'n ta: vaes y gans exceden el alcance de esta clase, pero se en frameworks como pytorch, la reconstruccio'n en decorelacionan con lo generativo.) ders suele emplear convtranspose2d o upsample+1× 1 conv; para clasificacio'n, crossentropyloss (con c. funciones de pe'rdida comunes logsoftmax interno) es esta'ndar. mse(meansquarederror):reconstruccio'np'ıxelap'ıxel vii. conclusiones (continuo). las cnn han transformado la visio'n por computadora mae: ma's robusto a outliers. al extraer jerarqu'ıas de caracter'ısticas de manera automa'tica bce/bcewithlogits: para ima'genes normalizay eficiente. no obstante, su interpretabilidad sigue siendo das/binarizadas. un reto; te'cnicas de visualizacio'n, embeddings y heatmaps perceptual/ssim (opcional): mejor correlacio'n percepayudan a entender y validar decisiones. los autoencoders tual que mse. extiendenestosconceptoshacialacompresio'n,reconstruccio'n ygeneracio'ndedatos,habilitandoaplicacionespra'cticascomo d. aplicaciones reduccio'ndedimensionalidad,deteccio'ndeanomal'ıasysuperresolucio'n. una ingenier'ıa cuidadosa de arquitectura, data y reduccio'n de dimensionalidad y almacenamiento efientrenamiento es clave para un desempen˜o robusto. ciente en bbdd vectoriales. deteccio'ndeanomal'ıas:entrenarcondatos\"normales\"; referencias altas pe'rdidas de reconstruccio'n sugieren anomal'ıas. [1] y.lecun,l.bottou,y.bengio,andp.haffner,\"gradient-basedlearning eliminacio'n"}
{"id_doc": "DOC_037", "segmentacion": "A", "chunk_id": "DOC_037_A_003", "idx": 3, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "jerarqu'ıas de caracter'ısticas de manera automa'tica bce/bcewithlogits: para ima'genes normalizay eficiente. no obstante, su interpretabilidad sigue siendo das/binarizadas. un reto; te'cnicas de visualizacio'n, embeddings y heatmaps perceptual/ssim (opcional): mejor correlacio'n percepayudan a entender y validar decisiones. los autoencoders tual que mse. extiendenestosconceptoshacialacompresio'n,reconstruccio'n ygeneracio'ndedatos,habilitandoaplicacionespra'cticascomo d. aplicaciones reduccio'ndedimensionalidad,deteccio'ndeanomal'ıasysuperresolucio'n. una ingenier'ıa cuidadosa de arquitectura, data y reduccio'n de dimensionalidad y almacenamiento efientrenamiento es clave para un desempen˜o robusto. ciente en bbdd vectoriales. deteccio'ndeanomal'ıas:entrenarcondatos\"normales\"; referencias altas pe'rdidas de reconstruccio'n sugieren anomal'ıas. [1] y.lecun,l.bottou,y.bengio,andp.haffner,\"gradient-basedlearning eliminacio'n de ruido (denoising). appliedtodocumentrecognition,\"proceedingsoftheieee,vol.86,no. super-resolucio'n: reconstruir versiones de mayor reso- 11,pp.2278-2324,1998. lucio'n. [2] c.szegedyetal.,\"goingdeeperwithconvolutions,\"cvpr,2015. e. hiperpara'metros relevantes taman˜o del latente: ma's pequen˜o = mayor compresio'n/menor fidelidad; ma's grande = mayor capacidad/costo. profundidaddelencoder/decoderytipodeupsampling (nearest/bilinear vs. convtranspose2d). pe'rdida de reconstruccio'n (mse/mae/bce/ssim) segu'n dominio. vi. buenaspra'cticasdeentrenamientoydisen˜o a. preprocesamiento y aumento de datos normalizacio'n por canal (media/desviacio'n del dataset). dataaugmentationmoderado:flips,crops,ligerosjitters; evita overfitting."}
{"id_doc": "DOC_039", "segmentacion": "A", "chunk_id": "DOC_039_A_000", "idx": 0, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "apuntes semana 11 clase #2 eder vega suazo escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica ic-6200 - inteligencia artificial gr2 resumen-este documento condensa la segunda leccio'n de la el profesor destaco' adema's las aplicaciones pra'cticas resemana11centradaenautoencodersysuaplicacio'naima'genesy visadas: la reduccio'n de dimensionalidad como alternativa a texto.seexplicanlaestructuraencoder-espaciolatente-decoder, me'todos tradicionales, la deteccio'n de anomal'ıas mediante variantes pra'cticas (denoising, vae, under/overcomplete) y arel ana'lisis del error de reconstruccio'n, y la restauracio'n de quitecturas relacionadas (u-net, skip-connections). se discuten tareas y aplicaciones: reduccio'n de dimensionalidad, deteccio'n ima'genes afectadas por ruido o baja resolucio'n. finalmente, de anomal'ıas, super-resolucio'n y segmentacio'n, adema's de la serepaso' elconceptodeespaciolatentecontinuo,introducido transicio'n a representaciones de texto (tokenizacio'n y embed- en los autoencoders variacionales (vae), el cual permite gedings)ymodelosdelenguaje.elapunteincluyerecomendaciones nerar nuevas muestras mediante la interpolacio'n entre puntos experimentales y criterios de evaluacio'n pra'cticos orientados a del espacio latente, estableciendo as'ı la base para los modelos la implementacio'n de proyectos y a la replicacio'n de resultados. generativos que se profundizar'ıan en la sesio'n actual. index terms-autoencoder, vae, denoising, reduccio'n de iii. apuntesdeclase dimensionalidad,tokenizacio'n,embeddings,u-net,deteccio'nde anomal'ıas. iii-a. organizacio'n y avisos las revisiones del proyecto sera'n presenciales, el proi. introduccio'n fesor aviso' a los apuntadores faltantes que lo tomen en cuenta ya que una semana se debera' de apartar para la este documento sintetiza los conceptos trabajados en la revisio'n del proyecto. sesio'n sobre arquitecturas basadas en redes convolucionales pro'ximamentehabra' dosentregablesprincipales:unejeraplicadas a autoencoders y la extensio'n hacia representaciocicio pra'ctico con autoencoders (ima'genes) y una tarea nes para texto. el documento ofrece una gu'ıa pra'ctica con ma's compleja sobre texto y agentes, esta puede que definiciones, fo'rmulas y recomendaciones operativas para la valga ma's porcentaje. ya que esta implica planificar implementacio'n de experimentos en ima'genes y texto. se suexperimentos y validaciones con tiempo para revisiones giere acompan˜ar este documento con las figuras referenciadas en laboratorio. parafacilitarlacomprensio'ndearquitecturasyvisualizaciones nota: zoom limita la validez de los enlaces; el profesor de espacios latentes. genero' un link que ya esta' en el grupo de telegram para las lecciones y la duracio'n t'ıpica de la sesio'n es ∼40 ii. repasodelaclase min, cuando acabe hay que ingresar nuevamente en el la sesio'n inicio' con un repaso de los temas vistos ante- mismo link. riormente, en los que se introdujeron los fundamentos de los iii-b. autoencoders: idea y componentes autoencoders y su relacio'n con las redes convolucionales. se recordo' que estas arquitecturas son una aplicacio'n directa de un autoencoder aprende una funcio'n f"}
{"id_doc": "DOC_039", "segmentacion": "A", "chunk_id": "DOC_039_A_001", "idx": 1, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "en el grupo de telegram para las lecciones y la duracio'n t'ıpica de la sesio'n es ∼40 ii. repasodelaclase min, cuando acabe hay que ingresar nuevamente en el la sesio'n inicio' con un repaso de los temas vistos ante- mismo link. riormente, en los que se introdujeron los fundamentos de los iii-b. autoencoders: idea y componentes autoencoders y su relacio'n con las redes convolucionales. se recordo' que estas arquitecturas son una aplicacio'n directa de un autoencoder aprende una funcio'n f : x (cid:55)→ xˆ donde xˆ las cnn en un contexto no supervisado, donde el objetivo intenta aproximarse a x. internamente: principalesreconstruirlaentradaoriginalapartirdeunarepre- encoder: transforma x en z =g (x). θ sentacio'n comprimida. el profesor enfatizo' que, a diferencia espacio latente: z es un vector de baja dimensio'n que de los modelos de clasificacio'n, los autoencoders no utilizan condensa caracter'ısticas relevantes. etiquetas externas, sino que aprenden de los propios datos, decoder: reconstruye xˆ=h (z). ϕ permitiendo capturar patrones y regularidades internas. en ima'genes el encoder usa convoluciones y pooling para durante el repaso, se analizo' la estructura general de un reducir resolucio'n y aumentar canales. el decoder usa operaautoencoder compuesta por un encoder, un espacio latente cionesdeupsamplingoconvolucio'ntranspuestapararecuperar y un decoder. el encoder transforma la entrada en una la forma espacial. ver figura 1 para un esquema general de representacio'ndemenordimensionalidadqueconcentralain- encoder/decoder. formacio'nesencial;eldecoder,asuvez,reconstruyelaimagen iii-c. entrenamiento y funciones de pe'rdida a partir de esa representacio'n. este proceso de codificacio'n y decodificacio'n se comparo' con una forma de \"compresio'n el objetivo del entrenamiento es reducir la diferencia entre aprendida\"dondeelmodelodecideque' informacio'nconservar laentradaoriginalylareconstruccio'nqueproduceelmodelo. y cua'l descartar. los elementos que generan las sen˜ales evaluadas por la laeleccio'nentrecomparacio'np'ıxelap'ıxelyunape'rdida paradatosbinariosdependedelrangoylainterpretacio'n de los p'ıxeles. cuando la calidad visual importa, adema's de la pe'rdida de entrenamiento suele evaluarse la reconstruccio'n con me'tricas perceptuales (p. ej. ssim) para complementar la evaluacio'n nume'rica. iii-d. variantes y su propo'sito iii-d0a. denoising: entrenar con x y objetivo ruidosa x limpio. el modelo aprende a eliminar ruido espec'ıfico (ej. salt-and-pepper). figura 1: esquema general de encoder, espacio latente y iii-d0b. under-/overcomplete: latente ma's pequen˜o decoder. obligaacomprimir;latentemayorpuedememorizarenexceso. iii-d0c. vae: permite muestrear y hacer interpolacio'n en un espacio continuo u'til para generacio'n. la combinacio'n funcio'n de pe'rdida son el encoder -que produce el vector de reconstruccio'n y kl genera latentes con estructura eslatente o, en variantes probabil'ısticas, los para'metros de una tad'ıstica. ver figura 2 para ilustracio'n de muestreo y espacio distribucio'n- y el decoder -que genera la reconstruccio'n"}
{"id_doc": "DOC_039", "segmentacion": "A", "chunk_id": "DOC_039_A_002", "idx": 2, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "espec'ıfico (ej. salt-and-pepper). figura 1: esquema general de encoder, espacio latente y iii-d0b. under-/overcomplete: latente ma's pequen˜o decoder. obligaacomprimir;latentemayorpuedememorizarenexceso. iii-d0c. vae: permite muestrear y hacer interpolacio'n en un espacio continuo u'til para generacio'n. la combinacio'n funcio'n de pe'rdida son el encoder -que produce el vector de reconstruccio'n y kl genera latentes con estructura eslatente o, en variantes probabil'ısticas, los para'metros de una tad'ıstica. ver figura 2 para ilustracio'n de muestreo y espacio distribucio'n- y el decoder -que genera la reconstruccio'n xˆ latente continuo. a partir de ese latente. iii-c0a. pe'rdidadereconstruccio'n.: eslamedidaprincipal que compara la entrada y la salida del autoencoder. su funcio'n es indicar cua'nto error comete el modelo al reconstruir.segu'neltipodedatosysunormalizacio'nseelige la forma pra'ctica de esta pe'rdida: para ima'genes normalizadas en [0,1] es comu'n usar una pe'rdida basada en la comparacio'n p'ıxel a p'ıxel (mencionada en clase como la opcio'n directa). el profesor comparo' esta te'cnica con el enfoque que usa'bamos en regresio'n para penalizar diferencias entre valores. para ima'genes con valores binarios o interpretadas como probabilidades se emplea una pe'rdida adecuada a ese caso (la alternativa binaria que se menciono' en la presentacio'n). lo que se obtiene con esta pe'rdida es un indicador directo de calidad de reconstruccio'n. en aplicaciones como deteccio'n de anomal'ıas se usa ese error (o una me'trica derivada) para decidir si una muestra es at'ıpica. iii-c0b. regularizacio'n en vae (pe'rdida adicional).: en la variante variacional el encoder no entrega un vector deterministasinopara'metrosdeunadistribucio'nenelespacio latente. adema's de la pe'rdida de reconstruccio'n, se incorpora figura 2: representacio'n del muestreo en vae unte'rminoqueobligaaqueladistribucio'nlatentesigaunareferencia(elprofesorlodescribio' comoforzarunadistribucio'n continua,porejemplo,normal).esete'rminoderegularizacio'n: iii-e. aplicaciones pra'cticas proviene directamente de los para'metros que calcula el encoder (media y dispersio'n en la clase). iii-e0a. reduccio'n de dimensionalidad: guardar z en su propo'sito es estructurar el espacio latente para que una base de datos vectorial. comparar vectores con similitud sea continuo y muestreable, permitiendo interpolacio'n y de coseno: generacio'n controlada. u-v sim(u,v)= . ∥u∥∥v∥ iii-c0c. notas pra'cticas (mencionadas en clase).: normalizar los p'ıxeles al rango adecuado facilita la usos: bu'squeda por similitud, indexacio'n y como entrada eleccio'n de la pe'rdida. comprimida para clasificadores simples (knn). iii-e0b. deteccio'n de anomal'ıas (ejemplo bancario): entrenarcontransaccionesva'lidas.paraunatransaccio'nnueva x: calcular err = l (x,xˆ). si el error err es mayor a un rec umbral, lo marca como posible fraude. seleccio'n del umbral τ medianterocovalidacio'nmanual.importanteevaluartasa de falsos positivos y costo operativo. iii-e0c. denoise y super-resolution: para superresolution el objetivo puede ser una imagen de alta resolucio'n x"}
{"id_doc": "DOC_039", "segmentacion": "A", "chunk_id": "DOC_039_A_003", "idx": 3, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "normalizar los p'ıxeles al rango adecuado facilita la usos: bu'squeda por similitud, indexacio'n y como entrada eleccio'n de la pe'rdida. comprimida para clasificadores simples (knn). iii-e0b. deteccio'n de anomal'ıas (ejemplo bancario): entrenarcontransaccionesva'lidas.paraunatransaccio'nnueva x: calcular err = l (x,xˆ). si el error err es mayor a un rec umbral, lo marca como posible fraude. seleccio'n del umbral τ medianterocovalidacio'nmanual.importanteevaluartasa de falsos positivos y costo operativo. iii-e0c. denoise y super-resolution: para superresolution el objetivo puede ser una imagen de alta resolucio'n x y la entrada x . arquitecturas con skip-connections hr lr (u-net style) mejoran la preservacio'n de detalles. se recomienda usar una figura comparativa de entrada/resultado en el informe experimental (ver figura 3). figura 3: ejemplo sugerido: comparacio'n entrada ruidosa / salida reconstruida / referencia. figura5:ejemplodevisualizacio'nt-snedevectoreslatentes. nota: aplicaciones forenses (p. ej. mejora de ca'maras) un compan˜ero plantea las consideraciones legales sobre manipulacio'n de evidencia. iii-g. transicio'n a nlp: tokenizacio'n y embeddings iii-e0d. segmentacio'n (u-net): u-net concatena mapas de caracter'ısticas del encoder en el decoder. esto restaura iii-g0a. tokenizacio'n: estrategias: palabra completa, informacio'n espacial perdida por pooling y mejora mapeo de subword (bpe), cara'cter, bytes. subword reduce oov y ma'scarasparasegmentacio'ndeobjetos.(sesugiereincluiruna controla longitud de secuencia. ver figura 6 para un esquema figura de arquitectura u-net y un ejemplo de ma'scara en la de tokenizacio'n subword. entrega.) iii-g0b. embeddings: cadatokensemapeaaunvector e∈rd mediantelacapaembedding.pararepresentarfrases se puede usar promedio de embeddings o agregadores ma's complejos. iii-g0c. modelosdelenguaje: evolucio'n:rnn/lstm → transformers con self-attention. la self-attention permite capturar dependencias largas y producir embeddings contextuales;esosembeddingssirvenpararecuperacio'n,clasificacio'n y agentes. figura 4: representacio'n de u-net iii-f. espacios latentes: visualizacio'n y utilidad (ampliado) visualizarz cont-sne/umapfacilitaverseparabilidadpor clases. cuando los clusters son n'ıtidos un clasificador simple sobre z funcionara' bien. en vae la continuidad del espacio figura 6: esquema ilustrativo de tokenizacio'n subword y permite interpolar entre muestras y generar ima'genes plausimapeo a ids. bles no vistas. ver figura 5 para un ejemplo de visualizacio'n t-sne. iii-h. recomendaciones operativas para la tarea probar al menos dos configuraciones: (1) denoising autoencoder, (2) vae con latente de prueba (p. ej. 32, 64) dependiendo de la gpu. guardar checkpoints y curvas de pe'rdida. evaluar mse y ssim. para anomal'ıas, definir umbral con validacio'n y reportar precisio'n/recall. paratexto,experimentartokenizacio'nsubwordyentrenar un embedding ba'sico antes de usar modelos preentrenados. iv. conclusiones los autoencoders representan una herramienta fundamental dentro del aprendizaje profundo no supervisado, al permitir que un modelo aprenda representaciones compactas de los datos sin depender de etiquetas externas. durante la sesio'n sedestaco'"}
{"id_doc": "DOC_039", "segmentacion": "A", "chunk_id": "DOC_039_A_004", "idx": 4, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "configuraciones: (1) denoising autoencoder, (2) vae con latente de prueba (p. ej. 32, 64) dependiendo de la gpu. guardar checkpoints y curvas de pe'rdida. evaluar mse y ssim. para anomal'ıas, definir umbral con validacio'n y reportar precisio'n/recall. paratexto,experimentartokenizacio'nsubwordyentrenar un embedding ba'sico antes de usar modelos preentrenados. iv. conclusiones los autoencoders representan una herramienta fundamental dentro del aprendizaje profundo no supervisado, al permitir que un modelo aprenda representaciones compactas de los datos sin depender de etiquetas externas. durante la sesio'n sedestaco' co'molaarquitecturaencoder-decoderconstituyela baseparamu'ltiplesaplicaciones,desdelareduccio'ndedimensionalidad hasta la generacio'n y reconstruccio'n de ima'genes. lacomprensio'ndelespaciolatenteresultaesencial,yaqueen e'l se concentra la informacio'n ma's relevante de las entradas y se posibilita la deteccio'n de patrones, la identificacio'n de anomal'ıas o la generacio'n de nuevos ejemplos a partir de distribuciones continuas como en los vae. asimismo, se vio la importancia de seleccionar correctamente las funciones de pe'rdida y de interpretar el error de reconstruccio'n segu'n el contexto de aplicacio'n. en tareas visuales, arquitecturas como u-net o las variantes con skipconnections ampl'ıan el potencial del modelo, mientras que en procesamiento de texto la nocio'n de codificacio'n latente se trasladaalosembeddingsyalatokenizacio'ncomopasospreviosalosmodelosdelenguaje.enconjunto,losautoencoders ofrecen una base conceptual y pra'ctica para desarrollar soluciones que integren visio'n e informacio'n textual, avanzando hacia sistemas ma's auto'nomos e interpretativos. referencias [1] stevenpachecop,\"autoencoder\"2025. [2] stevenpachecop,\"ragsyagentesusandollms\"2025. [3] compan˜erosd.clase,\"11 semana ai 20251014 (1,2,3).,\"2025."}
{"id_doc": "DOC_040", "segmentacion": "A", "chunk_id": "DOC_040_A_000", "idx": 0, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "apuntes semana 12 apuntes del 10 de octubre de 2025 andrey uren˜a bermu'dez - 2022017442 inteligencia artificial escuela de computacio'n, instituto tecnolo'gico de costa rica correo: andurena@estudiantec.cr abstract-estos apuntes corresponden a la semana 12 del curso de inteligencia artificial, impartido por el profesor steven pacheco portugue's en el instituto tecnolo'gico de costa rica. se abordan los temas relacionados con los modelos de lenguaje de gran escala (llm), la tokenizacio'n, embeddings, y la introduccio'nalparadigmaderetrieval-augmentedgeneration(rag) y agentes inteligentes. adema's, se presentan los anuncios del curso y el cronograma restante del semestre. i. introduccio'n durante esta sesio'n, se revisaron aspectos fundamentales de los modelos de lenguaje modernos y su relacio'n con las figura1. ejemplodeunmodeloderedneuronalpreentrenado. arquitecturas de inteligencia artificial actuales. tambie'n se analizaron conceptos claves para comprender co'mo los llm procesantexto,transformaninformacio'nenvectores,yaplican - semana 17: semana colcho'n (sin actividades prote'cnicas de recuperacio'n de conocimiento externo mediante gramadas). rag. finalmente, se discutieron las implicaciones e'ticas y el - semana 18: uso responsable de estos sistemas. ∗ martes 2 de diciembre: examen i. ii. anunciosdelcurso ∗ jueves 4 de diciembre: entrega del proyecto ii. - seasigno' latarea04sobreagentes,confechadeentrega el 6 de noviembre. la revisio'n sera' presencial y consiste en la creacio'n de un agente funcional. - se presento' el cronograma para el cierre del semestre, iii. repasodeconceptos organizado por semanas: - semana 13: a. modelos de lenguaje de gran escala (llm) ∗ martes 28 de octubre: quiz 6 y tema quantization - unsupervised. losllmsehanconvertidoenlabasedelossistemasmod- ∗ jueves30deoctubre:temaunsupervised-pca ernos de inteligencia artificial. permiten generar, comprender y entrega del proyecto i. y razonar sobre texto, co'digo, ima'genes y audio. - semana 14: cada entrada (input) es representada mediante valores ∗ martes 4 de noviembre: revisio'n presencial del nume'ricos en punto flotante que describen caracter'ısticas. el proyecto i. tratamiento var'ıa segu'n si la entrada corresponde a texto, ∗ jueves 6 de noviembre: revisio'n presencial del nu'meros o s'ımbolos. proyecto i y entrega de la tarea 04: agentes. - semana 15: ∗ martes 11 de noviembre: clase virtual sobre unsupervised - pca, asignacio'n del proyecto ii b. tokenizacio'n y la tarea 05: autoencoder - quantization. ∗ jueves 13 de noviembre: revisio'n virtual de la la tokenizacio'n convierte las palabras, signos o s'ımbolos tarea de agentes. en representaciones nume'ricas llamadas tokens. estos tokens - semana 16: permiten al modelo procesar texto de manera eficiente. ∗ martes 18 de noviembre: tema riesgos de la existen varios tipos de tokenizacio'n, resumidos en la inteligencia artificial. tabla"}
{"id_doc": "DOC_040", "segmentacion": "A", "chunk_id": "DOC_040_A_001", "idx": 1, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "11 de noviembre: clase virtual sobre unsupervised - pca, asignacio'n del proyecto ii b. tokenizacio'n y la tarea 05: autoencoder - quantization. ∗ jueves 13 de noviembre: revisio'n virtual de la la tokenizacio'n convierte las palabras, signos o s'ımbolos tarea de agentes. en representaciones nume'ricas llamadas tokens. estos tokens - semana 16: permiten al modelo procesar texto de manera eficiente. ∗ martes 18 de noviembre: tema riesgos de la existen varios tipos de tokenizacio'n, resumidos en la inteligencia artificial. tabla i. tablai iv. materianueva:retrieval-augmented tiposcomunesdetokenizacio'nysusprincipalesventajas. generation(rag) ventaja princi- un sistema rag conecta un llm con un mo'dulo tipo ejemplo pal de recuperacio'n de informacio'n (retriever) para incorporar palabra \"losmedios\" simplificada conocimiento externo relevante durante la generacio'n de recara'cter \"l\",\"o\",\"s\" sinoovs spuestas. subpalabra(bpe, equilibra vocab- \"super\"+\"vivencia\" wordpiece) ulario/contexto soportacualquier a. chunks byte-level bytesutf-8 s'ımbolo espacioenblanco \"hola\",\"mundo\" ra'pidoysimple el texto se divide en fragmentos denominados chunks, que suelen contener entre 200 y 500 tokens. cada fragmento se transforma en un vector mediante un modelo de embeddings, tras la tokenizacio'n, los tokens se representan como veccapturando su significado sema'ntico. tores en un espacio continuo. esto permite medir similitud sema'ntica entre palabras. b. consulta o recuperacio'n c. me'tricas de similitud dada una consulta, el sistema convierte la pregunta en las me'tricas ma's utilizadas incluyen: un embedding y calcula la similitud con los embeddings indexados, devolviendo los ma's cercanos sema'nticamente. - distancia euclidiana: mide que' tan separados esta'n dos puntos en el espacio vectorial. c. aumento y generacio'n - similitud del coseno: a-b los fragmentos recuperados se integran en el prompt envisim(a,b)= ado al llm, proporcionando contexto adicional que gu'ıa la ||a||||b|| respuesta hacia informacio'n verificada y relevante. evalu'a el a'ngulo entre los vectores; un a'ngulo menor implica mayor similitud. d. ventajas principales - reduccio'n de alucinaciones. tablaii - actualizacio'n continua del conocimiento. t e r j a e n m s p f l o o r s m i a m n pl e i n fic to a k d e o n d s e c t o o n k i e d n e i n z t a i c f i i o c 'n a : d l o a r s es pa n l u a m b e' r r a i s co se s. - eficiencia de costos en entrenamiento. - aplicabilidad en dominios especializados. palabra token idnume'rico - asistentes empresariales"}
{"id_doc": "DOC_040", "segmentacion": "A", "chunk_id": "DOC_040_A_002", "idx": 2, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "e i n fic to a k d e o n d s e c t o o n k i e d n e i n z t a i c f i i o c 'n a : d l o a r s es pa n l u a m b e' r r a i s co se s. - eficiencia de costos en entrenamiento. - aplicabilidad en dominios especializados. palabra token idnume'rico - asistentes empresariales enriquecidos. los los 105 - soporte a la investigacio'n y atencio'n al cliente. llm llm 2124 aprenden aprenden 893 v. llmtradicionalvsagenteinteligente patrones patrones 5749 un llm tradicional puede ofrecer informacio'n general, pero carece de personalizacio'n y accio'n. por ejemplo, si se le d. embeddings consulta\"¿cua'ntosd'ıasdevacacionesmequedan?\",nopodra' losembeddingssonrepresentacionesnume'ricasdensasque responder con precisio'n al no tener acceso a datos personales. asignan a cada token un vector en un espacio continuo de en cambio, un agente inteligente integra: alta dimensio'n. capturan significado sema'ntico y relaciones contextualesentrepalabrasuoracionescompletas,permitiendo - memoria: recuerda preferencias y contextos previos. comparaciones ma's profundas entre ideas o documentos. - herramientas: accede a apis externas (clima, vuelos, calendario). e. capacidades de los llm - planificacio'n: organiza y ejecuta tareas en funcio'n de objetivos. debido a su entrenamiento a gran escala y arquitecturas - accio'n: transforma planes en resultados concretos. basadas en transformers, los llm presentan capacidades este paradigma refleja la evolucio'n hacia sistemas que emergentes: razonan y actu'an, ma's alla' de solo responder texto. - comprensio'n contextual. - generacio'n coherente de texto. vi. escalamientoresponsable - razonamiento y planificacio'n ba'sica. - aprendizaje en el prompt (in-context learning). es fundamental evaluar cua'ndo realmente se requiere es- - multitarea sin reentrenamiento. calar de un modelo llm a un sistema de agentes o mul- - conocimiento esta'tico derivado de los datos de entre- tiagentes. esto implica garantizar seguridad, privacidad y el namiento. uso e'tico de los datos. los agentes deben ser disen˜ados bajo - costos computacionales elevados. principios de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligentes. estas herramientas representan un paso clave hacia una inteligencia artificial ma's contextual, adaptable y responsable. referencia pacheco portuguez, s. (2025). presentacio'n del curso de inteligencia artificial. instituto tecnolo'gico de costa rica."}
{"id_doc": "DOC_040", "segmentacion": "A", "chunk_id": "DOC_040_A_003", "idx": 3, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligentes. estas herramientas representan un paso clave hacia una inteligencia artificial ma's contextual, adaptable y responsable. referencia pacheco portuguez, s. (2025). presentacio'n del curso de inteligencia artificial. instituto tecnolo'gico de costa rica."}
{"id_doc": "DOC_042", "segmentacion": "A", "chunk_id": "DOC_042_A_000", "idx": 0, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "apuntes semana 12 - modelos de lenguaje extensos y sistemas avanzados (llms, rag y agentes inteligentes) fernando daniel brenes reyes escuela de ingeniería en computación instituto tecnológico de costa rica cartago, costa rica 21 de octubre 2020097446@estudiantec.cr resumen-el presente documento contiene un repaso y am- ii-b. embeddings y espacios vectoriales pliación de los conceptos fundamentales de los modelos de lenguaje extensos (llms), su representación del conocimiento una vez tokenizados, los ids numéricos se convierten en mediante la tokenización y los embeddings en espacios vec- embeddings, que son representaciones numéricas densas en toriales. se detalla la evolución del llm tradicional hacia un espacio continuo de alta dimensión. arquitecturas avanzadas como retrieval-augmented generation (rag), que resuelve las limitaciones de conocimiento estático, y captura semántica: los embeddings capturan el siglosagentesinteligentes,queintegranmemoria,planificaciónyla nificado y las relaciones contextuales entre palabras u capacidad de ejecutar acciones autónomas, reflejando el estado oraciones completas. del arte en la inteligencia artificial contextual y adaptable. indexterms-llm,rag,agentesinteligentes,tokenización, proximidad: las palabras con significados similares se embeddings, aprendizaje contextual. ubican próximas en el espacio vectorial. operaciones: este espacio permite realizar operacioi. introducción nes semánticas, como analogías (por ejemplo, rey - los modelos de lenguaje extensos (llms) se han hombre+mujer ≈reina). consolidado como la base de los sistemas modernos de inte- para medir la similitud entre dos vectores a y b en rn, la ligencia artificial generativa (iag). estos modelos no solo similitud del coseno es la métrica más utilizada: generan texto, sino que también permiten la comprensión y el razonamientosobretexto,códigoyotrainformacióncompleja. a-b sim(a,b)= (1) aunque son potentes, los llms poseen un conocimiento ||a||||b|| limitado a sus datos de entrenamiento (estático) y pueden incurrir en alucinaciones. para superar estas barreras, se han desarrollado enfoques como retrieval-augmented generation (rag) y los agentes inteligentes. ii. fundamentosdellmsyrepresentación ii-a. tokenización: de la palabra al número para que los llms puedan computar con el lenguaje, el texto de entrada debe convertirse en una representación numérica. el proceso de tokenización transforma palabras, signos o símbolos en unidades mínimas llamadas tokens, asignando a cada una un id numérico único. existen múltiples estrategias de tokenización, cada una optimizada para un objetivo distinto: por palabra: ofrece simplicidad. porcarácter:permitemanejarsímbolosopalabrasfuera del vocabulario (oov). subpalabra (bpe, wordpiece): logra un equilibrio óptimoentreeltamañodelvocabularioylapreservación figura1. representacióntridimensionaldetokens(realeza). del contexto. ii-c. capacidades emergentes el entrenamiento masivo de los llms les confiere capacidades avanzadas que emergen sin haber sido entrenados directamente para ellas: razonamiento y planificación. aprendizajeenelprompt(in-contextlearning):adaptan el comportamiento a partir de ejemplos dados"}
{"id_doc": "DOC_042", "segmentacion": "A", "chunk_id": "DOC_042_A_001", "idx": 1, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "signos o símbolos en unidades mínimas llamadas tokens, asignando a cada una un id numérico único. existen múltiples estrategias de tokenización, cada una optimizada para un objetivo distinto: por palabra: ofrece simplicidad. porcarácter:permitemanejarsímbolosopalabrasfuera del vocabulario (oov). subpalabra (bpe, wordpiece): logra un equilibrio óptimoentreeltamañodelvocabularioylapreservación figura1. representacióntridimensionaldetokens(realeza). del contexto. ii-c. capacidades emergentes el entrenamiento masivo de los llms les confiere capacidades avanzadas que emergen sin haber sido entrenados directamente para ellas: razonamiento y planificación. aprendizajeenelprompt(in-contextlearning):adaptan el comportamiento a partir de ejemplos dados en la entrada. multitarea: realizan traducción, clasificación y codificación sin reentrenamiento. iii. retrieval-augmentedgeneration(rag) rag es un paradigma que conecta un llm con un mó- figura3. agenteinteligente. dulo de recuperación (retriever) para inyectar conocimiento externo, actualizado y verificable durante la generación de respuestas. iv. dellmaagenteinteligente los agentes inteligentes basados en llms superan la iii-a. proceso y flujo de rag pasividaddelossistemasrag.estosagentespuedenrazonar, 1. preparación (chunking): los documentos se dividen planificar y actuar de manera autónoma, interactuando con en fragmentos (chunks), que suelen contener entre 200 el mundo real mediante herramientas externas. y 500 tokens, a menudo con overlap para preservar el iv-a. componentes clave del agente contexto. 2. indexación: cada chunk se convierte en un embedding 1. memoria: permite mantener coherencia y contexto a lo y se almacena en una base de datos vectorial (por largo del tiempo. ejemplo, faiss, qdrant, pinecone). corto plazo: ventana de contexto del modelo. 3. consulta y recuperación: la pregunta del usuario se largo plazo: bases de datos externas, incluyendo transforma en un embedding, se calcula la similitud sistemas rag para la recuperación contextual. con los vectores indexados y se seleccionan los top-k 2. planificación:permitedescomponerproblemascomplechunks más cercanos semánticamente. jos en pasos y razonar sobre ellos. 4. aumento y generación: los chunks recuperados se chains of thought (cot): razonamiento secuenintegran en una plantilla estructurada (prompt) como cial. contexto adicional, asegurando que la respuesta del trees of thought (tot):exploracióndemúltiples llm sea precisa y fundamentada. caminos de razonamiento antes de decidir. iii-b. ventajas y limitaciones 3. acción: capacidad de ejecutar tareas concretas mediante herramientas externas (apis, buscadores, sistemas rag ofrece la reducción de alucinaciones, la actuarag). por ejemplo, un agente puede acceder a un lización continua del conocimiento y la aplicabilidad en sistemaderecursoshumanospararesponder:\"¿cuántos dominios especializados. no obstante, los sistemas rag sidías de vacaciones me quedan?\". guen siendo pasivos; su función se limita a complementar la respuesta del llm con datos recuperados. iv-b. escalamiento responsable la implementación de agentes requiere evaluar cuándo"}
{"id_doc": "DOC_042", "segmentacion": "A", "chunk_id": "DOC_042_A_002", "idx": 2, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "iii-b. ventajas y limitaciones 3. acción: capacidad de ejecutar tareas concretas mediante herramientas externas (apis, buscadores, sistemas rag ofrece la reducción de alucinaciones, la actuarag). por ejemplo, un agente puede acceder a un lización continua del conocimiento y la aplicabilidad en sistemaderecursoshumanospararesponder:\"¿cuántos dominios especializados. no obstante, los sistemas rag sidías de vacaciones me quedan?\". guen siendo pasivos; su función se limita a complementar la respuesta del llm con datos recuperados. iv-b. escalamiento responsable la implementación de agentes requiere evaluar cuándo es necesarialacomplejidaddeunsistemamultiagente.escrucial garantizar la seguridad, privacidad y el uso ético de los datos, diseñando los agentes bajo principios de transparencia y responsabilidad. referencias [1] pacheco portuguez, s. (2025). presentación del curso de inteligencia artificial.institutotecnológicodecostarica. figura2. diagramadelflujodeunsistemarag,desdelaindexaciónhasta lageneracióndelarespuesta."}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_000", "idx": 0, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "1 apuntes semana 12, martes 21 de octubre carranza jiménez kevin instituto tecnológico de costa rica correo electrónico: kcarranza@estudiantec.cr resumen-el siguiente documento presenta el resumen de la modelos de lenguaje de gran escala (llm). los rag combiclasedeldíamartes21deoctubre,impartidaporelprofesorste- nan la capacidad generativa de los llm con mecanismos de venpachecoportugüezenelinstitutotecnológicodecostarica. recuperación de información externa, permitiendo respuestas la clase presenta el cronograma restante del curso, un resumen más precisas y actualizadas basadas en conocimiento relevande la clase anterior en el que se repasan los temas de modelos de lenguaje a gran escala (llm), tokenización, embeddings y la te [1]. por su parte, los agentes inteligentes extienden este introdución del paradigma de retrieval-augmented generation enfoque al incorporar razonamiento, planificación y toma de (rag) y agentes inteligentes. en la presente clase se profundiza decisiones, posibilitando sistemas que no solo generan texto, en el tema de llm, rags y agentes introducidos en la clase sino que también actúan de manera autónoma en función de anterior. objetivos específicos [2]. index terms-llm, rag, embedding, agente iii-a. ¿por qué los llm son tan utilizados? i. introducción los modelos de lenguaje de gran escala (llm) se han la sesión inició con una revisión del cronograma restante convertido en la base de numerosos sistemas modernos del curso. posteriormente, se realizó un repaso de la clase de inteligencia artificial, impulsando avances significativos anterior, en la cual se introdujeron conceptos fundamentales en tareas de generación, comprensión y razonamiento sobrelosmodelosdelenguajedegranescala(largelanguage sobre texto, código e incluso modalidades más complejas models,llm),elprocesodetokenizaciónylarepresentación como imágenes y audio. estos modelos son capaces semántica mediante embeddings. de representar conocimiento a gran escala mediante el a partir deeste punto, la clase se centró endos temas prin- aprendizaje de patrones lingüísticos y semánticos a partir de cipales: la integración de modelos mediante esquemas de re- enormes volúmenes de datos, lo que explica la sorprendente cuperación aumentada de generación (retrieval-augmented coherencia y versatilidad de sus resultados [3]. comprender generation, rag) y la noción de agentes inteligentes. los mecanismos internos que permiten estas representaciones, así como sus limitaciones y potencial de generalización, resultaesencialparaeldesarrollodeaplicacionesmásseguras ii. aspectosadministrativosdelaclase y efectivas basadas en inteligencia artificial generativa. se presentó el calendario en el cual se muestrán las próximas actividades y evaluaciones que restan del curso. en la la figura 1 ilustra la arquitectura de un modelo de red tabla i. neuronal preentrenado diseñado para la clasificación de eventos de colisión. este modelo recibe como entrada un conjunto iii. ragsyagentesutilizandollms"}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_001", "idx": 1, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "de agentes inteligentes. los mecanismos internos que permiten estas representaciones, así como sus limitaciones y potencial de generalización, resultaesencialparaeldesarrollodeaplicacionesmásseguras ii. aspectosadministrativosdelaclase y efectivas basadas en inteligencia artificial generativa. se presentó el calendario en el cual se muestrán las próximas actividades y evaluaciones que restan del curso. en la la figura 1 ilustra la arquitectura de un modelo de red tabla i. neuronal preentrenado diseñado para la clasificación de eventos de colisión. este modelo recibe como entrada un conjunto iii. ragsyagentesutilizandollms de características o features que incluyen la velocidad del vehículo,lacalidaddelterreno,elgradodevisióndisponibley los esquemas de recuperación aumentada de generación laexperienciatotaldelconductor.apartirdeestosparámetros, (retrieval-augmented generation, rag) y los agentes intelila red aprende a identificar patrones que permiten estimar la gentesrepresentanunaevoluciónsignificativaenelusodelos probabilidad de que ocurra una colisión bajo determinadas cuadroi cronogramadeclasesyactividades semana martes jueves 12 claseagentes-llmyasig- clasequantization nacióndetarea04agentes 13 aplicarquiz6yclasequan- clase unsupervised - pca y tization-unsupervised entregadeproyectoi 14 evaluación presencial proyec- entrega tarea 04 agentes y toi. evaluaciónpresencialproyectoi 15 clase virtual unsupervised - revisión virtual de tarea 04 pca, asignación de proyecto agentes ii y asignación de tarea 05 autoencoder-quantization 16 clasesesgosdeai 17 semanacolchón semanacolchón figura 1. modelo de red neuronal preentrenado para la clasificación de 18 exameni entregaproyectoii eventosdecolisión. 2 cuadroii ejemplosimplificadodetokenización palabra token idnumérico los los 105 llm llm 2124 aprenden aprenden 893 patrones patrones 5749 condiciones.elusodemodelospreentrenadosenestecontexto facilita una generalización más robusta y una convergencia más rápida durante el proceso de entrenamiento, lo cual resulta ventajoso en escenarios donde los datos etiquetados son limitados [4]. iii-b. tokenización figura2. 3dsemanticfeaturespace en el procesamiento del lenguaje natural, cada palabra, signo o símbolo debe transformarse en una representación numérica para que pueda ser comprendida y procesada por iii-c. representación de tokens en un espacio vectorial los modelos de lenguaje. este proceso se conoce como tokeuna vez que el texto ha sido tokenizado, cada token se nización, y consiste en dividir el texto en unidades mínimas convierte en un número que sirve únicamente como idendenominadas tokens, que pueden corresponder a palabras, tificador dentro del vocabulario del modelo. sin embargo, subpalabrasoinclusocaracteresindividuales.acadatokense estos valores numéricos carecen de significado semántico por leasignaunidentificadornuméricoúnicodentrodeunvocabusí mismos, ya que no reflejan las relaciones o similitudes lario previamente definido, lo que permite representar oracioentre las palabras. para que un modelo pueda comprender el nes completas como secuencias de números. existen diversas contextoyelsignificadodellenguaje,esnecesariotransformar estrategias de tokenización, como la basada en subpalabras dichos identificadores en representaciones continuas que cap- (byte pair encoding o wordpiece), que buscan equilibrar la"}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_002", "idx": 2, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "corresponder a palabras, tificador dentro del vocabulario del modelo. sin embargo, subpalabrasoinclusocaracteresindividuales.acadatokense estos valores numéricos carecen de significado semántico por leasignaunidentificadornuméricoúnicodentrodeunvocabusí mismos, ya que no reflejan las relaciones o similitudes lario previamente definido, lo que permite representar oracioentre las palabras. para que un modelo pueda comprender el nes completas como secuencias de números. existen diversas contextoyelsignificadodellenguaje,esnecesariotransformar estrategias de tokenización, como la basada en subpalabras dichos identificadores en representaciones continuas que cap- (byte pair encoding o wordpiece), que buscan equilibrar la turen las propiedades semánticas y sintácticas de las palabras eficiencia del vocabulario con la capacidad del modelo para dentro del texto. este proceso se logra mediante el uso de manejar palabras desconocidas o de diferentes idiomas [5]. embeddings, los cuales permiten a los modelos de lenguaje latablaiimuestraunejemplosimplificadodelprocesode aprenderrepresentacionesvectorialesquepreservanrelaciones tokenización,enelcualcadapalabradeltextoesdescompuesta de significado y proximidad contextual [6]. en su correspondiente token y asociada a un identificador la figura 2 representa un espacio vectorial tridimensional numérico dentro del vocabulario del modelo. este procedien el que las palabras se distribuyen según tres dimensiones miento permite representar de forma estructurada los elemensemánticas: edad, género y realeza. cada punto del espacio tos lingüísticos, facilitando que el modelo procese el texto corresponde a la proyección de una palabra en función de como una secuencia de valores discretos que posteriormente sus características aprendidas por el modelo, lo que permite serán transformados en vectores continuos mediante técnicas observar relaciones de similitud y diferencia entre concepde embedding. tos. por ejemplo, términos como \"rey\" y \"reina\" se ubican la tabla iii resume algunos de los tipos más comunes de próximos entre sí en la dimensión de realeza, pero difieren tokenizaciónutilizadosenmodelosdelenguaje.cadaenfoque en la dimensión de género, ilustrando cómo los embeddings difiere en el nivel de granularidad con que divide el texto: capturanrelacionessemánticascomplejasdentrodeunespacio desde unidades completas como palabras, hasta fragmentos continuo [6]. más pequeños como subpalabras, caracteres o incluso bytes individuales. esta diversidad de métodos permite adaptar la representación del texto según las necesidades del modelo, iii-d. similaridad entre vectores equilibrando la complejidad del vocabulario con la capacidad unavezquelaspalabrashansidotransformadasenvectores para manejar palabras desconocidas o símbolos especiales. dentro de un espacio continuo, es posible cuantificar su grado de similitud midiendo la distancia o el ángulo entre dichos vectores. en este contexto, dos vectores próximos representan cuadroiii palabras con significados semánticamente similares, mientras tiposcomunesdetokenización que aquellos que se encuentran alejados reflejan conceptos tipo ejemplo ventajaprincipal distintos o no relacionados. esta propiedad permite a los mopalabra"}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_003", "idx": 3, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "necesidades del modelo, iii-d. similaridad entre vectores equilibrando la complejidad del vocabulario con la capacidad unavezquelaspalabrashansidotransformadasenvectores para manejar palabras desconocidas o símbolos especiales. dentro de un espacio continuo, es posible cuantificar su grado de similitud midiendo la distancia o el ángulo entre dichos vectores. en este contexto, dos vectores próximos representan cuadroiii palabras con significados semánticamente similares, mientras tiposcomunesdetokenización que aquellos que se encuentran alejados reflejan conceptos tipo ejemplo ventajaprincipal distintos o no relacionados. esta propiedad permite a los mopalabra \"losmodelos\" simplicidad delos de lenguaje capturar relaciones latentes como analogías caracter \"l\",.o\",\"s\" sinoov* subpalabra .aprend-ïendo\" equilibrio vocabula- o asociaciones conceptuales, lo que ha sido fundamental para rio/contexto tareas como la búsqueda semántica, la traducción automática byte-level bytesutf-8 soportacualquiersímbolo espacioenblanco \"hola\",\"mundo\" rápidoysimple y la inferencia contextual [7]. 3 iii-e. métricas más comunes -como el razonamiento contextual, la inferencia lógica o la adaptación a tareas no vistas durante el entrenamiento- no las métricas más comunes para calcular similitud entre fueron programadas de forma directa, sino que surgen como vectores son: resultado del aprendizaje de patrones complejos a partir de distancia euclidiana: enormes volúmenes de datos textuales y contextuales. este (cid:115) d(a,b)= (cid:88) (a -b )2 (1) fenómenohasidoobjetodecrecienteinterés,yaqueevidencia i i cómo la escala y la estructura de los modelos pueden dar i lugar a comportamientos no lineales y sofisticados en el mide que tan lejos están los puntos. procesamiento del lenguaje natural [9]. similitud del coseno a-b sim(a,b)= (2) iv-b. capacidades de modelos de lenguaje ∥a∥∥b∥ comprensión textual: interpretan el significado de pamideelánguloentrevectores:cuantomáspequeño,más labras y frases según el entorno en el que aparecen. similares. generación coherente de texto: pueden redactar, tradula más usada en modelos de lenguaje es la similitud de cir o resumir información manteniendo estilo y consiscoseno, ya que se enfoca en la dirección del vector más que tencia. en su magnitud. razonamiento y planificación: resulven problemas, explican pasos y trazan estrategias. iv. embeddings aprendizaje de prompt: adaptan su comportamiento a partir de ejemplos dados en la misma conversación (inlosembeddingssonrepresentacionesnuméricasdensasque context learning). asignanacadatoken-yaseaunapalabra,subpalabraoinclumultitarea: realizan traducción, clasificación, codificasounafrase-unvectorenunespaciocontinuodealtadimención, análisis o dialogo sin requerir reentrenamiento. sión. estas representaciones permiten capturar el significado semántico y las relaciones contextuales entre los términos, de modoquepalabrasconsentidossimilaresseubiquenpróximas iv-c. limitación de los modelos de lenguaje entre sí dentro del espacio vectorial. además, los modelos alucinaciones: generan respuestas convincentes pero modernos son capaces de generar embeddings a nivel de frase incorrectas o inventadas. o enunciado (sentence embeddings), los cuales condensan"}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_004", "idx": 4, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "partir de ejemplos dados en la misma conversación (inlosembeddingssonrepresentacionesnuméricasdensasque context learning). asignanacadatoken-yaseaunapalabra,subpalabraoinclumultitarea: realizan traducción, clasificación, codificasounafrase-unvectorenunespaciocontinuodealtadimención, análisis o dialogo sin requerir reentrenamiento. sión. estas representaciones permiten capturar el significado semántico y las relaciones contextuales entre los términos, de modoquepalabrasconsentidossimilaresseubiquenpróximas iv-c. limitación de los modelos de lenguaje entre sí dentro del espacio vectorial. además, los modelos alucinaciones: generan respuestas convincentes pero modernos son capaces de generar embeddings a nivel de frase incorrectas o inventadas. o enunciado (sentence embeddings), los cuales condensan el memoria limitada: no recuerdan interacciones pasadas significado global de un texto. este tipo de representación más allá de su ventana de contexto. posibilita comparar oraciones, ideas o documentos en función conocimiento estático: su información proviene de los desucontenidosemántico,enlugardebasarseúnicamenteen datos de entrenamiento. coincidencias literales de palabras [8]. costos computacionales: requieren grandes recursos la figura 3 representa un ejemplo conceptual de embed- para entrenamiento e inferencia. dingsparafrasessimilaresenelquesemuestranlasdiferentes fases de forma general y simplificada, pasando des de la v. retrival-augmentedgeneration(rag) palabra,documentouoración,hastaelespaciodelembedding. el enfoque de recuperación aumentada de generación (retrieval-augmented generation, rag) combina la potencia iv-a. capacidad de los modelos de lenguaje generativa de los modelos de lenguaje de gran escala (llm) gracias a su entrenamiento a gran escala y al uso de conunmóduloderecuperacióndeinformaciónexterna,conoarquitecturas basadas en transformers, los modelos de len- cido como retriever. este componente permite inyectar conoguaje de gran escala (llm) han desarrollado un conjunto de cimientorelevanteprovenientedebasesdedatosocolecciones capacidades emergentes que trascienden las funciones para de documentos en el momento de la consulta, ampliando así las que fueron diseñados explícitamente. estas habilidades lacapacidaddelmodeloparagenerarrespuestasmásprecisas, actualizadas y fundamentadas en evidencia. de esta manera, el sistema integra razonamiento generativo con recuperación informativa,superandolaslimitacionesdelosllmentrenados únicamente con conocimiento estático [1]. v-a. ingesta y chunking elprimerpasoenlaconstruccióndeunsistemaderecuperación aumentada de generación (rag) consiste en preparar los documentos que servirán como fuente de información. para ello, el texto se segmenta en fragmentos manejables denominados chunks, que suelen tener una longitud entre 200 y 500 tokens, con el fin de preservar la coherencia semántica figura3. ejemploconceptualdeembeddingsdefrasessimilares y facilitar la recuperación eficiente de información relevante. 4 posteriormente, cada fragmento se transforma en un vector filtrado híbrido; pinecone, un servicio en la nube que ofrece mediante un módulo de embeddings, el cual codifica su signi- indexaciónvectorialescalableymantenimientoautomáticode ficado semántico en un espacio de alta dimensión. esta repre- índices;yfaiss(facebookaisimilaritysearch),unabibliosentaciónvectorialpermitemedirsimilitudesentreconsultasy teca desarrollada por meta que permite búsquedas eficientes fragmentosdetexto,habilitandolabúsquedacontextualbasada en grandes volúmenes de vectores mediante técnicas de cuanen significado y no en coincidencias literales [10]. tización y optimización"}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_005", "idx": 5, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "semántica figura3. ejemploconceptualdeembeddingsdefrasessimilares y facilitar la recuperación eficiente de información relevante. 4 posteriormente, cada fragmento se transforma en un vector filtrado híbrido; pinecone, un servicio en la nube que ofrece mediante un módulo de embeddings, el cual codifica su signi- indexaciónvectorialescalableymantenimientoautomáticode ficado semántico en un espacio de alta dimensión. esta repre- índices;yfaiss(facebookaisimilaritysearch),unabibliosentaciónvectorialpermitemedirsimilitudesentreconsultasy teca desarrollada por meta que permite búsquedas eficientes fragmentosdetexto,habilitandolabúsquedacontextualbasada en grandes volúmenes de vectores mediante técnicas de cuanen significado y no en coincidencias literales [10]. tización y optimización de memoria. estas herramientas son v-a1. chunking tamaño fijo: el proceso de chunking de esencialesparaelfuncionamientodesistemasragmodernos, tamaño fijo consiste en dividir los documentos en segmentos alpermitirunarecuperaciónrápidayprecisadelcontextomás detextodelongitudpredefinida,conelobjetivodeestandarizar relevante [14]-[16]. las unidades de información utilizadas en los sistemas de recuperación. esta técnica permite equilibrar la granularidad v-c. consulta o recuperación delcontenido:fragmentosdemasiadopequeñospuedenperder una vez construida la base vectorial, el siguiente paso contextosemántico,mientrasquefragmentosdemasiadogranconsisteenrealizarlarecuperaciónsemánticadeinformación. desdificultanlabúsquedaeficienteyaumentanlaambigüedad dada una consulta o pregunta formulada por el usuario, esta en la recuperación. al mantener un tamaño constante, los se transforma en un embedding que captura su significado chunks facilitan la indexación vectorial y mejoran la precien un espacio de alta dimensión. posteriormente, se calcula sión de los modelos que emplean embeddings para comparar la similitud -comúnmente mediante la métrica del cosenoconsultas y pasajes de texto [11]. entre este vector de consulta y todos los embeddings previav-a2. chunking recursivo: el chunking recursivo es una mente indexados. finalmente, el sistema devuelve los top-k técnica avanzada utilizada para segmentar texto de manera fragmentos más cercanos, es decir, aquellos cuya representajerárquica y adaptativa, en lugar de emplear longitudes fijas. ción vectorial es más similar a la de la consulta. este proceso este método divide los documentos siguiendo la estructura permiterealizarbúsquedasbasadasenelsignificadosemántico lingüística del contenido, como párrafos, oraciones o secciodel texto, en lugar de depender de coincidencias literales o nes,yaplicafragmentacionesadicionalescuandounsegmento palabrasexactas,loquemejorasignificativamentelaprecisión excedeunlímitedetokensdefinido.deestemodo,sepreserva contextual en aplicaciones basadas en retrieval-augmented el contexto semántico relevante en cada fragmento, evitando generation (rag) [1]. cortesarbitrariosquepodríanafectarlacoherenciadeltexto.el enfoquerecursivoresultaespecialmenteútilentareasderecuperaciónaumentadadegeneración(rag),dondemantenerla v-d. augmentación y generación (inyección de contexto) integridad semántica de los chunks mejora significativamente elpasofinalenunsistemaretrieval-augmentedgeneration la precisión de la recuperación contextual [12]. (rag) consiste en integrar la información recuperada dentro v-a3. chunkingsimilaridadsemántica: elchunkingbasa- delprompt queseenviaráalmodelodelenguaje.paraello,se do en similitud semántica emplea medidas vectoriales -prin- construye una plantilla o estructura de entrada que combina cipalmente la similitud del coseno- para dividir un texto la pregunta del usuario con los fragmentos de texto más en fragmentos coherentes según su significado, en lugar de relevantes obtenidos en la"}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_006", "idx": 6, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "(inyección de contexto) integridad semántica de los chunks mejora significativamente elpasofinalenunsistemaretrieval-augmentedgeneration la precisión de la recuperación contextual [12]. (rag) consiste en integrar la información recuperada dentro v-a3. chunkingsimilaridadsemántica: elchunkingbasa- delprompt queseenviaráalmodelodelenguaje.paraello,se do en similitud semántica emplea medidas vectoriales -prin- construye una plantilla o estructura de entrada que combina cipalmente la similitud del coseno- para dividir un texto la pregunta del usuario con los fragmentos de texto más en fragmentos coherentes según su significado, en lugar de relevantes obtenidos en la fase de recuperación. este contexto hacerlo por longitud o estructura gramatical. en este enfoque, adicional actúa como una fuente de conocimiento explícita segeneranembeddingsdeoracionesopárrafosconsecutivos,y que guía al llm, permitiéndole generar una respuesta más se calcula la similitud coseno entre ellos. cuando la similitud precisa, coherente y sustentada en la evidencia. de esta cae por debajo de un umbral predefinido, se considera que el contextocambiasignificativamente,estableciendoasíunnuevo límitedechunk.estemétodoproducedivisionesmásnaturales desde el punto de vista semántico, preservando la coherencia temática y mejorando la recuperación contextual en sistemas basados en retrieval-augmented generation (rag) [13]. v-b. indexación la indexación vectorial es un proceso fundamental en lossistemasderecuperaciónaumentada(retrieval-augmented generation, rag), que permite almacenar y buscar eficientemente representaciones numéricas de documentos o fragmentos de texto en un espacio vectorial de alta dimensión. su propósito es facilitar la recuperación de información semánticamentesimilaraunaconsultamediantelacomparación de vectores utilizando métricas como la similitud coseno o la distancia euclidiana. entre las soluciones más utilizadas se encuentran qdrant, un motor de búsqueda vectorial de código abierto optimizado para búsquedas por similitud y figura4. diagramadelprocesodelrag 5 manera,elmodelonodependeúnicamentedesuconocimiento referencias preentrenado, sino que se apoya en información actualizada y [1] p. lewis, e. perez, a. piktus, f. petroni, v. karpukhin, n. goyal, específica al dominio, lo cual mejora la fiabilidad y reduce la h. küttler, m. lewis, w. tau yih, t. rocktäschel, s. riedel, and alucinación de respuestas [17]. d.kiela,\"retrieval-augmentedgenerationforknowledge-intensivenlp tasks,\"advancesinneuralinformationprocessingsystems(neurips), la figura 4 ilustra de forma general el funcionamiento 2020. del proceso retrieval-augmented generation (rag). este [2] s. wang, y. qin, w. chen, z. wu, z. xi, y. xu, t. gui, x. qiu, and enfoque combina la recuperación de información relevante z.zhang,\"asurveyonlargelanguagemodelbasedautonomousagents,\" arxivpreprintarxiv:2401.03428,2024. desde una base vectorial con la generación de texto asistida [3] openai, \"gpt-4 technical report,\" arxiv preprint arxiv:2303.08774, por un modelo de lenguaje. a partir de una consulta del 2023. usuario, el sistema identifica los fragmentos más relacionados [4] y.lecun,y.bengio,andg.hinton,\"deeplearning,\"nature,vol.521, pp.436-444,2015. semánticamente, los integra dentro del prompt y genera una [5] r. sennrich, b. haddow, and a."}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_007", "idx": 7, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "qin, w. chen, z. wu, z. xi, y. xu, t. gui, x. qiu, and enfoque combina la recuperación de información relevante z.zhang,\"asurveyonlargelanguagemodelbasedautonomousagents,\" arxivpreprintarxiv:2401.03428,2024. desde una base vectorial con la generación de texto asistida [3] openai, \"gpt-4 technical report,\" arxiv preprint arxiv:2303.08774, por un modelo de lenguaje. a partir de una consulta del 2023. usuario, el sistema identifica los fragmentos más relacionados [4] y.lecun,y.bengio,andg.hinton,\"deeplearning,\"nature,vol.521, pp.436-444,2015. semánticamente, los integra dentro del prompt y genera una [5] r. sennrich, b. haddow, and a. birch, \"neural machine translation respuestafundamentadaendichasevidencias.deestamanera, of rare words with subword units,\" in proceedings of the 54th annual el modelo puede ofrecer respuestas más precisas, actualizadas meetingoftheassociationforcomputationallinguistics(acl),2016, pp.1715-1725. y contextualizadas que las obtenidas únicamente a partir del [6] t.mikolov,k.chen,g.corrado,andj.dean,\"efficientestimationof conocimiento interno del llm [1]. wordrepresentationsinvectorspace,\"inproceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2013. [7] j. pennington, r. socher, and c. d. manning, \"glove: global vectors vi. beneficios for word representation,\" in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), 2014, el uso de arquitecturas basadas en retrieval-augmented pp.1532-1543. generation (rag) ofrece múltiples beneficios frente al uso [8] n.reimersandi.gurevych,\"sentence-bert:sentenceembeddingsusing siamese bert-networks,\" in proceedings of the 2019 conference on de modelos de lenguaje puros. en primer lugar, permite empirical methods in natural language processing (emnlp), 2019, una significativa reducción de las alucinaciones, ya que pp.3982-3992. el modelo genera sus respuestas apoyándose en evidencia [9] j. wei, y. tay, r. bommasani, c. raffel, b. zoph, s. borgeaud, d. yogatama, m. bosma, d. zhou, d. metzler, e. chi, t. hashimoto, documental verificable en lugar de depender únicamente de o.vinyals,p.liang,j.dean,andw.fedus,\"emergentabilitiesoflarge suconocimientoimplícito.además,posibilitalaactualización languagemodels,\"arxivpreprintarxiv:2206.07682,2022. continuadelconocimiento,dadoquelabasederecuperación [10] v.karpukhin,b.oguz,s.min,p.lewis,l.wu,s.edunov,d.chen, andw.tauyih,\"densepassageretrievalforopen-domainquestionanspuede ser renovada con información reciente sin necesidad wering,\"inproceedingsofthe2020conferenceonempiricalmethods de reentrenar el modelo. este enfoque también contribuye a innaturallanguageprocessing(emnlp),2020,pp.6769-6781. una mayor eficiencia de costos, al disminuir la necesidad de [11] g. izacard and e. grave, \"leveraging passage retrieval with generative models for open domain question answering,\" in proceedings of entrenamientosextensivosyaprovecharmodelospreexistentes the 16th conference of the european chapter of the association for combinados con fuentes dinámicas de datos. finalmente, el computationallinguistics(eacl),2021,pp.874-880. paradigma rag favorece la aplicabilidad en dominios espe- [12] l. gilardi and d. steiner, \"recursive chunking strategies for improved context retrieval in large language models,\" arxiv preprint arcializados,permitiendoadaptarelcomportamientodelsistema xiv:2309.02706,2023. a contextos como medicina, derecho o ingeniería mediante la [13] y. liu, s. kumar, and p. gupta, \"semantic chunking with cosine incorporación de bases de conocimiento específicas [18]. similarity for enhanced context preservation in rag systems,\" arxiv preprintarxiv:2403.11892,2024. [14] j. johnson, m. douze, and h."}
{"id_doc": "DOC_043", "segmentacion": "A", "chunk_id": "DOC_043_A_008", "idx": 8, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "dinámicas de datos. finalmente, el computationallinguistics(eacl),2021,pp.874-880. paradigma rag favorece la aplicabilidad en dominios espe- [12] l. gilardi and d. steiner, \"recursive chunking strategies for improved context retrieval in large language models,\" arxiv preprint arcializados,permitiendoadaptarelcomportamientodelsistema xiv:2309.02706,2023. a contextos como medicina, derecho o ingeniería mediante la [13] y. liu, s. kumar, and p. gupta, \"semantic chunking with cosine incorporación de bases de conocimiento específicas [18]. similarity for enhanced context preservation in rag systems,\" arxiv preprintarxiv:2403.11892,2024. [14] j. johnson, m. douze, and h. jégou, \"billion-scale similarity search vii. casosdeuso withgpus,\"ieeetransactionsonbigdata,vol.7,no.3,pp.535-547, 2019. los sistemas basados en retrieval-augmented generation [15] p.s.inc.,\"pineconedocumentation,\"https://docs.pinecone.io/,2024. (rag)presentanaplicacionesprácticasenmúltiplesdominios. [16] q.team,\"qdrant:vectordatabasedocumentation,\"https://qdrant.tech/ documentation/,2024. por ejemplo, en el ámbito corporativo, los asistentes empre- [17] g. izacard, p. lewis, m. lomeli, l. hosseini, f. petroni, t. schick, sariales enriquecidos pueden ofrecer información precisa y s. riedel, and d. kiela, \"atlas: few-shot learning with retrieval augcontextualizada a partir de bases de conocimiento internas, mentedlanguagemodels,\"arxivpreprintarxiv:2208.03299,2022. [18] y.gao,s.li,j.lin,j.callanetal.,\"retrieval-augmentedgeneration mejorando la productividad y la toma de decisiones. en el forlargelanguagemodels:asurvey,\"arxivpreprintarxiv:2312.10997, campo de la investigación, los rag facilitan la recuperación 2023. de literatura relevante y la síntesis de información compleja, acelerandoelanálisisdegrandesvolúmenesdedatostextuales. asimismo, en el área de soporte al cliente, estos sistemas permiten generar respuestas fundamentadas y coherentes a consultas de usuarios, reduciendo errores y mejorando la experiencia de atención mediante información verificada y actualizada [18]. viii. tarea4:agenteconversacional al final de la clase se presentó la asignación y revisión del enunciadodelatarea4,centradaeneldesarrollodeunagente conversacional. la fecha de entrega se ha establecido para el jueves 6 de noviembre."}
{"id_doc": "DOC_044", "segmentacion": "A", "chunk_id": "DOC_044_A_000", "idx": 0, "autor": "Nelson Rojas Obando", "fecha": "2025-10-23", "tema": "Introducción al aprendizaje no supervisado y quantization como técnica de optimización de modelos de deep learning, reduciendo tamaño y consumo computacional sin pérdida significativa de precisión.", "texto": "apuntes de la clase del 23 de octubre de 2025 cursodeinteligenciaartificial nelson rojas obando estudiante ingeniería en computación nelson.rojas@estudiantec.cr resumen-este informe presenta una síntesis de los temas iii-a. ejemplo contextual abordadosenlasesióndel23deoctubredelcursodeinteligencia un modelo como llama 2 posee más de 70 mil millones artificial, centrada en el cierre del aprendizaje supervisado y la introducción al proceso de quantization como técnica de deparámetros,loqueequivaleaaproximadamente28gbsolo optimización de modelos de aprendizaje profundo. para almacenarlos en disco. cargar ese modelo en memoria index terms-inteligencia artificial, quantization sería inviable sin una gpu especializada, por lo que la quantization se convierte en una alternativa para reducir el i. introducción tamaño y mantener la funcionalidad. durante la clase se abordaron temas de actualidad relaiv. representaciónnumérica cionados con la evolución de los modelos de lenguaje y su impacto en el futuro del internet. con esta sesión concluye iv-a. números enteros la sección del curso dedicada al aprendizaje supervisado. a los computadores representan los números utilizando separtir de este punto, los contenidos se centran en métodos de cuencias de bits. con n bits se pueden representar 2n valores aprendizajenosupervisado,esdecir,aquellosquenodependen distintos. de etiquetas o resultados predeterminados para evaluar la por ejemplo, con 3 bits se pueden representar los números calidad del aprendizaje del modelo. del 0 al 7. el formato más común para representar números enteros ii. aspectosadministrativos con signo en cpus es el complemento a dos, donde: semencionólaintegracióndeherramientascomochatgpt el bit más significativo indica el signo (0 = positivo, 1 = atlas para chrome, que reflejan cómo las empresas están negativo). los demás bits representan el valor absoluto. orientando sus estrategias hacia la adopción de modelos de lenguaje extensos (llms) como núcleo de sus servicios 4.2. números de punto flotante digitales. los números de punto flotante se utilizan para representar asimismo, se compartieron noticias institucionales sobre valores reales que no pueden expresarse de manera exacta la rama ieee del tecnológico de costa rica, que organiza con enteros. en la norma ieee 754, un número flotante se reunionesperiódicasentredistintasuniversidades.elpropósito representa mediante tres componentes principales: el signo, el principal es identificar fuentes de financiamiento para eventos exponente y la mantissa (también conocida como fracción o tecnológicos, especialmente aquellos destinados a llevar co- significando). nocimiento a zonas rurales o con menor acceso. también se anunciólarealizacióndeuntallerdeteambuildingeldomingo parte descripción bitstípicos(float32) signo(s) indicasielnúmeroespositivoonegativo 1 9 de noviembre, con un costo de $20, que incluye almuerzo exponente(e) determinalaescalaorangodelnúmero 8 y transporte. mantissa(m) definelaprecisiónopartefraccionaria 23 cuadroi iii. temaprincipal:quantization estructuradelformatoieee754de32bits. quantization es"}
{"id_doc": "DOC_044", "segmentacion": "A", "chunk_id": "DOC_044_A_001", "idx": 1, "autor": "Nelson Rojas Obando", "fecha": "2025-10-23", "tema": "Introducción al aprendizaje no supervisado y quantization como técnica de optimización de modelos de deep learning, reduciendo tamaño y consumo computacional sin pérdida significativa de precisión.", "texto": "un número flotante se reunionesperiódicasentredistintasuniversidades.elpropósito representa mediante tres componentes principales: el signo, el principal es identificar fuentes de financiamiento para eventos exponente y la mantissa (también conocida como fracción o tecnológicos, especialmente aquellos destinados a llevar co- significando). nocimiento a zonas rurales o con menor acceso. también se anunciólarealizacióndeuntallerdeteambuildingeldomingo parte descripción bitstípicos(float32) signo(s) indicasielnúmeroespositivoonegativo 1 9 de noviembre, con un costo de $20, que incluye almuerzo exponente(e) determinalaescalaorangodelnúmero 8 y transporte. mantissa(m) definelaprecisiónopartefraccionaria 23 cuadroi iii. temaprincipal:quantization estructuradelformatoieee754de32bits. quantization es una técnica de optimización de modelos de aprendizaje profundo que busca reducir el tamaño y el con- el valor real que representa el número en punto flotante se sumo de recursos computacionales de un modelo sin compro- calcula mediante la siguiente ecuación: meter significativamente su precisión. la idea es convertir los parámetros del modelo (usualmente almacenados en formato x=(-1)s×(1+m)×2(e-127) de punto flotante de 32 bits, float32) a representaciones de menor precisión, como int8, int4 o incluso int1, dependiendo donde: del nivel de compresión deseado. s es el bit de signo. esto permite ejecutar modelos de gran tamaño en hardware m es la fracción o mantissa normalizada. con recursos limitados (por ejemplo, dispositivos móviles o e es el exponente con un sesgo de 127 (en el caso de microcontroladores). float32). este formato permite representar números muy grandes o ix. conclusiones muy pequeños, aunque implica un mayor uso de memoria y el estudio del quantization permite comprender cómo los recursos computacionales en comparación con representaciomodelos de inteligencia artificial pueden adaptarse a las limines de menor precisión. taciones del hardware sin comprometer significativamente su desempeño.estatécnicarepresentaunpuntodeconexiónentre v. quantizationderedesneuronales el desarrollo teórico de los algoritmos y su aplicación real en en redes neuronales, las matrices de pesos y sesgos están sistemas de producción, donde los recursos computacionales, representadascomoflotantes.elprocesodequantizationbusca laenergíayeltiempodeinferenciasonfactoresdeterminantes. convertir esos valores a enteros para reducir memoria y acelerar la inferencia. v-a. etapas del proceso quantize: los valores en punto flotante se transforman a enteros. inferencia el modelo realiza sus cálculos con aritmética entera. dequantize los resultados se transforman nuevamente a flotantes para la siguiente capa. el desafío está en mantener la precisión del modelo. los hardware modernos (gpu, tpu, cpu vectoriales) incluyen soporte para operaciones de baja precisión (por ejemplo, int8) para facilitar este proceso. vi. tiposdequantization vi-a. quantization simétrica usa un rango centrado en cero: vi-b. quantization asimétrica utiliza un rango desplazado [α,β]: vii. estrategiasyvariantes vii-a. dynamic quantization laescalayelrangosecalculanentiempodeinferencia.se aplican factores estadísticos derivados"}
{"id_doc": "DOC_044", "segmentacion": "A", "chunk_id": "DOC_044_A_002", "idx": 2, "autor": "Nelson Rojas Obando", "fecha": "2025-10-23", "tema": "Introducción al aprendizaje no supervisado y quantization como técnica de optimización de modelos de deep learning, reduciendo tamaño y consumo computacional sin pérdida significativa de precisión.", "texto": "inferencia el modelo realiza sus cálculos con aritmética entera. dequantize los resultados se transforman nuevamente a flotantes para la siguiente capa. el desafío está en mantener la precisión del modelo. los hardware modernos (gpu, tpu, cpu vectoriales) incluyen soporte para operaciones de baja precisión (por ejemplo, int8) para facilitar este proceso. vi. tiposdequantization vi-a. quantization simétrica usa un rango centrado en cero: vi-b. quantization asimétrica utiliza un rango desplazado [α,β]: vii. estrategiasyvariantes vii-a. dynamic quantization laescalayelrangosecalculanentiempodeinferencia.se aplican factores estadísticos derivados del conjunto de datos de prueba (\"calibration set\"). vii-b. post-training quantization (ptq) después del entrenamiento, se insertan observadores (observers) en el modelo para analizar las salidas de cada capa y determinar los mejores parámetros de escala y punto cero. este proceso no requiere reentrenamiento y es rápido, aunque puede perder algo de precisión. vii-c. quantization-aware training (qat) simula la quantization durante el entrenamiento. el modelo aprendeacompensarloserroresintroducidosporlareducción deprecisión,porloquemantieneunrendimientosuperiortras el proceso. viii. ventajasdelquantization menor consumo de memoria: los modelos comprimidos se cargan más rápido. menor tiempo de inferencia: cálculos más simples. menor consumo energético: ideal para dispositivos embebidos o móviles. portabilidad: permite ejecutar modelos complejos en hardware limitado."}
{"id_doc": "DOC_046", "segmentacion": "A", "chunk_id": "DOC_046_A_000", "idx": 0, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "notas de clase inteligenciaartificial-23deoctubre-semana12 luis alfredo gonza'lez sa'nchez escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica 2021024482 gonzal3z.luis@estudiantec.cr abstract-neural network quantization is a vital technique in modelos de aprendizaje automa'tico desarrollados en distinaithatreducesmodelsizeandcomputationalcostbyconverting tos frameworks como pytorch o tensorflow en una repreweights and activations from floating-point to lower-precision sentacio'n intermedia esta'ndar y eficiente. esta representacio'n formats, such as integers. this process enables deployment on facilita la interoperabilidad y el despliegue de modelos en resource-constrained devices, like mobile or embedded systems, while maintaining high accuracy. different methods include diferentes plataformas y hardware mediante optimizaciones symmetricandasymmetricquantization,withstrategiestailored en c++ u otros lenguajes, asegurando que el mismo modelo to specific data distributions and hardware constraints. dy- pueda ejecutarse con alto rendimiento en entornos variados. namic, granular, and post-training quantization further refine considerandoloanterior,lasplataformasposeendiversaslimthis approach by adjusting intervals per layer, per sample, or itacionesyrendimientotantoensoftwarecomoenhardware,si after training, respectively. these techniques involve calculating scaling factors and zero points to effectively map high-precision se entrenan modelos grandes, posiblemente un celular no este valuestolower-bitrepresentations,introducingminimalaccuracy adaptado para soportar dicho modelo, para ello se observara loss. overall, quantization enhances efficiency, reduces power el concepto de quantization. consumption, and facilitates real-time ai applications, making it a cornerstone of practical deep learning deployment. iii. quantization index terms-quantization in neural networks,model com- suponga que se tiene un modelo de deep learning con pression,qat quantization techniques,integer representation muchas capas, por ejemplo , llama 2 , con 70 mil millones de parametros aproximadamente, si cada parametro es de 32 i. introduction bits, se obtiene un taman˜o aproximado de 28 gb para solo almacenar el modelo, ¿co'mo podr'ıamos cargarlo a memoria? lacuantizacio'nenredesneuronalesesunate'cnicaesencial una alternativa es comprar una gpu con dicho taman˜o para para mejorar la eficiencia del co'mputo y reducir el taman˜o el procesamiento del modelo, pero gpus que soporten esos de los modelos, principalmente transformando los datos de taman˜os son costosas , lo que se busca es reducir el taman˜o punto flotante a formatos de menor precisio'n, como enteros. del modelo, una de sus te'cnicas es quantization esta transformacio'n permite que los modelos se ejecuten de manera ma's ra'pida y con menor consumo de memoria, lo a. definicion cual es fundamental para desplegar inteligencia artificial en quantization es una te'cnica de compresio'n de modelos de dispositivos con recursos limitados, como mo'viles y sistemas aprendizajeautoma'ticoquereduceelnumerodebitsutilizados embebidos. en el presente documento, se busca resumir la para representar los para'metros del modelo, transformando informacio'n vista"}
{"id_doc": "DOC_046", "segmentacion": "A", "chunk_id": "DOC_046_A_001", "idx": 1, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "a formatos de menor precisio'n, como enteros. del modelo, una de sus te'cnicas es quantization esta transformacio'n permite que los modelos se ejecuten de manera ma's ra'pida y con menor consumo de memoria, lo a. definicion cual es fundamental para desplegar inteligencia artificial en quantization es una te'cnica de compresio'n de modelos de dispositivos con recursos limitados, como mo'viles y sistemas aprendizajeautoma'ticoquereduceelnumerodebitsutilizados embebidos. en el presente documento, se busca resumir la para representar los para'metros del modelo, transformando informacio'n vista en la clase del 23 de octubre,donde se ha los valores de punto flotante a representaciones de menor revisado co'mo diferentes me'todos de cuantizacio'n -desde la precisio'n, generalmente enteros de 8, 5, 2 o incluso 1 bit. sime'trica y asime'trica hasta la dina'mica, granulada y post- lo que se busca es disminuir el taman˜o y la complejidad entrenamiento-manejanlaconversio'ndepesos,activaciones computacionaldelmodelo,manteniendounaprecisio'ncercana y sesgos, optimizando el balance entre precisio'n y eficiencia. al original. no se debe de confundir como una te'cnica de adema's, se menciona co'mo te'cnicas como la cuantizacio'n redondear pesos, sino de convertir y ajustar los tipos de datos conscienteduranteelentrenamiento(qat)ayudanamantener paraoptimizarelbalanceentretaman˜o,velocidaddeinferencia la precisio'n del modelo al considerar el efecto de la cuanti- yprecisio'n.sebuscauntradeoffoptimoentrecapacidadesdel zacio'n desde el inicio del aprendizaje. modelo vs rendimiento. b. ventajas ii. brevedefinicio'ndeonnix - menor consumo de memoria al cargar los modelos en para continuar el tema de quantization en supervised learn- memoria ing , es importante entender la herramienta onnix, suponga - permite insertar el modelo en sistemas con recursos un modelo llm ya entrenado¿co'mo empieza a funcionar el limitados / con propo'sito especifico, como celulares o productoosistema?laherramientaonnixpermiterepresentar embebidos asignan rangos de valores flotantes a niveles discretos enteros. - cuantizacio'n de entradas: las entradas a cada capa tambie'nseconviertenaenterosparamantenerlacoherencia en la representacio'n y facilitar operaciones eficientes. fig.1. partesdeunnu'meropuntoflotante - cuantizacio'n del sesgo (bias): los te'rminos de sesgo, que son sumados en cada neurona, se transforman de float. - genera un menor tiempo para hacer las inferencias, sus - normalizacio'ndelrango:sedefinenvaloresma'ximosy datos son ma's simples m'ınimosparapesos,entradasysesgos,quecorresponden - menor consumo energ'ıa debido a menor complejidad de a los valores l'ımite de la representacio'n entera (por computacio'n ejemplo, el rango de int8). esto asegura que los valores cuantizados este'n dentro de rangos representables. iv. breverepasoalasoperacionesconbits - ca'lculo en espacio entero: las operaciones de la capa se dara' un breve repaso a la manipulacio'n de bits en (multiplicacio'nysuma)serealizanenenteros,generando sistemas computacionales para entender mejor el proceso de un vector cuantizado. quantization - des-cuantizacio'n: despue's"}
{"id_doc": "DOC_046", "segmentacion": "A", "chunk_id": "DOC_046_A_002", "idx": 2, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "normalizacio'ndelrango:sedefinenvaloresma'ximosy datos son ma's simples m'ınimosparapesos,entradasysesgos,quecorresponden - menor consumo energ'ıa debido a menor complejidad de a los valores l'ımite de la representacio'n entera (por computacio'n ejemplo, el rango de int8). esto asegura que los valores cuantizados este'n dentro de rangos representables. iv. breverepasoalasoperacionesconbits - ca'lculo en espacio entero: las operaciones de la capa se dara' un breve repaso a la manipulacio'n de bits en (multiplicacio'nysuma)serealizanenenteros,generando sistemas computacionales para entender mejor el proceso de un vector cuantizado. quantization - des-cuantizacio'n: despue's de la capa, los valores encon2n bitssepuedenrepresentar2n valoresdistintos.esto teros se convierten nuevamente a punto flotante para significa que, por ejemplo, con 3 bits es posible representar continuar con el procesamiento de modo que las capas 23 =8 nu'meros diferentes. siguientes no requieren conocer el esquema de cuantiun ejemplo ba'sico que se vio' en clase es de conversio'n de zacio'n aplicado. binario a decimal es el nu'mero 6, que en binario se escribe durante la dequantization es donde puede ocurrir pe'rdida de como 110. la conversio'n se realiza sumando las potencias de precisio'n, ya que la conversio'n entre representaciones intro2 correspondientes a los bits activos (1) segu'n su posicio'n: duceaproximaciones.sinembargo,elobjetivoesquelasalida cuantizadasealosuficientementecercanaalaoriginalparano 6=1×22+1×21+0×20 =4+2+0=6 afectarelrendimientodelmodelo.esteprocesoesbeneficioso ya que permite que modelos originalmente pesados funcionen cada d'ıgito binario representa una potencia de 2, comen- eficientemente con menor consumo de memoria y tiempo zando desde la derecha con la potencia 0. de co'mputo, esencial sistemas embebidos o con capacidad los nu'meros enteros en sistemas digitales se representan limitada. es importante aclara que las capas que siguen a una normalmente en complemento a 2, donde el bit ma's significa- capa cuantizada generalmente no requieren modificaciones ni tivo indica el signo: 0 para positivo y 1 para negativo. esto conocen directamente la cuantizacio'n aplicada, manteniendo facilitarealizaroperacionesaritme'ticasconnu'merosnegativos transparencia en la mayor'ıa de frameworks. usando operaciones binarias esta'ndar. vi. tiposdequantization para nu'meros en punto flotante, la representacio'n se divide en tres partes: signo, exponente y mantisa (fraccio'n). el valor los tipos de consonantizan son los siguientes: decimal se calcula aproximadamente como: - quantizationsime'trica:mapeavalorespositivosynegativos de un rango m'ınimo a ma'ximo que incluye el cero. aqu'ı, el valor cero real se mapea exactamente a cero valor=(-1)signo×(1+mantisa)×2exponente-bias entero. esto simplifica el manejo de pesos y activaciones esta te'cnica de representacio'n permite expresar un amplio con signo, aplicando la misma escala en ambos lados del rangodenu'merosrealesconprecisio'nlimitadayeficienciaen cero. almacenamiento mediante manipulacio'n de bits. - quantization asime'trica: mapea valores entre 0 y un observe la"}
{"id_doc": "DOC_046", "segmentacion": "A", "chunk_id": "DOC_046_A_003", "idx": 3, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "los tipos de consonantizan son los siguientes: decimal se calcula aproximadamente como: - quantizationsime'trica:mapeavalorespositivosynegativos de un rango m'ınimo a ma'ximo que incluye el cero. aqu'ı, el valor cero real se mapea exactamente a cero valor=(-1)signo×(1+mantisa)×2exponente-bias entero. esto simplifica el manejo de pesos y activaciones esta te'cnica de representacio'n permite expresar un amplio con signo, aplicando la misma escala en ambos lados del rangodenu'merosrealesconprecisio'nlimitadayeficienciaen cero. almacenamiento mediante manipulacio'n de bits. - quantization asime'trica: mapea valores entre 0 y un observe la figura 1 donde se puede observar las partes valor ma'ximo entero, pero el valor m'ınimo real no se del numero flotante de 32 bits ahora bien , considerando las mapea a cero, sino a un valor entero llamado zero point partes del numero punto flotante, es importante detallar que (z), que representa el valor neutro o \"offset\" de la la precisio'n dada por la mantiza se va a disminuir con cuantizacio'n.estopermiterepresentarvaloresconundequantization . splazamiento, u'til cuando los valores no esta'n centrados en cero. v. procedimientodequantizationenmodelosde las fo'rmulas para la cuantizacio'n simetrica y asime'tricas se redesneuronales describen a continuacio'n : se toma un rango [b,a] y se mapea pasos generales del procedimiento: a un rango de salida, se calcula el para'metro de escalado y por ultimo se calcula el nu'mero neutro del mapeo. - transformacio'n de pesos:lospesosdelared,originalmente en formato de punto flotante (float), se convierten - x q =clamp (cid:0)(cid:4)x s f (cid:5) +z; 0; 2n-1 (cid:1) a valores enteros mediante mapas de cuantizacio'n que - x =valor flotante f - las convoluciones se realizan con mu'ltiples filtros que aprenden valores y distribuciones distintas. - cada filtro detecta diferentes caracter'ısticas (features) de la imagen. - no es posible aplicar el mismo intervalo a,b para todos los filtros, por lo que se calcula un intervalo a,b espec'ıfico para cada filtro respetando su distribucio'n. quantization post-training: - se realiza despue's del entrenamiento, utilizando datos fig.2. tiposdequantizaton:sime'tricavsasime'trica nunca antes vistos por el modelo. - introduce un componente llamado observer, que obtiene - para'metro de escalado s: estad'ısticas de cada capa para calibrar las salidas y calcular los para'metros de cuantizacio'n como la escala α-β s= (s) y punto cero (z). 2n-1 - permite cuantizar el modelo sin necesidad de re2n-1=el rango de salida entrenamiento completo. - para'metro neutro z: (cid:22) (cid:23) quantization aware training (qat): β z = -1- s - me'todoavanzadodondelacuantizacio'nsesimuladurante el entrenamiento. - n es el nu'mero de bits. - el modelo aprende"}
{"id_doc": "DOC_046", "segmentacion": "A", "chunk_id": "DOC_046_A_004", "idx": 4, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "un componente llamado observer, que obtiene - para'metro de escalado s: estad'ısticas de cada capa para calibrar las salidas y calcular los para'metros de cuantizacio'n como la escala α-β s= (s) y punto cero (z). 2n-1 - permite cuantizar el modelo sin necesidad de re2n-1=el rango de salida entrenamiento completo. - para'metro neutro z: (cid:22) (cid:23) quantization aware training (qat): β z = -1- s - me'todoavanzadodondelacuantizacio'nsesimuladurante el entrenamiento. - n es el nu'mero de bits. - el modelo aprende a compensar la pe'rdida de precisio'n y su respectiva des-cuantizacio'n : por la cuantizacio'n al utilizar la funcio'n de perdida para actualizar los pesos que constantemente sufren de este x =s×(x -z) f q efecto. para la cuantizacio'n sime'trica : - mejora el rendimiento en modelos cuantizados para en- - x q =clamp (cid:0)(cid:4)x s f (cid:5) ; - (cid:0) 2n-1-1 (cid:1) ; 2n-1-1 (cid:1) tornos de baja precisio'n. - para'metro de escalado s: viii. conclusio'n abs(α) s= la informacio'n presentada demostro' la importancia para 2n-1-1 la optimizacio'n del uso de recursos en modelos de redes - n es el nu'mero de bits. neuronales,especialmentesisedeseaimplementarensistemas y su respectiva descuantizacio'n se brinda por la siguiente embebidos o con recursos limitados, en esta clase se aprendio formula que : tex x f =s×x q - la cuantizacio'n consiste en transformar pesos, activaciones y sesgos de punto flotante a representaciones de vii. otrostiposdecuantizaciones menor precisio'n, principalmente enteros, con el fin de cuantizacio'n dina'mica: la cuantizacio'n dina'mica se en- reducir taman˜o y acelerar la inferencia. foca en cuantizar las activaciones de las neuronas seleccio- - existen distintos tipos de cuantizacio'n: sime'trica, nandoapropiadamentelosvaloresm'ınimos(a)yma'ximos(b) asime'trica, dina'mica, granulada y postpara el mapeo de cuantizacio'n de cada tensor. la estrategia entrenamiento,cadaunaconestrategiasespec'ıficaspara de seleccio'n del intervalo a,b es la siguiente: mapear y convertir datos. - para cuantizacio'n asime'trica, se seleccionan los valores extremos reales del tensor, es decir, b y a corresponden al ma'ximo y m'ınimo del tensor respectivamente. - para cuantizacio'n sime'trica, se toma el mayor valor en te'rminos absolutos y se define el intervalo como [-a,a], centrado en cero. - esta te'cnica puede inducir un mayor error debido a la sensibilidad a valores at'ıpicos (outliers). una solucio'n es utilizar percentiles basados en la distribucio'n del tensor, excluyendo los outliers y reduciendo el error cuadra'tico medio (mse). cuantizacio'n granulada en convoluciones:"}
{"id_doc": "DOC_046", "segmentacion": "A", "chunk_id": "DOC_046_A_005", "idx": 5, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "para cuantizacio'n sime'trica, se toma el mayor valor en te'rminos absolutos y se define el intervalo como [-a,a], centrado en cero. - esta te'cnica puede inducir un mayor error debido a la sensibilidad a valores at'ıpicos (outliers). una solucio'n es utilizar percentiles basados en la distribucio'n del tensor, excluyendo los outliers y reduciendo el error cuadra'tico medio (mse). cuantizacio'n granulada en convoluciones:"}
{"id_doc": "DOC_041", "segmentacion": "A", "chunk_id": "DOC_041_A_000", "idx": 0, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "inteligencia artificial apuntesdeclase-21deoctubrede2025 1st kendall rodr'ıguez camacho escuela de ingenieria en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica kenrodriguez@estudiantec.cr abstract-el presente documento contiene los apuntes de la tablei clase del martes 21 de octubre de 2025, que cubren conceptos calendarioprevistoparaelrestodelcurso clave sobre los modelos de lenguaje extensos (llms). los apuntes explican co'mo los llms representan el conocimiento semana martes jueves medianteembeddingsyespaciosvectoriales,eintroducente'cnicas 12 asignacio'ndetarea04(agentes) clasedequantization como retrieval-augmented generation (rag) y agentes in- 13 quiz 6, terminar tema quantiza- claseunsupervisedlearningyenteligentes que ampl'ıan las capacidades de los llms con infor- tion,empezarunsupervisedlearn- tregaproyecto01 macio'n externa y acciones auto'nomas. ingypca 14 revisio'ndeproyecto01deforma entrega de tarea 04 (agentes) y presencial (se sacara' cita en un continuarconrevisio'ndeproyectos i. introduccio'n forms) 15 clasevirtual,seasignaproyecto02 revisio'n tarea 04 (agentes) de ytarea05sobrequantization formavirtual los modelos de lenguaje extensos (llms) han transfor16 claseriesgosdeia visitaamicrosoft mado la interaccio'n con la inteligencia artificial gracias a 17 - - su capacidad para generar texto coherente, traducir idiomas, 18 examenpresencial entregadeproyecto02 redactar co'digo y analizar informacio'n compleja. su funcionamientosebasaenlarepresentacio'nnume'ricadepalabras y frases en espacios vectoriales, donde los embeddings cap- b. asignacio'n de tarea 04 turan relaciones sema'nticas y contextuales. se asigna la tarea 04, la cual consiste en desarrollar un si bien los llms ofrecen capacidades sorprendentes, su asistente conversacional que se desempen˜e ante diferentes conocimiento es limitado a los datos de entrenamiento y preguntas basadas en una base de documentos (apuntes de carecen de habilidades para actuar o buscar informacio'n acti- clase realizados por los estudiantes hasta la fecha). vamente. para superar estas limitaciones, se han desarrollado se requiere implementar te'cnicas de recuperacio'n y auenfoques como retrieval-augmented generation (rag), que mento de contexto (rag) y comparar emp'ıricamente los enriquece las respuestas con informacio'n externa relevante en resultados con distintos esquemas de segmentacio'n del texto. tiempo real, y agentes inteligentes basados en llms, capaces la fecha de entrega esta' prevista para el jueves 6 de de razonar, planificar y ejecutar tareas auto'nomas. noviembre. este documento explora estos me'todos y su evolucio'n, iii. fundamentosdelosllms mostrandoco'mosepuedepasardemodelospasivosasistemas que no solo comprenden el lenguaje, sino que tambie'n inter- a. funcionamiento general actu'an con el entorno, toman decisiones informadas y aplican los llms procesan los datos de entrada transforma'ndolos conocimiento actualizado. en representaciones nume'ricas que describen caracter'ısticas sema'nticas. cada palabra, s'ımbolo o cara'cter se convierte en una secuencia de valores nume'ricos mediante la tokenizacio'n, ii. aspectosdelcurso para luego ser procesados en redes neuronales profundas con a. calendario previsto para el"}
{"id_doc": "DOC_041", "segmentacion": "A", "chunk_id": "DOC_041_A_001", "idx": 1, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "explora estos me'todos y su evolucio'n, iii. fundamentosdelosllms mostrandoco'mosepuedepasardemodelospasivosasistemas que no solo comprenden el lenguaje, sino que tambie'n inter- a. funcionamiento general actu'an con el entorno, toman decisiones informadas y aplican los llms procesan los datos de entrada transforma'ndolos conocimiento actualizado. en representaciones nume'ricas que describen caracter'ısticas sema'nticas. cada palabra, s'ımbolo o cara'cter se convierte en una secuencia de valores nume'ricos mediante la tokenizacio'n, ii. aspectosdelcurso para luego ser procesados en redes neuronales profundas con a. calendario previsto para el resto del curso millones o miles de millones de para'metros. la siguiente tabla i muestra la planificacio'n de las ac- b. del lenguaje al nu'mero tividades restantes del curso, indicando las fechas y tareas eltextodebeconvertirseenrepresentacionesnume'ricaspara correspondientes para cada semana. ser interpretado por el modelo. el proceso de tokenizacio'n nota: en caso de que no se realice la visita a microsoft, la divideeltextoenunidadesm'ınimasllamadastokens(palabras, clase\"riesgosdeia\"setrasladar'ıaaljuevesdelasemana15, subpalabras o caracteres), asignando a cada una un identifiy el examen se aplicar'ıa el jueves 20 de noviembre (semana cador u'nico. estos identificadores se transforman en vectores 16). que los modelos utilizan como entrada. tableii tiposdetokenizacio'n tipo ejemplo ventajaprincipal porpalabra \"losmodelos\" simplicidad porcara'cter \"h\",\"o\",\"l\",\"a\" sin palabras fuera del vocabulario(oov) subpalabra(bpe,wordpiece) \"compu\",\"tadora\" equilibrioentrevocabularioycontexto byte-level co'digoasciioutf-8 soporta cualquier fig. 2. proceso de generacio'n de embeddings desde palabra, oracio'n o s'ımbolooidioma documentohastaelespaciovectorial. espaciosenblanco \"hola\",\"mundo\" ra'pidoysimple c. fo'rmulas de similitud entre vectores paracompararlasimilitudodistanciaentrevectores,seutilizan diversas fo'rmulas matema'ticas, entre las ma's comunes: - distancia euclidiana: mide la separacio'n entre puntos en el espacio. para dos vectores a,b∈rn: (cid:118) (cid:117) n (cid:117)(cid:88) d(a,b)=(cid:116) (a i -b i )2 i=1 - similitud del coseno: mide el a'ngulo entre vectores y su orientacio'nenelespacio,siendolama'susadaenmodelos de lenguaje: a-b sim(a,b)= fig.1. representacio'ndetokensenunespaciobidimensionalyejemplode ∥a∥∥b∥ operacionessema'nticas. d. embeddings losembeddingssonrepresentacionesnume'ricasdensasque iv. representacio'ndelconocimiento capturanelsignificadosema'nticoylasrelacionescontextuales a. tokenizacio'n depalabras,oracionesodocumentoscompletos.estosvectores permiten comparar ideas, medir similitud y realizar operaenlosllmsseutilizandistintosenfoquesdetokenizacio'n, ciones sema'nticas en un espacio continuo de alta dimensio'n. cada uno con caracter'ısticas particulares que afectan el el proceso de generacio'n de embeddings se puede resumir rendimiento del modelo: la tabla ii resume estos enfoques, en los siguientes pasos: sus ejemplos y ventajas principales. - entrada textual: la unidad de texto que se quiere repb. representacio'n en espacios vectoriales resentar, que puede ser una palabra, una oracio'n o un documento completo. una vez tokenizado el texto, los identificadores se trans- - modelo de embeddings: un modelo que transforma la forman en vectores dentro de un espacio continuo de alta entrada en un vector nume'rico denso, capturando su dimensio'n."}
{"id_doc": "DOC_041", "segmentacion": "A", "chunk_id": "DOC_041_A_002", "idx": 2, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "modelo: la tabla ii resume estos enfoques, en los siguientes pasos: sus ejemplos y ventajas principales. - entrada textual: la unidad de texto que se quiere repb. representacio'n en espacios vectoriales resentar, que puede ser una palabra, una oracio'n o un documento completo. una vez tokenizado el texto, los identificadores se trans- - modelo de embeddings: un modelo que transforma la forman en vectores dentro de un espacio continuo de alta entrada en un vector nume'rico denso, capturando su dimensio'n. las palabras con significados similares se ubican significado y contexto. pro'ximas entre s'ı, mientras que las palabras con significados - embedding resultante: el vector que representa la endistintos aparecen ma's alejadas. trada en el espacio continuo. vectores cercanos indican esto permite medir similitud sema'ntica y realizar operaconceptos sema'nticamente similares. ciones vectoriales, como analog'ıas entre conceptos, suma o - espacio de embeddings: el espacio vectorial donde cada resta de vectores. embedding ocupa una posicio'n. este espacio permite talcomosemuestraenlafigura1,losvectoresrepresentan medir similitudes y realizar bu'squedas por proximidad. tokens proyectados en un espacio bidimensional para facilitar como se ilustra en la figura 2, la figura representa el flujo la comprensio'n, ilustrando relaciones sema'nticas entre ellos. de generacio'n de embeddings: desde palabras, oraciones o por ejemplo, la conocida analog'ıa: documentosdeentrada,pasandoporelmodelodeembeddings, rey-hombre+mujer≈reina. hasta el vector resultante y su posicio'n en el espacio de embeddings. en la pra'ctica, estos vectores existen en un espacio de alta adema's, la figura 3 muestra un ejemplo simplificado de dimensio'n(ndimensiones),loquepermitecapturardemanera sentence embeddings proyectados en un plano bidimensional, ma's precisa la informacio'n sema'ntica y contextual de las donde frases con significado similar aparecen pro'ximas entre palabras. s'ı. que inyecta conocimiento externo relevante en tiempo real. esto permite generar respuestas ma's precisas y coherentes, accediendo a informacio'n actualizada y fundamentada. a. preparacio'n de los documentos los documentos que se desean utilizar para la recuperacio'n se dividen en fragmentos llamados chunks, normalmente de entre 200 y 500 tokens. para evitar pe'rdida de informacio'n, los chunks suelen tener un overlap entre s'ı. 1) chunkingdetaman˜ofijo: sesegmentanlosdocumentos en trozos de longitud fija, respetando en la medida de lo posible los l'ımites de las frases, y se mantiene un overlap para preservar contexto. 2) chunking recursivo: en este enfoque, los chunks no fig.3. ejemplodesentenceembeddingsenunespaciobidimensional se cortan estrictamente segu'n el taman˜o ma'ximo, sino que se ajustan para mantener la sema'ntica de las oraciones. se comparan oraciones con la similitud del coseno y, si son v. capacidadesylimitaciones suficientementesimilaressegu'nunumbral,secombinanenun a. capacidades"}
{"id_doc": "DOC_041", "segmentacion": "A", "chunk_id": "DOC_041_A_003", "idx": 3, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "chunks suelen tener un overlap entre s'ı. 1) chunkingdetaman˜ofijo: sesegmentanlosdocumentos en trozos de longitud fija, respetando en la medida de lo posible los l'ımites de las frases, y se mantiene un overlap para preservar contexto. 2) chunking recursivo: en este enfoque, los chunks no fig.3. ejemplodesentenceembeddingsenunespaciobidimensional se cortan estrictamente segu'n el taman˜o ma'ximo, sino que se ajustan para mantener la sema'ntica de las oraciones. se comparan oraciones con la similitud del coseno y, si son v. capacidadesylimitaciones suficientementesimilaressegu'nunumbral,secombinanenun a. capacidades emergentes chunkma'sgrande,lograndounalmacenamientoma'seficiente graciasasuentrenamientomasivoyalusodearquitecturas y contextual. basadas en transformers, los llms presentan capacidades b. transformacio'n en embeddings como: - compresio'ntextual:interpretanelsignificadodepalabras cada chunk se convierte en un vector mediante un modelo y frases segu'n el entorno en el que aparecen. deembeddings.estosvectoresseutilizanparamedirsimilitud - generacio'n coherente de texto: pueden redactar, traducir sema'ntica y permitir la recuperacio'n eficiente de fragmentos oresumirinformacio'nmanteniendoestiloyconsistencia. relevantes durante la consulta del usuario. - razonamiento y planificacio'n: resuelven problemas, ex- c. indexacio'n plican pasos y trazan estrategias simples. los vectores resultantes se almacenan en bases vectoriales - aprendizaje en el prompt: adaptan su comportamiento a especializadas, que pueden residir en memoria ram o disco: partir de ejemplos dados en la misma conversacio'n (incontext learning). - faiss: principalmente en ram, ra'pido para bu'squedas. - multitarea: realizan traduccio'n, clasificacio'n, codifi- - qdrant: almacenamiento en disco con soporte de cacio'n, ana'lisis o dia'logo sin requerir entrenamiento bu'squeda vectorial. adicional. - pinecone: almacenamiento en disco y nube, escalable. se almacena adema's la metadata asociada, como el texto b. limitaciones original del chunk, para permitir una recuperacio'n eficiente. a pesar de sus capacidades, los llms presentan limitad. consulta o recuperacio'n ciones importantes: - alucinaciones: pueden generar respuestas incorrectas o cuando llega una pregunta del usuario, el proceso consiste inventadas, especialmente cuando la informacio'n de en- en: trada es ambigua o insuficiente. 1) transformar la consulta en un embedding. - memorialimitada:olvidaninformacio'nqueseencuentra 2) calcular la similitud con todos los embeddings indexafuera del contexto actual, no recordando interacciones dos. previas a menos que se almacenen externamente. 3) seleccionar los top-k chunks ma's cercanos - conocimiento esta'tico: no tienen acceso a informacio'n sema'nticamente. posteriorasufechadecortedeentrenamiento,porloque e. augmentacio'n y generacio'n no esta'n actualizados en tiempo real. - altos costos computacionales: requieren hardware espe- paraenriquecerelpromptdelllm,loschunksrecuperados cializado y significativos recursos para entrenamiento e se organizan en una plantilla estructurada, que combina el inferencia eficiente, lo que puede limitar su uso pra'ctico. contextextra'ıdodelosdocumentosconlaquestiondelusuario. estaplantillaaseguraqueelmodelorecibatodalainformacio'n vi. retrieval-augmentedgeneration(rag) relevantedemaneracoherente,permitie'ndolegenerarrespuesdadas las limitaciones de los llms tradicionales, los en-"}
{"id_doc": "DOC_041", "segmentacion": "A", "chunk_id": "DOC_041_A_004", "idx": 4, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "previas a menos que se almacenen externamente. 3) seleccionar los top-k chunks ma's cercanos - conocimiento esta'tico: no tienen acceso a informacio'n sema'nticamente. posteriorasufechadecortedeentrenamiento,porloque e. augmentacio'n y generacio'n no esta'n actualizados en tiempo real. - altos costos computacionales: requieren hardware espe- paraenriquecerelpromptdelllm,loschunksrecuperados cializado y significativos recursos para entrenamiento e se organizan en una plantilla estructurada, que combina el inferencia eficiente, lo que puede limitar su uso pra'ctico. contextextra'ıdodelosdocumentosconlaquestiondelusuario. estaplantillaaseguraqueelmodelorecibatodalainformacio'n vi. retrieval-augmentedgeneration(rag) relevantedemaneracoherente,permitie'ndolegenerarrespuesdadas las limitaciones de los llms tradicionales, los en- tas precisas y fundamentadas. foques de retrieval-augmented generation (rag) entran en a modo de ejemplo, la figura 4 muestra la estructura escena. el enfoque rag potencia los llms conecta'ndolos de la plantilla, donde se pueden observar sus componentes con un mo'dulo de recuperacio'n de informacio'n (retriever) principales: prompt, context y question. planificar y actuar de manera auto'noma, ejecutando tareas en el mundo real, como consultar apis, buscar informacio'n o tomar decisiones basadas en conocimiento externo. esta capacidad se estructura en tres componentes principales: a. memoria la memoria permite al agente mantener coherencia y contexto a lo largo de la interaccio'n: - corto plazo: ventana de contexto del modelo. - largoplazo:basesdedatosexternas,incluyendosistemas rag donde la informacio'n se divide en chunks y se fig.4. estructuradeundocumentorag representan como embeddings para su recuperacio'n. b. planificacio'n la planificacio'n dota al agente de la habilidad de descomponer problemas complejos y razonar sobre mu'ltiples pasos: - chains of thought (cot): razonamiento secuencial paso a paso. - treesofthought(tot):exploracio'ndemu'ltiplesposibles caminos de razonamiento antes de tomar decisiones. c. accio'n fig.5. diagramadelflujoderagmostrandolapreparacio'ndedocumentos, generacio'ndeembeddings,indexacio'n,recuperacio'nygeneracio'nderespues- finalmente, la accio'n permite al agente ejecutar tareas tas. concretas y aplicar su razonamiento en el mundo real: - utilizacio'n de herramientas externas, como buscadores, apis o sistemas rag. f. beneficios de rag - enriquecimiento de respuestas con informacio'n recuper- - reduccio'n de alucinaciones. ada en tiempo real, basada en chunks de documentos - actualizacio'n continua con informacio'n reciente. relevantes. - eficiencia en costos y tiempo de respuesta. - aplicabilidad en dominios especializados con infor- viii. conclusio'n macio'n privada. los modelos de lenguaje extensos (llms) han revolucionado el procesamiento del lenguaje natural, permitiendo g. aplicaciones de rag tareas complejas como la generacio'n de texto coherente, el - asistentes empresariales enriquecidos: pueden consultar razonamiento contextual y la ejecucio'n multitarea sin necesidocumentacio'n interna y responder de forma precisa. dad de reentrenamiento. - investigacio'n: lectura automa'tica de papers, resu'menes y el uso de embeddings y espacios vectoriales permite que citas. los llms"}
{"id_doc": "DOC_041", "segmentacion": "A", "chunk_id": "DOC_041_A_005", "idx": 5, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "aplicabilidad en dominios especializados con infor- viii. conclusio'n macio'n privada. los modelos de lenguaje extensos (llms) han revolucionado el procesamiento del lenguaje natural, permitiendo g. aplicaciones de rag tareas complejas como la generacio'n de texto coherente, el - asistentes empresariales enriquecidos: pueden consultar razonamiento contextual y la ejecucio'n multitarea sin necesidocumentacio'n interna y responder de forma precisa. dad de reentrenamiento. - investigacio'n: lectura automa'tica de papers, resu'menes y el uso de embeddings y espacios vectoriales permite que citas. los llms comprendan relaciones sema'nticas profundas. adi- - soportealcliente:ana'lisisdeticketspreviosparagenerar cionalmente, te'cnicas como retrieval-augmented generation respuestas coherentes y ra'pidas. (rag) mejoran su precisio'n y acceso a informacio'n actualla figura 5 ilustra el flujo general de un sistema rag, izada, mientras que los agentes inteligentes basados en llms donde se integran la creacio'n de embeddings, la indexacio'n y les permiten actuar de manera auto'noma, planificar y utilizar labu'squedavectorialparaenriquecerlasrespuestasgeneradas herramientas externas, superando la pasividad de los modelos por el llm. tradicionales. si bien los rag mejoran significativamente el rendimiento a pesar de estas mejoras, los llms y sus extensiones ende un llm tradicional al proporcionarle informacio'n externa frentan limitaciones importantes, como memoria finita, costos yactualizada,estossistemassiguensiendopasivos:nopueden computacionales elevados y riesgo de alucinaciones. por ello, buscaractivamenteinformacio'nenlawebnitomardecisiones su implementacio'n requiere un disen˜o cuidadoso y un uso auto'nomas. su funcio'n se limita a complementar la respuesta responsable, asegurando que sus capacidades se aprovechen del llm con los datos recuperados. de manera eficiente y confiable. vii. dellmsaagentesinteligentes losagentesbasadosenllmsrepresentanunpasoma'salla' delosrags.mientrasquelosragssoloenriquecenrespuestas con informacio'n recuperada, los agentes pueden razonar,"}
{"id_doc": "DOC_045", "segmentacion": "A", "chunk_id": "DOC_045_A_000", "idx": 0, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-10-23", "tema": "Técnicas de cuantización en redes neuronales: reducción de parámetros, uso de enteros, métodos simétricos y asimétricos, y estrategias como QAT y post-training quantization para optimización de modelos.", "texto": "apuntes semana 12 apuntesdel23deoctubre juan pablo rodr'ıguez cano ic-6200 inteligencia artificial tecnolo'gico de costa rica jp99@estudiantec.cr abstract-la cuantizacio'n en una te'cnica en redes neuronales y hasta 1 bit. la cuantizacio'n resulta en un menor tiempo de para reducir el taman˜o de los para'metros de los modelos, inferencia y menor consumo de energ'ıa, adema's de facilitar principalmente transformando los datos de punto flotante a laopcio'ndecorrerestosmodelosensistemaspequen˜oscomo enteros, lo cual adema's reduce el tiempo de computacio'n de dispositivos mo'viles o sistemas embebidos. operaciones. esta te'cnica es esencial para distribuir modelos en sistemas comerciales y ampliar la cantidad de plataformas que a. representacio'n de nu'meros puedan correr estos modelos. index terms-cuantizacio'n, punto flotante, reduccio'n de se suelen utilizar nu'meros en bloques de 8 bits para los para'mtetros. enteros, para representar nu'meros negativos se utiliza el complementoa2enloscomputadores.encontraste,paralospunto i. actividaddeieee flotantesseutilizaelieee-754,cuyotaman˜oderepresentacio'n es un evento anual que se dara' esta vez en noviembre es de 32 bits, se utiliza la siguiente fo'rmula. en la sabana. es una oportunidad para conocer sobre temas innovadores en inteligencia artificial y biolog'ıa molecular. es 23 (cid:88) una oportunidad para crear contactos dentro de la industria v =(-1)sign×2e-127×(1+ b 23-i 2-i) ya que los presentadores suelen ser receptivos al pu'blico y i=1 disponen de tiempo para hablar. para no perder tanta informacio'n se tiene el siguiente mecanismo: ii. quantization 1) antes de que las entradas lleguen a la siguiente capa se una vez que entrenado un modelo de redes neuronales, se cuantizan los pesos debe colocar en un sistema para la distribucio'n de este. para 2) estos pesos se limitan a ciertos rangos, dependiendo de esto existen varias te'cnicas, entre ellas, una opcio'n comu'n es la cantidad de bits de la cuantizacio'n. lo que se quiere utilizar el framework onnx, que toma modelos escritos en es que la distribucio'n sea equivalente. diferentes lenguajes y bibliotecas y se crea una versio'n que 3) se hacen las operaciones con los datos de tipo entero. maximiza la eficiencia de recursos y computacio'n utilizando 4) alsalirdelacapa,sede-cuantizanlospesosparaquelas c++. siguientes capas operen con nu'meros de punto flotante, elmecanismoporelcualsedisminuyeeslacuantizacio'ny sin \"saber\" que fueron cuantizados. se enfoca en el hecho que los para'metros de los modelos son representados con tipos de datos de punto flotante, se reducen iii. tiposdecuantizacio'n para hacer los modelos ma's densos con te'cnicas especiales 1) asime'trica → el valor de 0 corresponde al valor menor para no afectar mucho la precisio'n de la inferencia. aunque y"}
{"id_doc": "DOC_045", "segmentacion": "A", "chunk_id": "DOC_045_A_001", "idx": 1, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-10-23", "tema": "Técnicas de cuantización en redes neuronales: reducción de parámetros, uso de enteros, métodos simétricos y asimétricos, y estrategias como QAT y post-training quantization para optimización de modelos.", "texto": "maximiza la eficiencia de recursos y computacio'n utilizando 4) alsalirdelacapa,sede-cuantizanlospesosparaquelas c++. siguientes capas operen con nu'meros de punto flotante, elmecanismoporelcualsedisminuyeeslacuantizacio'ny sin \"saber\" que fueron cuantizados. se enfoca en el hecho que los para'metros de los modelos son representados con tipos de datos de punto flotante, se reducen iii. tiposdecuantizacio'n para hacer los modelos ma's densos con te'cnicas especiales 1) asime'trica → el valor de 0 corresponde al valor menor para no afectar mucho la precisio'n de la inferencia. aunque y el ma'ximo es el peso ma'ximo no es posible no introducir error, es necesario asumir esta 2) sime'trica → el cero es el peso 0, el valor absoluto desventaja para desplegar los modelos. ma'ximo de los pesos se mapea a un extremo, si es llama 2 es un modelo muy popular y notorio por tener negativo se mapea al valor ma's negativo dentro de los un taman˜o muy grande, tiene 70 mil millones de para'metros, valores posibles con los bits cada uno esta' representado por un punto flotante de 32 bits, lo que resulta en 28gb que deber'ıan estar en memoria si a. cuantizacio'n asime'trica se quisiera utilizar en una ma'quina local. esto claramente no es viable porque la mayor'ıa de ma'quinas comerciales x q =clamp(x s f +z;0;2n-1) cuentan con una capacidad menor a eso. adema's, las opera- x f =valorflotante ciones que se se hacen con datos de punto flotante son muy z =-1× β s lentas en comparacio'n a datos representados por enteros. la *s es el para'metro de escalado cuantizacio'n hace una reduccio'n de los bits requeridos para s= α-β 2b-1 representar cada para'metro y lo convierte a enteros, que se x =s(x -z) → permite volver al valor original con un f q pueden representar en las siguientes configuraciones: 8, 5, 2 grado de error b. cuantizacio'n sime'trica rango: [-(2n-1),(2n-1)] s= abs(α) 2n-1-1 x =sx f q iv. estrategiasdeseleccio'ndelrango - cuantizacio'n dina'mica - ca'lculo estad'ıstico de cua'l sera' el valor de esa capa - se utiliza en la etapa post-training quantization - post training quantization - hay que tratar los pesos at'ıpicos porque puede confinar los dema's pesos en un rango muy pequen˜o e introduce ma's error - se puede utilizar el percentil en vez del min y max - agregamos observers que se encargan de hacer la estad'ısticas, calibran todas las salidas de la capa - se hace con los datos de prueba -"}
{"id_doc": "DOC_045", "segmentacion": "A", "chunk_id": "DOC_045_A_002", "idx": 2, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-10-23", "tema": "Técnicas de cuantización en redes neuronales: reducción de parámetros, uso de enteros, métodos simétricos y asimétricos, y estrategias como QAT y post-training quantization para optimización de modelos.", "texto": "de cua'l sera' el valor de esa capa - se utiliza en la etapa post-training quantization - post training quantization - hay que tratar los pesos at'ıpicos porque puede confinar los dema's pesos en un rango muy pequen˜o e introduce ma's error - se puede utilizar el percentil en vez del min y max - agregamos observers que se encargan de hacer la estad'ısticas, calibran todas las salidas de la capa - se hace con los datos de prueba - quantization aware training (qat) - insertarmo'dulosirrealesenlacomputacio'ndegrafo del modelo para similar el efector de cuantizacio'n durante el entrenamiento. - la funcio'n de perdida es usada para actualizar los pesos que constantemente sufren. -"}
{"id_doc": "DOC_001", "segmentacion": "A", "chunk_id": "DOC_001_A_000", "idx": 0, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "inteligencia artificial apuntesdeclases rodolfo david acun˜a lo'pez escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica rodolfoide69@estudiantec.cr abstract-en este documento podra' encontrar informacio'n a. yann lecun sobre la primera clase de ia donde se presentan distintas yann lecun es un investigador muy reconocido en el perspectivas sobre esta. desde la perspectiva de la ia podremos campo de la inteligencia artificial y es considerado uno de encontrar conceptos como autonom'ıa y adaptabilidad. tambie'n introducenlosprincipiosba'sicosdelaprendizajesupervisadoyno los tres padrinos del deep learning. es el fundador de las supervisado, destacando su aplicacio'n para resolver problemas convolutionalneuralnetworks,untipoderedneuronalcapaz complejos. de procesar ima'genes de manera eficiente. index terms-principios de la ia, conceptos de ia anteriormente, los costos computacionales para procesar i. introduccio'n ima'genes eran muy altos lo que limitaba su uso. por esta razo'n, e'l disen˜o' una arquitectura que permit'ıa realizar este lainteligenciaartificialpuedeserunconceptomuyvariado procesamiento con un costo computacional mucho menor, un desde diferentes perspectivas. en el a'rea de la computacio'n modelo ma's liviano y efectivo para tareas visuales. pueden ser sistemas que puedan tener y mostrar comporactualmenteyannlecunesdirectordeproyectosenmeta, tamientos de forma \"inteligente\". en la primer clase pudimos donde dirige todas lo relacionado con inteligencia artificial. ver conceptos que nos permiten comprender las bases de esta, su impacto en el a'rea fue reconocido con el premio turing, incluyendo distintas perspectivas sobre que' es la ia y co'mo otorgado por sus contribuciones en el desarrollo y avance de se puede definir desde puntos de vista. las redes neuronales. ii. noticias b. ¿que' es la inteligencia? a. sobre la ia noexisteunconsensoclaroquedefinaque'eslainteligencia ahora la ia ha llegado para facilitar el trabajo de los artificial. este concepto ha sido estudiado desde diversas programadores, por ejemplo, cursor es una herramienta que disciplinas como la psicolog'ıa y la filosof'ıa, pero podemos ayuda mucho en el area de la programacion. decir de forma abstracta que la inteligencia puede verse de b. benchmark varias maneras, como un grupo de animales que trabajan swe-lancer es un benchmark el cual fue desarrollado para realizar una tarea espec'ıfica, o como la manera en que por openai y servia como herramienta para evaluar los los humanos piensan y se comportan. adema's estos sistemas modelosdelenguajeentareasfreelanceextra'ıdasdediferentes pueden ser auto'nomos y adaptativos capaces de aprender y plataformas. este inclu'ıa ma's de 1400 tareas reales. ajustarse a nuevas situaciones. iii. introduccio'nalainteligenciaartificial c. auto'nomo y adaptativo ¿que' piensas cuando escuchas hablar de ia? ¿co'mo crees auto'nomo significa que no necesitamos realizar instrucque"}
{"id_doc": "DOC_001", "segmentacion": "A", "chunk_id": "DOC_001_A_001", "idx": 1, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "es un benchmark el cual fue desarrollado para realizar una tarea espec'ıfica, o como la manera en que por openai y servia como herramienta para evaluar los los humanos piensan y se comportan. adema's estos sistemas modelosdelenguajeentareasfreelanceextra'ıdasdediferentes pueden ser auto'nomos y adaptativos capaces de aprender y plataformas. este inclu'ıa ma's de 1400 tareas reales. ajustarse a nuevas situaciones. iii. introduccio'nalainteligenciaartificial c. auto'nomo y adaptativo ¿que' piensas cuando escuchas hablar de ia? ¿co'mo crees auto'nomo significa que no necesitamos realizar instrucque funciona? al inicio la inteligencia artificial pod'ıa basarse ciones para funcionar ya que es capaz de tomar decisiones en una lo'gica el cual estaba llena de instrucciones condi- y ejecutar acciones por s'ı mismo. cionales, donde se procesaban mu'ltiples datos para llegar a adaptativo implica que un sistema puede modificar su una conclusio'n. con el tiempo se llego a utilizar el modelo comportamiento si el entorno o el espacio del problema de machine learning, que ba'sicamente a partir de datos, cambian.porejemplo,enclasesemostro' unaimagendeunas comienza a identificar y resolver patrones. cuando hablamos hormigas que se apilaban unas sobre otras para alcanzar una de ia hay diferentes perspectivas de lo que es. la sociedad hoja. este comportamiento demuestra co'mo pueden adaptarse podr'ıa pensar que puede ser como un terminator que quiere a las condiciones de su ambiente para cumplir un objetivo. dominar al mundo, los amigos de otras carreras no relad. ¿que' puede ser la ia? cionasdasconcomputacionpuedenpensarqueeresunamente brillantehaciendoalgunatareacontecnolog'ıaosilepreguntas un sistema que muestra un comportamiento \"inteligente\". a otro de computacion puede que te vea como un vago gando porejemplo,laautomatizacio'ndetareasoalgoquenosayuda mucho dinero, y as'ı sucesivamente con diferentes analog'ıas. a resolver problemas complejos. tambie'n puede ser como ver unrobotquemueveunacajadeunlugaraaunlugarb.puede 1970s estar relacionado con simulacio'n, como la visio'n, escucha o - se popularizan los algoritmos evolutivos. sentidosnaturales.sonsolucionesquesoncapacesdeaprender - el robot freddy puede usar la percepcio'n visual. auto'nomamentemientrasseadaptananuevosdatos.entonces, - se inventa el lenguaje de programacio'n prolog para el en otras palabras ma's resumidas, puede ser: uso de reglas. - sistema mostrando comportamiento \"inteligente\" 1980s - algo \"inteligente\" que resuelva tareas complejas - esta'n las ma'quinas lisp para sistemas expertos. - simulacio'n de visio'n, escucha o sentidos naturales - redes neuronales a trave's de la introduccio'n de la retro- - soluciones adaptativas que aprenden de forma auto'noma propagacio'n. un ejemplo de lo que puede ser ia es observar carros de - se populariza la inteligencia de enjambre. uber que utilizan varias capacidades, como computer vision, 1990s la cual sirve"}
{"id_doc": "DOC_001", "segmentacion": "A", "chunk_id": "DOC_001_A_002", "idx": 2, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "- sistema mostrando comportamiento \"inteligente\" 1980s - algo \"inteligente\" que resuelva tareas complejas - esta'n las ma'quinas lisp para sistemas expertos. - simulacio'n de visio'n, escucha o sentidos naturales - redes neuronales a trave's de la introduccio'n de la retro- - soluciones adaptativas que aprenden de forma auto'noma propagacio'n. un ejemplo de lo que puede ser ia es observar carros de - se populariza la inteligencia de enjambre. uber que utilizan varias capacidades, como computer vision, 1990s la cual sirve para reconocer diversos para'metros, tales como veh'ıculos,personas,sen˜alesdetra'nsito,distancias,entreotros. - td-gammon muestra el poder del aprendizaje por refuerzo que es como optimizaje a prueba y error. otro ejemplo es la capacidad estad'ıstica que la ia puede emplear para extraer las caracter'ısticas de una imagen. - experimentos con coches auto'nomos. - auge de los bots de internet y la bu'squeda. e. ¿que' puede ser el corazo'n de la ia? 2000s comobiensabemoslaiapuedeseralimentadademultiples - reconocimiento facial con redes neuronales. maneras. sin embargo, llamamos como corazo'n de la ia a: - watson de ibm gana en jeopardy. - funcio'n matema'ticas - deteccio'n de movimiento avanzada de xbox kinect. - datos - alphago de google se convierte en campeo'n de go. apesardequeesta'nestasdos,puedenserma'simportantelos - agentes para realizar tareas especificas. datosyaquesinestos,nohayia.ademaslosdatospresentes v. machinelearning deben ser buenos datos ya que si son erro'neos, las funciones arthur samuel fue la persona en acun˜ar este termino en matema'ticas no van a hacer nada. 1959 el cual lo hace un juego llamado checkers. este hombre f. datos trabajo' para ibm y tenia que encontrar una manera de vender como mencionamos anteriormente, podemos decir que un el producto algoritmo va a ser malo si tenemos datos que son incorrectos entonces el machine learning cuenta con dos aristas, las o hay datos faltantes. un ejemplo puede ser el prono'stico cuales son ciencia y ingenier'ıa. la idea es que se construyen del tiempo en cartago erroneos ya que hacen faltan datos maquinas que puedan hacer tareas solo infiriendo entre los estad'ısticos, por lo que genera una incorrecta prediccio'n. hay datos existentes, aproximamos una funcio'n. este sigue un envarios tipos de datos pero principlamente tenemos: foque estad'ıstico para entrenar modelos. tengamos en cuenta que las aproximaciones son u'tiles pero no perfectas. - los datos cuantitativos o sin sesgos que puede tomar una cantidad exacta para que represente un para'metro vi. tiposdeaprendizajes medible. hay varios tipos de aprendizajes, sin embargo solo men- - losdatoscualitativoscomoelolordeunafloroelsabor cionare' las dos vistas"}
{"id_doc": "DOC_001", "segmentacion": "A", "chunk_id": "DOC_001_A_003", "idx": 3, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "solo infiriendo entre los estad'ısticos, por lo que genera una incorrecta prediccio'n. hay datos existentes, aproximamos una funcio'n. este sigue un envarios tipos de datos pero principlamente tenemos: foque estad'ıstico para entrenar modelos. tengamos en cuenta que las aproximaciones son u'tiles pero no perfectas. - los datos cuantitativos o sin sesgos que puede tomar una cantidad exacta para que represente un para'metro vi. tiposdeaprendizajes medible. hay varios tipos de aprendizajes, sin embargo solo men- - losdatoscualitativoscomoelolordeunafloroelsabor cionare' las dos vistas en clases. de una comida. estos son datos que no son sencillos de medir y que puede variar entre los humanos. a. supervisado iv. l'ineadeltiempo enestevoyatenerunconjuntosdedatosconcaracter'ısticas. por ejemplo, una casa con las variables x1, x2 y x3, que van a continuacio'n se muestran algunos eventos a travez de los a ser el numero de cuartos, metros cuadrado y cantidad de an˜os los cuales fueron mostrados en una imagen en clases. robos. con esto podemos tener una tabla que me va a dar la 1950s muestra y me etiqueta. - se presenta el nombre de ia - se introduce el concepto de la red neuronal artificial. - seinventaelmodelodelperceptro'nysecreequevaaser el mejor. sin embargo, este no poda resolver problemas sencillos. resolv'ıa solo problemas lineales. 1960s - se introducen modelos de machine learning. - el robot unimate trabaja en una l'ınea de montaje de automo'viles. - el robot shakey tiene movimiento natural y habilidades para resolver problemas. fig.1. ejemplodeaprendizajesupervisado. la muestra o conjunto de datos tiene caracter'ısticas y mi etiqueta que me asocia el conjunto de caracter'ısticas a un valor.entonces,mimodeloesdetiposupervisadoporquehago una aproximacio'n. podemos verlo como f(x) aproximado a y. si el y esta' muy lejos, penalizo mi modelo para que en ese puntosemejoreelentrenamiento.estosmodelosquetratande aproximar un valor, se llaman modelos de regresio'n. tambie'n hay modelos de clasificacio'n. b. no supervisado en estas a diferencia de las supervisadas, no tenemos etiquetas. en otras palabras, no tengo quien me supervise el aprendizaje. como por ejemplo, los algoritmos de clustering. este algoritmo lo que hace es que agrupa datos segu'n las caracter'ısticas que comparte, que generalmente usan algoritmos de distancia. references [1] apuntes de la clase de inteligencia artificial, profesor steven andrey pachecoportuguez,institutotecnolo'gicodecostarica,2025."}
{"id_doc": "DOC_001", "segmentacion": "A", "chunk_id": "DOC_001_A_004", "idx": 4, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "de clustering. este algoritmo lo que hace es que agrupa datos segu'n las caracter'ısticas que comparte, que generalmente usan algoritmos de distancia. references [1] apuntes de la clase de inteligencia artificial, profesor steven andrey pachecoportuguez,institutotecnolo'gicodecostarica,2025."}
{"id_doc": "DOC_002", "segmentacion": "A", "chunk_id": "DOC_002_A_000", "idx": 0, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "inteligencia artificial apuntesdelaclase07/08/2025 fernando daniel brenes reyes escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica 2020097446@estudiantec.cr abstract-estos apuntes organizan y ampl'ıan el material b. impacto en el empleo y en programadores introductoriodelaclasesobreinteligenciaartificial.seincluyen laautomatizacio'nconiaesta' permitiendoaorganizaciones noticias recientes , fundamentos te'cnicos , cuestiones pra'cticas, unal'ıneadetiempohisto'rica,yunadescripcio'ndelasprincipales reducir tiempo en tareas repetitivas (redaccio'n de correos, ramasdelaprendizajeautoma'tico.eldocumentoesta' preparado generacio'n de reportes, tests ba'sicos de software). para proen formato ieee, con lugares marcados para figuras y sugeren- gramadores esto significa: cias bibliogra'ficas. indexterms-inteligenciaartificial,gpt-5,autosauto'nomos, - aumento de productividad: asistentes que generan esdatos, aprendizaje supervisado, aprendizaje no supervisado. queleto de co'digo y pruebas unitarias. - cambio en habilidades requeridas: mayor e'nfasis en disen˜o, validacio'n, e'tica, pruebas adversariales y i. introduccio'n orquestacio'n. la inteligencia artificial (ia) es un campo multidisci- - riesgos: tareas de bajo nivel y rutinas repetitivas pueden verse desplazadas; se recomienda desarrollar habilidades plinario que combina informa'tica, estad'ıstica, matema'tica de alto valor (arquitectura, ingenier'ıa de datos, devops, y aspectos del dominio de aplicacio'n para crear sistemas ml ops). que pueden percibir, razonar, aprender y actuar. en esta compilacio'n ampliada se cubren conceptos teo'ricos, avances c. autos auto'nomos en california recientes y aplicaciones pra'cticas relevantes para un curso introductorio. california es uno de los centros donde empresas realizan pruebas y despliegues de veh'ıculos auto'nomos; estos sistemas ii. noticiasycontextoreciente integran lidar, ca'maras, mapas hd y planificacio'n en tiempo real. las a'reas de intere's incluyen: a. gpt-5 y modelos de lenguaje avanzados - sensores y fusio'n: lidar + ca'maras + radar + gps. a mediados de 2025 emergieron nuevas generaciones de - percepcio'n: deteccio'n y clasificacio'n de peatones, grandes modelos de lenguaje con capacidades multimodales, veh'ıculos y sen˜ales. mejor manejo del contexto y mejoras en razonamiento. estos - planificacio'n: toma de decisiones en entornos urbanos modelos impactan fuertemente en herramientas de productivi- complejos. dad(resu'menes,generacio'ndeco'digo,asistenciadeescritura) - protocolos de emergencia: procedimientos para fallas y han generado debate acerca de su adopcio'n responsable y del sistema, intervencio'n humana y registro de eventos. efectos laborales. fig. 1. l'ınea simplificada deevolucio'n de modelos delenguaje: gpt-1 → gpt-2→gpt-3→gpt-4 fig.2. veh'ıculoauto'nomo:sensores,percepcio'n,planificacio'nycontrol. iii. definicionesyconceptosba'sicos a. ¿que' es inteligencia? noexisteunadefinicio'nu'nicaaceptada.eniaoperativase sueleentendercomolacapacidaddeunsistemaparapercibir su entorno, razonar, tomar decisiones auto'nomas y adaptarse a cambios. distintos campos (psicolog'ıa, filosof'ıa, ciencias de la computacio'n) aportan matices: aprendizaje, simbolismo, fig.3. esquemadeunaredconvolucional razonamiento probabil'ıstico, entre otros. b. auto'nomo vs. adaptativo v. datos:elcorazo'ndelaia a. calidad y preprocesamiento auto'nomo: actu'a sin intervencio'n humana constante. adaptativo:modificasucomportamientobasa'ndoseennueva los datos deben ser: informacio'n o retroalimentacio'n."}
{"id_doc": "DOC_002", "segmentacion": "A", "chunk_id": "DOC_002_A_001", "idx": 1, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "humana y registro de eventos. efectos laborales. fig. 1. l'ınea simplificada deevolucio'n de modelos delenguaje: gpt-1 → gpt-2→gpt-3→gpt-4 fig.2. veh'ıculoauto'nomo:sensores,percepcio'n,planificacio'nycontrol. iii. definicionesyconceptosba'sicos a. ¿que' es inteligencia? noexisteunadefinicio'nu'nicaaceptada.eniaoperativase sueleentendercomolacapacidaddeunsistemaparapercibir su entorno, razonar, tomar decisiones auto'nomas y adaptarse a cambios. distintos campos (psicolog'ıa, filosof'ıa, ciencias de la computacio'n) aportan matices: aprendizaje, simbolismo, fig.3. esquemadeunaredconvolucional razonamiento probabil'ıstico, entre otros. b. auto'nomo vs. adaptativo v. datos:elcorazo'ndelaia a. calidad y preprocesamiento auto'nomo: actu'a sin intervencio'n humana constante. adaptativo:modificasucomportamientobasa'ndoseennueva los datos deben ser: informacio'n o retroalimentacio'n. - representativos del problema real. un sistema puede ser auto'nomo pero no adaptativo (p. ej. un - limpios: sin errores obvios (p. ej. mezcla celrobot con una ruta fija) o adaptativo pero no completamente sius/fahrenheit). auto'nomo (p. ej. un asistente que sugiere cambios que un - balanceados o bien tratados para evitar sesgos. humano valida). - steven pacheco 2025 - si tenemos mal los datos, mala es la salida de nuestra funcio'n c. capacidad de generalizacio'n te'cnicas comunes: normalizacio'n, imputacio'n (media/mediana), deteccio'n y tratamiento de outliers, la generalizacio'n es la habilidad de un modelo de deingenier'ıa de caracter'ısticas y enriquecimiento. sempen˜arse bien sobre datos no vistos durante el entrenamiento. es el objetivo central al medir la utilidad pra'ctica b. sesgos y equidad de un modelo. los datasets reflejan las desigualdades del mundo real. un modelo entrenado con datos sesgados puede perpetuar iv. deeplearningyredesneuronales discriminaciones. ejemplos pra'cticos: a. perceptro'n y or'ıgenes - reconocimiento facial con peor desempen˜o en ciertos grupos demogra'ficos. el perceptro'n (de'cada de 1950) es un modelo de unidad - modelos de cre'dito que penalizan poblaciones subreprede decisio'n lineal que calcula una combinacio'n ponderada de sentadas. entradas y aplica una funcio'n de activacio'n. funciona bien buenas pra'cticas: auditor'ıas de sesgo, conjuntos de prueba paraproblemaslinealmenteseparables,peronopuederesolver estratificados, transparencia en datos y procesos. problemas no lineales (ej. xor). c. ejemplo del profe: celsius vs fahrenheit b. redes profundas y arquitecturas comunes un error cla'sico en datasets es mezclar unidades. si un campo de temperatura contiene valores en ambas escalas sin el deep learning usa redes con muchas capas: perceptrones etiqueta, el modelo puede aprender patrones erro'neos. es multicapa (mlp), redes convolucionales (cnn) y redes reesencial normalizar unidades y validar rangos. currentes (rnn/lstm/transformer). cada arquitectura esta' orientada a distintos tipos de datos: vi. brevehistoriadelaia(l'ineadeltiempo) - cnn: ima'genes y datos con estructura espacial. - 1950s: perceptro'n y primeras investigaciones (rosen- - rnn / lstm: secuencias temporales y texto blatt). (histo'ricamente). - 1960s: nacimiento temprano"}
{"id_doc": "DOC_002", "segmentacion": "A", "chunk_id": "DOC_002_A_002", "idx": 2, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "de temperatura contiene valores en ambas escalas sin el deep learning usa redes con muchas capas: perceptrones etiqueta, el modelo puede aprender patrones erro'neos. es multicapa (mlp), redes convolucionales (cnn) y redes reesencial normalizar unidades y validar rangos. currentes (rnn/lstm/transformer). cada arquitectura esta' orientada a distintos tipos de datos: vi. brevehistoriadelaia(l'ineadeltiempo) - cnn: ima'genes y datos con estructura espacial. - 1950s: perceptro'n y primeras investigaciones (rosen- - rnn / lstm: secuencias temporales y texto blatt). (histo'ricamente). - 1960s: nacimiento temprano del machine learning y - transformers: atencio'n y modelado de dependencias a primeros sistemas simbo'licos. larga distancia (estado del arte en nlu y nlg). - 1970s: lenguajes lo'gicos (prolog), algoritmos cla'sicos (dijkstra). c. yann lecun y las cnn - 1980s:iniciodelaexperimentacio'nconautosauto'nomos. - 1990s: resurgimiento con redes multicapa y aprendizaje yann lecun fue pionero en redes convolucionales (lenet) por refuerzo. y en su aplicacio'n al reconocimiento de d'ıgitos. las cnn - 2000s: auge del reconocimiento facial y visio'n por aprenden filtros que detectan caracter'ısticas locales (bordes, computadora. texturas) y luego construyen representaciones de alto nivel - 2010s-2020s: deep learning, grandes modelos de mediante capas sucesivas. lenguaje, despliegues comerciales. vii. ramasdelaprendizajeautoma'tico b. aprendizaje no supervisado a. aprendizaje supervisado - definicio'n: modelo identifica patrones en datos sin etiquetas. consiste en aprender una funcio'n f : x → y a partir - ejemplo: agrupamiento de clientes por ha'bitos de comde ejemplos etiquetados (x ,y ). te'cnicas: regresio'n lineal, i i pra. svm,a'rboles,redesneuronales.seevalu'aconme'tricascomo - te'cnicas: rmse, accuracy, f1. - clustering (ej. k-means). b. aprendizaje no supervisado - reduccio'n de dimensionalidad (ej. pca). no hay etiquetas; el objetivo es encontrar estructura. c. comparacio'n te'cnicas: clustering (k-means, dbscan), reduccio'n de di- aspecto supervisado no supervisado mensionalidad (pca, t-sne, umap). datos etiquetados sin etiquetas objetivo prediccio'n descubrir patrones c. aprendizaje por refuerzo ejemplos regresio'n, clasificacio'n clustering agentes aprenden interactuando con un entorno y recibiendo recompensas. aplicaciones: juegos (atari, go), control robo'tico. referencia cla'sica: sutton & barto. fig.5. ejemplodeaprendizajeporrefuerzo fig.4. ejemplodeaprendizajeporrefuerzo conclusio'n viii. cient'ificovsingenieroenia la ia es un campo en ra'pida evolucio'n: combina teor'ıa matema'tica, ingenier'ıa de software y consideraciones e'ticas. cient'ıfico de ia: foco en investigacio'n, nuevos modelos, comprender fundamentos, cuidar la calidad de los datos y experimentos. adquirir habilidades pra'cticas (keras, mlops, validacio'n) son ingeniero de ia / ml engineer: foco en produccio'n, claves para trabajar efectivamente en este a'mbito. rendimiento, escalabilidad, mlops y despliegue. ambos roles se complementan; en proyectos reales conviven y colaboran. ix. tiposdeaprendizajeenia a."}
{"id_doc": "DOC_002", "segmentacion": "A", "chunk_id": "DOC_002_A_003", "idx": 3, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "ejemplodeaprendizajeporrefuerzo conclusio'n viii. cient'ificovsingenieroenia la ia es un campo en ra'pida evolucio'n: combina teor'ıa matema'tica, ingenier'ıa de software y consideraciones e'ticas. cient'ıfico de ia: foco en investigacio'n, nuevos modelos, comprender fundamentos, cuidar la calidad de los datos y experimentos. adquirir habilidades pra'cticas (keras, mlops, validacio'n) son ingeniero de ia / ml engineer: foco en produccio'n, claves para trabajar efectivamente en este a'mbito. rendimiento, escalabilidad, mlops y despliegue. ambos roles se complementan; en proyectos reales conviven y colaboran. ix. tiposdeaprendizajeenia a. aprendizaje supervisado - definicio'n: modelo que aprende a partir de datos etiquetados (caracter'ısticas + valores conocidos). - ejemplo: predecir el precio de una casa usando sus caracter'ısticas. - proceso: - divisio'n: muestra (caracter'ısticas) + etiqueta (valor objetivo). - aproximacio'n: minimiza el error mediante una funcio'n de pe'rdida (l). - te'cnicas: - regresio'n (valores continuos). - clasificacio'n (categor'ıas discretas)."}
{"id_doc": "DOC_003", "segmentacion": "A", "chunk_id": "DOC_003_A_000", "idx": 0, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "apuntes de clase inteligencia artificial - semana 2 - 12 de agosto priscilla jime'nez salgado escuela de ingenier'ıa en computacio'n, tecnolo'gico de costa rica cartago, costa rica - 2021022576@estudiantec.cr abstract-el presente documento recopila los - medicina:usodemodelosdemachine learning apuntes de la segunda semana del curso de in- para analizar ima'genes me'dicas, como escaneos teligencia artificial. se introducen conceptos de cerebrales, y detectar patrones o anomal'ıas que machine learning y deep learning, los principales ayudeneneldiagno'stico. tipos de aprendizaje, el rol de la calidad de los datos y el ciclo completo de desarrollo, validacio'n y despliegue de modelos. i. introduccio'nalainteligenciaartificial se hace un repaso de los fundamentos de la fig. 1. ana'lisis de ima'genes me'dicas con algoritmos de aprendizaje auinteligencia artificial (ia), sus caracter'ısticas de toma'tico autonom'ıa y adaptabilidad, y su capacidad para - agricultura:ana'lisisdedatossobreclima,tipo resolver problemas complejos. se destacan ejemplos deplanta,composicio'nyhumedaddelsuelopara pra'cticos y analog'ıas para comprender el comporoptimizar que' cultivar, cua'ndo sembrar, cua'ndo tamiento de sistemas inteligentes. regaryque' fertilizantesusar. a. definicio'n de la inteligencia artificial la inteligencia artificial (ia) es, en pocas palabras, un conjunto de tecnolog'ıas capaces de realizar tareas que requieren inteligencia humana, como ver, escuchar, aprender y adaptarse. su objetivo es resolver problemas complejos de forma eficaz y generar valor. b. ¿que' significaserauto'nomoyadaptativoenesta a'rea? fig.2. aplicacio'ndedatosparaperfeccionarlaspra'cticasdecultivo en ia, un sistema puede considerarse inteligente cuando combina dos capacidades clave: la au- - videojuegos:enlaimagensemuestraunejemtonom'ıaparaactuarsininstruccionesconstantesyla plo donde la ia analiza el entorno de un videoadaptabilidad para modificar su comportamiento en juegotipoplataformas,identificandoobsta'culos, respuestaacambiosdelentornoodelascondiciones enemigos y recompensas, para decidir que' acque se presenten. ciones ejecutar mediante diferentes entradas y salidasdeunmodeloderedneuronal. c. usos de la inteligencia artificial la inteligencia artificialtiene aplicaciones en una gran variedad de a'reas. algunos ejemplos destacados son: en resumen, se le entrega u'nicamente la entrada y su etiqueta, y el modelo debe inferir o aproximar el resultadoconlamayorprecisio'nposible. en este contexto, se cuenta con dos perspectivas: cienciaeingenier'ıa. machine learning: ciencia - generar conocimiento: se refiere a investigar y desarrollar nuevas ideas para mejorar el aprendizaje automa'tico. esto puede incluir crear modelos desde cero, encontrar formas ma's fig.3. iaidentificandoaccioneso'ptimasenunentornodeplataformas eficientes de optimizar funciones o aprovechar datossinetiquetarparaentrenarsistemas. d. modelos deterministas y estoca'sticos en inteligencia artificial y machine learning, un - me'tricas: se emplean me'tricas para determinar modelo puede clasificarse segu'n co'mo responde a que' modelo ofrece un mejor desempen˜o, evalunaentrada: uando su rendimiento en funcio'n del problema espec'ıficoquesebuscaresolver. - determinista:paraunaentradaespec'ıfica,siempredevuelvelamismasalida.ejemplo:determinarsihayluzded'ıaalas12p.m. - data scientist: se encarga de trabajar con los datos desde su recoleccio'n hasta"}
{"id_doc": "DOC_003", "segmentacion": "A", "chunk_id": "DOC_003_A_001", "idx": 1, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "mejorar el aprendizaje automa'tico. esto puede incluir crear modelos desde cero, encontrar formas ma's fig.3. iaidentificandoaccioneso'ptimasenunentornodeplataformas eficientes de optimizar funciones o aprovechar datossinetiquetarparaentrenarsistemas. d. modelos deterministas y estoca'sticos en inteligencia artificial y machine learning, un - me'tricas: se emplean me'tricas para determinar modelo puede clasificarse segu'n co'mo responde a que' modelo ofrece un mejor desempen˜o, evalunaentrada: uando su rendimiento en funcio'n del problema espec'ıficoquesebuscaresolver. - determinista:paraunaentradaespec'ıfica,siempredevuelvelamismasalida.ejemplo:determinarsihayluzded'ıaalas12p.m. - data scientist: se encarga de trabajar con los datos desde su recoleccio'n hasta su preparacio'n final, aplicando te'cnicas de ana'lisis y manipu- - estoca'stico: para una entrada espec'ıfica, puede lacio'nparaconvertirloseninformacio'ncomprendevolver diferentes salidas de un conjunto de sibleyaplicable. posibilidades, incorporando aleatoriedad. ejemplo:predecirelclimaexactoalas12p.m. - research scientist: centra su labor en investigar y proponer nuevos modelos, algoritmos y enfoques teo'ricos que impulsen el avance del ii. machinelearning machine learning. esto incluye experimentar conaprendizajenosupervisado. el concepto de machine learning consiste en disen˜ar ma'quinas capaces de realizar tareas sin estar programadasdeformaexpl'ıcita,extrayendolalo'gica directamentedelosdatos. por ejemplo, no se le indicara' a la computadora que' es un perro ni las reglas que lo definen (cola, fig.4. descripcio'ngeneraldelusodemachinelearningenlaciencia. ojos, raza, etc.); en su lugar, se le proporcionara' una imagen y retroalimentacio'n para que aprenda por s'ı en esta misma l'ınea, google publico' un art'ıculo misma. as'ı, si se dispone de ima'genes etiquetadas titulado towards an ai co-scientist, en el que como x e y, el modelo recibe cada imagen junto presenta una inteligencia artificial capaz de generar consuetiquetaydebe,demaneraimpl'ıcita,aprender hipo'tesiscient'ıficasapartirdelana'lisisdedatos.esta a distinguir entre ambas, sin que se le explique el tecnolog'ıa se probo' en el a'mbito me'dico y permitio' procedimiento. obteneravancessignificativosenesaa'rea. machine learning: ingenier'ıa ahorabien,consideremoselsiguientegra'fico: en la etapa de puesta en produccio'n de un modelo, lo primero que se debe considerar es la necesidad de refactorizar el co'digo, ya que sera' necesario seguir monitoreando el modelo e, incluso, volveraentrenarlopasadociertotiempo.estoimplica modificar su estructura para que pueda ofrecer la misma funcionalidad, pero de manera ma's eficiente. porejemplo,sisedisponedeunmodelomuypesado, es fundamental transformar el modelo para que puedaserservidoaunclientedeformapra'ctica. para lograrlo, existen diversas herramientas u'tiles. una de ellas es onnx, que permite tomar un modelo desarrollado en un framework espec'ıfico y optimizarlo, generando una versio'n ma's ligera y compacta. tambie'n pueden aplicarse te'cnicas como fig.6. s'ıntesisdelmachinelearningenelcampodelaingenier'ıa. elmodel distillation,dondeunmodeloma'spequen˜o aprendeaimitarelcomportamientodeunodemayor taman˜o, logrando realizar las mismas tareas con elprocesocomienzaconlarecoleccio'n de datos. a partir de ellos, se genera un conjunto estructurado menor peso. otra estrategia es la que reduce la cantidad de bits en los para'metros del modelo. estos (dataset) que se"}
{"id_doc": "DOC_003", "segmentacion": "A", "chunk_id": "DOC_003_A_002", "idx": 2, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "para lograrlo, existen diversas herramientas u'tiles. una de ellas es onnx, que permite tomar un modelo desarrollado en un framework espec'ıfico y optimizarlo, generando una versio'n ma's ligera y compacta. tambie'n pueden aplicarse te'cnicas como fig.6. s'ıntesisdelmachinelearningenelcampodelaingenier'ıa. elmodel distillation,dondeunmodeloma'spequen˜o aprendeaimitarelcomportamientodeunodemayor taman˜o, logrando realizar las mismas tareas con elprocesocomienzaconlarecoleccio'n de datos. a partir de ellos, se genera un conjunto estructurado menor peso. otra estrategia es la que reduce la cantidad de bits en los para'metros del modelo. estos (dataset) que se dividira' en dos partes: una para para'metros,normalmenteenpuntoflotante,seajustan el entrenamiento y otra para las pruebas. esta informacio'nsealmacenaparasuusoposterior. para mejorarel rendimiento, reduciendo eltaman˜o y acelerandosuejecucio'n. en la fase de entrenamiento, el modelo aprende a partir del conjunto de datos de entrenamiento, en el a'mbito del mlops, el objetivo es gestionar ajustando sus para'metros segu'n el feno'meno que se elmodelodeformama'sdetallada.unconceptoclave desea modelar. durante esta etapa tambie'n se define es el data shift, que ocurre cuando el modelo fue que' tipo de modelo utilizar, justificando la eleccio'n entrenado con una distribucio'n de datos espec'ıfica, enfuncio'ndelasnecesidadesdelproblema. pero con el tiempo los patrones cambian, afectando suprecisio'n.poreso,unadelasresponsabilidadesdel luego se pasa a la validacio'n, donde se utilizan equipodemlopsesmonitorearelcomportamiento datos que el modelo no ha visto antes para evaluar del modelo y, si detectan un cambio en los datos, su rendimiento en condiciones similares a las de volver a entrenarlo. este monitoreo continuo no solo produccio'n. esto permite identificar que' modelo incluye el rendimiento del modelo, sino tambie'n la obtiene los mejores resultados segu'n los criterios de calidad y actualidad de los datos, la infraestructura evaluacio'nestablecidos. de despliegue y la trazabilidad de las versiones el modelo seleccionado se guarda en un repositoutilizadas. rio o almacenamiento de modelos, desde el cual se prepara para su implementacio'n en el entorno elegido. poru'ltimo,sellevaacaboeldespliegueoservicio de predicciones, donde el modelo ya entrenado y fig.5. descripcio'ngeneraldelusodemachinelearningenlaingenier'ıa. validadocomienzaaprocesardatosrealesyagenerar resultados. todo este flujo puede representarse me- continuacio'n. diante un diagrama que describe cada paso, desde la - supervisado: el modelo aprende a partir de obtencio'n inicial de los datos hasta la puesta en datos que incluyen etiquetas, las cuales sirven marcha del sistema en produccio'n. como referencia durante el entrenamiento. un ejemplocomu'neslaclasificacio'ndeima'genes. iii. jerarqu'ıadeconceptosenia - no supervisado:elmodelotrabajacondatossin etiquetas y se encarga de encontrar patrones en los datos ocultos. un ejemplo claro de esto son losclusters. - semi-supervisado: combina datos etiquetados y no etiquetados, u'til cuando el proceso de etiquetadoescostosoodif'ıcilderealizar. - auto-supervisado: el propio dato de entrada sirve"}
{"id_doc": "DOC_003", "segmentacion": "A", "chunk_id": "DOC_003_A_003", "idx": 3, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "supervisado: el modelo aprende a partir de obtencio'n inicial de los datos hasta la puesta en datos que incluyen etiquetas, las cuales sirven marcha del sistema en produccio'n. como referencia durante el entrenamiento. un ejemplocomu'neslaclasificacio'ndeima'genes. iii. jerarqu'ıadeconceptosenia - no supervisado:elmodelotrabajacondatossin etiquetas y se encarga de encontrar patrones en los datos ocultos. un ejemplo claro de esto son losclusters. - semi-supervisado: combina datos etiquetados y no etiquetados, u'til cuando el proceso de etiquetadoescostosoodif'ıcilderealizar. - auto-supervisado: el propio dato de entrada sirve como etiqueta. se emplea en autoencoders y modelos de representacio'n, principalmente fig.7. relacio'nentreia,machinelearningydeeplearning para reducir el taman˜o de vectores o extraer - machinelearning:utilizadatosparaqueunsis- caracter'ısticasrelevantes. temapuedaentrenarseymejorarsurendimiento. - aprendizaje por refuerzo:elmodelo,llamado para lograrlo, emplea me'todos como algoritmos agente, aprende mediante un sistema de recomestad'ısticos, regresio'n lineal, regresio'n log'ıstica pensas, mejorando su desempen˜o a trave's de la o a'rboles de decisio'n. ma's alla' de la te'cnica interaccio'nconunentorno. usada, su base siempre es la misma: aprender apartirdelosdatosdisponibles. - few-shot learning: el modelo necesita solo unospocosejemplosparaaprenderarealizaruna - deep learning: es una especialidad dentro del tareaespec'ıfica. machine learning que utiliza redes neuronales profundas,formadasporvariascapasconectadas. - one-shot learning: basta con mostrarle una u'nica vez co'mo realizar la tarea para que el esteenfoqueesmuyefectivopararesolvertareas modelopuedareproducirla. complejas, aunque requiere grandes volu'menes dedatosparafuncionarcorrectamente. - zero-shot learning: el modelo es capaz de realizar tareas sin haber sido entrenado previaenresumen,lasdiferenciasesquelainteligencia menteparaellasdeformaespec'ıfica. artificialeselconceptoma'samplio,queengloba todo lo relacionado con lograr que una ma'quina v. pipelinedemachinelearning actu'edeforma\"inteligente\".elmachinelearnel desarrollo de un modelo de machine learning ing agrupa algoritmos que aprenden con datos, pasaporvariasetapasclave: y el deep learning es una te'cnica dentro de este que esta' disen˜ada para trabajar con canti1) data adquisition: este es muy importante dades masivas de informacio'n y resolver tareas ya que se necesitan obtener datos de calidad y espec'ıficascongranprecisio'n. representativos, evitando errores como valores iv. tiposdeaprendizajeenmachinelearning faltantesyduplicados. sehizounrepasodelaclaseanterioryseretomaron 2) data preparation:paraestaetapasetieneque los dema's tipos de aprendizaje y se describen a preparar los datos que se tienen para tener un dataset de calidad, ya sea, eliminando datos raciones hasta encontrar la que produce el mejor duplicados o se descartan datos que no tienen rendimientodelmodelo. utilidad. vii. paradigmasderesolucio'ndeproblemasen 3) feature engineering: esta fase consiste en machinelearning crear nuevas variables u'tiles a partir de los datos que se encuentran disponibles o eliminar 1) agrupamiento: busca patrones o relaciones aquellas que no aporten informacio'n relevante, ocultas en los datos para formar grupos, u'til especialmenteenelcasodedatostabulares. para descubrir conexiones que no se hab'ıan considerado. 4) modelo selection: elegir el modelo ma's"}
{"id_doc": "DOC_003", "segmentacion": "A", "chunk_id": "DOC_003_A_004", "idx": 4, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "datos raciones hasta encontrar la que produce el mejor duplicados o se descartan datos que no tienen rendimientodelmodelo. utilidad. vii. paradigmasderesolucio'ndeproblemasen 3) feature engineering: esta fase consiste en machinelearning crear nuevas variables u'tiles a partir de los datos que se encuentran disponibles o eliminar 1) agrupamiento: busca patrones o relaciones aquellas que no aporten informacio'n relevante, ocultas en los datos para formar grupos, u'til especialmenteenelcasodedatostabulares. para descubrir conexiones que no se hab'ıan considerado. 4) modelo selection: elegir el modelo ma's adecuadosegu'nelproblemayrecursosdisponibles, puedeirdesdeopcionessimplescomoregresio'n log'ıstica hasta redes neuronales profundas para casoscomplejos. 5) model training: esta fase de entrenamiento delprocesodelmodelo,sedividenlosdatosen training set y validation set y se ajustan hiperpara'metrosconme'todoscomogridsearch. 6) model deployment: en este proceso de deployment se implementa el modelo en profig.8. ejemplodelprocesodeagrupamiento duccio'n y se supervisa para poder garantizar usubuenrendimiento. 2) prediccio'n y clasificacio'n: vi. hiperpara'metrosypara'metros es importante diferenciar entre para'metros e - prediccio'n: estima un valor nume'rico hiperpara'metros: basa'ndoseenpatronesdelosdatos. - clasificacio'n:asignadatosaunacategor'ıa - para'metros: son los valores internos que un segu'nsuscaracter'ısticas. modelo aprende automa'ticamente a partir de los datosduranteelentrenamiento.estosvaloresson los que el modelo ajusta para minimizar el error ymejorarsucapacidaddeprediccio'n. - hiperpara'metros: son valores definidos manualmente antes de iniciar el entrenamiento, y controlan el comportamiento del algoritmo. un ejemplo es el batch size (por ejemplo, 32 muestras por iteracio'n). estos no se aprenden del conjunto de datos, sino que se configuran para guiarelprocesodeentrenamiento. el ajuste de hiperpara'metros requiere un proceso fig.9. ejemplodelprocesodeprediccio'nyclasificacio'n de experimentacio'n, probando distintas configu3) optimizacio'n: encuentra la mejor solucio'n viii. notaimportante entremuchasposibles. - enunciado tarea moral mencionar un aporte - local:lamejorenuna'reaconcreta. decadaunodeellos: - global: la mejor en todo el espacio de -yannlecun bu'squeda. -yoshuabengio -samaltman -geoffreyhinton -timnitgebru -iangoodfellow incluya un resumen del funcionamiento de las siguientesherramientas/conceptos: -onnx -mlflow -vertex -langchain -huggingface -ollama -chain-of-thought fecha de entrega: 19 de agosto, pero esta es moral. ix. aspectosadministrativos la pro'xima seccio'n corresponde a a'lgebra linfig.10. ejemplodelprocesodeoptimizacio'n eal, la cual es fundamental para los temas que se abordara'nma'sadelante. 4) bu'squeda: encuentra el camino ma's eficiente hacia una solucio'n, representando la mejor la pro'xima semana la modalidad sera' virtual. opcio'ncomoelcaminodemenorcosto. hasta el martes 26 de agosto las clases sera'n presencialesyesemarteshabra' quiz acumulativo. fig.11. ejemplodelprocesodebu'squeda"}
{"id_doc": "DOC_003", "segmentacion": "A", "chunk_id": "DOC_003_A_005", "idx": 5, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "ma's eficiente hacia una solucio'n, representando la mejor la pro'xima semana la modalidad sera' virtual. opcio'ncomoelcaminodemenorcosto. hasta el martes 26 de agosto las clases sera'n presencialesyesemarteshabra' quiz acumulativo. fig.11. ejemplodelprocesodebu'squeda"}
{"id_doc": "DOC_004", "segmentacion": "A", "chunk_id": "DOC_004_A_000", "idx": 0, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "notas de clase inteligenciaartificial-12deagosto-semana2 luis alfredo gonza'lez sa'nchez escuela de ingenier'ıa en computadores instituto tecnolo'gico de costa rica cartago, costa rica 2021024482 gonzal3z.luis@estudiantec.cr abstract-this document provides a concise summary of the c. la jerarqu'ıa de conceptos en ia key concepts and examples covered in week two of the artificial intelligence course. it begins with an overview of general ai observe la figura 1. concepts, followed by an explanation of the types and main approaches of machine learning. the document also explores various problem-solving paradigms and concludes with a brief review of the essential components of the machine learning pipeline. this summary aims to enhance understanding and reinforce the material presented in class. index terms-ai, machine learning, pipeline. i. introduction el presente documento busca brindar un breve resumen a los conceptos y ejemplos vistos en la clase de la semana 2 del curso de inteligencia artificial , comenzando por un breve recorrido a determinados conceptos generales de la ia, luego se procedera' a explicar los tipos de aprendizaje en machine fig.1. jerarquiadeconceptosia learning, sus principales enfoques , algunos paradigmas de resolucio'n de problemas y por u'ltimo un breve repaso a los conceptos del pipeline de machine learning. d. ¿que' es machine learning ? ii. conceptosgenerales consiste en generar ma'quinas o programas a partir de se presentara' un breve repaso a conceptos vistos en clase. algoritmos que sean capaces de resolver un problema a partir deinferencias.enestecontextosebuscaque,apartirdedatos a. definicio'n de modelos deentrada(yenocasionesconsusresultados)dondeelsistema - modelo determinista : es aquel modelo que, bajo una sea capaz de resolver problemas en base a los datos que ya misma entrada se produce los mismos resultados. no inposeesinqueseleexpliquecomoresolverlo.considerandolo corpora aleatoriedad, por lo que los resultados obtenidos anterior , se describe a continuacio'n los tipos de aprendizaje se vuelven predecibles , por ejemplo, bajo caracter'ısticas en machine learning. me'dicas se puede determinar una enfermedad asociada. - modelo estoca'stico : es aquel modelo en que , bajo una entradadeterminadalassalidassondiferentesdebidoaun iii. tiposdeaprendizaje conjuntodeposiblesresultadosasociadosadichaentrada a. aprendizaje supervisado , existe control respecto al grado de aleatoriedad, por ejemplo, consultar el clima a la misma hora todos los tipodeaprendizajequeinvolucraunconjuntodedatoscon d'ıas. caracter'ısticas y una respectiva etiqueta, esa etiqueta asocia b. definicio'n de para'metros dichas caracter'ısticas con un valor. la etiqueta supervisa y corrigelasaproximacionesdadasporlafuncio'nplanteadapara - para'metros : son los datos que se le configuran al obtener mejores resultados, una forma de visualizarlo es de la modelo, permiten controlar el comportamiento de sus siguiente manera : algoritmos"}
{"id_doc": "DOC_004", "segmentacion": "A", "chunk_id": "DOC_004_A_001", "idx": 1, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "tiposdeaprendizaje conjuntodeposiblesresultadosasociadosadichaentrada a. aprendizaje supervisado , existe control respecto al grado de aleatoriedad, por ejemplo, consultar el clima a la misma hora todos los tipodeaprendizajequeinvolucraunconjuntodedatoscon d'ıas. caracter'ısticas y una respectiva etiqueta, esa etiqueta asocia b. definicio'n de para'metros dichas caracter'ısticas con un valor. la etiqueta supervisa y corrigelasaproximacionesdadasporlafuncio'nplanteadapara - para'metros : son los datos que se le configuran al obtener mejores resultados, una forma de visualizarlo es de la modelo, permiten controlar el comportamiento de sus siguiente manera : algoritmos - hiperpara'metros : son todos los valores que el mod- x={x i ,y i } elo aprende a partir de los datos de entrenamiento, le permiten al modelo identificar errores y ajustar sus donde x representa mi caracteristica, xi representa la etiqueta algoritmos. y yi, representa el resultado \"supervisado\" por la etiqueta. b. aprendizaje no supervisado f. otros tipos de aprendizaje este tipo de aprendizaje es aquel que se entrena con datos se detallan a continuacio'n algunos tipos de aprendizaje sin etiquetas, es decir, sin informacio'n previa que le permita adicionales determina cua'l es el \"resultado correcto\".a diferencia del - fev-shot : me'todo en el cual el modelo aprende a aprendizaje supervisado, donde las etiquetas sirven como una partir de breves ejemplos. una posible aplicacio'n es con \"supervisio'n\" o gu'ıa para corregir y entrenar el modelo, en respecto a los llms ,si el llms no sabe como realizar el aprendizaje no supervisado no hay una referencia expl'ıcita latarea,selebrindaejemplosyapartirdeesosejemplos paraevaluarocorregir.unejemplomencionadoenlaclase,son es capaz de resolver la tarea. los algoritmos de clu'ster , donde analiza en base a sus - one shot:me'todosimilaralanterior,soloquereducela caracter'ısticas si pertenece a un grupo u otro, sin hacer uso candidaddeejemplosa1,yapartirdeeseu'nicoejemplo de etiquetas. logra resolver. c. aprendizaje semi-supervisado - zero shot : en este me'todo el sistema resuelve en base a lo que \"sabe\" , no existen ejemplos del que pueda tomar en palabras sencillas es una combinacio'n de los me'todos para aprender. deaprendizajeanteriores,enestetipodealgoritmo,seutilizan tanto datos etiquetados como datos no etiquetados para entre- iv. enfoquesdelmachinelearning nar el modelo, es decir no siempre va a existir una etiqueta existen dos principales enfoques, ciencia e ingenieria: asociada a las caracter'ısticas, es principalmente u'til cuando se posee etiquetas en solo en una parte de los datos para a. machine learning: ciencia entrenar,yendatosdondelasetiquetassondif'ıcilesdeobtener - se genera conocimiento : es la parte acade'mica que se ,aprovechalainformacio'ndedatossinetiquetasparamejorar dedica a investigar y desarrollar nuevas ideas respecto a el rendimiento del modelo. aprendizaje automa'tico, incluye la elaboracio'n de"}
{"id_doc": "DOC_004", "segmentacion": "A", "chunk_id": "DOC_004_A_002", "idx": 2, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "etiquetados para entre- iv. enfoquesdelmachinelearning nar el modelo, es decir no siempre va a existir una etiqueta existen dos principales enfoques, ciencia e ingenieria: asociada a las caracter'ısticas, es principalmente u'til cuando se posee etiquetas en solo en una parte de los datos para a. machine learning: ciencia entrenar,yendatosdondelasetiquetassondif'ıcilesdeobtener - se genera conocimiento : es la parte acade'mica que se ,aprovechalainformacio'ndedatossinetiquetasparamejorar dedica a investigar y desarrollar nuevas ideas respecto a el rendimiento del modelo. aprendizaje automa'tico, incluye la elaboracio'n de modd. aprendizaje auto-supervisado elos desde cero y optimizar funciones, por lo general, esta' en manos de investigadores con un fuerte trasfondo para este tipo de aprendizaje, el modelo no depende de matema'tico que les permite conocer a gran detalle los datos con etiquetas \"pre- insertadas\" para el entrenamiento, procesos que suceden durante el desarrollo de dichos en lugar de eso, genera automa'ticamente etiquetas o sen˜ales modelos o algoritmos. de supervisio'n a partir de los propios datos. un ejemplo visto en clase es el uso en modelos de lenguaje, donde el sistema - me'tricas : a partir de los datos recolectados, los data scientist generan me'tricas que permiten evaluar el depredice a partir de palabras previas cual es la palabra que sempen˜o del modelo o algoritmo. sigue a dicha palabra, puede visualizarse la palabra previa comodato,peroalavezcomounamismaetiquetaparapoder - data scientist : son las personas encargadas de realizar larecoleccio'nylapreparacio'ndelosdatos,detalmanera definir la palabra siguiente. queseanaplicablesparaelentrenamientodelosmodelos e. aprendizaje por refuerzo u algoritmos. el aprendizaje por refuerzo consiste en un aprendizaje con - research scientist : a diferencia de los data scientist , \"premiacio'n\",existeunagenteydichoagenteaprendeatomar losresearchscientistseencargandeinvestigaryproponer decisionesatrave'sdelainteraccio'nconunentorno,buscando proponer nuevos modelos, algoritmos o modelos. maximizarunarecompensa,sepuedenpenalizarporacciones b. machine learning: ingenieria que llevaran a un resultadoinsatisfactorio.ejemplo, observe la figura 2. en el videojuego representado, se puede entrenar al - puesta en produccio'n del modelo : es necesario la continua monitorizacio'n del modelo antes de la puesta a produccio'n y pasado un tiempo, se busca realizar correcciones, monitorizar si los resultados o salidas mantienen unadistribucio'nnormalprobabil'ısticaycomoelprogreso deentrenamientoluegodelapuestaenproduccio'nmueve dichadistribucio'n,esdecir,darseguimientoalassalidas. - transformar el modelo :hace referencia a todas las estrategias y cambios requeridos para adaptar el modelo segu'n los requerimientos especificados , por ejemplo, si se dispone de un modelo en python, cua'les son las estrategias para adaptarlo en otras plataformas como c++ fig.2. videojuegodeejemplo o mo'vil , una de dichas estrate'gias es la reduccion del personaje (agente) para que cruce al otro lado basa'ndose en peso del modelo. si el salto se realiza"}
{"id_doc": "DOC_004", "segmentacion": "A", "chunk_id": "DOC_004_A_003", "idx": 3, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "o salidas mantienen unadistribucio'nnormalprobabil'ısticaycomoelprogreso deentrenamientoluegodelapuestaenproduccio'nmueve dichadistribucio'n,esdecir,darseguimientoalassalidas. - transformar el modelo :hace referencia a todas las estrategias y cambios requeridos para adaptar el modelo segu'n los requerimientos especificados , por ejemplo, si se dispone de un modelo en python, cua'les son las estrategias para adaptarlo en otras plataformas como c++ fig.2. videojuegodeejemplo o mo'vil , una de dichas estrate'gias es la reduccion del personaje (agente) para que cruce al otro lado basa'ndose en peso del modelo. si el salto se realiza correctamente (accio'n) se recompensa, y - onnx:esunaherramientaquefacilitalaportabilidadde si la accio'n es insatisfactoria (caer al vac'ıo y perder una vida) modelos entre plataformas y dispositivos, optimizando y se castiga por dicha accio'n. reduciendo el taman˜o del modelo. - mlops : los equipos de mlops tiene la finalidad de monitorear y gestionar el modelo de manera detallada ,tanto con los datos como la calidad de los mismos, por ejemplo, en el seguimiento que se da en puesta en produccio'n estan encargados de monitorear el comportamiento del modelo, en casos donde suceda algun cambio en los datos, deben de entrenarlo nuevamente. v. paradigmasderesolucio'ndeproblemas se detallan a continuacio'n ciertos paradigmas o estrategias para resolver problemas en el a'mbito de la inteligencia artififig.4. ejemplodegra'ficaparavisualizarproblemasdeprediccio'n cial: a. problemas de bu'squeda d. problemas de agrupamiento : son todos aquellos algoritmos que permiten encontrar la para los problemas de agrupamiento no existen etiquetas mejor solucio'n, representada por el camino ma's barato o la con las cuales entrenar, se busca encontrar pertenencia a un ruta ma's corta, se detallan algoritmos como dijkstra , dfs, grupodadassimilitudesacaracter'ısticas,lassolucionesaestos bfs. problemas permite encontrar relaciones a preguntas que quiza' no se ten'ıan previamente. fig.3. ejemplodealgoritmosdebu'squeda b. problemas de optimizacio'n son todas aquellas soluciones que pretenden encontrar la fig.5. ejemplodeproblemasdeagrupamiento solucio'nma'soptimadentrodeungrupodesoluciones.existen 2 tipos: vi. pipelinedelmachinelearning - solucio'n local : es aquella seleccionada dentro de un grupo o a'rea especifica, un ejemplo es el visto en clase - data adquisition :una de las partes ma's importantes respecto a bares, el mejor bar cercano al tec de cartago del piplene de machine learning, es todo el proceso de es la nave, pero esto es por que en el a'rea cercana al tec recoleccio'ndedatosrelevantesdelproblemaaresolver, es la mejor o la u'nica opcion. se deben de resolver preguntas como ¿cua'l es la calidad - solucio'nlocal :serefierealamejorsolucio'nencontrada de los datos que estoy tomando? ¿de do'nde los saco? entre todas las posibles soluciones, tomando el ejemplo ¿es mi fuente libre de sesgo?."}
{"id_doc": "DOC_004", "segmentacion": "A", "chunk_id": "DOC_004_A_004", "idx": 4, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "respecto a bares, el mejor bar cercano al tec de cartago del piplene de machine learning, es todo el proceso de es la nave, pero esto es por que en el a'rea cercana al tec recoleccio'ndedatosrelevantesdelproblemaaresolver, es la mejor o la u'nica opcion. se deben de resolver preguntas como ¿cua'l es la calidad - solucio'nlocal :serefierealamejorsolucio'nencontrada de los datos que estoy tomando? ¿de do'nde los saco? entre todas las posibles soluciones, tomando el ejemplo ¿es mi fuente libre de sesgo?. del bar, si se evalu'a a nivel provincia , probablemente se - data preparation : la etapa de la preparacio'n de encuentre un mejor bar que la nave. los datos, considerando que el computador trabaja con nu'meros, se debe de resolver las pregunta ¿co'mo voy c. problemas de prediccio'n y clasificacio'n a representar los datos a mi algoritmo o modelo? , por en los problemas de prediccio'n , existe una serie de datos ejemplo, si los datos son ima'genes, le asigno valores respecto a un evento, comportamiento, entre otros y se desea puestoquelosalgoritmossonmatema'ticos.enestaetapa encontrar patrones para realizar una prediccio'n ,generalmente tambie'n se realizan otras labores como la revisio'n de se representa con una funcio'n, que podra' generar una esti- datos duplicados y la bu'squeda de datos faltantes, por macio'ndelarelacio'ndelosdatosconsussalidas,unejemplo ejemplo en un set de datos, con algu'n dato faltante, es el visto en clase , donde en base al motor del veh'ıculo , se elimino dicho set o calculo la mediana y la introduzco obtienen datos respecto al consumo de gasolina y se genera como el dato faltante. una funcio'n la cual sera' capaz de predecir cua'l es el consumo - freature engineering : los freatures son los atributos de gasolina de otro veh'ıculo. estos datos son cuantificables o que proporcionan informacio'n u'til de los datos, en esta nume'ricos. etapa se pueden generar nuevos freatures a partir de los existentes,seleccionarsololosrelevantesparalasolucio'n o eliminar freatures que generen algun sesgo. - modelselection:eslaetapadeseleccio'ndelmodeloma's adecuadoparacubrirlademandaderecursosyrequisitos que genera el problema. entra un concepto importante para la seleccio'n del modelo, la explicabilidad, si tengo quedar explicacio'ndel porque' de los resultadosgeneradosconstantemente,quiza' lamejorsolucio'nnoesutilizar ia, si no , podr'ıa escoger un algoritmo que brinde una solucio'n ma's elegante, un ejemplo es para un sistema de un banco que detecta si un usuario es elegible para un pre'stamo, entrenar una ia que determine esto puede no ser la mejor manera , si no , un algoritmo que con datos del usuario"}
{"id_doc": "DOC_004", "segmentacion": "A", "chunk_id": "DOC_004_A_005", "idx": 5, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "que genera el problema. entra un concepto importante para la seleccio'n del modelo, la explicabilidad, si tengo quedar explicacio'ndel porque' de los resultadosgeneradosconstantemente,quiza' lamejorsolucio'nnoesutilizar ia, si no , podr'ıa escoger un algoritmo que brinde una solucio'n ma's elegante, un ejemplo es para un sistema de un banco que detecta si un usuario es elegible para un pre'stamo, entrenar una ia que determine esto puede no ser la mejor manera , si no , un algoritmo que con datos del usuario pueda determinar si es posible o no darle el pre'stamo. - modeltraining:eslafasedeentrenamientodelmodelo, los datos son divididos en datos de entrenamiento y datos de validacio'n, en esta parte los algoritmos realizan optimizaciones en base a los datos de entrenamiento. - model deployment: en esta etapa el modelo se manda a produccio'n en donde se debe de supervisar para garantizar un correcto funcionamiento."}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_000", "idx": 0, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "inteligencia artificial apuntesdeclase-14deagostode2025 1st kendall rodr'ıguez camacho escuela de ingenieria en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica kenrodriguez@estudiantec.cr abstract-en este documento se presentan los apuntes cor- iii. tiposdeaprendizaje respondientes a la clase del 14 de agosto de 2025 del curso de enmachinelearningexistendistintostiposdeaprendizaje, inteligenciaartificial.enprimerlugar,seincluyeunresumende cada uno con aplicaciones y caracter'ısticas particulares: lasesio'nanterior,enelqueserevisanconceptosdeiaymachine learning.posteriormente,seintroduceela'lgebralinealaplicada - supervisado: se dispone de datos con entradas y salidas mediante python, empleando librer'ıas como pytorch, numpy y conocidas x ={x ,y },i=1..n. el modelo aprende a i i pandas, con el objetivo de familiarizarse con las herramientas y predecir la salida y a partir de la entrada x. su implementacio'n pra'ctica. index terms-machine learning, mlops, models, deep - no supervisado: solo se tienen entradas sin etiquetas. el learning, generative ai, vectores, tensores, matrices modelo identifica patrones o estructuras ocultas en los datos. i. introduction - semi-supervisado: algunos datos esta'n etiquetados y la inteligencia artificial incluye diversas te'cnicas, entre otrosno.u'tilcuandoetiquetartodoslosdatosescostoso. ellas el machine learning o aprendizaje automa'tico, que se - auto-supervisado: el modelo genera etiquetas a partir de encarga de que los modelos aprendan de los datos. en esta los propios datos, sin necesidad de intervencio'n manual. clase se presento' un resumen de los diferentes paradigmas es ampliamente utilizado en procesamiento de lenguaje de resolucio'n de problemas en ia, los distintos tipos de natural (nlp) y en redes neuronales concurrentes. ejemaprendizaje y de las etapas que componen el ciclo de vida plo: predecir la siguiente palabra en una oracio'n. de un modelo de ml. adema's, se introdujo el a'lgebra lineal - aprendizaje por refuerzo: el agente aprende mediante de forma pra'ctica utilizando python y librer'ıas como numpy, recompensas y penalizaciones, ajustando sus acciones pandas y pytorch, trabajando con conceptos como tensores, para maximizar la recompensa futura. ejemplo: entrenar matrices y vectores. un agente para jugar mario bros, donde el modelo aprende a avanzar, evitar obsta'culos y recolectar recomii. noticias pensas. a. chatgpt 5, ¿un fracaso? - few-shot: aprende a partir de pocos ejemplos (3-4 apocosd'ıasdesulanzamientoporpartedeopenai,chat- muestras). ejemplo: modelos de lenguaje que generan gpt 5 comenzo' a recibir cr'ıticas por parte de la comunidad. respuestas correctas con muy pocos ejemplos. muchos usuarios cuestionaron si realmente representaba una - one-shot: aprende con un solo ejemplo. ejemplo: remejorafrenteachatgpt4,sen˜alandoqueenvariasocasiones conocimiento facial usando una sola foto de referencia. el modelo tarda demasiado en generar una respuesta y, en - zero-shot:generalizaatareasnuevassinejemplosdirecalgunoscasos,produceerroresorespuestasdecalidadinferior tos, basa'ndose en conocimientos previos. a lo esperado. machinelearningvistodesdelaciencia b."}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_001", "idx": 1, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "- few-shot: aprende a partir de pocos ejemplos (3-4 apocosd'ıasdesulanzamientoporpartedeopenai,chat- muestras). ejemplo: modelos de lenguaje que generan gpt 5 comenzo' a recibir cr'ıticas por parte de la comunidad. respuestas correctas con muy pocos ejemplos. muchos usuarios cuestionaron si realmente representaba una - one-shot: aprende con un solo ejemplo. ejemplo: remejorafrenteachatgpt4,sen˜alandoqueenvariasocasiones conocimiento facial usando una sola foto de referencia. el modelo tarda demasiado en generar una respuesta y, en - zero-shot:generalizaatareasnuevassinejemplosdirecalgunoscasos,produceerroresorespuestasdecalidadinferior tos, basa'ndose en conocimientos previos. a lo esperado. machinelearningvistodesdelaciencia b. alexandr wang y la inversio'n de meta desde la perspectiva cient'ıfica, el fin es generar alexandr wang, un empresario de 28 an˜os, es el fundador conocimiento, entender patrones y crear teor'ıas. las me'tricas descaleai,unastartupespecializadaeninteligenciaartificial se utilizan para cuantificar que' tan bien funciona un modelo, y en el etiquetado masivo de datos para entrenar modelos. en aportando una base objetiva para comparar resultados y enfo2025, meta realizo' una inversio'n significativa en la compan˜'ıa, ques. destacando el intere's de la empresa en reforzar su estrategia en este contexto, los roles principales incluyen: en ia. - data scientist: experimenta con modelos, selecciona alwang se convirtio' en uno de los multimillonarios ma's goritmos, ajusta hiperpara'metros y evalu'a resultados. jo'venes creados por s'ı mismos, y su trabajo en scale ai - research scientist: investiga nuevos algoritmos, publica lo posiciona como una figura relevante en el desarrollo y art'ıculos cient'ıficos y desarrolla teor'ıas para avanzar la aplicacio'n de tecnolog'ıas de inteligencia artificial avanzada. inteligencia artificial. machinelearningvistodesdelaingenier'ia desde la perspectiva de la ingenier'ıa, el enfoque esta' en implementar, mantener y optimizar modelos en produccio'n. entre las principales tareas se incluyen transformar modelos parareducirsutaman˜oyaumentarlavelocidad,crearpipelines de datos que alimenten los modelos de forma automa'tica y monitorear me'tricas de rendimiento y tiempos de respuesta. adema's, las pra'cticas de mlops se aplican para operacionalizarelaprendizajeautoma'tico,demanerasimilaraco'mo devops se utiliza para el desarrollo y despliegue de software. iv. paradigmasderesolucio'ndeproblemas entrelosprincipalesparadigmasderesolucio'ndeproblemas en inteligencia artificial se incluyen: float fig.2. ejemplodesolucioneslocalesyglobalesenoptimizacio'n. c. prediccio'n y clasificacio'n a. problemas de bu'squeda 1) prediccio'n: estimar un valor futuro o desconocido, ense busca encontrar el camino ma's eficiente hacia una contrando patrones segu'n los datos disponibles. por ejemplo, solucio'n. cua'nta gasolina quedara' en un carro. 2) clasificacio'n: asignar elementos a categor'ıas predefinidas, basa'ndose en sus caracter'ısticas. por ejemplo, identificar el modelo de un carro segu'n sus partes. fig.1. ejemplodeproblemasdebu'squeda. b. problemas de optimizacio'n fig.3. ejemplodeprediccio'nyclasificacio'n. cuandoexisteungrannu'merodesolucionesposibles,hallar la mejor solucio'n absoluta puede ser dif'ıcil. d. agrupamiento (clustering) 1) solucio'n local: mejor solucio'n"}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_002", "idx": 2, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "de bu'squeda 1) prediccio'n: estimar un valor futuro o desconocido, ense busca encontrar el camino ma's eficiente hacia una contrando patrones segu'n los datos disponibles. por ejemplo, solucio'n. cua'nta gasolina quedara' en un carro. 2) clasificacio'n: asignar elementos a categor'ıas predefinidas, basa'ndose en sus caracter'ısticas. por ejemplo, identificar el modelo de un carro segu'n sus partes. fig.1. ejemplodeproblemasdebu'squeda. b. problemas de optimizacio'n fig.3. ejemplodeprediccio'nyclasificacio'n. cuandoexisteungrannu'merodesolucionesposibles,hallar la mejor solucio'n absoluta puede ser dif'ıcil. d. agrupamiento (clustering) 1) solucio'n local: mejor solucio'n dentro de un a'rea espec'ıfica. descubrir patrones o grupos naturales en los datos, uti2) solucio'n global: mejor solucio'n en todo el espacio de lizando diferentes aspectos para formar grupos con distintas posibles soluciones. formas. fig.4. ejemplodeagrupamiento(clustering). v. modelos 1) determinista: un modelo determinista produce siempre el mismo resultado para un mismo conjunto de entradas. fig.6. jerarqu'ıadelaia ejemplo: ¿hay luz a mediod'ıa? para las mismas condiciones, la respuesta siempre sera' la misma: s'ı. 2) estoca'stico: un modelo estoca'stico puede producir diferentes resultados para el mismo conjunto de entradas, dependiendodeprobabilidadesofactoresaleatorios.ejemplo: vii. pipelinedemachinelearning ¿cua'l sera' el clima a mediod'ıa? aunque las condiciones iniciales sean similares, el clima puede variar: puede estar soleado, nublado o lloviendo. 1) data acquisition: recolectar datos relevantes y de calidad. 2) data preparation: limpiar y transformar los datos, fig. 5. comparacio'n entre modelos determinista y estoca'stico usando el entrega'ndolos en un formato adecuado para su ana'lisis, elimiejemplodelclima. nando duplicados, valores faltantes o incorrectos, y aplicando normalizacio'n o escalamiento de los valores. vi. jerarqu'iadelainteligenciaartificial 3) feature engineering: crear y seleccionar las variables (features) ma's relevantes para entregar al modelo u'nicamente las necesarias para su entrenamiento y ana'lisis. 1) inteligencia artificial (ia): algoritmos que imitan la inteligencia humana, capaces de tomar decisiones o resolver 4) model selection: elegir el modelo que mejor se adapta problemas. alproblema,considerandonosolosudesempen˜o,sinotambie'n su explicabilidad, es decir, que' tan fa'cil es interpretar y 2) machine learning (ml): me'todos estad'ısticos que perentender co'mo toma decisiones. miten a los modelos aprender de los datos, como regresio'n o' a'rboles de decisio'n. 5) model training: entrenar el modelo y ajustar los hiper3) deep learning (dl): redes neuronales profundas uti- para'metros, utilizando te'cnicas como grid search. durante lizadasparaproblemascomplejos,incluyendovisio'nporcom- este proceso, el modelo realiza optimizacio'n basada en los putadora y procesamiento de lenguaje natural (nlp). datos para mejorar su desempen˜o y reducir errores. 4) generative ai (genai): modelos capaces de generar contenido nuevo, como texto, ima'genes o audio, a partir de 6) model deployment:"}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_003", "idx": 3, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "aprender de los datos, como regresio'n o' a'rboles de decisio'n. 5) model training: entrenar el modelo y ajustar los hiper3) deep learning (dl): redes neuronales profundas uti- para'metros, utilizando te'cnicas como grid search. durante lizadasparaproblemascomplejos,incluyendovisio'nporcom- este proceso, el modelo realiza optimizacio'n basada en los putadora y procesamiento de lenguaje natural (nlp). datos para mejorar su desempen˜o y reducir errores. 4) generative ai (genai): modelos capaces de generar contenido nuevo, como texto, ima'genes o audio, a partir de 6) model deployment: poner el modelo en produccio'n, patrones aprendidos. integrarlo con aplicaciones y monitorear su desempen˜o. import torch # para crear y manipular tensores import numpy as np # para arreglos y operaciones numericas import pandas as pd # para manejar datos tabulares c. creacio'n de tensores un tensor es un arreglo de nu'meros que puede tener una o varias dimensiones. - si tiene una dimensio'n, se llama vector. - con dos dimensiones se llama matriz. - con ma's de dos dimensiones, simplemente se denomina tensor de orden k. pytorch permite crear tensores ya inicializados. por ejemplo, arange(n) genera un vector con valores de 0 hasta n-1, almacenado en memoria principal y listo para operaciones en cpu. cada valor dentro del tensor se llama elemento. por ejemplo,eltensorxcreadoconarange(12)tiene12elementos. se puede inspeccionar el nu'mero total de elementos con numel() y la forma del tensor (taman˜o de cada eje) con fig. 7. pipeline t'ıpico de machine learning desde la adquisicio'n de datos shape: hastaeldesplieguedelmodelo. x = torch.arange(12, dtype=torch.float32) x.numel() # 12 elementos viii. manejoymanipulacio'ndedatoscon x.shape # (12,) pytorchypandasenpython enestaseccio'nseabordanlaste'cnicasba'sicasparaprocesar 1) redimensionamiento (reshape): el me'todo reshape y manipular datos utilizando pytorch, junto con pandas y permite cambiar la forma de un tensor sin copiar sus datos. numpy. se explicara' co'mo crear y transformar tensores, por ejemplo, un vector de 12 elementos puede transformarse realizar operaciones sobre ellos y cargar datos desde archivos en una matriz de 3 filas y 4 columnas: csv para su ana'lisis y uso en modelos de aprendizaje aux = x.reshape(3, 4) toma'tico. x # matriz de 3x4 a. anaconda y manejo de ambientes 2) tensores pre-inicializados: se pueden crear tensores anaconda es una distribucio'n que incluye lenguajes como ya con valores espec'ıficos, como ceros, unos o nu'meros python y r, junto con herramientas para gestionar paquetes aleatorios, u'tiles por ejemplo para para'metros de modelos: y entornos de forma aislada. su principal utilidad es permitir zeros = torch.zeros((2, 3, 4)) # tensor de"}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_004", "idx": 4, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "uso en modelos de aprendizaje aux = x.reshape(3, 4) toma'tico. x # matriz de 3x4 a. anaconda y manejo de ambientes 2) tensores pre-inicializados: se pueden crear tensores anaconda es una distribucio'n que incluye lenguajes como ya con valores espec'ıficos, como ceros, unos o nu'meros python y r, junto con herramientas para gestionar paquetes aleatorios, u'tiles por ejemplo para para'metros de modelos: y entornos de forma aislada. su principal utilidad es permitir zeros = torch.zeros((2, 3, 4)) # tensor de manejar dependencias en diferentes ambientes, evitando que ceros un proyecto interfiera con otro, lo cual es muy u'til en ciencia ones = torch.ones((2, 3, 4)) # tensor de de datos y machine learning. unos por ejemplo, para instalar pytorch en un ambiente es- randn = torch.randn(3, 4) # valores aleatorios pec'ıfico se puede usar: # activar el ambiente la forma 2×3×4 indica 2 bloques, cada uno con 3 filas conda activate ml # o el nombre del ambiente y 4 columnas. # instalar pytorch dentro del ambiente 3) creacio'n de tensores desde listas de python: pytorch activado permite convertir listas de python o arreglos de numpy pip install torch directamente en tensores: esto asegura que pytorch se instale u'nicamente en el a = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], ambiente seleccionado, sin afectar otros proyectos o config- [4, 3, 2, 1]], dtype=torch. uraciones. float32) a b. configuracio'n para trabajar con datos y tensores, se necesita importar estogenerauntensorde3filasy4columnasconlosvalores algunas librer'ıas clave: especificados. d. indexacio'n y segmentacio'n (slicing) h. broadcasting pytorch permite acceder a elementos, filas, columnas o el broadcasting permite realizar operaciones entre tensores submatrices de manera similar a numpy: de distintas formas, expandiendo automa'ticamente sus dimensiones sin duplicar datos: fila_ultima = a[-1] # ultima fila submatriz = a[1:3] # filas 1 y 2 a = torch.arange(3).reshape((3, 1)) # forma 3 a[1, 2] = 9 # asigna 9 al elemento en la x1 fila 1, columna 2 b = torch.arange(2).reshape((1, 2)) # forma 1 a[:2, :] = 35 # asigna 35 a todas las x2 columnas de las dos primeras filas broadcast = a + b # tensor resultante de forma 3x2 esto funciona tambie'n para tensores de ma's de dos dimensiones. i. operaciones in-place las operaciones in-place modifican directamente el tensor e. operaciones elemento a elemento original, ahorrandose memoria. esto es u'til cuando se maneja muchospara'metrosysequiereevitarcrearcopiasinnecesarias. enpytorch,lasoperacionesaritme'ticasseaplicanelemento por elemento, generando un nuevo tensor: before = id(y) # se"}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_005", "idx": 5, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "torch.arange(2).reshape((1, 2)) # forma 1 a[:2, :] = 35 # asigna 35 a todas las x2 columnas de las dos primeras filas broadcast = a + b # tensor resultante de forma 3x2 esto funciona tambie'n para tensores de ma's de dos dimensiones. i. operaciones in-place las operaciones in-place modifican directamente el tensor e. operaciones elemento a elemento original, ahorrandose memoria. esto es u'til cuando se maneja muchospara'metrosysequiereevitarcrearcopiasinnecesarias. enpytorch,lasoperacionesaritme'ticasseaplicanelemento por elemento, generando un nuevo tensor: before = id(y) # se guarda la direccion de memoria original de y x = torch.tensor([1.0, 2, 4, 8]) y = y + x # se crea un nuevo y = torch.tensor([2, 2, 2, 2]) tensor con la suma; y ahora apunta a nueva memoria add, sub, mul, div, exp = x + y, x - y, x * y, id(y) == before # false, la memoria de x / y, x ** y y cambio esto permite realizar suma, resta, multiplicacio'n, divisio'n y z = torch.zeros_like(y) # se crea un tensor z con la misma forma que y, lleno de ceros potencia de forma directa sobre cada elemento. z[:] = x + y # modifica el contenido de z directamente (in-place), f. concatenacio'n de tensores sin cambiar su direccion de memoria se pueden unir varios tensores en uno solo usando se recomienda usar in-place para eficiencia, pero con torch.cat, especificando el eje sobre el cual concatenar. cuidadosivariasvariablesapuntanalmismotensor,paraevitar por ejemplo, concatenando dos matrices a lo largo de las filas inconsistencias. (dim=0) se suman las filas, y a lo largo de las columnas (dim=1) se suman las columnas: j. conversio'n a numpy pytorch permite convertir tensores a arreglos de numpy y x = torch.arange(12, dtype=torch.float32). reshape((3,4)) viceversa, sin duplicar los datos en memoria: y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, a_np = a.numpy() # tensor a arreglo 4], [4, 3, 2, 1]]) numpy torch.cat((x, y), dim=0) # concatenar filas type(a_np) torch.cat((x, y), dim=1) # concatenar columnas a_back = torch.from_numpy(a_np) # arreglo numpy a tensor adema's, se puede construir tensores binarios mediante type(a_back) comparaciones.porejemplo,x == ygenerauntensordonde cada elemento es 1 si coincide y 0 si no: k. carga de datos desde csv x == y # comparacion elemento a elemento para trabajar con datos externos, se puede usar pandas y luego convertir a tensores de pytorch. se pueden aplicar codificacio'n *one-hot* y completar valores faltantes: g. indexacio'n lo'gica import pandas as pd se pueden crear"}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_006", "idx": 6, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "concatenar columnas a_back = torch.from_numpy(a_np) # arreglo numpy a tensor adema's, se puede construir tensores binarios mediante type(a_back) comparaciones.porejemplo,x == ygenerauntensordonde cada elemento es 1 si coincide y 0 si no: k. carga de datos desde csv x == y # comparacion elemento a elemento para trabajar con datos externos, se puede usar pandas y luego convertir a tensores de pytorch. se pueden aplicar codificacio'n *one-hot* y completar valores faltantes: g. indexacio'n lo'gica import pandas as pd se pueden crear ma'scaras booleanas para seleccionar el- import torch ementos que cumplan cierta condicio'n. por ejemplo, comdf = pd.read_csv('../data/house_tiny.csv') parando dos tensores se obtiene un tensor de valores true # leer csv o false: inputs = df.iloc[:, :2] # seleccionar mask = x == y # true donde los elementos columnas coinciden, false en caso contrario inputs = pd.get_dummies(inputs, dummy_na=true) # one-hot encoding inputs = inputs.fillna(inputs.mean()) # completar valores faltantes tensor: x_csv = torch.tensor(inputs.to_numpy(dtype= [[[ 1 2 3] float)) # convertir a tensor [ 4 5 6]] x_csv [[ 7 8 9] [10 11 12]]] ix. conceptosba'sicosdea'lgebralineal en esta seccio'n, como continuacio'n de la anterior, se pre- e. hadamard product sentan los fundamentos matema'ticos que sustentan la manip- el producto hadamard corresponde a la multiplicacio'n ulacio'n de datos. se introducen escalares, vectores, matrices elementoaelementodedosmatricesotensoresdeigualforma. y tensores, junto con sus operaciones ba'sicas. en python se puede realizar usando el operador *. a. escalar a = np.array([[1, 2], [3, 4]]) un escalar es un valor nume'rico u'nico que representa una b = np.array([[5, 6], solacantidad.enpytorchsepuederepresentarcomountensor [7, 8]]) con un solo elemento: c = a * b escalar = 6 c: b. vector [[ 5 12] un vector es un arreglo unidimensional de escalares. cada [21 32]] elemento del vector es un escalar. import numpy as np f. propiedades ba'sicas de la aritme'tica de tensores sumar o multiplicar un escalar con un tensor produce un vector = np.array([1, 2, 3]) tensordelamismaformaqueeloriginal,dondecadaelemento se ve afectado por el escalar: vector: a = 2 [1 2 3] x = torch.arange(24).reshape(2, 3, 4) a + x # suma escalar elemento a elemento c. matriz (a * x).shape # multiplicacion escalar, mantiene la forma al igual que los escalares son tensores de orden 0 y los vectores son tensores de orden 1, una matriz es un tensor de tensor([[[ 2, 3, 4, 5], orden 2. es un arreglo bidimensional de escalares. [ 6, 7, 8, 9], [10, 11, 12, 13]],"}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_007", "idx": 7, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "el escalar: vector: a = 2 [1 2 3] x = torch.arange(24).reshape(2, 3, 4) a + x # suma escalar elemento a elemento c. matriz (a * x).shape # multiplicacion escalar, mantiene la forma al igual que los escalares son tensores de orden 0 y los vectores son tensores de orden 1, una matriz es un tensor de tensor([[[ 2, 3, 4, 5], orden 2. es un arreglo bidimensional de escalares. [ 6, 7, 8, 9], [10, 11, 12, 13]], import numpy as np [[14, 15, 16, 17], [18, 19, 20, 21], matriz = np.array([[1, 2, 3], [22, 23, 24, 25]]]) [4, 5, 6], torch.size([2, 3, 4]) [7, 8, 9]]) 1) reduccio'n: sepuedensumarloselementosdeuntensor matriz: usando sum(): [[1 2 3] [4 5 6] x = torch.arange(3, dtype=torch.float32) [7 8 9]] x, x.sum() # vector [0,1,2] y su suma tensor([0., 1., 2.]), tensor(3.) d. tensor cuandosetrabajacondatosdema'sdedosdimensiones,se parauntensormultidimensional,sum()pordefectoreduce utilizan tensores. son arreglos de orden 3 o superior. todos los ejes: import numpy as np a = torch.arange(6, dtype=torch.float32). reshape(2, 3) tensor = np.array([[[1, 2, 3], [4, 5, 6]], a.shape, a.sum() # suma de todos los [[7, 8, 9], [10, 11, 12]]]) elementos torch.size([2, 3]), tensor(15.) se puede especificar un eje para sumar a lo largo de filas o columnas: a, a.sum(axis=0), a.sum(axis=1) tensor([[0., 1., 2.], [3., 4., 5.]]), tensor([3., 5., 7.]), tensor([3., 12.]) reducir a mu'ltiples ejes simulta'neamente es equivalente a sumar todos los elementos: a.sum(axis=[0,1]) == a.sum() true la media se calcula con mean(), equivalente a la suma dividida por el nu'mero de elementos: a.mean(), a.sum()/a.numel() a.mean(axis=0), a.sum(axis=0)/a.shape[0] tensor(2.5000), tensor(2.5000) tensor([1.5, 2.5, 3.5]), tensor([1.5, 2.5, 3.5]) 2) sumasinreduccio'n: sisequiereconservarlaformadel tensor tras sumar: sum_a = a.sum(axis=1, keepdims=true) a_normalized = a / sum_a # broadcasting para normalizar filas a, sum_a, a_normalized tensor([[0., 1., 2.], [3., 4., 5.]]), tensor([[ 3.], [12.]]), tensor([[0.0000, 0.3333, 0.6667], [0.2500, 0.3333, 0.4167]]) 3) sumaacumulada: sepuedecalcularlasumaacumulada con cumsum(): a.cumsum(axis=0) # suma acumulada a lo largo de las filas tensor([[0., 1., 2.], [3., 5., 7.]]) references [1] stevenpachechoportuguez,clasesobrealgebralinealymanipulacio'n de datos con pytorch, pandas y numpy, tecnolo'gico de costa rica, 2025."}
{"id_doc": "DOC_005", "segmentacion": "A", "chunk_id": "DOC_005_A_008", "idx": 8, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "[3., 5., 7.]]) references [1] stevenpachechoportuguez,clasesobrealgebralinealymanipulacio'n de datos con pytorch, pandas y numpy, tecnolo'gico de costa rica, 2025."}
{"id_doc": "DOC_006", "segmentacion": "A", "chunk_id": "DOC_006_A_000", "idx": 0, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "inteligencia artificial apuntesdeclase14/08/2025 jose pablo quesada rodr'ıguez escuela ingenier'ıa en computacio'n cartago, costa rica josepabloqr15@estudiantec.cr abstract-elpresentedocumentopretendefuncionaramanera oproponermejorasmetodolo'gicasparaquelosresultados de resumen detallado de la clase del d'ıa jueves 14/08/2025, se sean cient'ıficamente va'lidos. abarca un repaso de los temas vistos en la clase anterior a esta, - research scientist: ma's enfocado en la teor'ıa y la as'ıtambiencomoconceptosnuevosyexplicacio'ndeherramientas innovacio'n que en la aplicacio'n pra'ctica. su objetivo utiles para el curso. se busca ampliar la informacio'n mediante referencias bibliogra'ficas y adjuntar el apartado de noticias al principal es desarrollar nuevos modelos, marcos teo'ricos final del documento. o enfoques de aprendizaje automa'tico que expandan el campo. i. introduccio'n se inicia la clase con una introduccio'n a los tipos de b. ingenier'ıa aprendizaje, estos siendo un repaso ra'pido de estos ya que fueron vistos en la clase anterior - puesta en produccio'n de un modelo: bajo lo seleccionadoporeldatascientistdelmodelocient'ıfico,tomael modelo dado y busca ponerlo en produccio'n para usarios tablei tiposdeaprendizajel masivos - transformar el modelo: busca la optimizacio'n de este tipo descripcio'n supervisado conjunto de datos que tienen una etiqueta, la cual - onnx:seencargadehacertransformacionesenmodelos supervisaelaprendizajeydeterminaquetanbieno para optimizar los modelos. malseencuentraladescripcion. - mlops: debe pensar como tomar un modelo masivo y no-supervisado norequiereetiquetas,utilizaalgoritmosdemachine disponibilizarlo para los usuarios learning disen˜ados para descubrir patrones ocultos o agrupaciones de datos sin la necesidad de intervencio'nhumana iii. pipelineia semi-supervisado usa pocos datos etiquetados junto a muchos no etiquetados. auto-supervisado genera etiquetas a partir de los propios datos de entrada.(sus dataset/samples funcionan como sus propiasetiquetas) refuerzo aprende con recompensas o castigos segu'n sus acciones. few-shot generalizaapartirdepocosejemplos. one-shot aprendeconunu'nicoejemplo. zero-shot realiza tareas sin ejemplos previos, usando conocimientoprevio. ii. enfoquesamachinelearning a. ciencia - generan conocimiento: se centran en la investigacio'n y en la produccio'n de nuevo conocimiento y tecnicas especificas sobre como utilizar modelos para generar nuevo conocimiento o pulir el existente - me'tricas: se enfocan en criterios cientificos para definir la me'tricas como por ejemplo la capacidad explicativa (que' tan bien un modelo ayuda a entender un feno'meno) - data scientist: orientado a extraer conocimiento de los datos mediante experimentacio'n rigurosa, pero con una el pipeline de machine learning indica como se debe crear, visio'n pra'ctica. a menudo trabajan con datos reales y hacer y mantener una inteligencia artificial segu'n [1] la linea aplican te'cnicas existentes, pero tambie'n pueden ajustar de este pipeline es: a. conseguir data b. problemas de optimizacio'n enestaseccionseconsiguenladata,seobtienedediferentes en estos problemas se tiene una funcio'n matema'tica con formas, esta data"}
{"id_doc": "DOC_006", "segmentacion": "A", "chunk_id": "DOC_006_A_001", "idx": 1, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "a entender un feno'meno) - data scientist: orientado a extraer conocimiento de los datos mediante experimentacio'n rigurosa, pero con una el pipeline de machine learning indica como se debe crear, visio'n pra'ctica. a menudo trabajan con datos reales y hacer y mantener una inteligencia artificial segu'n [1] la linea aplican te'cnicas existentes, pero tambie'n pueden ajustar de este pipeline es: a. conseguir data b. problemas de optimizacio'n enestaseccionseconsiguenladata,seobtienedediferentes en estos problemas se tiene una funcio'n matema'tica con formas, esta data va a alimentar el modelo para su entre- un punto minimo, al encontrar ese punto minimo se tiene la namiento, sin embargo esta puede contener informacio'n inutil mejorsolucio'n,existelasolucio'nlocallacualnosdalamejor para el entrenamiento del modelo o en formatos diferentes, solucio'n en un a'rea especifica de bu'squeda (sin embargo no por lo tanto se necesita limpiar la misma la mejor) y la solucio'n global que da la mejor solucio'n en todo el espacio, (se busca encontrar esta solucio'n) b. limpiar la data(data preparation c. prediccio'n y clasificacio'n se procesa la data, buscando eliminar los datos inutiles o 1) prediccio'n: cuando se quiere determinar el valor real rellenar elementos faltantes de los mismos, tambien convertir de una funcio'n o predecir los valores de esta de acuerdo formatos, pueden llegar data de diferentes formas, sql o a las caracteristicas que tienen las muestras del data set nosql por ejemplo, todo con el objetivo de hacerlos fun- que lo componen, un ejemplo dado por el profesor fue las cionales como datos de entrenamiento, estandarizandolos. partesdelosvehiculosenlaprediccio'ndecuantocombustible consume, variables dependientes que dependen de variables c. feature engineering independientes 2) clasificacio'n: enestecasoenvezdebuscarpredecirun ayuda a definir las caracteristicas que vayan a definir valor, se busca hallar la categor'ıa a la que pertenece basado las predicciones correctas, consiste en la seleccio'n correcta en sus caracter'ısticas, ejemplo la marca del vehiculo con base de que variables considerar como relevantes para obtener a sus piezas. mejores resultados, el objetivo es proporcionar features ma's informativos y relevantes, elegir que caracteristicas conservar d. agrupamiento y que deshechar conjunto de datos no etiquetados y se busca encontrar patro'nes de estos mismos datos, aqu'ı aplica el k-means d. seleccio'n del modelo v. modelosdeterministaoestoca'stico consiste en saber identificar el problema y que modelo 1) determinista: este modelo indica que para una entrada ameritasuuso,unabuenasintesisparadefinirestoesmediante de datos retoran una salida siempre consistente, un ejemplo la caracter'ıstica de explicabilidad, si se trabaja con problemas de esto ser'ıa por ejemplo ¿hay luz"}
{"id_doc": "DOC_006", "segmentacion": "A", "chunk_id": "DOC_006_A_002", "idx": 2, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "informativos y relevantes, elegir que caracteristicas conservar d. agrupamiento y que deshechar conjunto de datos no etiquetados y se busca encontrar patro'nes de estos mismos datos, aqu'ı aplica el k-means d. seleccio'n del modelo v. modelosdeterministaoestoca'stico consiste en saber identificar el problema y que modelo 1) determinista: este modelo indica que para una entrada ameritasuuso,unabuenasintesisparadefinirestoesmediante de datos retoran una salida siempre consistente, un ejemplo la caracter'ıstica de explicabilidad, si se trabaja con problemas de esto ser'ıa por ejemplo ¿hay luz al medio d'ıa? quevananecesitarunaretroalimentacio'nclaradedondeviene 2) estoca'stico: este modelo indica que para una entrada la informacio'n, un modelo de red neuronal no sera' lo ma's puede retornar un resultado a partir de un conjunto de los indicado. posiblesresultados,muyaleatorio,unejemploser'ıa¿cualsera' el clima a medio d'ıa? e. entrenar el modelo vi. conjuntoai el modelo es entrenado con alguno de los metodos de aprendizaje vistos, teniendo en consideracio'n parametros e hiperpara'metros, siendo estos ultimos los que ayudara'n a dirigir al modelo por el camino deseado, un ejemplo de esto esk-means,el cualesunalgoritmo pararealizarclusteringen un modelo de aprendizaje no-supervisado f. validar el modelo en esta parte ya se tiene el modelo entrenado y se busca el testeo del mismo y la validacio'n, se prueba con situacio'nes similaresalasdeentrenamiento,peronoexactamenteiguales, con el objetivo de verificar que se comporte de la manera deseada y al final se realiza una validacio'n del modelo, como unaespeciedeexamenfinalysecomparaconproduccio'npara verificar que el mismo iv. paradigmaderesolucio'ndeproblemas a. problemas de bu'squeda en estos problemas, los algoritmos tratan de seguir un - inteligencia artificial: algoritmos generativos camino para llegar a una solucio'n optima, la solucio'n optima - machine learning: utiliza metodos estadisticos, como se considera siempre la opcio'n ma's \"barata\" regresio'n lineal, log'ıstica y arboles de decisio'n - deep learning: redes neuronales (utilizadas para re- x. tensores solver problemas complejos) ¿que son tensores? segu'n [2] los tensores han existido - generative ai: generacio'n de nuevo contenido uti- desde que william hamilton acun˜o' el te'rmino hace 200 an˜os lizando deep learning para describir un objeto matema'tico que representa un conjunto de nu'meros con algunas propiedades de transformacio'n. vii. iniciodematerialnuevo a. definicio'n de tensor viii. jupyternotebook untensoresunobjetomatema'ticoquegeneralizaescalares, en el curso se ha explicado que para todas las entregas vectoresymatricesenespaciosdedimensionessuperiores.es se van a utilizar jupyter notebooks debido a la facilidad que un arreglo de nu'meros y funciones que abarca magnitudes tiene esto para representar tanto celadas de texto como celdas f'ısicas, transformaciones geome'tricas y diversas entidades de co'digo. se puede usar una"}
{"id_doc": "DOC_006", "segmentacion": "A", "chunk_id": "DOC_006_A_003", "idx": 3, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "learning para describir un objeto matema'tico que representa un conjunto de nu'meros con algunas propiedades de transformacio'n. vii. iniciodematerialnuevo a. definicio'n de tensor viii. jupyternotebook untensoresunobjetomatema'ticoquegeneralizaescalares, en el curso se ha explicado que para todas las entregas vectoresymatricesenespaciosdedimensionessuperiores.es se van a utilizar jupyter notebooks debido a la facilidad que un arreglo de nu'meros y funciones que abarca magnitudes tiene esto para representar tanto celadas de texto como celdas f'ısicas, transformaciones geome'tricas y diversas entidades de co'digo. se puede usar una extensio'n de visual studio matema'ticas. code para esto existen tensores unidimensionales los cuales son llamados vectores, bidimensionales en formas de matriz y cuando k ¿ a. recomendaciones del profesor 2 ejes se deja de usar un nombre especifico y se le conoce como tensor de orden k 1) utilizar conda: conda es una herramienta que permite tener diferentes ambientes de desarrollo por separado, lo cual xi. introduccio'napytorch permite trabajar con sin errores de compatibilidad al volver a a. librerias necesarias para la manipulacio'n de tensores, proyectos antiguos arreglos y estructuras de datos tabulares ix. tutorialdeinstalacio'ndeanaconday jupyternotebook import torch import numpy as np 1) paso uno: ingresar a la pagina oficial de anaconda import pandas as pd https://anaconda.org/ y descargar la versio'n requerida de anaconda de acuerdo a su sistema operativo b. ¿como crear tensores? 2) paso dos: instalarlo y crear una nuevo ambiente la funcio'n arange (n) funciona para generar tensores prellenados con valores espaciados uniformemente, comenzando en 0 (inlcuido) hasta el n (no incluido) los tensores recien creados se almacenan en memoria principal x= torch.arange(12,dtype=torch.float32) x# tensor unidimensional que puede ser operado con diversas funciones que pueden ser invocadas sobre el 3) paso tres: seleccionar la versio'n del nuevo ambiente c. funciones sobre tensores 4) paso cuatro: instalar la extensio'n de jupyter en vscode y crear un archivo con extensio'n .ipynb - .numel() indica el numero de elementos que tiene el tensor 5) pasocinco:seleccioneenlaopcio'ndekernel,elambiente que creo en anaconda - .shape se usa para acceder al taman˜o de cada eje del tensor - .reshape se usa para reorganizar las dimensiones del tensor sin copiar los datos se puede pasar de un tensor de una dimensio'n por ejemplo el ejemplo anterior de 12 elementos, a un tensor bidimensional de 3 filas por 4 columnas - torch.zeros((z,x,y)) / torch.ones((z,x,y)) / torch.grandn(x,y) se utilizan para crear tensores de diferentes dimensiones, relleno con ceros, unos o a. recomendaciones de profesor numeros random en caso"}
{"id_doc": "DOC_006", "segmentacion": "A", "chunk_id": "DOC_006_A_004", "idx": 4, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": ".shape se usa para acceder al taman˜o de cada eje del tensor - .reshape se usa para reorganizar las dimensiones del tensor sin copiar los datos se puede pasar de un tensor de una dimensio'n por ejemplo el ejemplo anterior de 12 elementos, a un tensor bidimensional de 3 filas por 4 columnas - torch.zeros((z,x,y)) / torch.ones((z,x,y)) / torch.grandn(x,y) se utilizan para crear tensores de diferentes dimensiones, relleno con ceros, unos o a. recomendaciones de profesor numeros random en caso de que falten dependencias utilizar pip install - operaciones elemento a elemento pytorch permite opcon las dependencias requeridas, es buena idea tener un eracionesaritmeticasentretensoreslascualesseaplicara'n requirements.txt para agilizar el proceso elemento a elemento - concatenaciones de tensores mediante torch.cat((x.y),dim=k se pueden concatenar tensores, df = pd.read_csv('../data/house_tiny.csv') siendo k el eje donde sobre el que se apilara'n inputs = df.iloc[:, :2] - indexacio'n lo'gica mediante x==y siendo x y y ten- inputs = pd.get_dummies(inputs, dummy_na=true) sores se pueden realizar mascaras booleanas inputs = inputs.fillna(inputs.mean()) x_csv = torch.tensor(inputs.to_numpy(dtype= xii. creaciondesdelistasdepython float)) x_csv a= torch.tensot([[2,1,4,3], [1,2,3,4], [4,3,2,1]], dtype=torch.float32) xvii. algebralinealintroduccio'n a a. escalar tensor([[2.,1.,4.,3.], [1.,2.,3.,4.], esunvalornume'ricoquerepresentaunasitaucio'nalavez, [4.,3.,2.,1.]]) basicamente es un nu'mero, un unico elemento xiii. indexacionysegmentacio'n(slicing) b. vectores se puede pensar en un vector como un arreglo de taman˜o fila_ultima = a[-1] # ltima fila de x submatriz = a[1:3] # filas 1 y 2 de x fijo de escalares, un vector no ser'ıa ma's que un conjunto de fila_ultima, submatriz escalares xiv. broadcasting c. matrices el broadcasting en tensores permite operar tensores de sepuedevercomounarreglodearreglosocomountensor diferentes formas al expandir automaticamente, de orden 2 a = torch.arange(3).reshape((3, 1)) # forma 3 x1 d. tensores de orden superior b = torch.arange(2).reshape((1, 2)) # forma 1 x2 ya no tienen nombre espec'ıfico ma's que tensores de orden broadcast = a + b # resultado de forma 3x2 k gracias al broadcasting una propiedad util de los escalares, vectores, matrices y tensores es que las operaciones elemento a elemento xv. operacionesin-place generan resultados que tienen la misma forma que sus operandos se indica que se tiene que tener cuidado debido a que cuando se tratan con modelos de machine learning o deep learning, nos encontramos con el hecho de que se ocupa xviii. productohadamard muchisima memoria y hay que procurar ser o'ptimos en este sentidoyalhacerejecutaroperacionessepuededejarmemoria consiste en la multiplicacio'n de elemento a elemento de asignada o se apunta a nuevas secciones de memoria, lo dos matrices de un"}
{"id_doc": "DOC_006", "segmentacion": "A", "chunk_id": "DOC_006_A_005", "idx": 5, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "elemento xv. operacionesin-place generan resultados que tienen la misma forma que sus operandos se indica que se tiene que tener cuidado debido a que cuando se tratan con modelos de machine learning o deep learning, nos encontramos con el hecho de que se ocupa xviii. productohadamard muchisima memoria y hay que procurar ser o'ptimos en este sentidoyalhacerejecutaroperacionessepuededejarmemoria consiste en la multiplicacio'n de elemento a elemento de asignada o se apunta a nuevas secciones de memoria, lo dos matrices de un mismo taman˜o cual puede ser innecesario. por eso se recomienda el uso de import numpy as np operacio'nes totalmente in-place # definicin de matrices xvi. conversio'nanumpyycargadedatosdesde x = np.array([[1, 2, 3], csv [4, 5, 6]]) se puede interoperar con numpy, convirtiendo tensores a y = np.array([[7, 8, 9], arreglos y viceversa sin copiar datos [10, 11, 12]]) a_np = a.numpy() # producto de hadamard (elemento a elemento) print(type(a_np)) z = x * y a_back = torch.from_numpy(a_np) print(type(a_back)) print(z) # resultado: para cargar datos de un archivo csv usaremos pandas # [[ 7 16 27] para convertir sus columnas a tensores. adema's de usar # [40 55 72]] codificacio'n one-hot para completar valores faltantes. xix. propiedadesbasicasdelaaritmeticade a.mean(), a.sum() / a.numel() #se obtiene el tensores mismo resultado sumando o multiplicando un escalan y un tensor, producira un tensor del mismo taman˜o como el tensor original. cada elementodeeltensosessumadoomultiplicadoporelescalar. xxi. sumasinreduccio'n si se desea conservar el numero de ejes al sumar como a = 2 x = torch.arange(24).reshape(2, 3, 4) cuando se desea aprovechar el broadcasting, se usa a + x, (a * x).shape sum_a = a.sum(axis=1, keepdims=true) sum_a, sum_a.shape resultado: (tensor([[ 3.], (tensor([[[ 2, 3, 4, 5], [12.]]), [ 6, 7, 8, 9], [10, 11, 12, 13]], torch.size([2, 1])) [[14, 15, 16, 17], si se desea calcular la suma acumulada de los elementos [18, 19, 20, 21], de un tensor, se puede usar cumsum [22, 23, 24, 25]]]), torch.size([2, 3, 4])) a.cumsum(axis=0) tensor([[0., 1., 2.], xx. reduccio'n [3., 5., 7.]]) podemos realizar la suma de tensores se puede invocar sum() sin argumentos, esto hara' que se reduzca a un escalar xxii. noticiashabladasenclase a. alexander wang fundador de scale # crear una matriz a de forma (2, 3) con valores [0,1,2,3,4,5] se hablo' del caso de alexander wang un joven de solo 28 a=torch.arange(6,dtype=torch.float32).reshape an˜osquefundo' laempresascaleai,empresaporlacualmeta (2,3) invirtio' 13 mil millones de euros # mostrar la forma de a y la suma de todos"}
{"id_doc": "DOC_006", "segmentacion": "A", "chunk_id": "DOC_006_A_006", "idx": 6, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "1., 2.], xx. reduccio'n [3., 5., 7.]]) podemos realizar la suma de tensores se puede invocar sum() sin argumentos, esto hara' que se reduzca a un escalar xxii. noticiashabladasenclase a. alexander wang fundador de scale # crear una matriz a de forma (2, 3) con valores [0,1,2,3,4,5] se hablo' del caso de alexander wang un joven de solo 28 a=torch.arange(6,dtype=torch.float32).reshape an˜osquefundo' laempresascaleai,empresaporlacualmeta (2,3) invirtio' 13 mil millones de euros # mostrar la forma de a y la suma de todos sus elementos b. guerra de plataformas de llm a.shape, a.sum() semenciono' quelacompetenciaporserlamejoria,existe (torch.size([2, 3]), tensor(15.)) actualmente como si fuera una guerra entre plataformas, de la misma forma que ha ocurrido en otras cosas en el pasado, como lo reduce a lo largo de sus ejes, se puede especificar como las plataformas de streaming. alguna de sus ejes x o y para sumar a lo largo del respectivo eje usando el parametro axis references [1] s.pachecoportuguez,\"clasealgebralinealymanipulacio'ndetensores a, a.sum(axis=0), a.sum(axis=1) mediantepytorch,\"tecnolo'gicodecostarica,2025. [2] i.valchanov,\"¿que'sonlostensores?\"365datascience,2023.[online]. available:https://365datascience.com/tutorials/python-tutorials/tensor/ a--->(tensor([[0., 1., 2.], [3., 4., 5.]]) a.sum(axis=0) ---> tensor([3., 5., 7.]), a.sum(axis=1)--->tensor([ 3., 12.])) si se reduce a lo largo de todos sus ejes equivale a sumar todos los elementos de la matriz a.sum(axis=[0, 1]) == a.sum() tensor(true) la media se calcula usando mean, la cual se puede definir como la suma de todos los elementos dividido entre el total de estos a.mean(), a.sum() / a.numel() #comparacin entre ambas formas de sacar la media"}
{"id_doc": "DOC_007", "segmentacion": "A", "chunk_id": "DOC_007_A_000", "idx": 0, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "inteligencia artificial apuntesdeclase-19deagosto2025 javier rojas rojas escuela de ingenier'ıa en computacio'n tecnolo'gico de costa rica cartago, costa rica javrojas@estudiantec.cr abstract-this paper summarizes the topics covered in class on august 19, 2025. the session focused on two main areas: a reviewoflinearalgebraconcepts,withemphasisonvectors,and u=b-a=(b 1 -a 1 , b 2 -a 2 , ..., b n -a n ) an introduction to supervised learning. in the first part, vectors de esta manera, cada componente del vector se obtiene were examined in terms of their definitions, norms, distances, and operations, highlighting their importance as mathematical restandolascoordenadascorrespondientesdelpuntodeorigen tools for representing and comparing data. in the second part, a las del punto final. supervised learning was introduced through the notions of b. norma de un vector features, labels, regression, and classification. the connection between these two areas illustrates how algebraic concepts serve lanormadeunvectorrepresentasumagnitud,esdecir,que' as the foundation for machine learning techniques. tan grande es o cua'nto mide desde el punto de inicio hasta el i. introduccio'n punto final. se denota como ∥x∥. existen diferentes formas de medir la norma de un estosapuntesesta'nbasadosenlaclasedel19deagostode vector: 2025 [1]. el a'lgebra lineal es la base matema'tica de muchas 1) norma l1: distancia manhattan: la distancia mante'cnicas de inteligencia artificial. en particular, los vectores hattan entre dos puntos se calcula sumando las distancias permitenrepresentardatos,medirsimilitudesyrealizaroperaabsolutas de cada componente, desplaza'ndose u'nicamente a ciones fundamentales en espacios multidimensionales. estos lo largo de los ejes del espacio desde el punto de inicio hasta conceptos resultan esenciales para entender el aprendizaje elpuntofinal.estame'tricaseutilizaespecialmentecuandono supervisado, paradigma en el que los datos se expresan mese permiten desplazamientos negativos. diante vectores de caracter'ısticas y etiquetas asociadas. dicho enfoque permite construir modelos de regresio'n, orientados a (cid:88) n |x -y | predecir valores continuos, y de clasificacio'n, enfocados en i i asignar categor'ıas. este documento resume lo visto en clase, i=1 dondesetocaronambostemas,destacandolaconexio'nentrela teor'ıaalgebraicaysuaplicacio'nenelaprendizajeautoma'tico. ii. repasodea' lgebralineal a. ¿que' es un vector? un vector es una estructura de datos que permite organizar informacio'n de forma secuencial y que tambie'n puede interpretarse geome'tricamente. 1) caracter'ısticas: - tieneunpuntodeorigen(normalmente(0,0))yunpunto final. - posee una direccio'n espec'ıfica. - tiene una magnitud (distancia entre el punto de origen y punto final). fig.1. ejemplodedistanciamanhattan 2) vector como desplazamiento: un vector puede inter- 2) norma l2: distancia euclidiana: la distancia eupretarse como un desplazamiento entre dos puntos en un clidiana representa la distancia directa entre dos puntos y espacio n-dimensional. si se considera un punto de origen se calcula usando el"}
{"id_doc": "DOC_007", "segmentacion": "A", "chunk_id": "DOC_007_A_001", "idx": 1, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "de forma secuencial y que tambie'n puede interpretarse geome'tricamente. 1) caracter'ısticas: - tieneunpuntodeorigen(normalmente(0,0))yunpunto final. - posee una direccio'n espec'ıfica. - tiene una magnitud (distancia entre el punto de origen y punto final). fig.1. ejemplodedistanciamanhattan 2) vector como desplazamiento: un vector puede inter- 2) norma l2: distancia euclidiana: la distancia eupretarse como un desplazamiento entre dos puntos en un clidiana representa la distancia directa entre dos puntos y espacio n-dimensional. si se considera un punto de origen se calcula usando el teorema de pita'goras. para un vector a = (a 1 ,a 2 ,...,a n ) y un punto final b = (b 1 ,b 2 ,...,b n ), x=(x 1 ,x 2 ,...,x n ), su norma es: elvectorquerepresentaeldesplazamientodeaabsecalcula (cid:113) como: ∥x∥= x2+x2+...+x2 1 2 n c. propiedades de la norma - positividad: ∥x∥≥0 y ∥x∥=0 si y solo si x=0. - homogeneidad: ∥αx∥=|α|-∥x∥ para cualquier escalar α. - desigualdad triangular: ∥x+y∥≤∥x∥+∥y∥. d. vector unitario - es aquel vector que tiene una longitud o magnitud de 1. fig.3. ejemploproductopunto - se puede obtener el vector unitario de un vector u f. identidad del coseno dividiendo el vector u entre su norma. la identidad del coseno para el producto punto entre dos vectores u y v en funcio'n de sus magnitudes y el a'ngulo θ u es: uˆ = ∥u∥ u-v=∥u∥-∥v∥-cos(θ) al multiplicar este vector por un escalar, se puede hacer crecerodecrecer,osisetieneunescalarespec'ıfico,sepodr'ıa llegar al vector original. fig.4. a'nguloentredosvectoresayb[2] ¿por que' es esto importante? esta es una de las me'tricas ma's importantes para determinar la similitud entre vectores, ya que entre menor sea el a'ngulo, los vectores sera'n ma's similares. en nlp (natural language processing), esto puede ser usado para identificar que' tan similares son dos frases, transformando las frases en vectores y midiendo ese a'ngulo. 1) ejemplo de ca'lculo de a'ngulo entre dos vectores: se fig.2. ejemplodeunvectorysuvectorunitario consideran los vectores u=(1,2) y v=(3,4). paso 1: calcular el producto punto u-v=1-3+2-4=3+8=11 e. producto punto paso 2: calcular las normas es el producto interno entre dos vectores, siempre genera (cid:112) √ (cid:112) √ ∥u∥= 12+22 = 5, ∥v∥= 32+42 = 25=5 un escalar. paso 3: aplicar la identidad del coseno u-v 11 11 cos(θ)= = √ = √ ≈0.9839  y  ∥u∥-∥v∥ 5-5 5 5 1 xty∈rn = (cid:2) x 1 x 2 ... x n (cid:3)     y . . . 2    = (cid:88) n"}
{"id_doc": "DOC_007", "segmentacion": "A", "chunk_id": "DOC_007_A_002", "idx": 2, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "producto punto paso 2: calcular las normas es el producto interno entre dos vectores, siempre genera (cid:112) √ (cid:112) √ ∥u∥= 12+22 = 5, ∥v∥= 32+42 = 25=5 un escalar. paso 3: aplicar la identidad del coseno u-v 11 11 cos(θ)= = √ = √ ≈0.9839  y  ∥u∥-∥v∥ 5-5 5 5 1 xty∈rn = (cid:2) x 1 x 2 ... x n (cid:3)     y . . . 2    = (cid:88) n x i y i paso 4: calcul θ ar = el ar a' c n c g o u s l ( o 0.9839)≈10.3◦ i=1 y n g. vector co-direccional son dos vectores que siguen la misma direccio'n, pero con es especialmente u'til en inteligencia artificial, ya que se magnitudes distintas: puede usar como indicador de importancia de features, utiu=k-v lizandounvectordepesos,yalrealizarlaoperacio'nnosdar'ıa un indicador de la importancia del feature. estos al ser iguales en direccio'n, su a'ngulo es de 0◦. 1) ejemplo:vectorysuvectorunitario: seconsiderav= j. ortogonalidad (4,3) y su vector unitario u=(0.80,0.60). dos vectores x y y son ortogonales si y so'lo si: v=(4,3), u=(0.80,0.60) x-y=0 paso 1: calcular el producto punto y se denota como x⊥y. k. ortonormalidad u-v=0.80-4+0.60-3=3.2+1.8=5.0 siadema'sdeserortogonales,∥x∥=1y∥y∥=1,esdecir, paso 2: calcular las normas ambos vectores son unitarios, entonces se dice que x y y son ortonormales. (cid:112) √ √ ∥u∥= 0.802+0.602 = 0.64+0.36= 1=1 iii. supervisedlearning (cid:112) √ √ setieneundataset,dondesetieneunconjuntodeejemplos ∥v∥= 42+32 = 16+9= 25=5 que se toma de algu'n feno'meno, y unas etiquetas que nos paso 3: usar la identidad del coseno ayudara'n a realizar nuestro proceso de aprendizaje. a. feature u-v 5 cos(θ)= = =1⇒θ =cos-1(1)=0◦ - es una propiedad o atributo medible de una entidad. ∥u∥-∥v∥ 1-5 - normalmente se representa nume'ricamente para ser conclusio'n:sedemuestraqueela'nguloentredosvectores procesada co-direccionales es cero. - ejemplos: h. ¿que' ocurresicalculaelproductopuntoconsigomismo? - altura de una casa - peso de un individuo sesabequeunvectoresco-direccionalconsigomismo,por lo tanto su a'ngulo es de 0◦: - intensidad de un pixel en una imagen u-u=∥u∥-∥u∥-cos(0◦) b. vector de caracter'ısticas se denota un vector de caracter'ısticas como u-u=∥u∥2 x=(x ,x ,...,x )∈rd se puede expresar la distancia l2 en te'rminos de producto 1 2 d punto: √ donde cada x i es un feature. este vector agrupa todas las u-u=∥u∥ propiedades de un ejemplo en una sola estructura. si se tiene solamente el vector u, se puede calcular su magnitud, o hacer la operacio'n inversa. nu'merodehabitaciones metroscuadrados cantidaddejardines 2 250 1"}
{"id_doc": "DOC_007", "segmentacion": "A", "chunk_id": "DOC_007_A_003", "idx": 3, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "pixel en una imagen u-u=∥u∥-∥u∥-cos(0◦) b. vector de caracter'ısticas se denota un vector de caracter'ısticas como u-u=∥u∥2 x=(x ,x ,...,x )∈rd se puede expresar la distancia l2 en te'rminos de producto 1 2 d punto: √ donde cada x i es un feature. este vector agrupa todas las u-u=∥u∥ propiedades de un ejemplo en una sola estructura. si se tiene solamente el vector u, se puede calcular su magnitud, o hacer la operacio'n inversa. nu'merodehabitaciones metroscuadrados cantidaddejardines 2 250 1 i. ¿que' sucede con vectores que tienen a'ngulo de 90◦? cuando dos vectores forman un a'ngulo de 90◦: tablei ejemplodevectordecaracter'isticas u-v=∥u∥-∥v∥-cos(90◦)=∥u∥-∥v∥-0=0 c. label elproductopuntoenestecasoser'ıacero.sedicequeestos vectores son perpendiculares. - valor objetivo que se quiere predecir - puede ser - continuo: y ∈r (regresio'n) - discreto: y ∈ {1,...,k} (clasificacio'n, donde en el conjunto finito se representar'ıa la categor'ıa) d. dataset conjunto de datos que se utilizara'n que tiene la forma {(x ,y )}n i i i=1 nu'merodehabitaciones metroscuadrados cantidaddejardines precio 2 250 1 250000 3 350 5 650000 tableii ejemplodedataset fig.5. ejemplovectorescona'ngulode90◦ donde las primeras columnas son features y \"precio\" ser'ıa clasificacio'n? respuesta: regresio'n, ya que se esta' el label. prediciendo valores continuos de esperanza de vida. 2) basado en datos sobre animales, se dispone del peso de e. subcategor'ıas principales cada ejemplar y de si tiene alas o no. se esta' tratando 1) regresio'n: de determinar cua'les animales son pa'jaros. ¿es este - consiste en ajustar una curva o l'ınea que pase lo ma's un problema de regresio'n o clasificacio'n? respuesta: cerca posible de un conjunto de puntos de datos. clasificacio'n, ya que se esta' determinando si pertenece - se emplea para analizar tendencias, por ejemplo: a la categor'ıa \"pa'jaro\" o no. - ¿existe una relacio'n directa entre las iniciativas de 3) basado en datos sobre dispositivos informa'ticos, se marketing (anuncios en l'ınea) y las ventas reales de cuenta con el taman˜o de pantalla, el peso y el sistema un producto? operativo de varios dispositivos. se quiere determinar - ¿co'mo afecta el tiempo al valor de una crip- cua'les dispositivos son tablets, laptops o tele'fonos. ¿es tomoneda? ¿aumentara' exponencialmente su valor este un problema de regresio'n o clasificacio'n? recon el paso del tiempo? spuesta: clasificacio'n, ya que se esta' asignando dispositivos a categor'ıas discretas. 4) basado en datos meteorolo'gicos, se tiene la cantidad de precipitacio'n y un valor de humedad. se quiere determinar la humedad en diferentes e'pocas del an˜o."}
{"id_doc": "DOC_007", "segmentacion": "A", "chunk_id": "DOC_007_A_004", "idx": 4, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "producto? operativo de varios dispositivos. se quiere determinar - ¿co'mo afecta el tiempo al valor de una crip- cua'les dispositivos son tablets, laptops o tele'fonos. ¿es tomoneda? ¿aumentara' exponencialmente su valor este un problema de regresio'n o clasificacio'n? recon el paso del tiempo? spuesta: clasificacio'n, ya que se esta' asignando dispositivos a categor'ıas discretas. 4) basado en datos meteorolo'gicos, se tiene la cantidad de precipitacio'n y un valor de humedad. se quiere determinar la humedad en diferentes e'pocas del an˜o. ¿es este un problema de regresio'n o clasificacio'n? respuesta:regresio'n,yaqueseesta' prediciendovalores continuos de humedad. references [1] s.a.p.portuguez,\"apuntesdelaclasedeinteligenciaartificial,\"cartago, costarica,agosto2025,clasedel19deagostode2025. [2] f. explicadas. (2025) a'ngulo entre dos vectores. [online]. available: https://www.formulasexplicadas.com/angulo-entre-dos-vectores/ fig.6. ejemploderegresiones 2) clasificacio'n: - tienecomoobjetivopredecirlacategor'ıaoclasealaque pertenece un ejemplo, segu'n sus caracter'ıstica. - ejemplo pra'ctico: - ¿sepuededeterminarsiunveh'ıculoesunautomo'vil o un camio'n basa'ndonos en su nu'mero de ruedas, peso y velocidad ma'xima? fig.7. ejemplodeclasificacio'n iv. ejercicioscomprobatoriosdesupervised learning 1) basado en datos sobre ratas, se tiene una caracter'ıstica de esperanza de vida y una de obesidad. se esta' intentando encontrar una correlacio'n entre las dos caracter'ısticas. ¿es este un problema de regresio'n o"}
{"id_doc": "DOC_008", "segmentacion": "A", "chunk_id": "DOC_008_A_000", "idx": 0, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "repaso de álgebra lineal y aprendizaje supervisado instituto tecnológico de costa rica escuela de ingeniería en computación inteligencia artificial mariana quesada sánchez 19 de agosto de 2024 abstract-this paper reviews concepts of linear algebra necesario para trasladarse de un punto a otro. por ejemplo, relevant to artificial intelligence, including vectors, norms, el vector (4,3) puede interpretarse como la posición de un distances, dot product, orthogonality, and orthonormality. it puntoenelplanocartesiano,perotambiénpuederepresentar alsointroducestheprinciplesofsupervisedlearning,describing eldesplazamientorequeridoparapasardelorigen(0,0)hasta datasets as feature-label pairs and distinguishing between regressionandclassificationtasksthroughillustrativeexamples. dicho punto. y i. introduction el álgebra lineal es la base para representar datos en (4,3) espacios multidimensionales y para definir operaciones que 3 permiten medir magnitudes, direcciones y similitudes. estos fundamentos son indispensables en algoritmos de machine learning, en particular dentro del aprendizaje supervisado, donde los datos se representan como vectores de características asociados a etiquetas. x 4 ii. álgebralineal a. vectores un vector se define como una entidad matemática car- fig.1. representacióngráficadelvector(4,3)enelplanocartesiano. acterizada por magnitud y dirección. en espacios de dos o tres dimensiones, puede visualizarse como un segmento b. norma o magnitud orientado que parte del origen y termina en un punto (x,y,z). en espacios de dimensión n, se representa como una tupla la norma mide longitud de un vector y se denota como ordenada (x , x , ..., x ). los vectores constituyen la base ∥x∥.geométricamente,puedeinterpretarsecomoladistancia 1 2 n delarepresentacióndedatosenespaciosmultidimensionales desde el punto de origen hasta el punto final definido y permiten operaciones como la suma, la resta y la multipli- por x. de esta manera, la norma proporciona una medida cación por escalares. cuantitativa de la magnitud del vector, independientemente el desplazamiento de un vector se define como la difer- de su dirección. encia entre un punto final b = (b 1 ,b 2 ,...,b n ) y un para un vector x = (x 1 ,x 2 ,...,x n ), las normas más punto inicial a = (a 1 ,a 2 ,...,a n ). formalmente, el vector comunes son: desplazamiento se expresa como - norma l1 o manhattan: a⃗b =b-a=(b -a , b -a , ..., b -a ), la distancia manhattan entre dos puntos a = 1 1 2 2 n n (x ,y ,z ,...,n ) y b = (x ,y ,z ,...,n ) en un 1 1 1 1 2 2 2 2 lo cual indica cuánto debe recorrerse en cada componente espacio n-dimensional se calcula mediante la fórmula: parapasardeaab.porejemplo,sia=(1,2)yb =(4,6),"}
{"id_doc": "DOC_008", "segmentacion": "A", "chunk_id": "DOC_008_A_001", "idx": 1, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "formalmente, el vector comunes son: desplazamiento se expresa como - norma l1 o manhattan: a⃗b =b-a=(b -a , b -a , ..., b -a ), la distancia manhattan entre dos puntos a = 1 1 2 2 n n (x ,y ,z ,...,n ) y b = (x ,y ,z ,...,n ) en un 1 1 1 1 2 2 2 2 lo cual indica cuánto debe recorrerse en cada componente espacio n-dimensional se calcula mediante la fórmula: parapasardeaab.porejemplo,sia=(1,2)yb =(4,6), entonces a⃗b = (3,4), lo que representa un movimiento de n tres unidades en el eje x y cuatro en el eje y. (cid:88) ∥x∥ = |x -y | 1 i i es importante distinguir entre un vector de posición y i=1 un vector de desplazamiento. un vector de posición ubica un punto específico en el espacio con respecto al origen, se interpreta como la distancia recorrida siguiendo los mientrasqueunvectordedesplazamientodescribeelcambio ejes de la cuadrícula. - norma l2 o euclidiana: el producto punto entre dos vectores u y v se define de (cid:118) dos formas equivalentes: (cid:117) n ∥x∥ 2 = (cid:117) (cid:116) (cid:88) x2 i - definición algebraica: n i=1 (cid:88) u-v = u v i i correspondealadistanciaenlínearectaentreelorigen i=1 yelpuntofinal,deacuerdoconelteoremadepitágoras. ej 1. sea     1 4 x=2, y =5. 3 6 calculamos el producto punto:   4 xty = (cid:2) 1 2 3 (cid:3) 5 6 =1-4+2-5+3-6=4+10+18= 32 - definición geométrica: fig.2. comparaciónentredistanciamanhattanyeuclidiana[1]. u-v =∥u∥∥v∥cos(θ) donde θ es el ángulo entre ambos vectores. una función es considerada una norma si cumple las ej 2. sea siguientes propiedades: (cid:20) (cid:21) (cid:20) (cid:21) 1 3 1) positividad: ∥x∥ ≥ 0 y ∥x∥ = 0 si y sólo si x es el u= , v = . 2 4 vector nulo. paso 1: calcular el producto punto 2) homogeneidad:∥αx∥=|α|∥x∥paracualquierescalar α∈r. u-v =1-3+2-4=3+8=11 3) desigualdad triangular: ∥x+y∥ ≤ ∥x∥+∥y∥ para paso 2: calcular las normas todos los vectores x e y. (cid:112) √ (cid:112) √ ∥u∥= 12+22 = 5, ∥v∥= 32+42 = 25=5 c. vectores unitarios finalmente, usando la definición geométrica del producto unvectorunitarioesaquelcuyanormaesigualauno.se punto: obtiene normalizando un vector v mediante su magnitud: u= v √ 11 ∥v∥ 11= 5-5-cos(θ) ⇒ cos(θ)= √ ≈0.9839 5 5 de esta manera, u conserva la dirección de v, pero con longitud unitaria. θ =cos-1(0.9839)≈10.3◦ e. vectores codireccionales dos vectores se consideran codireccionales cuando mantienen la misma dirección, aunque difieran en magnitud. estarelaciónsecumplesiexisteunescalarktalquev =k-u."}
{"id_doc": "DOC_008", "segmentacion": "A", "chunk_id": "DOC_008_A_002", "idx": 2, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "y. (cid:112) √ (cid:112) √ ∥u∥= 12+22 = 5, ∥v∥= 32+42 = 25=5 c. vectores unitarios finalmente, usando la definición geométrica del producto unvectorunitarioesaquelcuyanormaesigualauno.se punto: obtiene normalizando un vector v mediante su magnitud: u= v √ 11 ∥v∥ 11= 5-5-cos(θ) ⇒ cos(θ)= √ ≈0.9839 5 5 de esta manera, u conserva la dirección de v, pero con longitud unitaria. θ =cos-1(0.9839)≈10.3◦ e. vectores codireccionales dos vectores se consideran codireccionales cuando mantienen la misma dirección, aunque difieran en magnitud. estarelaciónsecumplesiexisteunescalarktalquev =k-u. en este caso, el ángulo entre ambos vectores es nulo y el coseno del ángulo es igual a uno. el vector unitario es un caso particular, ya que al ser multiplicado por un escalar recupera la magnitud del vector original sin alterar su dirección. sesabequesidosvectoressoncodireccionales,elángulo entre ellos es de 0◦. en consecuencia, el producto punto se fig.3. vectorunitario.[2] expresa como d. producto punto u-u=∥u∥-∥u∥-cos(0)=∥u∥2. el producto punto entre dos vectores es la suma de las de esta forma, la norma de un vector puede escribirse multiplicaciones de sus componentes, lo que produce un como √ valor real. esta operación es fundamental en inteligencia ∥u∥= u-u. artificial, ya que un vector puede representar características asimismo, la distancia euclidiana puede expresarse en de los datos y otro vector puede representar los pesos términos de producto punto: asociados a dichas características. si un peso es cero, la √ (cid:112) característica correspondiente no contribuye al resultado. u-u= ∥u∥2 =∥u∥. f. ortogonalidad y ortonormalidad a. ejercicios de aprendizaje supervisado resueltos dosvectoressonortogonalessisuproductopuntoescero: 1) ratas: esperanza de vida vs. obesidad dado un conjunto de datos con dos características u-v =0 (esperanza de vida y obesidad) se busca modelar la relación entre ambas. además, un conjunto de vectores es ortonormal si además tipo: regresión (la salida es un valor continuo). de ser ortogonales, cada vector es unitario. 2) animales: identificar aves tomando en cuenta datos sobre animales, el peso y si iii. aprendizajesupervisado tiene alas, se desea determinar cuáles son pájaros. tipo: clasificación (se clasifica cada ejemplar como elaprendizajesupervisadoconsisteenentrenarunmodelo ave o no empleando el peso y la presencia de alas). a partir de un conjunto de datos donde cada ejemplo se 3) dispositivos: tablet, laptop o teléfono encuentra representado por un vector de características x i con tamaño de pantalla, peso y sistema operativo, se y una etiqueta asociada y . las características describen i debe asignar cada dispositivo a una de varias catepropiedadescuantificablesdelfenómenoobservado,mientras gorías. que la etiqueta"}
{"id_doc": "DOC_008", "segmentacion": "A", "chunk_id": "DOC_008_A_003", "idx": 3, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "cuáles son pájaros. tipo: clasificación (se clasifica cada ejemplar como elaprendizajesupervisadoconsisteenentrenarunmodelo ave o no empleando el peso y la presencia de alas). a partir de un conjunto de datos donde cada ejemplo se 3) dispositivos: tablet, laptop o teléfono encuentra representado por un vector de características x i con tamaño de pantalla, peso y sistema operativo, se y una etiqueta asociada y . las características describen i debe asignar cada dispositivo a una de varias catepropiedadescuantificablesdelfenómenoobservado,mientras gorías. que la etiqueta corresponde al valor que se desea predecir. tipo: clasificación. existen dos tareas principales dentro del aprendizaje su4) meteorología: precipitación → humedad. pervisado con cantidad de precipitación y un valor de humedad, la regresión busca predecir valores continuos, como el se desea predecir la humedad en distintas épocas del precio de una vivienda en función de atributos como área, año. número de habitaciones o ubicación. la fig. 4 corresponde tipo: regresión (humedad como variable continua). aunproblemaderegresiónporquebuscaajustarunafunción que modele la relación entre una variable independiente references (carat) y una variable dependiente continua (precio). [1] s.raniandg.sikka,\"recenttechniquesofclusteringoftimeseries data:asurvey,\"artificialintelligencereview,vol.46,no.1,pp.2744,2016.available:https://www.researchgate.net/figure/comparativebetween-euclidean-and-manhattan-distancef ig1332432569 [2] s. pacheco, \"repaso de matemática: álgebra lineal,\" presentación, institutotecnológicodecostarica,2025. fig.4. ejemploderegresión.[2]. la clasificación, en cambio, asigna cada instancia a una categoríadiscretaapartirdesuscaracterísticas.porejemplo, predecir el tipo de vehículo dependiendo de cuántas llantas tiene y cuánto pesa. fig.5. ejemplodeclasificación.[2]. en ambos casos, el objetivo es construir un modelo que generalice más allá de los datos de entrenamiento y que logre realizar predicciones confiables sobre ejemplos no observados."}
{"id_doc": "DOC_009", "segmentacion": "A", "chunk_id": "DOC_009_A_000", "idx": 0, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "inteligencia artificial apuntes de la clase del dia 21/08/2025 julio varela venegas-2019008041 escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica juliojvv20@estudiantec.cr resumen-this session of the artificial intelligence course iii. repasodematema'tica:a' lgebralineal began with a review of fundamental linear algebra concepts, emphasizing their relevance for data analysis and manipulation. en esta parte de la clase retomamos conceptos ba'sicos de the class then introduced the principles of supervised learning a'lgebra lineal, que son fundamentales para entender la inteliand its main characteristics. using visual studio code with a genciaartificial.comotextosdeapoyoseusaronintroduccio'n jupyternotebook,practicalexampleswerepresentedtoillustrate a la inteligencia artificial y dive into machine learning - dataset operations. the professor highlighted the advantages of applying vectorized programming over traditional iterative algebra, cap'ıtulo 2. a continuacio'n se resumen los puntos approaches,showinghowvectorizationoptimizesdataprocessing ma's importantes. and facilitates the use of linear algebra operations. index terms-inteligencia artificial, aprendizaje supervisado, a'lgebra lineal, programacio'n vectorial, jupyter notebook, iii-a. vectores y representacio'n procesamiento de datos. un vector es un objeto matema'tico con direccio'n y magnitud, definido en un espacio n-dimensional. en 2d o 3d se i. introduccio'n puede representar gra'ficamente, pero en dimensiones mayores (4d, 500d, etc.) aunque no se pueda visualizar, s'ı se puede en esta sesio'n del curso de inteligencia artificial se reforoperar matema'ticamente. zaron conceptos de a'lgebra lineal vistos en la clase anterior, los cuales son fundamentales para el manejo eficiente de datos, destacando su aplicacio'n en el contexto de modelos de aprendizaje automa'tico. luego, se hablo' sobre algunos de los conceptos clave del aprendizaje supervisado como una de las ramas centrales de la disciplina, explicando sus caracter'ısticas y objetivos. a nivel pra'ctico, la clase incluyo' el uso de visual studio code junto con jupyter notebook con codigos de ejemploenfocadosendarunademostracio'nsobrelostemasde algebra vistos en clase y como estos pueden permitir trabajar conmayoreficienciasobreundeterminadodataset.poru'ltimo se logro' evidenciar co'mo las te'cnicas vectorizadas no solo simplifican la implementacio'n, sino que tambie'n optimizan el rendimiento en tareas de procesamiento de datos. figura1. representacio'ngra'ficadeunvector. ii. noticiasycontextoreciente un vector se puede ver como un segmento con un punto inicial y uno final; la operacio'n fundamental es restar las en esta parte de la clase se discutio' brevemente la impor- coordenadas correspondientes. normalmente se asume que tanciadeasistiracharlasorganizadasporgruposestudiantiles todoslosvectorespartendelorigen(0,0)ylosejesdependen comoieee,yaqueestasactividadespermitenelcontactocon de la dimensio'n que estemos usando. profesionales del a'rea y fomentan tanto el aprendizaje complementario como la creacio'n de redes de colaboracio'n que iii-b. vectores en lenguaje natural pueden abrir oportunidades laborales en el futuro. asimismo, se menciono'"}
{"id_doc": "DOC_009", "segmentacion": "A", "chunk_id": "DOC_009_A_001", "idx": 1, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "noticiasycontextoreciente un vector se puede ver como un segmento con un punto inicial y uno final; la operacio'n fundamental es restar las en esta parte de la clase se discutio' brevemente la impor- coordenadas correspondientes. normalmente se asume que tanciadeasistiracharlasorganizadasporgruposestudiantiles todoslosvectorespartendelorigen(0,0)ylosejesdependen comoieee,yaqueestasactividadespermitenelcontactocon de la dimensio'n que estemos usando. profesionales del a'rea y fomentan tanto el aprendizaje complementario como la creacio'n de redes de colaboracio'n que iii-b. vectores en lenguaje natural pueden abrir oportunidades laborales en el futuro. asimismo, se menciono' el surgimiento de nuevas versiones de modelos enia,laspalabrassepuedenrepresentarcomovectoresen de lenguaje como gpt-5. aunque se destaco' su relevancia un espacio sema'ntico. por ejemplo, las palabras rey, reina, en el a'mbito de la inteligencia artificial, se comento' que la hombre, mujer pueden combinarse de forma vectorial para recepcio'n por parte de los usuarios no fue del todo positiva. ver relaciones: rey - hombre + mujer da un vector cercano ma's alla' de esta observacio'n, no se abordaron en detalle otras a reina. esto muestra co'mo los vectores son la base para noticias recientes durante la sesio'n. representar conceptos en ia. iii-c. magnitud y distancias iii-f. identidad del coseno lamagnitudindicaladistanciaentreelpuntoinicialyfinal la identidad del coseno mide la similitud entre vectores. del vector, calculada con la norma ∥v∥. es u'til para comparar palabras en un espacio vectorial. la distancia manhattan (l1): suma de los valores abso- fo'rmula es: lutos de las diferencias en cada eje: u-v =||u||-||v||-cos(θ) n (cid:88) d(x,y)= |x -y | i i i=1 distancia euclidiana (l2): hipotenusa del tria'ngulo formado por los vectores: (cid:118) (cid:117) n (cid:117)(cid:88) figura4. analog'ıassema'nticasusandovectores. d(x,y)=(cid:116) (x i -y i )2 i=1 interpretacio'n: - a'ngulo pequen˜o → vectores similares. - a'ngulo 0◦ → mismo vector. - a'ngulo grande → vectores lejanos, sin relacio'n. aunque se ilustre en 2d, normalmente se calcula en espacios de 500 o 1000 dimensiones. iii-f1. ejemplo de ca'lculo: para u=(1,2) y v =(3,4): 1. calcular u-v. 2. calcular ||u|| y ||v||. 3. sustituir en la fo'rmula del coseno. resultado:a'ngulo∼10,3◦ →vectorescasicodireccionales. iii-f2. vectores codireccionales: dos vectores con la misma direccio'n pero diferente magnitud cumplen u = k-v. a'ngulo 0◦, misma direccio'n. figura2. distanciaeuclidianaentredospuntos. producto punto consigo mismo: √ iii-d. propiedades de la norma u-u=||u||2 =⇒ ||u||= u-u la norma cumple varias propiedades: positividad: siempre es mayor o igual a cero. homogeneidad: escalar el vector escala tambie'n su norma. desigualdad triangular: la norma de la suma es menor o igual que la suma de las normas."}
{"id_doc": "DOC_009", "segmentacion": "A", "chunk_id": "DOC_009_A_002", "idx": 2, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "la fo'rmula del coseno. resultado:a'ngulo∼10,3◦ →vectorescasicodireccionales. iii-f2. vectores codireccionales: dos vectores con la misma direccio'n pero diferente magnitud cumplen u = k-v. a'ngulo 0◦, misma direccio'n. figura2. distanciaeuclidianaentredospuntos. producto punto consigo mismo: √ iii-d. propiedades de la norma u-u=||u||2 =⇒ ||u||= u-u la norma cumple varias propiedades: positividad: siempre es mayor o igual a cero. homogeneidad: escalar el vector escala tambie'n su norma. desigualdad triangular: la norma de la suma es menor o igual que la suma de las normas. un caso especial es el vector unitario, cuya norma es 1. normalizar un vector lo convierte en unitario y simplifica figura5. demostracio'ndeu-u=||u||2. ca'lculos. iii-f3. ortogonalidad y ortonormalidad: vectores con iii-e. producto punto a'ngulo90◦→productopunto=0→ortogonales().siadema's tienen norma 1 → ortonormales. el producto punto se define como: n (cid:88) iv. supervisedlearning x-y= x y i i i=1 en aprendizaje supervisado tenemos dos cosas importantes: features y labels. - features: propiedades medibles de una entidad(altura,peso,intensidaddep'ıxel,etc.).-labels:valor objetivo que queremos predecir. un vector de features x = (x ,x ,...) describe todas 1 2 las propiedades de un ejemplo, y la variable dependiente y depende de estas. figura3. fo'rmuladelproductopunto. regresio'n:predecirvalorescontinuos(porejemplo,precio de una casa). el resultado siempre es un escalar. en ia, x puede ser un clasificacio'n: asignar a categor'ıas discretas (tipo de vector de caracter'ısticas y y un vector de pesos, permitiendo veh'ıculo: moto, carro, camio'n). cuantificar la importancia relativa de cada feature. v-e. exploracio'n y visualizacio'n de datos - ver primeras/u'ltimas filas. - contar elementos por columna. - acceder a columnas individuales. - calcular estad'ısticas descriptivas (media, desviacio'n, cuartiles, percentiles). - graficar histogramas para ver distribuciones. v-f. clasificacio'n de nuevos samples figura6. estructuradeundatasetconfeaturesylabels. se entran 100 samples nuevos y se quiere clasificarlos usando el dataset previo. estrategia: k-nearest neighbors (knn) 1. tomar dataset v. notebook:programacio'nvectorialy de entrenamiento. 2. para cada nuevo sample: - calcular operacio'ncondatasets distancia a todos los samples de entrenamiento. - tomar los k se exploraron ventajas de operar un dataset con programa- ma's cercanos. - asignar la clase ma's frecuente entre ellos. cio'nvectorialya'lgebralinealvsusarciclostradicionales.esto notas: - permite clasificar segu'n proximidad a ejemplos mejoralaeficienciaypermitetrabajarcongrandesvolu'menes conocidos.-kesunhiperpara'metroquedefinecua'ntosvecinos de datos. considerar. - puede aplicarse a clasificacio'n o, en variantes, a regresio'n. v-a. introduccio'n a numpy vi. conclusio'n numpy permite: - arrays multidimensionales (vectores, la inteligencia artificial combina matema'ticas, programamatrices, tensores). - operaciones vectorizadas. - ca'lculos cio'n y consideraciones pra'cticas. entender los fundamentos, ra'pidos gracias a c y fortran. - funciones de a'lgebra lineal y cuidar la calidad"}
{"id_doc": "DOC_009", "segmentacion": "A", "chunk_id": "DOC_009_A_003", "idx": 3, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "ma's cercanos. - asignar la clase ma's frecuente entre ellos. cio'nvectorialya'lgebralinealvsusarciclostradicionales.esto notas: - permite clasificar segu'n proximidad a ejemplos mejoralaeficienciaypermitetrabajarcongrandesvolu'menes conocidos.-kesunhiperpara'metroquedefinecua'ntosvecinos de datos. considerar. - puede aplicarse a clasificacio'n o, en variantes, a regresio'n. v-a. introduccio'n a numpy vi. conclusio'n numpy permite: - arrays multidimensionales (vectores, la inteligencia artificial combina matema'ticas, programamatrices, tensores). - operaciones vectorizadas. - ca'lculos cio'n y consideraciones pra'cticas. entender los fundamentos, ra'pidos gracias a c y fortran. - funciones de a'lgebra lineal y cuidar la calidad de los datos y practicar con herramientas estad'ısticas.-integracio'nconpandas,matplotlib,scikit-learn. como numpy y pandas es clave para agilizar el manejo de - compatibilidad con gpu mediante bibliotecas externas. grandes cantidades de datos y la optimizacio'n de tiempos de ejecucio'n. v-b. creacio'n y manipulacio'n de arrays (pseudoco'digo) referencias - crear array 1d con nu'meros consecutivos. - crear matriz identidad 3x3. - realizar operaciones ba'sicas (suma, mul- [1] apuntes de la clase de inteligencia artificial, profesor steven andres pachecoportuguez,institutotecnolo'gicodecostarica,21dejuliode tiplicacio'n). - calcular media, desviacio'n esta'ndar y otras 2025. estad'ısticas. v-c. ca'lculo de la distancia euclidiana (pseudoco'digo) ciclos tradicionales: sumar las diferencias al cuadrado y sacar ra'ız. programacio'n vectorial: restar vectores, elevar al cuadrado, sumar y sacar ra'ız. comparacio'n: con vectores grandes, vectorial es mucho ma's ra'pido. v-d. creacio'n de datasets con pandas y numpy (pseudoco'digo) - generar 3 clases con distribucio'n normal. - asignar etiquetas 0, 1, 2. - combinar datos en un solo dataset. - crear dataframe con features y labels. - visualizar distribucio'n con gra'ficos. figura7. distribucio'ndeclaseseneldatasetgenerado."}
{"id_doc": "DOC_010", "segmentacion": "A", "chunk_id": "DOC_010_A_000", "idx": 0, "autor": "Andrés Sánchez Rojas", "fecha": "2025-08-26", "tema": "Implementación del algoritmo KNN y fundamentos de regresión lineal, incluyendo función de pérdida, descenso del gradiente y comparación entre MSE y MAE.", "texto": "1 apuntes semana 4 andrés sánchez rojas escuela de ingeniería en computación instituto tecnológico de costa rica 26/8/2025 abstract-la clase comenzó con un quiz de 4 preguntas relacionadas a la materia vista en clases anteriores, luego el ventajas: profesor nos explicó las respuestas del quiz antes de comenzar - es sencillo de implementar con la materia de la clase. durante la clase vimos el algoritmo de knn, hicimos un repaso de derivadas y pasamos a ver cómo - sirve para regresión y clasificación se construye y optimiza un modelo de regresión lineal. desventajas: - es muy costoso - features irrelevantes pueden distorcionar las distancias i. quiz - noesmuyconsistenteyaquelaclasificaciónpuedevariar 1) 1.anoteydescribalas3propiedadesdelanorma.30pts dependiendo del k usado r// a) positividad:∥x∥≥0y∥x∥=0siysolosix=0. iii. regresiónlineal b) homogeneidad: ∥αx∥=|α|∥x∥ para todo escalar método estadístico que intenta hallar la relación entre una α. variable dependiente y un conjunto de variables independic) desigualdad triangular: ∥x+y∥≤∥x∥+∥y∥. entes. 2) 2. describa los tipos de aprendizaje supervised, unsupervised y one-shot learning. 30 pts r// a) supervised: se utiliza un conjunto de datos con característicasyetiquetas.lasetiquetassirvenpara validar y corregir las aproximaciones del sistema. b) unsupervised:nohayetiquetasconlasqueevaluar o corregir, se usa en algoritmos de cluster para agrupar valores. c) one-shotlearning:seledaunejemploalmodelo y luego debe resolver un ejercicio similar fig.1. ejemploderegresiónlineal. 3) 3.siuyvsondosvectorescolinealesconmagnitudesde 5 y 6 respectivamente. ¿desarrolle cuál es el resultado del producto punto entre u y v? a. ¿qué queremos hacer? r// buscamos construir un modelo a) ∥u∥=5, ∥v∥=6. f (x)=wx+b b) u-v =∥u∥ ∥v∥ cosθ. w,b c) cos0=1 - x es un vector d-dimensional d) 5-6-1. - w es un vector d-dimensional e) u-v =30 - b es un número real 4. ¿quién propone las redes generativas adversarias - y=f w,b (x) r// ian goodfellow lo que queremos es encontrar los valores de w y b óptimos para nuestro modelo. es importante recordar que no tiene que ser perfecto (mínimo absoluto) pero debemos buscar que sea ii. k-nearestneighbors(knn) óptimo (mínimo local) para las necesidades que tengamos. se tiene un conjunto de datos tiquetados y se le quiere asignar una etiqueta a un dato basado en otros datos similares b. loss function a este. estos datos similares son los k vecinos más cercanos. esta función nos permite calcular qué tan bueno es nuestro una vez que se tiene a los vecinos más cercanos se revisa las modelo. con esta función calculamos el error cometido por el etiquetas de estos en una \"votación\" la etiqueta más común"}
{"id_doc": "DOC_010", "segmentacion": "A", "chunk_id": "DOC_010_A_001", "idx": 1, "autor": "Andrés Sánchez Rojas", "fecha": "2025-08-26", "tema": "Implementación del algoritmo KNN y fundamentos de regresión lineal, incluyendo función de pérdida, descenso del gradiente y comparación entre MSE y MAE.", "texto": "tengamos. se tiene un conjunto de datos tiquetados y se le quiere asignar una etiqueta a un dato basado en otros datos similares b. loss function a este. estos datos similares son los k vecinos más cercanos. esta función nos permite calcular qué tan bueno es nuestro una vez que se tiene a los vecinos más cercanos se revisa las modelo. con esta función calculamos el error cometido por el etiquetas de estos en una \"votación\" la etiqueta más común modelo en cada muestra. la función de pérdida penaliza más en estos k vecinos se le asigna al dato nuevo. este k es los errores grandes por el error cuadrático. un hiperparámetro y normalmente es un número impar para (cid:0) (cid:1)2 evitar empates. f (x )-y w,b i i 2 c. cost function f. descenso de gradiente el profe puso un ejemplo para explicar este concepto. eselerrorpromediodellossfunctionsobretodoeldataset. estamos en la cima de una montaña con los ojos vendados y nuestro objetivo es minimizarla ajustando los parámetros w debemosencontrarlarutamáscortaalpuntomásbajoposible. y b. si tenemos un l grande quiere decir que el modelo da el proceso para esto sería: valores muy distintos a las etiquetas. un l pequeño indica lo opuesto. - buscar la dirección de mayor pendiente hacia abajo - descender por ese camino hacia abajo n l= 1 (cid:88)(cid:0) f (x )-y (cid:1)2 - en cada paso repetimos el proceso. n w,b i i tenemos la función: i=1 x =x -α∇f(x ) nuevo antiguo t d. repaso de derivadas α es la taza de aprendizaje que es un hiperparámetro y ∇f(x ) es el gradiente o la derivada. debemos tener cuidado propiedades de las derivadas: t al definir el α. si se utiliza un α muy grande el algoritmo probablementevaasaltarseelpuntoóptimomuchasveces.un d [k]=0, α muy pequeño nos va a forzar a hacer muchas iteraciones. dx debemos pensar bien en el learning rate que se utilizará pero d [x]=1, serecomiendaquesearelativamentepequeñoparanosaltarnos dx d (cid:2) xn(cid:3) =nxn-1, e e l n p d u e n fi to ni ó r p u t n im v o al o or us ra a z r o e n l a e b a e rl d y e s l to y pp d in et g en m er et l h a o f d u . n e c s ió te n c c o u n a s n"}
{"id_doc": "DOC_010", "segmentacion": "A", "chunk_id": "DOC_010_A_002", "idx": 2, "autor": "Andrés Sánchez Rojas", "fecha": "2025-08-26", "tema": "Implementación del algoritmo KNN y fundamentos de regresión lineal, incluyendo función de pérdida, descenso del gradiente y comparación entre MSE y MAE.", "texto": "xn(cid:3) =nxn-1, e e l n p d u e n fi to ni ó r p u t n im v o al o or us ra a z r o e n l a e b a e rl d y e s l to y pp d in et g en m er et l h a o f d u . n e c s ió te n c c o u n a s n is d t o e dx d (cid:2) f(x)+g(x) (cid:3) =f′(x)+g′(x), se llega a ese valor de l. dx d (cid:2) f(x)-g(x) (cid:3) =f′(x)-g′(x), dx d (cid:2) kf(x) (cid:3) =kf′(x), dx d (cid:2) f(x)g(x) (cid:3) =f′(x)g(x)+f(x)g′(x), dx d (cid:20) f(x) (cid:21) f′(x)g(x)-f(x)g′(x) = , dx g(x) (cid:2) g(x) (cid:3)2 d (cid:2) f (cid:0) g(x) (cid:1)(cid:3) =f′(cid:0) g(x) (cid:1) - g′(x). dx ejemplo de derivada parcial: sea f(x,y)=2x+3y, fig.3. ilustracióndedescensodegradiente. ∂f al calcular , tratamos x como constante, ∂y ∂f =3 ∂y e. función convexa vs no convexa lafunciónconvexasólotieneunmínimoabsolutomientras fig.4. ejemplodedescensodegradientecondiferenteslearningrates que la no convexa puede tener múltiples mínimos locales. fig.2. ejemplodeunafunciónconvexayunanoconvexa. 3 g. ¿por qué utilizamos mse (mean squared error) y no mae(mean absolute error) - el mse penaliza más los errores grandes, el mae los penaliza de manera lineal - mae no tiene una derivada continua ya que no es derivable en 0 fig.5. ilustracióndemaevsmse references [1] stevenpachecoportuguez,clasesobreregresiónlineal,tecnológicode costarica,2025."}
{"id_doc": "DOC_011", "segmentacion": "A", "chunk_id": "DOC_011_A_000", "idx": 0, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "apuntes de clase luis felipe calderón pérez escuela de ingeniería en computación tecnológico de costa rica cartago, costa rica 2021048663 26-08-2025 resumen-este documento presenta los apuntes de la cuarta one-shot: basta con mostrarle una única vez como semana del curso de inteligencia artificial. primeramente se dan realizarlatareaparaqueelmódelopuedareproducirla. lasrespuestasdelprimerquiz.serepasalatareadeclasificación, 3. si u y v son dos vectores colineales con magnitudes 5 elalgoritmodelosk-nearestneighbors.seintroduceeltemade y 6 respectivamente.¿desarrolle cuál es el resultado del laregresiónlíneal,funcióndepérdida,mínimoslocales,mínimos globales, el descendo del gradiente. además, se repasaron las producto punto entre u y v? derivadas y se terminó con la pregunta de porque escoger mse respuesta: y no mae. indexterms-ia,derivadas,descensodelgradiente,regresión u-v =∥u∥-∥v∥-cos(θ) lineal u-v = 5- 6-cos(0) i. preguntasyrespuestasdelprimerquiz ∴u-v =30 1. anote y describa las tres propiedades de la norma 4. ¿quién propone las redes generativas adversarias?. respuesta: respuesta: ian goodfellow positividad: ∥x∥≥0 y ∥x∥=0 si y solo si x=0. ii. clase homogeneidad:∥αx∥=|α|-∥x∥paracualquierescalar ii-a. clasificación α. desigualdad triangular: ∥x+y∥≤∥x∥+∥y∥. ii-a1. k-nearest neighbors: algoritmo en donde un conjunto de datos etiquetados recibe un nuevo dato. a ese nuevo 2. describa los tipos de aprendizaje supervised, unsupervidato se le calcula la distancia con sus datos vecinos, una sed y one-shot learning. vez se encuentra a los vecinos más cercanos, se realiza una respuesta: votación,paradeterminaraquecategoriaotipopertenece.en supervised: el modelo aprende a partir de datos que este algoritmo el hiperparámetro es el k. incluyen etiquetas, las cuales sirven como referencia durante el entrenamiento. 1. ventajas unsupervised: : el modelo trabaja con datos sin eti- no requiere fase de entrenamiento. quetas y se encarga de encontrar patrones en los datos fácil de implementar. ocultos. flexible: regresión y clasificación. 2. desventajas donde w y x son vectores, y su producto punto genera poco eficiente. un escalar; b es un escalar. los valores de w y b afectan las features irrelevantes distorcionan las distancias directamentelosresultados,porloquedebemosencontrar entre los datos. los valores óptimos de ambos para obtener un modelo puede ser costoso a nivel computacional. óptimo. dependiendo del k usado, cambia la clasificación del nota: se van a trabajar desde modelos simples de regredato ingresado. sión líneal, hasta multiple linear. nota: se requiere normalización o estandarización en caso de figura1. regresiónlínealsimple que se dispare o haya gran diferencia en las distancia entre los datos. ii-b. regresión líneal es un algoritmo usado para encontrar un modelo en el conjunto de los números reales, para predecir valores contiguos. 1. existen 2 tipos de variables: variables independientes, representan los features que"}
{"id_doc": "DOC_011", "segmentacion": "A", "chunk_id": "DOC_011_A_001", "idx": 1, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "k usado, cambia la clasificación del nota: se van a trabajar desde modelos simples de regredato ingresado. sión líneal, hasta multiple linear. nota: se requiere normalización o estandarización en caso de figura1. regresiónlínealsimple que se dispare o haya gran diferencia en las distancia entre los datos. ii-b. regresión líneal es un algoritmo usado para encontrar un modelo en el conjunto de los números reales, para predecir valores contiguos. 1. existen 2 tipos de variables: variables independientes, representan los features que introducimos en el modelo. figura2. multiplelinear variables dependientes, representan las etiquetas o el objetivo que deseamos predecir. cuadroi relaciónhorasdeestudioconnotas horas de estudio (x) nota (y) 1 50 2 55 3 65 4 70 5 75 enelejemploanteriorlaregresiónlínealquecorresponde es: y =5x+45 en donde 5x representa la inclinación de la función y 45 ii-c. función de pérdida el intercept o en donde corta el eje y. lafuncióndepérdidamideelerrorcometidoporelmodelo el modelo debe cumplir con la siguiente función: en cada muestra (sample). una de sus caracteristicas es el f (x)=w-x+b errorcuadrático,quepenalizademaneramásfuerteloserrores w,b grandes.lafuncióndepérdidadeunamuestrasedenotacomo ii-e. descenso del gradiente l i =(f w,b (x i )-y i )2. se propone una analogía sobre que se esta en una montaña para evaluar el desempeño del modelo en todo el conjunto muy elevada, se tiene los ojos vendados y la meta es bajar de datos, se calcula la función de costo (cost function), que con la menor cantidad de esfuerzo y los más rápido posible. es el promedio de la función de pérdida sobre todas las n y la solución es desde el punto inicial, calcular el lado que muestras: tiene más pendiente, dar un paso y repetir ese mismo proceso hasta llegar a un punto mínimo de altura. n 1 (cid:88)(cid:0) (cid:1)2 l= f (x )-y , (1) matemáticamente, esto se formaliza mediante la regla de n w,b i i i=1 actualización: dondef (x )eslaprediccióndelmodeloparalamuestra w,b i i, y i es el valor real, y n es el número total de muestras. x nuevo =x antiguo -α∇f(x t ), si logramos minimizar l, reducimos la discrepancia entre donde α es la tasa de aprendizaje (learning rate) y ∇f(x ) t lasprediccionesdelmodeloylosvaloresreales,obteniendoasí representa el gradiente de la función en el punto x . antiguo unmejorajuste.otraformademejorarelmodeloesajustando el valor de α es crítico: un valor demasiado grande puede los valores de w y b, evitando el underfitting, o modificando provocar que el algoritmo oscile y no converja, mientras que el conjunto de"}
{"id_doc": "DOC_011", "segmentacion": "A", "chunk_id": "DOC_011_A_002", "idx": 2, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "total de muestras. x nuevo =x antiguo -α∇f(x t ), si logramos minimizar l, reducimos la discrepancia entre donde α es la tasa de aprendizaje (learning rate) y ∇f(x ) t lasprediccionesdelmodeloylosvaloresreales,obteniendoasí representa el gradiente de la función en el punto x . antiguo unmejorajuste.otraformademejorarelmodeloesajustando el valor de α es crítico: un valor demasiado grande puede los valores de w y b, evitando el underfitting, o modificando provocar que el algoritmo oscile y no converja, mientras que el conjunto de datos (dataset). un valor demasiado pequeño ocasiona una convergencia muy lenta.paramitigarestosproblemas,sesuelenemplearestrateii-d. función convexa vs no convexa gias como la búsqueda de una tasa de aprendizaje óptima o el al realizar regresiones lineales, como parte de la fórmula earlystopping,quedetieneelentrenamientocuandolafunción está elevada al cuadrado, sabemos que es posible encontrar de pérdida deja de mejorar significativamente o alcanza un una solución óptima (mínimo local). sería ideal siempre que valor aceptable. la función sea convexa, porque a diferencia de la no convexa nota: los términos derivada, pendiente y gradiente son es fácil identificar un mínimo global. equivalentes. d derivadadeunaconstante: [c]=0 dx d derivadadeunavariable: [x]=1 figura3. funciónconvexavsnoconvexa. dx d derivadadeconstanteporvariable: [c-x]=c dx d regladelapotencia: [xn]=nxn-1 dx d derivadadeunasuma: [f(x)+g(x)]=f′(x)+g′(x) dx d regladelproducto: [f(x)g(x)]=f′(x)g(x)+f(x)g′(x) dx ∂f derivadasparciales: ∂xi =derivadadefrespectoaxi ∂f ∂f ejemplodeparciales:f(x,y)=x2y+3xy2, =2xy+3y2, =x2+6xy ∂x ∂y pregunta final se concluye la clase con la siguiente pregunta, ¿porque escoger mse y no mae? respuesta: mae no es derivable en 0 y nos lleva a errores de cálculo referencias [1] s. a. p. portuguez, \"apuntes de la clase de inteligencia artificial,\" cartago,costarica,agosto2025,clasedel26deagostode2025."}
{"id_doc": "DOC_012", "segmentacion": "A", "chunk_id": "DOC_012_A_000", "idx": 0, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "inteligencia artificial apuntes del 28 de agosto de 2025 - semana 4 juan diego jiménez valverde - 2019199111 juand0908@estudiantec.cr abstract-estos apuntes resumen la clase del 28 de agosto de b. regresión lineal 2025, en la que se analizaron noticias recientes sobre modelos concepto básico de lenguaje y sus impactos socioeconómicos, así como conceptos matemáticos clave para algoritmos de aprendizaje supervisado. - busca construir un modelo estadístico lineal. se cubrieron temas como knn, regresión lineal, funciones de - la relación entre variables debe representarse como una pérdida,derivadas,descensodelgradienteysusvariantes,epochs recta. ybatches,yladiferenciaentremseymae.elresumenenfatiza larelaciónentreteoríayprácticaenlaoptimizacióndemodelos - si no es lineal, cae en otra categoría de modelos. predictivos. variables index terms-modelos de lenguaje, aprendizaje supervisado, - variable dependiente (y): valor que queremos predecir. knn, regresión lineal, mse, descenso del gradiente, optimización, epochs, batches. - variable independiente (x): valor usado para explicar/predicir. i. introducción - x: vector d-dimensional (características o features). la clase del 28 de agosto de 2025 se centró primero en - w: vector d-dimensional (pendientes/pesos). discutircómolosavancesenmodelosdelenguaje,desdesmall - b: número real (intersección con el eje y). language models hasta llms, están afectando el empleo modelo y la economía, especialmente en ocupaciones susceptibles f w,b (x)=w-x+b de automatización. luego, se abordaron los fundamentos - f(x): predicción del modelo. matemáticos que sustentan algoritmos de aprendizaje super- - w-x: producto punto → asegura que el resultado sea un visado, incluyendo knn y regresión lineal, así como las escalar. herramientas necesarias para evaluar y optimizar modelos, - interpretación: combinación lineal de características. como funciones de pérdida, derivadas, descenso del gradiente parámetros del modelo y sus variantes (batch, stochastic y mini-batch), epochs y batches, y la comparación entre mse y mae. lo que sigue - f: vector de variables independientes. son mis apuntes y reflexiones personales sobre estos temas, - w: pendientes. explicando cómo los entendí y cómo se aplican en la práctica - b: intersección con el eje y. de la modelación predictiva. - modelo parametrizado por w y b. - objetivo: encontrar valores óptimos de w y b que permiii. noticiasdeldía tan predicciones más acertadas. se mencionaron dos trabajos importantes: primero, un - óptimo ̸= perfecto, siempre existe error. artículo que argumenta que small language models (slms) - restricción: solo se pueden modificar w y b, el x es fijo pueden ser más prácticos que llms en muchos despliegues (sample). por costo y adaptabilidad; y segundo, un estudio que muestra función de pérdida efectos tempranos de la ia generativa"}
{"id_doc": "DOC_012", "segmentacion": "A", "chunk_id": "DOC_012_A_001", "idx": 1, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "- objetivo: encontrar valores óptimos de w y b que permiii. noticiasdeldía tan predicciones más acertadas. se mencionaron dos trabajos importantes: primero, un - óptimo ̸= perfecto, siempre existe error. artículo que argumenta que small language models (slms) - restricción: solo se pueden modificar w y b, el x es fijo pueden ser más prácticos que llms en muchos despliegues (sample). por costo y adaptabilidad; y segundo, un estudio que muestra función de pérdida efectos tempranos de la ia generativa en el empleo, espe- - midequétanbienomalestáfuncionandoelmodelo(qué cialmente perjudicando a jóvenes en ocupaciones altamente tan lejos están las predicciones de los valores reales). automatizables. estas observaciones nos ayudaron a contextuplot residual alizarporquélaeficienciacomputacionalylainterpretabilidad son temas relevantes hoy. [1], [2] - un residual es la diferencia entre el valor real y la predicción. iii. repaso:conceptosclave - el plot residual muestra gráficamente esas diferencias a. knn - k nearest neighbor para analizar la calidad del ajuste. knn es un algoritmo lazy (perezoso): no aprende un c. función de costo: error cuadrático medio (mse) modelo global, simplemente guarda los datos y en tiempo de definición consulta busca los k vecinos más cercanos. k: es el hiperparámetro a seleccionar. 1 (cid:88) n l= (f (x )-y )2 ventajas: sencillo, interpretable, sin entrenamiento costoso. n w,b i i desventajas: costoso en memoria y consulta; sensible a la i=1 escala de las features. conceptos clave - lossfunction:(f w,b (x i )-y i )2 midelapenalidadoerror importancia del α (learning rate) de cada ejemplo individual. - el tamaño del paso α debe ser pequeño (ejemplo: 0.1) - error cuadrático: penaliza más los errores grandes. para no sobrepasar el mínimo. - cost function: promedio de la loss function en todo el - al acercarnos al mínimo, los saltos se reducen porque el dataset;esunamedidaglobaldeldesempeñodelmodelo. gradiente disminuye. - objetivo: minimizar l ajustando los parámetros w y b. - un α muy grande puede provocar oscilaciones o incluso interpretación alejarse del mínimo. - l pequeño → mejor modelo. - un α demasiado pequeño ralentiza la convergencia. - l grande → peor modelo. nota - reducir l implica mejorar la capacidad predictiva del - el learning rate (α) es un hiperparámetro que debe modelo. seleccionarse cuidadosamente. funciones convexas vs. no convexas - convexa: garantiza un único mínimo global. - no convexa: pueden aparecer mínimos locales y globales. d. repaso de derivadas reglas básicas - f(x)=k ⇒ f′(x)=0 ejemplo: f(x)=2 ⇒ f′(x)=0 fig.1. impactodellearningrateengradientdescent - f(x)=x"}
{"id_doc": "DOC_012", "segmentacion": "A", "chunk_id": "DOC_012_A_002", "idx": 2, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "pequeño → mejor modelo. - un α demasiado pequeño ralentiza la convergencia. - l grande → peor modelo. nota - reducir l implica mejorar la capacidad predictiva del - el learning rate (α) es un hiperparámetro que debe modelo. seleccionarse cuidadosamente. funciones convexas vs. no convexas - convexa: garantiza un único mínimo global. - no convexa: pueden aparecer mínimos locales y globales. d. repaso de derivadas reglas básicas - f(x)=k ⇒ f′(x)=0 ejemplo: f(x)=2 ⇒ f′(x)=0 fig.1. impactodellearningrateengradientdescent - f(x)=x ⇒ f′(x)=1 - f(x)=kx ⇒ f′(x)=k f. ¿por qué usar mse y no mae? ejemplo: f(x)=2x ⇒ f′(x)=2 el mean squared error (mse) es más utilizado que el potencias mean absolute error (mae) en optimización con descenso - f(x)=xn ⇒ f′(x)=nxn-1 del gradiente porque: ejemplo: f(x)=x2 ⇒ f′(x)=2x - la función mse es suave (diferenciable en todos sus suma puntos), lo que permite calcular derivadas de forma - f(x)=u(x)+v(x) ⇒ f′(x)=u′(x)+v′(x) sencilla y aplicar métodos basados en gradiente. ejemplo: u(x) = 2x, v(x) = 3x ⇒ f(x) = - en contraste, la función mae no es diferenciable en 5x, f′(x)=5 0 (presenta una esquina), lo que complica el uso de derivadas directas y hace más difícil la optimización con producto gradiente puro. - f(x)=u(x)v(x) ⇒ f′(x)=u′(x)v(x)+u(x)v′(x) - gracias a su naturaleza cuadrática, el mse penaliza más constante sumada fuertemente los errores grandes, favoreciendo un ajuste - f(x)=u(x)+z ⇒ f′(x)=u′(x) más preciso en esos casos. ejemplo: f(x)=2x+5 ⇒ f′(x)=2 error cuadrático medio (mse): derivadas parciales n - sea f(x,y)=2x+3y mse = n 1 (cid:88) (f w,b (x i )-y i )2 i=1 ∂f ∂f =2, =3 ∂x ∂y e. descenso del gradiente concepto básico - la cantidad de pasos se calcula como: pendiente × α (learning rate). - ejemplo: si x=1, el gradiente es dy =2x=2. dx - para acercarnos al mínimo, nos movemos en la dirección del gradiente negativo con un paso de tamaño α. regla de actualización fig.2. mse x =x -α-(2x) nuevo antiguo error absoluto medio (mae): - donde 2x es el gradiente. n - el proceso se repite hasta que el gradiente sea 0 (punto mae = 1 (cid:88) |f (x )-y | de mínimo). n w,b i i i=1 ∂ (wx +b-y )=0+1-0=1. ∂b i i sustituyendo (forma idéntica a la de tu imagen, antes de simplificar): ∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) -1. ∂b i i forma final por muestra: fig.3. mae ∂ ∂"}
{"id_doc": "DOC_012", "segmentacion": "A", "chunk_id": "DOC_012_A_003", "idx": 3, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "x =x -α-(2x) nuevo antiguo error absoluto medio (mae): - donde 2x es el gradiente. n - el proceso se repite hasta que el gradiente sea 0 (punto mae = 1 (cid:88) |f (x )-y | de mínimo). n w,b i i i=1 ∂ (wx +b-y )=0+1-0=1. ∂b i i sustituyendo (forma idéntica a la de tu imagen, antes de simplificar): ∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) -1. ∂b i i forma final por muestra: fig.3. mae ∂ ∂ ℓ b i =2 (cid:0) (wx i +b)-y i (cid:1) . iv. materialdeclase sumando y normalizando para el mse completo: a. derivadasdelafuncióndepérdida(mse)-notacióncon n n ∂/∂ ∂l = 1 (cid:88)∂ℓ i = 2 (cid:88)(cid:0) (wx +b)-y (cid:1) . ∂b n ∂b n i i recordemos la función: i=1 i=1 n 1 (cid:88)(cid:0) (cid:1)2 nota: l= (wx +b)-y . n i i - observaquelasexpresionespormuestracoincidenconlo i=1 que aparece en tu imagen: para w aparece el factor extra derivada por muestra (desglose) - respecto a w: para la x (porque (wx )′=x ), y para b queda solo 2((wx + contribución de la muestra i: i i i i b)-y ) (porque la derivada de b es 1). i ℓ i = (cid:0) (wx i +b)-y i (cid:1)2 . - estas son las cantidades que se usan en la regla de actualización por descenso de gradiente: aplicando la regla de la cadena con derivadas parciales: ∂l ∂l ∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) - ∂ (cid:0) wx +b-y (cid:1) . w ←w-α ∂w , b←b-α ∂b . ∂w i i ∂w i i b. epochs, batches y tipos de descenso por gradiente desglose término a término en la parte interior: epoch: ∂ ∂ ∂ ∂ ∂w (wx i +b-y i )= ∂w (wx i )+ ∂w (b)- ∂w (y i ), - una epoch es una iteración completa sobre todo el conjunto de entrenamiento. ∂ ∂ ∂ (wx )=x , (b)=0, (y )=0, - es un hiperparámetro (p. ej. epochs = 5). ∂w i i ∂w ∂w i - ejemplo: si hay 10000 samples y ejecutamos 5 epochs, ∂ recorremos los 10000 samples 5 veces en total. (wx +b-y )=x . ∂w i i i - podemos aplicar el descenso del gradiente al finalizar un sustituyendo: epoch (actualizaciones por epoch) o antes (por batches). batch: ∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) x . ∂w i i i - un batch es un"}
{"id_doc": "DOC_012", "segmentacion": "A", "chunk_id": "DOC_012_A_004", "idx": 4, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "- es un hiperparámetro (p. ej. epochs = 5). ∂w i i ∂w ∂w i - ejemplo: si hay 10000 samples y ejecutamos 5 epochs, ∂ recorremos los 10000 samples 5 veces en total. (wx +b-y )=x . ∂w i i i - podemos aplicar el descenso del gradiente al finalizar un sustituyendo: epoch (actualizaciones por epoch) o antes (por batches). batch: ∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) x . ∂w i i i - un batch es un subconjunto del conjunto de entrenamiento usado para calcular la gradiente y actualizar sumando sobre las muestras y normalizando: parámetros. ∂l = 1 (cid:88) n ∂ℓ i = 2 (cid:88) n (cid:0) (wx +b)-y (cid:1) x . - e ne je c m es p it l a o n : 1 1 0 00 b 0 at 0 ch s e a s m p p a l r e a s c y om b p a l t e c t h ar s 1 ize epo = ch 1 . 000 ⇒ se ∂w n ∂w n i i i i=1 i=1 - no se espera a procesar todo el dataset: cada partición (batch) sirve para calcular la gradiente y actualizar los derivada por muestra (desglose) - respecto a b: para la parámetros. contribución de la muestra i: - cada vez que procesamos un batch actualizamos los (cid:0) (cid:1)2 ℓ = (wx +b)-y . parámetros (o acumulamos gradientes según la estratei i i gia). regla de la cadena (forma no simplificada): tipos de descenso por gradiente: ∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) - ∂ (cid:0) wx +b-y (cid:1) . a) batch gradient descent (vanilla): ∂b i i ∂b i i - calcula la gradiente usando todo el dataset: ∇l = desglose término a término en la parte interior: 1 (cid:80)n .... n i=1 ∂ ∂ ∂ ∂ - actualización cuando se ha procesado el conjunto com- (wx +b-y )= (wx )+ (b)- (y ), ∂b i i ∂b i ∂b ∂b i pleto. ∂ ∂ ∂ - ventajas: gradiente estable, pasos consistentes. ∂b (wx i )=0, ∂b (b)=1, ∂b (y i )=0, - desventajas: - requiere tener todo el dataset en memoria. - ayuda a evitar mínimos locales y aporta robustez en - en datasets grandes, las actualizaciones son lentas la optimización. (cada paso es costoso). - desventajas: - gradiente muy estable puede ocultar señales útiles"}
{"id_doc": "DOC_012", "segmentacion": "A", "chunk_id": "DOC_012_A_005", "idx": 5, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "(wx +b-y )= (wx )+ (b)- (y ), ∂b i i ∂b i ∂b ∂b i pleto. ∂ ∂ ∂ - ventajas: gradiente estable, pasos consistentes. ∂b (wx i )=0, ∂b (b)=1, ∂b (y i )=0, - desventajas: - requiere tener todo el dataset en memoria. - ayuda a evitar mínimos locales y aporta robustez en - en datasets grandes, las actualizaciones son lentas la optimización. (cada paso es costoso). - desventajas: - gradiente muy estable puede ocultar señales útiles - introduce un hiperparámetro adicional: batch size. y hacer que el proceso converja a parámetros no - hay que elegir el tamaño del batch cuidadosamente deseados según la topología (según el problema). (trade-off entre estabilidad y velocidad). fig.4. batchgradientdescent fig.6. mini-batchgradientdescent b) stochastic gradient descent (sgd): v. comentariosprácticosytareas - actualizalosparámetrosporcadasampledeltrainingset se mencionó que se asignará una tarea práctica: imple- (o mezcla aleatoria de samples). mentar (solo con numpy) un pipeline de regresión lineal que - ventajas: detecta rápidamente si el algoritmo puede con- incluya: verger; útil para datasets muy grandes. 1) exploración visual del dataset. - desventajas: 2) ingeniería simple de features (transformaciones no lin- - señales de gradiente ruidosas (alto ruido en las eales cuando aplique). actualizaciones). 3) implementación de mse y pasos de descenso (batch / - muchas actualizaciones (computacionalmente cosmini-batch). toso si no se optimiza). esoayudaaentenderporquéalgunasfuncionesnosonsmooth - la trayectoria del parámetro es muy oscilatoria: y cómo afecta a las derivadas y la optimización. ∂l w ←w-α ∂w vi. conclusión en esta clase se consolidó la comprensión de conceptos ejecutado por muestra puede producir movimientos fundamentales para implementar algoritmos de aprendizaje muy erráticos. supervisado de manera eficiente y correcta. se destacó la relevancia de elegir adecuadamente funciones de pérdida, hiperparámetroscomoellearningrateylaestrategiadeactualización de gradientes, así como la importancia de comprender la teoría detrás de knn y regresión lineal. asimismo, los apuntes reflejan la relación entre teoría y práctica, preparando al estudiante para aplicar estos conceptos en tareas concretas y proyectos de programación. references [1] belcaketal.,\"smalllanguagemodelsarethefutureofagenticai\", nvidiaresearch,2025.https://arxiv.org/abs/2506.02153 fig.5. stochasticgradientdescent [2] e.brynjolfsson,a.chandar,yz.chen,\"canariesinthecoalmine?six facts about the recent employment effects of artificial intelligence\", c) mini-batch gradient descent: stanford digital economy lab, 2025. https://digitaleconomy.stanford. edu/publications/canaries-in-the-coal-mine/ - combina ambas estrategias: se calcula la gradiente sobre batches de tamaño intermedio. - ventajas: - reduce el ruido respecto a sgd (más estable) y es más eficiente que batch gd. - mejora la explotación de hardware (vectorización, gpus)."}
{"id_doc": "DOC_012", "segmentacion": "A", "chunk_id": "DOC_012_A_006", "idx": 6, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "de programación. references [1] belcaketal.,\"smalllanguagemodelsarethefutureofagenticai\", nvidiaresearch,2025.https://arxiv.org/abs/2506.02153 fig.5. stochasticgradientdescent [2] e.brynjolfsson,a.chandar,yz.chen,\"canariesinthecoalmine?six facts about the recent employment effects of artificial intelligence\", c) mini-batch gradient descent: stanford digital economy lab, 2025. https://digitaleconomy.stanford. edu/publications/canaries-in-the-coal-mine/ - combina ambas estrategias: se calcula la gradiente sobre batches de tamaño intermedio. - ventajas: - reduce el ruido respecto a sgd (más estable) y es más eficiente que batch gd. - mejora la explotación de hardware (vectorización, gpus)."}
{"id_doc": "DOC_013", "segmentacion": "A", "chunk_id": "DOC_013_A_000", "idx": 0, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "apuntes semana 4 clase #2 28/08/2025 alex steven naranjo masis instituto tecnolo'gico de costa rica cartago, costa rica email: alnaranjo@estudiantec.cr resumen-para esta clase se repasaron temas de la clase b. regresio'n lineal anteriorcomolosonknn,regresio'nlineal,meansquareerror, descensodelgradienteyunrepasogeneraldederivadas.yluego lo que queremos hacer es encontrar la l'ınea que mejor se del repaso continuamos viendo temas como lo son: derivadas ajuste a los datos, para poder realizar una prediccio'n de un parciales con respecto a w y b en la funcio'n de pe'rdida con valor. el fin de actualizarlos y ajustar la funcio'n, y por u'ltimo vimos epoch y batch. b1. variables: index terms-knn, regresio'n lineal, mean square error, variables independientes: son las caracter'ısticas de la mae, descenso del gradiente, epoch y batch muestra. i. noticasdelasemana variables dependientes: es el valor a predecir y es afectada por las varibales independientes a. small language models are the future of agentic ai con esto lo que queremos hacer es encontrar un modelo enelart'ıculosedicequelosmodelosdelenguajepequen˜os estad'ıstico lineal: f (x)=wx+b (slms) son ma's adecuados que los grandes (llms) para w,b donde: ciertossistemasinteligentesauto'nomos(agenticai),especialmente en tareas especializadas y repetitivas. [1] x es un vector d-dimensional. w es un vector d-dimensional. b. canaries in the coal mine? six facts about the recent b un nu'mero real. employment effects of artificial intelligence wx es un producto punto, da'ndonos como resultado un el estudio analiza co'mo la adopcio'n de la inteligencia escalar. artificialgenerativahaafectadoalmercadolaboralenee.uu., el modelo esta' parametrizado por w y b, por lo que utilizando datos administrativos mensuales de no'minas de debemos encontrar los valores o'ptimos de w y b que hara'n adp,elmayorprocesadordeno'minasdelpa'ıs,elcualabarca quelafuncio'nrealicelasprediccionesma'sprecisas.peroojo, millonesdetrabajadoresendecenasdemilesdeempresas.[2] optimo̸=perfecto ii. repasoclaseanterior a. k nearest neighbor (knn) en resumen, cuando obtenemos una nueva instancia, medimos contra todos los elementos del dataset, y tomamos las distancias ma's cercanas, y en base a eso determina'bamos la clase de la nueva instancia. contamos con el hiperpara'metro k. es un algoritmo de lazy learning, porque realmente no se aprende de los datos. a1. ventajas: sencillo de implementar. es flexible: aplica tanto para regresion como clasificafigura1. tiposderegresio'n cio'n. a2. desventajas: c. funcio'n de pe'rdida las caracter'ısticas irrelevantes pueden distorsionar las distancias necesitamos de un me'todo que' nos permita cuantificar que' es computacionalmente costoso. tan bien se ajusta nuestro modelo a los datos. funcio'n de poco eficiente en grandes volu'menes de datos. pe'rdida = medida del error del modelo d. error cuadra'tico medio (mse) es el resultado del modelo contra la etiqueta. sumamos todos los errores"}
{"id_doc": "DOC_013", "segmentacion": "A", "chunk_id": "DOC_013_A_001", "idx": 1, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "sencillo de implementar. es flexible: aplica tanto para regresion como clasificafigura1. tiposderegresio'n cio'n. a2. desventajas: c. funcio'n de pe'rdida las caracter'ısticas irrelevantes pueden distorsionar las distancias necesitamos de un me'todo que' nos permita cuantificar que' es computacionalmente costoso. tan bien se ajusta nuestro modelo a los datos. funcio'n de poco eficiente en grandes volu'menes de datos. pe'rdida = medida del error del modelo d. error cuadra'tico medio (mse) es el resultado del modelo contra la etiqueta. sumamos todos los errores de los samples y lo promediamos. n l= 1 (cid:88) (f (x )-y )2 n w,b i i i=1 d1. conceptos clave): figura2. comparacio'ndedistintosvaloresparaalpha loss function: (f (x )-y )2 es la medida de penaw,b i i lidad que cuantifica el error de cada ejemplo. iii. contenidodelaclase error cuadra'tico: penaliza los errores grandes. a. funcio'n de pe'rdida y sus derivadas parciales costfunction:eselpromediodelalossfunctionsobre todo el dataset. para optimizar los para'metros w y b de nuestro modelo, objetivo: minimizar l para ajustar los parametros w,b. necesitamos actualizar sus valores de manera que la funcio'n de pe'rdida se minimice. para esto, evaluamos co'mo cada el motivo por el cual queremos minimizar l, es porque para'metro afecta la pe'rdida utilizando derivadas parciales con entre menor sea l, significa que tenemos un mejor modelo, y respecto a w y b. entre ma's grande significa que tenemos un peor modelo. considerando la funcio'n de pe'rdida basada en el error cuadra'ticomedio(mse)paranuestromodelolinealf (x)= w,b e. ¿por que' mse y no mae? wx+b, tenemos: esdebidoaque' escuadra'tica,yestonosaseguraquevamos n a tener un punto m'ınimo. y ta'mbien es porque la funcio'n no l(w,b)= 1 (cid:88) ((wx +b)-y )2 mae no es smooth, por lo que no nos va a permitir obtener n i i i=1 las derivadas en todos los puntos, lo que induce a errores de las derivadas parciales de l con respecto a w y b se ca'lculo calculan como: f. derivadas generales n ∂l 2 (cid:88) = ((wx +b)-y )x ∂w n i i i regla funcio'nf(x) derivadaf′(x) i=1 constante k 0 n identidad x 1 ∂l 2 (cid:88) = ((wx +b)-y ) constantemultiplicativa kx k ∂b n i i potencia xn nxn-1 i=1 suma u(x)+v(x) u′(x)+v′(x) estas derivadas nos indican la direccio'n y magnitud del producto u(x)v(x) u′(x)v(x)+u(x)v′(x) constantesumada u(x)+z u′(x) ajuste necesario para cada para'metro, permitiendo aplicar derivadasparciales f(x,y)=2x+3y ∂f =2,∂f =3 algoritmos de optimizacio'n como el gradient descent para ∂x ∂y actualizar w y b. cuadroi"}
{"id_doc": "DOC_013", "segmentacion": "A", "chunk_id": "DOC_013_A_002", "idx": 2, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": ")x ∂w n i i i regla funcio'nf(x) derivadaf′(x) i=1 constante k 0 n identidad x 1 ∂l 2 (cid:88) = ((wx +b)-y ) constantemultiplicativa kx k ∂b n i i potencia xn nxn-1 i=1 suma u(x)+v(x) u′(x)+v′(x) estas derivadas nos indican la direccio'n y magnitud del producto u(x)v(x) u′(x)v(x)+u(x)v′(x) constantesumada u(x)+z u′(x) ajuste necesario para cada para'metro, permitiendo aplicar derivadasparciales f(x,y)=2x+3y ∂f =2,∂f =3 algoritmos de optimizacio'n como el gradient descent para ∂x ∂y actualizar w y b. cuadroi repasodederivadasba'sicas b. epoch unaepochesunaiteracio'ncompletasobretodoelconjunto deentrenamiento.esunhiperpara'metroquedefinecua'ntasveg. descenso del gradiente ces se recorrera' el dataset completo durante el entrenamiento, por ejemplo, epochs = 5. el descenso del gradiente es un algoritmo iterativo de opti- si tenemos 10 000 muestras y ejecutamos 5 epochs, signimizacio'n para encontrar el m'ınimo de una funcio'n. funciona fica que se procesara'n todas las muestras 5 veces en total. la actualizando repetidamente los para'metros en la direccio'n actualizacio'n de los para'metros puede realizarse al finalizar opuesta al gradiente de la funcio'n de costo. cada epoch o de manera ma's frecuente utilizando batches. g1. regla de actualizacio'n: c. batch x nuevo =x antiguo -α-(2x) un batch es un subconjunto del conjunto de entrenamiento que se utiliza para calcular la gradiente y actualizar los g2. importancia del α: es el learning rate, debe ser para'metros del modelo. pequen˜o para no pasarnos del punto m'ınimo. este es un por ejemplo, si tenemos 10 000 muestras y un batch hiperpara'metro size = 1 000, necesitaremos 10 batches para completar una epoch. cada batch permite calcular la gradiente y actua- c3. mini-batch gradient descent: el mini-batch gralizar los para'metros sin esperar a procesar todo el dataset. dient descent combina las estrategias anteriores: calcula la dependiendo de la estrategia, se puede actualizar los para'me- gradiente sobre batches de taman˜o intermedio. tros despue's de cada batch o acumular gradientes antes de la ventajas: actualizacio'n. reduce el ruido respecto a sgd y es ma's estable. c1. batch gradient descent (vanilla): el batch gra- ma's eficiente que batch gd. dient descent calcula la gradiente utilizando todo el dataset: mejora la explotacio'n de hardware (vectorizacio'n, gpus). n 1 (cid:88) ∂l ∇l= n ∂θ i i=1 y actualiza los para'metros solo despue's de procesar el conjunto completo. ventajas: gradiente estable y pasos consistentes. ayuda a evitar m'ınimos locales y aporta robustez en la optimizacio'n. figura5. mini-batchgradientdescent desventajas: requiere todo el dataset en memoria. referencias las actualizaciones son lentas para"}
{"id_doc": "DOC_013", "segmentacion": "A", "chunk_id": "DOC_013_A_003", "idx": 3, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "batch gradient descent (vanilla): el batch gra- ma's eficiente que batch gd. dient descent calcula la gradiente utilizando todo el dataset: mejora la explotacio'n de hardware (vectorizacio'n, gpus). n 1 (cid:88) ∂l ∇l= n ∂θ i i=1 y actualiza los para'metros solo despue's de procesar el conjunto completo. ventajas: gradiente estable y pasos consistentes. ayuda a evitar m'ınimos locales y aporta robustez en la optimizacio'n. figura5. mini-batchgradientdescent desventajas: requiere todo el dataset en memoria. referencias las actualizaciones son lentas para datasets grandes. [1] belcak, p., et al, \"small language models are the future of agentic la gradiente muy estable puede ocultar sen˜ales u'tiles. ai\"2025. [2] e.brynjolfssonetal.,\"canariesinthecoalmine?sixfactsaboutthe recentemploymenteffectsofartificialintelligence\"2025. figura3. batchgradientdescent c2. stochastic gradient descent (sgd): el stochastic gradient descent actualiza los para'metros despue's de cada muestra del dataset (o un pequen˜o conjunto aleatorio de muestras). ventajas: detecta ra'pidamente si el algoritmo puede converger. u'til para datasets muy grandes. desventajas: las actualizaciones pueden ser muy ruidosas. la trayectoria de los para'metros es oscilatoria. muchasactualizacionespuedensercostosascomputacionalmente. ∂l w ←w-α ∂w figura4. stochasticgradientdescent"}
{"id_doc": "DOC_014", "segmentacion": "A", "chunk_id": "DOC_014_A_000", "idx": 0, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "repaso de derivadas, regresio'n lineal y sesgo-varianza en aprendizaje supervisado ian murillo campos instituto tecnolo'gico de costa rica escuela de ingenier'ıa en computacio'n inteligencia artificial gr 2 abstract-this paper reviews key elements of supervised posterior a esto se aplica el algoritmo del decenso learning.itintroducestheuseofpartialderivativesandgradient del gradiente, el cual permite optimizar la posicio'n redescent to optimize the mean squared error function. it also specto a w y b representado por la siguiente fo'rmula: examines issues in linear regression such as nonlinearity and outliers,describingstatisticalmethodstoaddressthem.finally,it outlinesdatasetpartitioningintotraining,validation,andtesting sets,andexplainsthebias-variancetradeoffasatooltoevaluate model generalization. i. introduction el aprendizaje supervisado entrena modelos predictivos a partir de ejemplos con etiquetas. las derivadas parciales permiten calcular la influencia de cada para'metro sobre la b. vocabulario funcio'n de pe'rdida y se aplican en el descenso de gradiente. - epoch:todaslasiteracionesquehacemossobretodoslos en la regresio'n lineal, los problemas comunes incluyen la samples.esunhiperparametroquemidetodoelrecorrido no linealidad de la relacio'n entre variables y la presencia de de inicio a fin de todo mi set de entrenamiento outliers, que pueden corregirse con te'cnicas estad'ısticas. - batch: tomar ciertos subconjuntos del epoch. funciona la divisio'n de datos en entrenamiento, validacio'n y prueba para la optimizacio'n de las pruebas. permitemedirlacapacidaddegeneralizacio'ndelmodelo.este ana'lisisserelacionaconelsesgoylavarianza,cuyoequilibrio iii. potencialesproblemasalrealizarregresio'n evita tanto el sobreajuste como el subajuste. lineal entrelospotencialesproblemasquenospodemosencontrar ii. repasodederivadas esta'n la no linealidad de la relacio'n respuesta predictor, los a. funcio'n de pe'rdida (mse) datos sobresalientes y la colinealidad. deesteu'ltimonosevaahablarenlaclase,quedacomotema de investigacio'n personal. a. no linealidad de la relacio'n respuesta predictor uno de los principales supuestos de la regresio'n lineal es se busca optimizar esta funcio'n con valores que aumentan que existe una relacio'n lineal entre las variables predictoras y l, siendo l una parabola. dicho de otro modo, se busca la variable respuesta. encontrar la pendiente de algu'n punto de la parabola en el 1) ¿que' ocurre si la verdadera relacio'n no es lineal?: que nos ubiquemos y buscamos descender sobre la funcio'n para llegar a su punto m'ınimo. - el modelo lineal no podra' captar adecuadamente la relacio'n. se busca calcular cuanto influye el valor w sobre el valor l, calculando sus derivadas parciales. - se obtendra'n errores sistema'ticos en los residuos, definidos como: esto deja como resultado lo siguiente: ∂l 1 (cid:88) n (cid:0) (cid:1) e i =y i -yˆ i = 2 (wx +b)-y -x ∂w n i i i donde i=1 y tambien se tiene que realizar el procedimiento derivando con i base en el bias, dando el siguiente resultado: es"}
{"id_doc": "DOC_014", "segmentacion": "A", "chunk_id": "DOC_014_A_001", "idx": 1, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "captar adecuadamente la relacio'n. se busca calcular cuanto influye el valor w sobre el valor l, calculando sus derivadas parciales. - se obtendra'n errores sistema'ticos en los residuos, definidos como: esto deja como resultado lo siguiente: ∂l 1 (cid:88) n (cid:0) (cid:1) e i =y i -yˆ i = 2 (wx +b)-y -x ∂w n i i i donde i=1 y tambien se tiene que realizar el procedimiento derivando con i base en el bias, dando el siguiente resultado: es el valor real y yˆ n i ∂l 1 (cid:88) (cid:0) (cid:1) = 2 (wx +b)-y ∂b n i i es la prediccio'n del modelo. i=1 esta'n ma's alejados, provocando que el modelo busque hacer un \"trade off\" entre los datos. esto no esta' bien ya que el modelo quedar'ıa sesgado por los outliers y no se utilizar'ıan correctamente los datos que si quiero buscar predecir. estos datos en su mayoria son ocasionados por errores de captura, y la forma de corregirlos puede ser eliminarlos directamente del dataset. otras te'cnicas pueden ser: - standardized residuals: escalar el residuo crufo por una desviacio'n esta'ndar global de los errores. donde: fig.1. ejemplodegra'ficaderegresio'nlineal. - e =y -yˆ es el residuo crudo i i i - n=nu'mero de observaciones enlafigura1sevealaizquierdaunplotresidualquemuestra - p=nu'mero de para'metros estimados en el modelo (incluye el intercepto) la fiderencia entre los puntos que tenia que predecir y cuanto la razo'n de utilizar una desviacio'n estandar es que se alejan entre si, el modelo de plot residual ma's correcto es teniendotodoestandarizado,sepuedesaberqueaciertas el que este' ma's cercano al cero. desviaciones estandar de la media se encuentra un poren la figura de la derecha es el arreglo a los datos, utilizando centajedelosdatos.conestosepuededefinirunumbral te'cnicas para que una funcio'n cuadratica como la de la donde los datos son sobresalientes. izquierda se comporte ma's como una funcio'n lineal. - regla del rango intercuart'ılico: utilizar directamente los la forma de solucinarlo es extender el modelo lineal incorpodatos en lugar del modelo, tomamos el rango intercuanrando transformaciones po'linomicas del predictor, con eso: tilico que existe entre todos los datos, por definicio'n se - aunque la relacio'n es no lineal en los datos, el modelo ve de la siguiente forma: sigue siendo lineal en los para'metros. - se puede resolver con regresio'n lineal esta'ndar. iqr=q 3 -q 1 b. outliers la regla para detectar los outliers es la siguiente: son datos que se salen de la distribucio'n que se"}
{"id_doc": "DOC_014", "segmentacion": "A", "chunk_id": "DOC_014_A_002", "idx": 2, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "en lugar del modelo, tomamos el rango intercuanrando transformaciones po'linomicas del predictor, con eso: tilico que existe entre todos los datos, por definicio'n se - aunque la relacio'n es no lineal en los datos, el modelo ve de la siguiente forma: sigue siendo lineal en los para'metros. - se puede resolver con regresio'n lineal esta'ndar. iqr=q 3 -q 1 b. outliers la regla para detectar los outliers es la siguiente: son datos que se salen de la distribucio'n que se trata de (cid:2) (cid:3) q -1.5-iqr, q +1.5-iqr predecir. 1 3 al tratar de entrenar el modelo de ia, este se va a centrar - valores <q -1.5-iqr ⇒ outliers inferiores. 1 - valores >q +1.5-iqr ⇒ outliers superiores. 3 en la figura 3 se puede ver de forma gra'fica la regla del rango intercuart'ılico. nota: el 1.5 * iqr es aproximadamente equivalente a fig.3. ejemplodeoutlier. 2-2.7 desviaciones esta'ndar de la media (depende de la forma de la distribucio'n). - winsorizacio'n: te'cnica que reemplaza los valores extremos por percentiles l'ımite, en lugar de eliminarlos. fig.2. ejemplodeoutlier. el procedimiento es el siguiente: tanto en los modelos cercanos a la linea como a los que - elegir percentiles de corte (ej. 5% y 95%). - valores menores al percentil 5 se reemplazan por el midiendo el rendimiento del modelo real'ısticamente simuvalor del percentil 5. lando datos que nunca ha vistohaciendo posible la com- - valores mayores al percentil 95 se reemplazan por paracio'n. el valor del percentil 95. 1) caso overfitting: una solucio'n es dividir ptra parte de dentro de sus ventajas se encuentra que: los datos en datos de validacio'n, que se ejecuten como tests - conserva el taman˜o de la muestra. cada cierto tiempo durante la etapa de entrenamiento y que - reduce la influencia de valores extremos. esos datos aseguren que no se da un overfitting. iv. sesgoyvarianza a. dataset losdatossedividenentredatosdeentrenamientoypruebas, como se ve en la figura 4, los datos de entrenamiento son con los que se optimiza el modelo de ia, mientras que los de pruebas son para verificacio'n, una divisio'n de 80% y 20% es lo ma's comu'n. d. validation set son un conjunto de datos que sirven para valorar la capacidad de generalizacio'nde mi modelo a datos nunca vistos, este set de datos brinda resultados con los que puedo tomar fig.4. ejemplodedataset. decisiones sobre el proceso de entrenamiento y es un set esencial para el ajuste"}
{"id_doc": "DOC_014", "segmentacion": "A", "chunk_id": "DOC_014_A_003", "idx": 3, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "datos de entrenamiento son con los que se optimiza el modelo de ia, mientras que los de pruebas son para verificacio'n, una divisio'n de 80% y 20% es lo ma's comu'n. d. validation set son un conjunto de datos que sirven para valorar la capacidad de generalizacio'nde mi modelo a datos nunca vistos, este set de datos brinda resultados con los que puedo tomar fig.4. ejemplodedataset. decisiones sobre el proceso de entrenamiento y es un set esencial para el ajuste de hiperpara'metros. b. training set se utiliza para ajustar el modelo ajustando los para'metros e. te'cnicas para subdividir el dataset de acuerdo a las muestras disponibles. el modelo identifica patrones basado en estos datos, ya que 1) random sampling: se divide aleatoriamente el dataset, estos deber'ıan representar la diversidad de escenarios que se es u'til para datos con clases balanceados ya que no se agrega esperaencontrar.deestaformapermitira' almodeloentrenado ningu'n sesgo al momento de hacer la divisio'n. predecirdatosnuncavistosantesyencontrarpatronesentrelas los datos imbalanceados pueden producir validation o testing entradasysalidas.porlotanto,sirveparaestablecerrelaciones sets con menos datos o ninguno, de las clases menos repreentre las variables y los pesos o para'metros del modelo. sentadas. este debe ser duficientemente grande para que sea significa2) stratifiedsampling: seutilizaparadatosimbalanceados tivo, pero sin causar overfitting. el overfitting ocurre cuando ya que asegura una representacio'n de todas las clases en cada los datos son muy especializados y adaptados al conjunto de split. entrenamiento por lo que el modelo se vuelve incapaz de mantiene la misma distribucio'n de datos para cada clase en generalizar adecuadamente. cada subconjunto, lo que da un modelo ma's robusto. c. testing set 3) k-fold cross-validation: se divide el subconjunto en k se utiliza para evaluar el modelo con ejemplos que no se partes y el modelo se entrena con k-1 partes ya que una se utilizaron en el entrenamiento. reserva para validacio'n. debe ser independiente del set de entrenamiento. secontinuaesteprocesorotandolossubconjuntosusadospara simula la aplicacio'n de un examen a nuestro modelo y con el el entrenamiento y validacio'n. permite tomar el promedio del se calculan me'tricas como : accuracy, loss, etc... rendimiento del modelo y es u'til cuando tenemos pocos datos el objetivo de este set es crear un modelo que generalice y deseamos validar nuestro modelo. se puede ver de forma adecuadamente todos los escenarios. ma's gra'fica en la siguiente imagen: f. posibles escenarios dentro de los posibles escenarios de estos me'todos se encueentran: - bajo error en training, bajo error"}
{"id_doc": "DOC_014", "segmentacion": "A", "chunk_id": "DOC_014_A_004", "idx": 4, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "con el el entrenamiento y validacio'n. permite tomar el promedio del se calculan me'tricas como : accuracy, loss, etc... rendimiento del modelo y es u'til cuando tenemos pocos datos el objetivo de este set es crear un modelo que generalice y deseamos validar nuestro modelo. se puede ver de forma adecuadamente todos los escenarios. ma's gra'fica en la siguiente imagen: f. posibles escenarios dentro de los posibles escenarios de estos me'todos se encueentran: - bajo error en training, bajo error en testing. otro escenario es el siguiente: - escenario ideal. - alto error en training, alto error en testing. - modelo evita el ruido existente en los datos. - underfitting. - puede generalizar correctamente. - el modelo no esta' aprendiendo nada de los datos. - modelo muy simple. visualmente se puede ver de la siguiente forma: - alto sesgo visualmente se ve de la siguiente forma: otro escenario es cuando se tiene lo siguiente: - bajo error en training, alto error en testing. para solucionar este u'ltimo caso se utiliza un bias-variance - overfitting. tradeoff. - no es capaz de generalizar. v. bias-variancetradeoff - alta varianza. se busca un modelo que tenga baja varianza y visualmente se ve de la siguiente forma: bajo sesgo, para eso se editan valores en las pruebas, visualmente se ve un arreglo de la siguiente forma: references [1] s. pacheco, \"repaso de matema'tica: a'lgebra lineal,\" presentacio'n, institutotecnolo'gicodecostarica,2025. [2] s.pacheco,\"sesgoyvarianza,\"presentacio'n,institutotecnolo'gicode costarica,2025."}
{"id_doc": "DOC_015", "segmentacion": "A", "chunk_id": "DOC_015_A_000", "idx": 0, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "apuntes semana 5 clase #1s eder vega suazo escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica ic-6200 - inteligencia artificial gr2 resumen-este documento es un resumen de la clase de ii. desaf'iosenmodeladopredictivo inteligencia artificial correspondiente a la semana 5, enfocando en los fundamentos del aprendizaje supervisado. se abordan ii-a. relaciones no lineales entre variables temas clave como la optimizacio'n de modelos mediante ca'lculo la regresio'n lineal presume una relacio'n lineal entre prediferencial y el algoritmo de descenso de gradiente aplicado a la dictoresyvariablerespuesta.cuandoestasuposicio'nseviola, funcio'ndeerrorcuadra'ticomedio.adema's,seexaminandesaf'ıos comunes en el modelado predictivo, incluyendo el manejo de el modelo resulta inadecuado y muestra patrones sistema'ticos relaciones no lineales entre variables y la deteccio'n de valores en los residuos: at'ıpicos.tambiendiscutenestrategiasparalaevaluacio'ndemoe =y -yˆ delos mediante particio'n de datasets y se analiza el compromiso i i i entre sesgo y varianza, crucial para desarrollar modelos con la solucio'n implica transformar las variables predictoras capacidad de generalizacio'n efectiva. mediante expansio'n polinomial o otras transformaciones que i. optimizacio'nmedianteca'lculodiferencial permitancapturarrelacionesnolinealesmanteniendolalineai-a. funcio'n de error cuadra'tico medio lidad en los para'metros. en problemas de regresio'n, la funcio'n de costo ma's comu'n esta' dada por: n l= 1 (cid:88) (f (x )-y )2, i=1,...,n n w,b i i i=1 donde h (x ) representa la prediccio'n del modelo para la θ i instancia i-e'sima. el proceso de optimizacio'n busca minimizar esta funcio'n mediante el ca'lculo de gradientes: n ∂l 1 (cid:88) = 2((wx +b)-y )-x figura 1: ejemplo de relacio'n no lineal y su ajuste mediante ∂w n i i i transformacio'n polinomial. i=1 n ∂l 1 (cid:88) = 2((wx +b)-y ) ∂b n i i ii-b. manejo de valores at'ıpicos i=1 i-b. algoritmo de descenso de gradiente las observaciones extremas pueden distorsionar significativamentelosmodelosderegresio'n.existenmu'ltiplesenfoques la actualizacio'n de para'metros se realiza de forma iterativa para su identificacio'n y tratamiento: mediante: w(t+1) =w(t)-α ∂l ii-b1. identificacio'n de valores at'ıpicos: b(t+1) =b(t)-α ∂ ∂ w l (t) r de e s s v id ia u c o io' s n e e s s t t a a' n nd d a a r ri d z e ad lo o s s: re z s i idu = os. σ ei e donde σ e es la ∂b(t) rango intercuart'ılico: valores fuera de [q - 1,5 - 1 donde α representa la tasa de aprendizaje que controla la iqr,q +1,5-iqr] se consideran at'ıpicos. 3 magnitud de cada actualizacio'n. ii-b2. te'cnicas de tratamiento:"}
{"id_doc": "DOC_015", "segmentacion": "A", "chunk_id": "DOC_015_A_001", "idx": 1, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "ia u c o io' s n e e s s t t a a' n nd d a a r ri d z e ad lo o s s: re z s i idu = os. σ ei e donde σ e es la ∂b(t) rango intercuart'ılico: valores fuera de [q - 1,5 - 1 donde α representa la tasa de aprendizaje que controla la iqr,q +1,5-iqr] se consideran at'ıpicos. 3 magnitud de cada actualizacio'n. ii-b2. te'cnicas de tratamiento: i-c. terminolog'ıa fundamental eliminacio'n:removerobservacionesidentificadascomo e'poca(epoch):ciclocompletodepresentacio'ndetodos at'ıpicas. los ejemplos de entrenamiento al modelo. winsorizacio'n: reemplazar valores extremos por perlote (batch): subconjunto de ejemplos utilizados para centiles espec'ıficos (ej. percentil 5 y 95). calcular una actualizacio'n de para'metros. transformaciones: aplicar funciones como logaritmo tasa de aprendizaje: hyperpara'metro que determina la o ra'ız cuadrada para reducir la influencia de valores velocidad de convergencia del algoritmo. extremos. cuadro ii: caracter'ısticas de modelos con sesgo o varianza elevados me'trica altosesgo altavarianza errorentrenamiento alto bajo errorvalidacio'n alto alto comportamiento subajuste sobreajuste soluciones modelos ma's comple- regularizacio'n, ma's jos datos figura2:efectodevaloresat'ıpicosenunmodeloderegresio'n iv. sesgoyvarianza lineal. iv-a. diagno'stico de problemas comunes iv-b. estrategias de mejora iii. evaluacio'nyvalidacio'ndemodelos para alto sesgo: aumentar la complejidad del modelo, agregar caracter'ısticas adicionales o reducir regularizaiii-a. particio'n de datasets cio'n. para alta varianza: aumentar datos de entrenamiento, la divisio'n adecuada de los datos es crucial para evaluar la aplicar te'cnicas de regularizacio'n o reducir la complejicapacidad de generalizacio'n: dad del modelo. compromiso o'ptimo: seleccionar la complejidad del cuadro i: propo'sitos de los diferentes subconjuntos de datos modelo que minimice el error de generalizacio'n. subconjunto propo'sito entrenamiento ajuste de para'metros del modelo mediante optimizacio'n validacio'n seleccio'n de hyperpara'metros y monitorizacio'n del sobreajuste prueba evaluacio'n final del rendimiento con datos nunca vistos iii-b. te'cnicas de muestreo 1. muestreoaleatorio:divisio'nrandomizadaquepreserva la distribucio'n original de los datos. 2. muestreo estratificado: mantiene la proporcio'n de clasesencadaparticio'n,crucialparadatosdesbalanceados. 3. validacio'n cruzada: divide los datos en k particiones y realiza k iteraciones de entrenamiento/validacio'n. figura 4: relacio'n entre complejidad del modelo y error de generalizacio'n. v. conclusiones la efectividad de los modelos de aprendizaje supervisado dependecr'ıticamentedelaadecuadaoptimizacio'ndepara'metros, el manejo de relaciones complejas entre variables, la identificacio'n y tratamiento de valores at'ıpicos, y la evaluacio'n rigurosa mediante te'cnicas de validacio'n apropiadas. el entendimiento del compromiso entre sesgo y varianza permite desarrollar modelos que generalizan efectivamente a nuevos datos, balanceando complejidad y capacidad predictiva. figura 3: esquema de validacio'n cruzada con k =5 particioreferencias nes. [1]"}
{"id_doc": "DOC_015", "segmentacion": "A", "chunk_id": "DOC_015_A_002", "idx": 2, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "entrenamiento/validacio'n. figura 4: relacio'n entre complejidad del modelo y error de generalizacio'n. v. conclusiones la efectividad de los modelos de aprendizaje supervisado dependecr'ıticamentedelaadecuadaoptimizacio'ndepara'metros, el manejo de relaciones complejas entre variables, la identificacio'n y tratamiento de valores at'ıpicos, y la evaluacio'n rigurosa mediante te'cnicas de validacio'n apropiadas. el entendimiento del compromiso entre sesgo y varianza permite desarrollar modelos que generalizan efectivamente a nuevos datos, balanceando complejidad y capacidad predictiva. figura 3: esquema de validacio'n cruzada con k =5 particioreferencias nes. [1] apuntes de la clase de inteligencia artificial, profesor s. pacheco, institutotecnolo'gicodecostarica,2025."}
{"id_doc": "DOC_016", "segmentacion": "A", "chunk_id": "DOC_016_A_000", "idx": 0, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "inteligencia artificial apuntes semana 5, clase #2 luis fernando benavides villegas instituto tecnolo'gico de costa rica cartago, costa rica lubenavides@estudiantec.cr abstract-este documento recopila los apuntes de la clase del patrones sistema'ticos (por ejemplo, en forma de para'bola) en jueves 04 de septiembre de 2025 para el curso de inteligencia lugar de distribuirse de manera aleatoria. esto indica que el artificial. se repasan conceptos clave de regresio'n lineal y sus modelo lineal no es adecuado. para resolverlo, una opcio'n es limitaciones, as'ı como los problemas de overfitting y underfitting. aplicar feature engineering, agregando te'rminos polino'micos tambie'n se describen te'cnicas de subdivisio'n de datasets y estrategias para mejorar la capacidad de generalizacio'n de los que transformen las variables originales y permitan que la modelos.finalmente,seintroducelaregresio'nlog'ısticacomoun relacio'nseaproximemejoraunaformalineal.deestamanera, modelo de clasificacio'n binaria, explicando la funcio'n sigmoide, aunque la relacio'n real sea curva, el modelo puede ajustarse suderivadayelprocesodeoptimizacio'ndepara'metrosmediante con menor error. descenso del gradiente. 2) datos sobresalientes: surgen por ruido, errores de indexterms-inteligenciaartificial,regresio'nlineal,regresio'n log'ıstica,funcio'nsigmoide,overfitting,underfitting,optimizacio'n medicio'n o datos at'ıpicos y pueden afectar el ajuste del modelo. una forma de tratarlos es estandarizar los residuos dividiendoentreladesviacio'nesta'ndar.unaveznormalizados, i. noticiasdelasemana se mide cua'ntas desviaciones esta'ndar se aleja cada dato. si un dato esta' muy lejos (ma's de 2 o 3 desviaciones esta'ndar), a. ingenieeer'ıa costa rica se considera sobresaliente. otras te'cnicas que vimos fueron el un evento de ingenier'ıa que organiza ieee costa rica. rangointercuart'ılico,queeselqueseusaengra'ficosdecaja habra' charlas de profesores distinguidos en diversas a'reas y ybigotes,ylawinsorizacio'n,dondeenvezdeeliminardatos participacio'n de empresas. [1] at'ıpicos se reemplazan por valores en percentiles l'ımite. 3) colinealidad: sedacuandodosoma'spredictoresesta'n b. referencias falsas en ia altamente correlacionados entre s'ı. esto hace dif'ıcil separar las\"alucinaciones\"eninteligenciaartificialsoncuandolos el efecto de cada variable en la prediccio'n, afectando la modelos generan referencias aparentemente va'lidas pero que estabilidad de los coeficientes del modelo. en consecuencia, en realidad no existen. esto fue debatido en el grupo parma los para'metros estimados se vuelven poco confiables y muy del tec y se resalto' la importancia de siempre verificar las sensibles a cambios en los datos. para detectarla, se pueden fuentes. la responsabilidad recae en el usuario de confirmar usar medidas como el vif (variance inflation factor). una la veracidad de la informacio'n antes de tomarla como cierta. solucio'n comu'n es eliminar variables redundantes o aplicar te'cnicas de regularizacio'n. c. google nano banana b. dataset el nuevo modelo de google enfocado en la edicio'n de ima'genes que se llama nano banana. a diferencia de otros es el conjunto completo de"}
{"id_doc": "DOC_016", "segmentacion": "A", "chunk_id": "DOC_016_A_001", "idx": 1, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "sensibles a cambios en los datos. para detectarla, se pueden fuentes. la responsabilidad recae en el usuario de confirmar usar medidas como el vif (variance inflation factor). una la veracidad de la informacio'n antes de tomarla como cierta. solucio'n comu'n es eliminar variables redundantes o aplicar te'cnicas de regularizacio'n. c. google nano banana b. dataset el nuevo modelo de google enfocado en la edicio'n de ima'genes que se llama nano banana. a diferencia de otros es el conjunto completo de datos disponibles para entrenar generadores que recrean la imagen completa desde cero, este y evaluar un modelo. normalmente se subdivide en diferentes modelo conserva mejor los detalles originales y el contexto. partesparapodermedirlacapacidaddegeneralizacio'nyevitar as'ı,aleditarunafotomantienelacoherenciaentreiteraciones. problemas como el overfitting. se hablo' tambie'n de posibles sesgos en los modelos, al notar que repeticiones en ima'genes de personas modificaban rasgos c. training set hacia un perfil ma's latino. subconjunto usado para entrenar el modelo y ajustar sus para'metros. es donde el algoritmo aprende los patrones preii. repasodelaclaseanterior sentes en los datos. a. potenciales problemas al aplicar una regresio'n lineal d. validation set 1) no linealidad: un supuesto de la regresio'n lineal es que la relacio'n entre las variables predictoras y la variable re- subconjunto usado durante el entrenamiento para evaluar spuestaeslineal.cuandonosecumple,losresiduosmuestran el rendimiento intermedio del modelo. sirve para medir si lo aprendido se generaliza a datos no vistos y para ajustar hiperpara'metros.permitedetectarproblemasdesobreajustede manera temprana sin necesidad de esperar a la prueba final. e. te'cnicas para subdividir el dataset 1) random sampling: consiste en dividir aleatoriamente los datos entre entrenamiento y prueba. es adecuado cuando lasclasesesta'nbalanceadas,yaquegarantizarepresentatividad sin introducir sesgos. el problema surge si las clases esta'n desbalanceadas, porque puede que un subconjunto quede con muy pocos o incluso sin ejemplos de alguna clase. fig.4. ej.deregresio'ndeunderfitfig.3. ej.deunderfitting 2) stratified sampling: se usa cuando las clases esta'n ting desbalanceadas. mantiene la misma proporcio'n de clases en los conjuntos de entrenamiento y prueba. de esta forma, si en 3) caso ideal: el error en training es bajo y tambie'n lo el dataset original una clase representa el 90% y otra el 10%, es en validation. el modelo logra ajustarse a los datos sin esa relacio'n se conserva en las divisiones. sobreajustarsealruidoypuedegeneralizarbienaejemplosno 3) k-fold cross-validation: el conjunto de entrenamiento vistos. representa un buen equilibrio entre sesgo y varianza. se divide en k partes (folds). en cada iteracio'n se usa k -1 folds para entrenar y el fold restante para"}
{"id_doc": "DOC_016", "segmentacion": "A", "chunk_id": "DOC_016_A_002", "idx": 2, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "3) caso ideal: el error en training es bajo y tambie'n lo el dataset original una clase representa el 90% y otra el 10%, es en validation. el modelo logra ajustarse a los datos sin esa relacio'n se conserva en las divisiones. sobreajustarsealruidoypuedegeneralizarbienaejemplosno 3) k-fold cross-validation: el conjunto de entrenamiento vistos. representa un buen equilibrio entre sesgo y varianza. se divide en k partes (folds). en cada iteracio'n se usa k -1 folds para entrenar y el fold restante para validar. el proceso se repite k veces, rotando el fold de validacio'n. esto permite aprovechar mejor los datos disponibles y obtener una evaluacio'n ma's robusta del modelo. f. posibles escenarios de comportamiento de training y validation 1) overfitting: el error en training es bajo pero el error en validation comienza a aumentar despue's de cierto punto. el modelo memoriza los datos de entrenamiento en lugar de aprenderpatronesgenerales.secapturatambie'nelruidodelos fig.5. ej.delcasoideal fig.6. ej.deregresio'ndelcasoideal datos,loqueprovocaquenopuedageneralizar.secaracteriza por tener alta varianza. 4) bias-variance tradeoff: es un caso su'per raro porque el error en training es alto pero el error en validation es bajo. si sucede, puede deberse a errores de ca'lculo o valores mal tomados, no a un aprendizaje real del modelo. iii. altobias se presenta cuando el modelo es demasiado simple y no logra capturar el patro'n real de los datos, provocando underfitting. tanto el error en training como en validation son altos, ya que el modelo asume demasiado sobre la forma de los datos. fig.1. ej.deoverfitting fig.2. ej.deregresio'ndeoverfitting a. causas - modelo demasiado simple (ej. lineal para datos con una te'cnica para evitarlo es el early stopping, que consiste relaciones cuadra'ticas). en detener el entrenamiento en la e'poca donde el error de - no se utilizan todas las variables relevantes. validacio'n empieza a empeorar. - los features disponibles no son buenos predictores de la 2) underfitting: tanto el error en training como en val- variable objetivo. idation son altos. el modelo no logra aprender patrones de b. posibles soluciones los datos porque es demasiado simple o incorrecto para el problema. se caracteriza por alto sesgo, es decir, asume una - incrementar la complejidad del modelo (por ejemplo, forma equivocada de los datos (por ejemplo, usar un modelo pasar de lineal a cuadra'tico o a un modelo ma's flexible). lineal para datos con comportamiento cuadra'tico). - incorporar ma's features o transformar los existentes. - sustituirorecolectarmejoresfeaturesquerepresentende manera adecuada el problema. iv. altavarianza se presenta cuando"}
{"id_doc": "DOC_016", "segmentacion": "A", "chunk_id": "DOC_016_A_003", "idx": 3, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "aprender patrones de b. posibles soluciones los datos porque es demasiado simple o incorrecto para el problema. se caracteriza por alto sesgo, es decir, asume una - incrementar la complejidad del modelo (por ejemplo, forma equivocada de los datos (por ejemplo, usar un modelo pasar de lineal a cuadra'tico o a un modelo ma's flexible). lineal para datos con comportamiento cuadra'tico). - incorporar ma's features o transformar los existentes. - sustituirorecolectarmejoresfeaturesquerepresentende manera adecuada el problema. iv. altavarianza se presenta cuando el modelo se ajusta demasiado a los datos de entrenamiento pero falla al generalizar en el conjuntodevalidacio'n.estoprovocaoverfitting,dondepequen˜as variaciones en los datos de entrada pueden generar malas predicciones. fig.7. regresio'nlinealvslog'ıstica a. causas b. distribucio'n de bernoulli - el modelo es demasiado complejo y aprende patrones irrelevantes o ruido. cada etiqueta y es una variable aleatoria que sigue una i - exceso de dimensionalidad: agregar muchas variables distribucio'n de bernoulli. la probabilidad de que ocurra el aumenta el riesgo de overfitting. evento (y =1) o no ocurra (y =0) se define como: - muy pocos ejemplos en el conjunto de entrenamiento, especialmente en problemas con clases desbalanceadas. p(y =k)=pk(1-p)1-k, k ∈{0,1} b. posibles soluciones donde: - reducirlacomplejidaddelmodelo(ej.usarmenoscapas - p es la probabilidad de e'xito (y =1). o un modelo ma's simple). - k es la etiqueta observada (0 o 1). - disminuir la dimensionalidad eliminando variables irrelas'ı, si k = 1, la probabilidad es p; y si k = 0, la evantes. probabilidad es 1-p. - obtener ma's ejemplos de entrenamiento para mejorar la representacio'n de todas las clases. vi. funcio'nsigmoide - aplicar te'cnicas de regularizacio'n que penalizan la complejidad del modelo, como: lafuncio'nsigmoideesunaherramientaclaveporqueintroducenolinealidadalmodeloytieneunrangodesalidaentre - l1 y l2 (penalizacio'n sobre los para'metros). 0 y 1, lo cual la hace ideal para trabajar con probabilidades. - dropout (apagar ciertas neuronas durante el entrenamiento). se define como: 1 σ(x)= v. regresio'nlog'istica 1+e-x al tomar valores de entrada muy negativos, la salida se aunque su nombre incluya \"regresio'n\", la regresio'n acerca a 0; mientras que con valores grandes y positivos, se log'ıstica es un modelo de clasificacio'n, no de regresio'n. acerca a 1. se utiliza principalmente para problemas binarios, donde las etiquetas y toman los valores 0 o 1. a. diferencia con la regresio'n lineal - en regresio'n lineal se predicen valores continuos en los reales (r). - en regresio'n log'ıstica se predice la probabilidad de pertenecer a una clase u otra. el resultado final"}
{"id_doc": "DOC_016", "segmentacion": "A", "chunk_id": "DOC_016_A_004", "idx": 4, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "nombre incluya \"regresio'n\", la regresio'n acerca a 0; mientras que con valores grandes y positivos, se log'ıstica es un modelo de clasificacio'n, no de regresio'n. acerca a 1. se utiliza principalmente para problemas binarios, donde las etiquetas y toman los valores 0 o 1. a. diferencia con la regresio'n lineal - en regresio'n lineal se predicen valores continuos en los reales (r). - en regresio'n log'ıstica se predice la probabilidad de pertenecer a una clase u otra. el resultado final es una clasificacio'n: 0 o 1. por ejemplo, con una variable como el taman˜o de una calabaza: fig.8. gra'ficadelafuncio'nsigmoide - regresio'nlineal:prediceelprecioaproximadoenvalores reales. adema's,elargumentoxpuedesercualquiervaloroincluso - regresio'n log'ıstica: predice si la calabaza es naranja (1) otrafuncio'n(composicio'ndefunciones),loquedaflexibilidad o no lo es (0). para modelar relaciones ma's complejas. la idea es tomar la salida de un modelo lineal y conver- en regresio'n lineal usamos el error cuadra'tico medio tirla en una probabilidad. si partimos de una funcio'n lineal (mse), pero en clasificacio'n esto deja de ser u'til, porque ya f (x)=wx+b,alaplicarlelafuncio'nsigmoideobtenemos: no predecimos valores continuos, sino probabilidades. w,b el procedimiento general sigue siendo el mismo: 1 yˆ=σ(f (x))= w,b 1+e-(wx+b) - definimos una funcio'n de pe'rdida l apropiada para probabilidades. de esta forma: - calculamos sus derivadas respecto a w y b. - si yˆ<0.5, se clasifica como 0. - usamos esas derivadas en el algoritmo de descenso del - si yˆ≥0.5, se clasifica como 1. gradiente,iterandosobrelosdatosdeentrenamientopara ir actualizando los para'metros y minimizar la pe'rdida. esto convierte la regresio'n log'ıstica en un modelo de clasificacio'n binaria. queremos hacerlo as'ı porque calcular c. derivada de la funcio'n sigmoide una funcio'n lineal es simple computacionalmente, es un buen me'todo para mantener la relacio'n entre variables y pesos y 1 permite modelar problemas con mayor complejidad. σ(x)= 1+e-x a. diagrama computacional de la regresio'n log'ıstica 1′-(1+e-x)-1-(1+e-x)′ σ′(x)= (1+e-x)2 e-x σ′(x)= (1+e-x)2 e-x+1-1 σ′(x)= (1+e-x)2 e-x+1 1 σ′(x)= - (1+e-x)2 (1+e-x)2 fig.9. diagrama 1 1 σ′(x)= - 1) los inputs (features) x ingresan junto con un vector de 1+e-x (1+e-x)2 pesos w y un bias b. 2) secalculaelproductopuntoentreelvectorxyelvector 1 (cid:18) 1 (cid:19) σ′(x)= - 1w. 1+e-x 1+e-x 3) se le aplica la funcio'n no lineal σ(z), obteniendo como salida una probabilidad. σ′(x)=σ(x)(1-σ(x)) 4) finalmente,estaprobabilidadsecomparaconunumbral para asignar una etiqueta de clase (0 o 1). d. hallar la funcio'n de pe'rdida en algunos textos, al valor lineal z = wx + b se le llama pre-activacio'n, y a la aplicacio'n"}
{"id_doc": "DOC_016", "segmentacion": "A", "chunk_id": "DOC_016_A_005", "idx": 5, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "los inputs (features) x ingresan junto con un vector de 1+e-x (1+e-x)2 pesos w y un bias b. 2) secalculaelproductopuntoentreelvectorxyelvector 1 (cid:18) 1 (cid:19) σ′(x)= - 1w. 1+e-x 1+e-x 3) se le aplica la funcio'n no lineal σ(z), obteniendo como salida una probabilidad. σ′(x)=σ(x)(1-σ(x)) 4) finalmente,estaprobabilidadsecomparaconunumbral para asignar una etiqueta de clase (0 o 1). d. hallar la funcio'n de pe'rdida en algunos textos, al valor lineal z = wx + b se le llama pre-activacio'n, y a la aplicacio'n de la sigmoide se le ¿mse? esto y ma's en la siguiente clase. llama activacio'n. la salida de la activacio'n corresponde a la probabilidad estimada yˆ. vii. conclusio'n en esta clase se reforzaron conceptos esenciales para b. optimizacio'n comprender co'mo los modelos de aprendizaje supervisado nuestroobjetivoesoptimizarlospara'metroswybparaque aprenden a partir de datos. se revisaron las limitaciones de el modelo aprenda correctamente. tenemos: la regresio'n lineal y los problemas comunes asociados al sesgo y la varianza, as'ı como te'cnicas para evaluar y mejorar 1 yˆ=σ(f w,b (x))= 1+e-(wx+b) la generalizacio'n de los modelos. adema's, se introdujo la regresio'n log'ıstica como un modelo de clasificacio'n, destapara ajustar los para'metros, necesitamos calcular las cando el papel de la funcio'n sigmoide y su derivada en el derivadas parciales de la funcio'n de pe'rdida respecto a w y proceso de optimizacio'n. estos fundamentos sientan la base b. sin embargo, antes de derivar, debemos definir una funcio'n para profundizar en funciones de pe'rdida espec'ıficas y en el de pe'rdida adecuada. entrenamiento de modelos ma's complejos en futuras sesiones. referencias [1] ieee costa rica. \"ingenieeer'ıa costa rica.\" [en l'ınea]. disponible: https://r9.ieee.org/costarica/ingenieeeria [2] a. shervine. \"hoja de referencia de aprendizaje automa'tico.\" stanford university. [en l'ınea]. disponible: https://stanford.edu/∼shervine/l/es/teaching/cs-229/ hoja-referencia-aprendizaje-automatico-consejos-trucos"}
{"id_doc": "DOC_017", "segmentacion": "A", "chunk_id": "DOC_017_A_000", "idx": 0, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "apuntes de semana 5, clase #2 mauricio campos cerdas instituto tecnolo'gico de costa rica cartago, costa rica maucampos@estudiantec.cr abstract-this document presents class notes on handling outliers, the concepts of bias and variance, and an introduction tologisticregressionasaclassificationalgorithm.techniquesfor identifying and addressing outlying values are discussed, along with methods for splitting datasets and common scenarios encounteredduringtrainingandvalidation.aswellasthesigmoid function and parameter optimization in logistic regression are introduced, including the derivation of the sigmoid function. indexterms-outliers,bias,variance,logisticregression,classification,sigmoidfunction,parameteroptimization,trainingand validation, overfitting, underfitting i. noticiasdelasemana fig.1. residualplots a. evento ieee ieee esta' organizando un evento donde se tocara'n temas muyinteresantes,incluyendolainteligenciaartificial.vendra'n - datos sobresalientes: siempre existira'n outliers, ya sea por ruido o error humano. lo que pasa es que nos afecta personas de gran renombre a dar charlas, habra' comida y a nuestro modelo, siempre habra' cierta sensibilidad hay dema's. se pide registrarse para calcular la alimentacio'n para que tratarlos para evitar que nos afecte en gran medida el d'ıa del evento. nuestro modelo. b. problema con las referencias y la ia a. me'todos para tratar outliers se esta' produciendo un feno'meno en el que cada vez ma's art'ıculos, notas y sitios web son generados con inteligencia - standardized residuals: tenemos el ca'lculo de artificial y se referencian entre s'ı. esto puede llevar a que los residuos y calculamos la desviacio'n esta'ndar, la propia ia se cite a s'ı misma, provocando un aumento de para asegurarnos de que nuestros datos siguen una referencias generadas artificialmente. distribucio'n normal. a partir de que los tenemos estandarizados, calculamos a cua'ntas desviaciones c. modelo nano banana esta'ndar se encuentra ese dato. lo que nos dira' google lanzo' un nuevo modelo llamado nano banana. su es el l'ımite de hasta do'nde se consideran datos atractivo se encuentra que a diferencia de otros modelos, este sobresalientes. agarra la imagen que esta' como input y la modifica sin tener ∗ |z|>2: posible outlier. que generarla otra vez. se dio' un ejemplo de un experimento ∗ |z| > 3: outlier muy probable, se recomienda donde una ia ten'ıa que modificar una foto varias veces y se excluir. llego' a evidenciar que hubo un sesgo de generar la imagen de - regla del rango intercuart'ılico (iqr): definido la persona cada vez con rasgos ma's latinos. comoiqr=q3-q1.losdatosqueseencuentran ii. potencialesproblemasdelaregresio'nlineal fuera del intervalo [q1-1.5-iqr,q3+1.5-iqr] se consideran outliers. - no linealidad: en regresio'n lineal se asume que existe una relacio'n lineal entre las variables predictoras - winsorizacio'n: te'cnica que consiste en reemplazar y"}
{"id_doc": "DOC_017", "segmentacion": "A", "chunk_id": "DOC_017_A_001", "idx": 1, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "outlier muy probable, se recomienda donde una ia ten'ıa que modificar una foto varias veces y se excluir. llego' a evidenciar que hubo un sesgo de generar la imagen de - regla del rango intercuart'ılico (iqr): definido la persona cada vez con rasgos ma's latinos. comoiqr=q3-q1.losdatosqueseencuentran ii. potencialesproblemasdelaregresio'nlineal fuera del intervalo [q1-1.5-iqr,q3+1.5-iqr] se consideran outliers. - no linealidad: en regresio'n lineal se asume que existe una relacio'n lineal entre las variables predictoras - winsorizacio'n: te'cnica que consiste en reemplazar y la variable respuesta. sin embargo, esto no siempre los valores extremos por los percentiles l'ımite (por se cumple, lo que provoca que el modelo no capture ejemplo, 5% y 95%). adecuadamente la relacio'n y que los residuos presenten patronessistema'ticos(porejemplo,conformaparabo'lica) iii. sesgoyvarianza en lugar de distribuirse aleatoriamente (ver fig. 1). el dataset suele dividirse en train y test (80/20) una estrategia para enfrentar este problema es aplicar a. training set feature engineering. un ejemplo es incorporar te'rminos polino'micos adicionales a las variables, lo que permite se utiliza para ajustar el modelo. nos puede pasar que aproximar mejor relaciones no lineales. entrenemos el modelo mucho tiempo, lleguemos al final y fig.2. underfitting,ideal,overfittingplots nos damos cuenta de que fallamos el examen. si dedicamos mucho al entrenamiento pero nada a generalizar, se llama fig.3. linearvslogisticregression overfitting. por eso queremos hacer tests pequen˜os durante el entrenamiento, con el validation set. e. alto bias b. validation set cuando el modelo comete muchos errores en el training nos dice si los hiperpara'metros son adecuados o no, para set, se produce underfitting. esto ocurre porque el modelo no continuar si no lo son y as'ı no desperdiciar recursos. asume demasiado del training set, no utiliza todas los features disponibles y es demasiado simple para capturar la complejic. te'cnicas de subdividir el dataset dad de los datos. para evitar un alto sesgo, se puede utilizar un modelo ma's complejo. adema's, es importante revisar que - randomsampling:seusasiemprequetengamosclases los features del training set sean adecuadas para la naturaleza balanceadas. si los datos no esta'n balanceados, pueden del problema, ya que si no tienen la capacidad de capturar la quedar mal distribuidos, con ma's datos de una clase que informacio'n relevante, el modelo no podra' hacer predicciones de la otra. correctas. - stratified sampling: usado para datos imbalanceados, asegura una representacio'n de todas las clases por sepa- f. alta varianza rado. ocurre cuando el modelo se ajusta demasiado a los datos -"}
{"id_doc": "DOC_017", "segmentacion": "A", "chunk_id": "DOC_017_A_002", "idx": 2, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "training set sean adecuadas para la naturaleza balanceadas. si los datos no esta'n balanceados, pueden del problema, ya que si no tienen la capacidad de capturar la quedar mal distribuidos, con ma's datos de una clase que informacio'n relevante, el modelo no podra' hacer predicciones de la otra. correctas. - stratified sampling: usado para datos imbalanceados, asegura una representacio'n de todas las clases por sepa- f. alta varianza rado. ocurre cuando el modelo se ajusta demasiado a los datos - k-fold cross-validation: divisio'n en k partes, en cada de entrenamiento y no es capaz de generalizar correctamente. iteracio'n se usan k - 1 para entrenamiento y 1 para esto suele suceder cuando los datos son de alta dimensionvalidacio'n. alidad y hay pocos ejemplos disponibles. para evitar la alta varianza, se pueden usar modelos ma's simples, reducir la d. escenarios posibles dimensionalidad de los datos, obtener ma's ejemplos y aplicar te'cnicas de regularizacio'n. - escenario ideal: el modelo presenta bajo error tanto en training como en testing. puede evitar el ruido de los iv. regresio'nlog'istica datos y generalizar correctamente. por cada e'poca de entrenamientoelerrordeber'ıairdisminuyendo,tendiendo aunque su nombre contenga la palabra regresio'n, en resiempre a la baja. alidad la regresio'n log'ıstica es un algoritmo de clasificacio'n binaria. distingue entre dos clases (0 y 1), estimando proba- - overfitting: ocurre cuando el error en el validation set bilidades. fig. 3). empieza a crecer o se estanca. esto indica que el modelo erabuenohastaciertae'pocadeentrenamiento,peroluego a. distribucio'n de bernoulli empieza a sobreajustarse a los datos de entrenamiento, produciendo overfitting. a esta te'cnica de detener el utilizamos una distribucio'n de bernoulli para la ocurrencia entrenamiento antes de que esto suceda se le llama early de un evento binario. stopping. p(y =k)=pk(1-p)1-k, k ∈{0,1} - underfitting: se da cuando el error es alto tanto en training como en testing. esto se conoce como under- b. funcio'n sigmoide fitting, que ocurre cuando el modelo no logra ajustarse es una funcio'n que no se comporta linealmente. tiene un correctamente a los datos. es lo opuesto al overfitting y codominio de [0, 1] se caracteriza por un alto sesgo. para ver gra'ficamente estos escenarios, ver fig. 2). 1 σ(x)= 1+e-x - bias-variance tradeoff: validacio'n con buen resultado, pero entrenamiento con alto error. es raro que suceda y nota:xpuedesercualquiernu'mero,hastaelresultadodeotra tal vez hay errores de ca'lculo. funcio'n. ver fig. 4). e-x+1-1 σ′(x)= (1+e-x)2 e-x+1 1 σ′(x)= - (1+e-x)2 (1+e-x)2 de la fraccio'n izquierda,"}
{"id_doc": "DOC_017", "segmentacion": "A", "chunk_id": "DOC_017_A_003", "idx": 3, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "funcio'n que no se comporta linealmente. tiene un correctamente a los datos. es lo opuesto al overfitting y codominio de [0, 1] se caracteriza por un alto sesgo. para ver gra'ficamente estos escenarios, ver fig. 2). 1 σ(x)= 1+e-x - bias-variance tradeoff: validacio'n con buen resultado, pero entrenamiento con alto error. es raro que suceda y nota:xpuedesercualquiernu'mero,hastaelresultadodeotra tal vez hay errores de ca'lculo. funcio'n. ver fig. 4). e-x+1-1 σ′(x)= (1+e-x)2 e-x+1 1 σ′(x)= - (1+e-x)2 (1+e-x)2 de la fraccio'n izquierda, puedo cancelar 1 1 σ′(x)= - (1+e-x) (1+e-x)2 aplicamos factor comu'n 1 1 σ′(x)= -(1- ) (1+e-x) (1+e-x) como 1 =σ(x), decimos que: (1+e-x) fig.4. sigmoidplot σ′(x)=σ(x) (cid:0) 1-σ(x) (cid:1) references c. clasificador [1] amazon web services, \"model fit: underfitting vs. overfit- - si y <0.5, se clasifica como 0. ting,\". available: https://docs.aws.amazon.com/machine-learning/latest/ dg/model-fit-underfitting-vs-overfitting.html. - si y ≥0.5, se clasifica como 1. [2] university of virginia library, \"understanding diagnostic plots for el umbral puede ajustarse segu'n el problema. linearregressionanalysis,\".available:https://library.virginia.edu/data/ articles/diagnostic-plots. [3] ml4a, \"neural networks,\". available: https://ml4a.github.io/ml4a/es/ d. modelo combinado neural networks/. alaplicarlasigmoideaunafuncio'nlinealf (x)=wx+ w,b b, obtenemos: 1 f (x)= w,b 1+e-(wx+b) la relacio'n de los features y pesos se da por regresio'n lineal. lo que nos da es la probabilidad de que un evento suceda. e. optimizacio'n en la regresio'n log'ıstica necesitamos optimizar los pesos w y el sesgo b. para actualizar estos pesos, es necesario contar con una funcio'n de pe'rdida l que sea adecuada para probabilidades, ya que el mse ya no es lo apropiado en este caso. f. derivada de la sigmoide 1 σ(x)= 1+e-x usando la regla del cociente: 1′-(1+e-x)-(1-(1+e-x)′) σ′(x)= (1+e-x)2 0-1-(1′+(e-x)′) σ′(x)= (1+e-x)2 -(0-(e-x)) σ′(x)= (1+e-x)2 e-x σ′(x)= (1+e-x)2"}
{"id_doc": "DOC_018", "segmentacion": "A", "chunk_id": "DOC_018_A_000", "idx": 0, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "apuntes semana 6 apuntesdel09deseptiembre juan pablo rodr'ıguez cano ic-6200 inteligencia artificial tecnolo'gico de costa rica jp99@estudiantec.cr abstract-en este documento se detallan las indicaciones de - elme'tododescribe()resumelosdatosanal'ıticosqueson la tarea 1 de inteligencia artifical y se introduce el tema importantes para saber co'mo se comportan los features de regresio'n log'ıstica como un modelo de clasificacio'n cuyas - no debe haber co'digo en el informe, solo resultados, propiedades de funcio'n son aptas para modelar problemas ana'lisis etc. complejos y la optimizacio'n de recursos. index terms- - el notebook sera' evidencia del trabajo - elobjetivoesversilarelacio'nconlaprediccio'neslineal, i. preguntasdelquiz y si no aplicar un feature engineering 1) describa que' es \"overfitting\" y \"underfitting\". - figurasenieeesiemprevanenlapartesuperioroinferior de las columnas. r/ \"overfitting\" es cuando el modelo tiene una mejor me'trica con el conjunto de entrenmaiento que con el - el formato es de ieee para conferencias conjunto de testing, lo cual indica una pobre generaliii. actividaddeieee izacio'n con datos nuevos. \"underfitting\" es cuando el es un evento anual que se dara' esta vez en noviembre model no logra captar la relacio'n entre los features de en la sabana. es una oportunidad para conocer sobre temas manera que los puntajes de me'trica son bajos para el innovadores en inteligencia artificial y biolog'ıa molecular. es conjunto de entrenamiento y testeo. una oportunidad para crear contactos dentro de la industria 2) describa k-fold cross-validation ya que los presentadores suelen ser receptivos al pu'blico y sesubdivideelconjuntodeentrenamientoenk-1partes. disponen de tiempo para hablar. en cada e'poca se entrenan k-1 partes y se utiliza el otro subconjunto para la validacio'n, el iv. contenidodeclase 3) ¿que' es un m'ınimo global y m'ınimo local en una a. regresio'n log'ısitca funcio'n? un m'ınimo local es el valor m'ınimo de una funcio'n en a diferencia de la regresio'n lineal que es un modelo que una vecindad reducida, mientras que el m'ınimo global predice un nu'mero real a partir de los features, la regresio'n se refiere al m'ınimo global a trave's de todo el dominio log'ıstica es un modelo de clasificacio'n binaria. el resultado de la funcio'n. de dicho modelo es la probabilidad de que suceda un evento y esta' basado en la distribucio'n de bernoulli: p(x = k) = 4) desarrolleladerivadaparcialdelconrespectoawde: pk(1-p)1-k 1 (cid:88) l= ((wx +b)-y ) n i i b. funcio'n sigmoide ∂l 2 (cid:88) = ((wx +b)-y )x ) ∂w n i i i ii. indicacionesdelatarea - la tarea se deber realizar en grupos de 3"}
{"id_doc": "DOC_018", "segmentacion": "A", "chunk_id": "DOC_018_A_001", "idx": 1, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "trave's de todo el dominio log'ıstica es un modelo de clasificacio'n binaria. el resultado de la funcio'n. de dicho modelo es la probabilidad de que suceda un evento y esta' basado en la distribucio'n de bernoulli: p(x = k) = 4) desarrolleladerivadaparcialdelconrespectoawde: pk(1-p)1-k 1 (cid:88) l= ((wx +b)-y ) n i i b. funcio'n sigmoide ∂l 2 (cid:88) = ((wx +b)-y )x ) ∂w n i i i ii. indicacionesdelatarea - la tarea se deber realizar en grupos de 3 personas. - la fecha de entrega es el 16 de septiembre. - solo hace falta que una persona del grupo suba la tarea. en el nombre del archivo zip debe venir el nombre de todos. - nosepuedeutilizarningunabibliotecaquenoseanumpy o pandas - kagg;e es una plataforma con datasets para machine learning para el pu'blico y tambie'n presentan oportunidades para participar en concursos de ml. esta funcio'n es conveniente porque puede modelar com- - la funcio'n de pe'rdida y la gra'ficacio'n debe se manual portamientos no lineales, el cual es un comportamiento muy comu'n en la mayor'ıa de problemas. trae consigo una mayor porloquehayqueconvertirunproblemademaximizacio'nen complejidad pero a su vez logra resolver problemas ma's minimizacio'n.paraesto,simplementesedavueltaalafuncio'n complejos. de ln multiplicando por -1. sucodominioesde0a1yestoesmuyconvenienteyaque los valores probabil'ısticos comparten ese mismo espacio. la funcio'n sigmoide se expresa de la siguiente manera 1 1 σ(x)= ⇒σ(f (x))= 1+e-x w,b 1+e-fw,bx la manera en que esta funcio'n se convierte en un clasificador es al escoger un umbral. este umbral se utiliza para definir un punto a partir de cua'l se calsifica un evento con unaetiquetaolaotra.porlogeneralsesueleescogerunvalor umbral de 0.5. c. derivada de la funcio'n sigmoide como la regresio'n log'ıstica es un clasificador, se debe encontrar una funcio'n de pe'ridica adecuada para el problema. para esto se debe analizar la derivada de la funcio'n sigmoide, ya que es necesario para cualquier problema de optimizacio'n. 1′(1+e-x)-(1(1+e-x)′) σ′(x)= (1+e-x)2 ⇒σ′(x)=σ(x)(1-σ(x)) como se puede notar, la derivada se puede expresar en te'rminosdelafuncio'nmisma,locuallohacemuyconveniente ya que no se requieren operaciones muy complejas y con esto se obtiene una mayor eficiencia. d. funcio'n de pe'rdida: verosimilitud en vez de utilizar mse o mae, se utiliza la verosimilitud. esta esta' dada por la siguiente ecuacio'n (cid:89) l= f (x )yi(1-f (x ))1-yi w,b i w,b i el resultado que se obtiene para un punto en esta ecuacio'n es la probabilidad de que su etiqueta sea y con los pesos i w actuales. como se quiere optimizar los pesos"}
{"id_doc": "DOC_018", "segmentacion": "A", "chunk_id": "DOC_018_A_002", "idx": 2, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "no se requieren operaciones muy complejas y con esto se obtiene una mayor eficiencia. d. funcio'n de pe'rdida: verosimilitud en vez de utilizar mse o mae, se utiliza la verosimilitud. esta esta' dada por la siguiente ecuacio'n (cid:89) l= f (x )yi(1-f (x ))1-yi w,b i w,b i el resultado que se obtiene para un punto en esta ecuacio'n es la probabilidad de que su etiqueta sea y con los pesos i w actuales. como se quiere optimizar los pesos para los cuales se obtiene una mejor me'trica, se debe derivar esta funcio'n. sin embargo, existe un problema con esta expresio'n donde una multiplicacio'n incluye polinomios muy grandes, y calcular la dervida respectiva se vuelve muy complejo y computacionalmente costoso. adema's, como se trata de valores probabil'ısticos, o sea, de 0 a 1, su multiplicacio'n se vuelve extremadamente pequen˜a y as'ı la derivada de la funcio'n se vuelve virtualmente cero, y esto no cambia los pesosenelpasodeentrenamiento.aestoseleconocecomoel feno'meno de \"vanishing gradients\". por esta razo'n se aplican los teoremas de logaritmo y se obtiene la siguiente expresio'n. (cid:88) ln(l)= ln(f (x )yi +ln((1-fw,b(x ))1-yi) w,b i i (cid:88) ⇒ln(l)= y ln(f (x )+1-y ln((1-fw,b(x ))) i w,b i i i estoseconvierteenunatareama'sfa'cildeoptimizacio'n.sin embargo, la funcio'n de logaritmo es estrictamente creciente,"}
{"id_doc": "DOC_019", "segmentacion": "A", "chunk_id": "DOC_019_A_000", "idx": 0, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": "apuntes de inteligencia artificial - semana 6 ashley vasquez apuntes del 09 de septiembre abstract-estedocumentoreu'neyreformulalosapuntesdela iii. actividadieee semana 6 del curso de inteligencia artificial. incluye preguntas del quiz, instrucciones de la tarea i, una breve nota sobre una en noviembre se llevara' a cabo un evento ieee en la actividad de ieee y los contenidos principales de clase sobre sabana.estecongresoreu'nepresentacionessobreinteligencia regresio'n log'ıstica. asimismo, se profundizo' en la funcio'n de artificial y biolog'ıa molecular, y es una oportunidad para verosimilitud,elusodelogaritmosparasimplificarderivadas,la establecer conexiones con investigadores y profesionales. regla de la cadena y la actualizacio'n de para'metros. se an˜aden ejemplos pra'cticos (como el caso de la calabaza naranja / no naranja) para reforzar la comprensio'n del modelo. iv. regresio'nlog'istica a. definicio'n i. preguntasdelquiz la regresio'n log'ıstica es un modelo de clasificacio'n binaria 1) overfittingyunderfitting:eloverfittingocurrecuando que estima la probabilidad de que un dato pertenezca a una elmodeloaprendedemasiadobienelconjuntodeentre- clase.adiferenciadelaregresio'nlineal,queentregaunvalor namiento, pero no logra generalizar en datos nuevos. el continuo,estemodelotransformalasalidaenunaprobabilidad underfitting, en cambio, refleja que el modelo no logra entre 0 y 1, y se basa en la distribucio'n de bernoulli. captar la relacio'n entre las variables, obteniendo bajo rendimiento en ambos conjuntos. b. funcio'n sigmoide 2) k-fold cross-validation: se divide el conjunto de entrenamiento en k subconjuntos. en cada iteracio'n se 1 σ(x)= entrenan k-1 y el restante se utiliza para validar. al 1+e-x finalizar, se promedian los resultados. la funcio'n sigmoide transforma cualquier nu'mero real en un 3) m'ınimos locales y globales: un m'ınimo local es el valor en [0,1]. se define un umbral (generalmente 0.5) para valor ma's bajo dentro de una regio'n reducida de la decidir la clase asignada. como se observa en la figura 1, es funcio'n. el m'ınimo global es el valor ma's bajo en todo la base para convertir salidas lineales en probabilidades. el dominio. 4) derivada parcial de l con respecto a w: 1 (cid:88)(cid:0) (cid:1)2 ∂l 2 (cid:88)(cid:0) (cid:1) l= (wx +b)-y , = (wx +b)-y x . n i i ∂w n i i i ii. indicacionesdelatareai - realizar la tarea en equipos de tres personas. fecha de entrega: 16 de septiembre. - solo un integrante debe subir el archivo comprimido con los nombres de todos los miembros. - se permite u'nicamente el uso de numpy y pandas. - elinformenodebecontenerco'digo,u'nicamenteana'lisis, resultados y conclusiones. fig. 1: funcio'n sigmoide - el notebook sera' evidencia del trabajo realizado. - la funcio'n de pe'rdida y las gra'ficas deben hacerse de"}
{"id_doc": "DOC_019", "segmentacion": "A", "chunk_id": "DOC_019_A_001", "idx": 1, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": ". n i i ∂w n i i i ii. indicacionesdelatareai - realizar la tarea en equipos de tres personas. fecha de entrega: 16 de septiembre. - solo un integrante debe subir el archivo comprimido con los nombres de todos los miembros. - se permite u'nicamente el uso de numpy y pandas. - elinformenodebecontenerco'digo,u'nicamenteana'lisis, resultados y conclusiones. fig. 1: funcio'n sigmoide - el notebook sera' evidencia del trabajo realizado. - la funcio'n de pe'rdida y las gra'ficas deben hacerse de forma manual. c. derivada de la sigmoide - el formato debe ser ieee. - se debe comprobar si la relacio'n entre las variables es lineal; si no, aplicar feature engineering. σ′(x)=σ(x)(1-σ(x)). - el me'todo describe() ayuda a resumir los datos de forma estad'ıstica. el hecho de que la derivada se exprese en funcio'n de la - figuras deben colocarse en parte superior o inferior de propia sigmoide la hace eficiente y pra'ctica en optimizacio'n. columnas. la figura 2 ilustra este comportamiento. fig. 2: derivada de la funcio'n sigmoide fig. 4: ejemplo: calabaza no es naranja d. verosimilitud vs. error cuadra'tico mientrasqueelerrorcuadra'ticomedio(mse)esidealpara predecir valores continuos, la verosimilitud se utiliza cuando el resultado es una probabilidad. en regresio'n log'ıstica, se busca maximizar la probabilidad de que el modelo asigne la clase correcta a cada ejemplo. la figura 3 compara ambos enfoques. fig. 5: ejemplo: calabaza s'ı es naranja g. composicio'n de funciones y regla de la cadena el modelo puede expresarse como: z(x)=wx+b, a(z)=σ(z), f (x)=a(z). w,b entonces la funcio'n de costo es: fig. 3: comparacio'n entre mse y verosimilitud l=y ln(a(z))+(1-y )ln(1-a(z)). i i e. interpretacio'n de la verosimilitud aplicando la regla de la cadena: laverosimilitudseentiendecomolaprobabilidaddeobser- ∂l ∂l ∂a ∂z var los datos dados los para'metros actuales. analicemos los = - - . ∂w ∂a ∂z ∂w casos: - caso y i = 1: la probabilidad es σ(wx+b). ejemplo: de manera ana'loga para b. wx+b = 1.458 =⇒ σ(1.458) = 0.81. esto significa h. derivadas parciales paso a paso que hay un 81% de probabilidad de que la calabaza no sea naranja, como se muestra en la figura 4. - con respecto a a(z): - casoy i =0:laprobabilidades1-σ(wx+b).ejemplo: ∂l y 1-y wx + b = -1.32 =⇒ σ(-1.32) = 0.21. entonces =- i + i . ∂a a(x) 1-a(x) 1-0.21 = 0.79, lo que se interpreta como un 79% de probabilidaddequelacalabazas'ıseanaranja(figura5). - con respecto a z: f. uso de logaritmos ∂a"}
{"id_doc": "DOC_019", "segmentacion": "A", "chunk_id": "DOC_019_A_002", "idx": 2, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": "esto significa h. derivadas parciales paso a paso que hay un 81% de probabilidad de que la calabaza no sea naranja, como se muestra en la figura 4. - con respecto a a(z): - casoy i =0:laprobabilidades1-σ(wx+b).ejemplo: ∂l y 1-y wx + b = -1.32 =⇒ σ(-1.32) = 0.21. entonces =- i + i . ∂a a(x) 1-a(x) 1-0.21 = 0.79, lo que se interpreta como un 79% de probabilidaddequelacalabazas'ıseanaranja(figura5). - con respecto a z: f. uso de logaritmos ∂a =σ(z)(1-σ(z)). multiplicar probabilidades pequen˜as genera valores cer- ∂z canos a cero, causando inestabilidad. aplicando logaritmos se - con respecto a w y b: transforma en sumas: (cid:88)(cid:2) (cid:3) ∂z ∂z ln(l)= y ln(f (x ))+(1-y )ln(1-f (x )) . =x, =1. i w,b i i w,b i ∂w ∂b i. actualizacio'n de para'metros finalmente, los para'metros se actualizan con descenso de gradiente: ∂l ∂l w =w-α , b=b-α . ∂w ∂b el valor de α (tasa de aprendizaje) es crucial. el flujo de ca'lculo se muestra en la figura 6. fig. 6: flujo de ca'lculo y actualizacio'n de para'metros j. aspectos pra'cticos - epochs: nu'mero de veces que el modelo recorre todo el dataset. ma's epochs permiten aprender mejor, pero tambie'n aumenta el riesgo de overfitting. - batch size: cantidad de ejemplos procesados antes de actualizar para'metros. un batch pequen˜o hace el entrenamiento ma's ruidoso pero puede mejorar la generalizacio'n. - gradiente descendente estoca'stico (sgd): actualiza para'metros con un ejemplo a la vez, lo que lo hace ma's ra'pido pero inestable. v. conclusiones durante esta semana se consolidaron los fundamentos de la regresio'n log'ıstica. se estudiaron sus bases matema'ticas, la funcio'n sigmoide y su derivada, la diferencia entre mse y verosimilitud, el uso de logaritmos para simplificar expresiones y la actualizacio'n de para'metros mediante gradiente descendente. los ejemplos pra'cticos de la calabaza facilitaron lainterpretacio'ndeprobabilidades,yladescomposicio'npasoa pasodederivadasmostro' co'moseaplicalaregladelacadena en la pra'ctica. con esto se sientan las bases para enfrentar algoritmos ma's avanzados en aprendizaje supervisado."}
{"id_doc": "DOC_019", "segmentacion": "A", "chunk_id": "DOC_019_A_003", "idx": 3, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": "en aprendizaje supervisado."}
{"id_doc": "DOC_020", "segmentacion": "A", "chunk_id": "DOC_020_A_000", "idx": 0, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "apuntes semana 6 apuntes del 11 de setiembre de 2025 andrey ureña bermúdez - 2022017442 inteligencia artificial andurena@estudiantec.cr resumen-en este documento, se resume la clase del 11 de caso y i =0: setiembrede2025,enlacuálserealizóprimeramenteunrepaso delovistoenlaclaseanterior.demanerageneral,estedocumento f w,b (x i )yi(1-f w,b (x i ))(1-yi) =f w,b (x i )0(1-f w,b (x i ))1 recopilainformaciónsobreverosimilitudenlaregresiónlogística, (1) lafuncióndecostoeinformaciónsobreunnotebookderegresión =(1-f (x ))1 (2) logística compartido por el profesor. w,b i indexterms-verosimilitud,regresiónlogística,gradientedescon la misma fórmula puedo estudiar de que ocurra o no cendiente, función sigmoide, derivada. un evento. ejemplo: calabaza es naranja: i. notasobretareai wx+b=-1,32 sehacerecordatoriosobredarleimportanciaynodescuidar f (x )=σ(wx +b)=σ(-1,32)=0,21 w,b i i eltrabajoescritodelatarea,asícomosudocumentación,pues =f (x )0(1-f (x ))1 w,b i w,b i de este se dará el feedback para los escritos que haya que =(1-f (x ))1 =(1-σ(-1,32)) realizar en tareas próximas y etapas del proyecto. w,b i =(1-0,21)=0,79 ii. repasosobreclasedelmartes la probabilidad de que x sea naranja es 0,79. i ii-a. verosimilitud al final lo que obtenemos es la probabilidad de que la es la probabilidad de observar cada uno de los datos muestra x tenga la etiqueta y . cambiando ciertos parámetros. lo que se busca es maximizar i i para llegar al punto de máxima probabilidad. ii-b. derivada de la función de costo ladiferenciaentremseymaximumlikelihoodradicaensu primero, se debe calcular la probabilidad de que x tome la aplicación: para la predicción de valores continuos, se utiliza i etiqueta de y , así con cada muestra. mse, mientras que para modelar probabilidades, se utiliza i dado que esto implica la multiplicación de probabilidades, maximum likelihood. el cálculo de la derivada se vuelve complejo. para simasí, nuestra función de costo es: plificarlo, se busca una expresión equivalente que evite la n multiplicación, lo cual se logra aplicando logaritmos. (cid:89) l= f (x )yi -(1-f (x ))(1-yi) (1) w,b i w,b i ii-c. logaritmos i=1 ln(an)=n-ln(a) se vió el desarrollo de cada uno de los casos de y en i f w,b (x i )yi(1-f w,b (x i ))(1-yi): ln(a-b)=ln(a)+ln(b) ln(an-bn)=n-ln(a)+n-ln(b) caso y =1: i f (x )yi(1-f (x ))(1-yi) =f (x )1(1-f (x ))0 ii-d. aplicación de logaritmo a la verosimilitud w,b i w,b i w,b i w,b i (1) l= (cid:81) f w,b (x i )yi -(1-f w,b (x i ))(1-yi) =f w,b (x i )1 (2) ln(l)= (cid:80) ln(f w,b (x i )yi)+ln((1-f w,b (x i ))(1-yi)) (cid:80) acá el modelo nos da el valor directo. ejemplo: calabaza ln(l)= y -ln(f (x ))+(1-y"}
{"id_doc": "DOC_020", "segmentacion": "A", "chunk_id": "DOC_020_A_001", "idx": 1, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "(x i ))(1-yi): ln(a-b)=ln(a)+ln(b) ln(an-bn)=n-ln(a)+n-ln(b) caso y =1: i f (x )yi(1-f (x ))(1-yi) =f (x )1(1-f (x ))0 ii-d. aplicación de logaritmo a la verosimilitud w,b i w,b i w,b i w,b i (1) l= (cid:81) f w,b (x i )yi -(1-f w,b (x i ))(1-yi) =f w,b (x i )1 (2) ln(l)= (cid:80) ln(f w,b (x i )yi)+ln((1-f w,b (x i ))(1-yi)) (cid:80) acá el modelo nos da el valor directo. ejemplo: calabaza ln(l)= y -ln(f (x ))+(1-y )-ln(1-f (x )) i w,b i i w,b i no es naranja: esto lo vamos a llamar log-likelihood. es mucho más fácil wx+b=1,458 decomputaryderivar,ademásdequequitaerroresalmomento f w,b (x i )=σ(wx i +b)=σ(1,458)=0,81 decomputarlasmultiplicacionesdeprobabilidades.ahoraesta =f (x )1(1-f (x ))0 es la función de costo que se va a usar. w,b i w,b i =f (x )1 =σ(1,458) paraminimizarmaximizandoloquesepuedehaceresdarle w,b i vuelta a la función, para eso se multiplica por -1: =0,81 1 (cid:88) la probabilidad de que x i no sea naranja es 0,81. l= n y i -ln(f w,b (x i ))+(1-y i )-ln(1-f w,b (x i )) 1 (cid:104)(cid:88) (cid:105) l=- y -ln(f (x ))+(1-y )-ln(1-f (x )) 1) cálculo de derivadas parciales n i w,b i i w,b i importante recordar que el l que se está usando es: ahora puedo minimizar la función, lo que permite aplicar el descenso del gradiente que se ha estado trabajando. l=-[y -ln(a(z(x)))+(1-y )-ln(1-a(z(x)))] i i primero se inicia calculando la derivada parcial de l con respecto a la función sigmoide: (cid:20)(cid:18) (cid:19) (cid:18) (cid:19)(cid:21) ∂l 1 1 =- y - -a(x)′ + (1-y )- -(1-a(x))′ ∂a i a(x) i 1-a(x) (cid:20)(cid:18) (cid:19) (cid:18) (cid:19)(cid:21) y (1-y ) =- i -1 + i --1 a(x) 1-a(x) (cid:20)(cid:18) (cid:19) (cid:18) (cid:19)(cid:21) y (1-y ) =- i - i a(x) 1-a(x) -y (1-y ) = i + i a(x) 1-a(x) figura1. gráficaminimizandol ahora,secalculaladerivadaparcialdelafunciónsigmoide aquí lo ideal es intentar que el loss llegue a cero; si se respecto a z. importante recordar que la derivada de sigmoide obtiene un loss negativo, significa que algo se está haciendo es σ(x)-(1-σ(x)), por lo que la derivada parcial sería: mal. ∂a =σ(z(x))-(1-σ(z(x))) ii-e. actualización de parámetros ∂z por último, se debe calcular de manera individual la deries necesario actualizar los parámetros w y b, ya que son vada parcial de cada uno de los parámetros con respecto a la losquepermitenmodificarlosresultadosdelasprobabilidades regresión lineal: obtenidas. z(x)=wx+b para actualizar el parámetro w"}
{"id_doc": "DOC_020", "segmentacion": "A", "chunk_id": "DOC_020_A_002", "idx": 2, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "se respecto a z. importante recordar que la derivada de sigmoide obtiene un loss negativo, significa que algo se está haciendo es σ(x)-(1-σ(x)), por lo que la derivada parcial sería: mal. ∂a =σ(z(x))-(1-σ(z(x))) ii-e. actualización de parámetros ∂z por último, se debe calcular de manera individual la deries necesario actualizar los parámetros w y b, ya que son vada parcial de cada uno de los parámetros con respecto a la losquepermitenmodificarlosresultadosdelasprobabilidades regresión lineal: obtenidas. z(x)=wx+b para actualizar el parámetro w se necesita: ∂l ∂w ∂z para actualizar el parámetro b se necesita: ∂l =x ∂b ∂w ii-f. composición de funciones ∂z =1 se va a utilizar el concepto de composición de funciones ∂b para que el cálculo de derivadas sea más sencillo. ya que se hizo el cálculo de cada derivada de manera derivada función de costo para un sample: individual, se prosigue a realizar las multiplicaciones: l=y -ln(f (x ))+(1-y )-ln(1-f (x )) i w,b i i w,b i modelo: f (x)=a(z(x)) w,b a(x)=σ(x)= 1 1+e-x z(x)=wx+b el resultado de combinar ambas es: l=y -ln(a(z(x)))+(1-y )-ln(1-a(z(x))) i i cuandosehabladelatécnicadecomposicióndefunciones seaplicalaregladelacadena.sedebencalcularlasderivadas figura2. derivadaparcialdelrespectoaz parciales: l=y -ln(a(z(x)))+(1-y )-ln(1-a(z(x))) i i ∂l ∂l ∂a ∂z = - - ∂w ∂a ∂z ∂w ∂l ∂l ∂a ∂z = - - figura3. derivadaparcialdelrespectoawyb ∂b ∂a ∂z ∂b se procede a actualizar parámetros: se actualizan los valores de w y b, y se calcula el error en z(x)=wx+b cada iteración. w =w-α∂l la función predict se encarga de predecir la clase de una ∂w nueva muestra en base al umbral que se define. una vez b=b-α∂l ∂b obtenida la probabilidad, se asigna la clase correspondiente. donde α es un hiperparámetro (learning rate). iii. código se muestra un notebook con el fin de comprender mejor cómo hacer una regresión logística. enlace a notebook. figura4. códigoclasificación como se muestra en la figura 4, se hace la importación de libreríasnecesarias,muchasdelascualespertenecenasklearn. luego, se hace la clasificación con make_classification, el cual es un método para crear un dataset de clasificación. en este caso, se indica que sea de 1000 samples, con 2 features informativas, sin features redundantes y con un solo clúster por clase. posteriormente, se visualiza el conjunto de datos utilizando plt.scatter, donde los puntos se colorean según su clase (y). luego, se crea un dataframe con pd.dataframe que contiene las dos características (feature_1 y feature_2) y la variable figura5. códigoregresiónlogística objetivo (target). finalmente, se divide el dataset en"}
{"id_doc": "DOC_020", "segmentacion": "A", "chunk_id": "DOC_020_A_003", "idx": 3, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "cual es un método para crear un dataset de clasificación. en este caso, se indica que sea de 1000 samples, con 2 features informativas, sin features redundantes y con un solo clúster por clase. posteriormente, se visualiza el conjunto de datos utilizando plt.scatter, donde los puntos se colorean según su clase (y). luego, se crea un dataframe con pd.dataframe que contiene las dos características (feature_1 y feature_2) y la variable figura5. códigoregresiónlogística objetivo (target). finalmente, se divide el dataset en entrenamiento y prueba finalmente, el modelo se implementa instanciando la clase contrain_test_split,reservandoel80%delosdatosparaentrey entrenándola con x train y y train. luego, se evalúa con namiento y el 20% para prueba, asegurando reproducibilidad x test y y test, calculando la accuracy y generando un clas con random_state=225. sification report para medir su desempeño, el cuál muestra la figura 5 muestra la implementación manual de la remétricas como el nivel de accuracy, precision, recall, f1-score gresiónlogística.laclaserecibecomoparámetroslacantidad y support. de epochs a ejecutar, el learning rate que se aplicará y los parámetros de la regresión w y b, que serán ajustados durante el entrenamiento. primero, se define la función sigmoide, utilizada para convertir la predicción lineal en una probabilidad. luego, se implementa la función de costo binary_cross_entropy_loss, quecalculalapérdidanegativaconelobjetivodeminimizarla durante el entrenamiento. enlafunciónfit,serecibentodoslosfeaturesylasetiquetas correspondientes. antes de iniciar el ajuste de los parámetros, se inicializan aleatoriamente los valores de w, cuyo tamaño figura6. entercaption corresponde al número de features, ya que cada uno necesita un peso asociado. luego, se ejecuta el ciclo de entrenamiento de igual forma, en lugar de implementar la regresión por la cantidad de epochs definida, donde primero se calcula logísticamanualmente,sepuedeutilizarelmétodoquefacilita la predicción lineal, que luego pasa por la función sigmoide sklearn,talycomosemuestraenlafigura6.enestecaso, para obtener una probabilidad. a partir de esta probabilidad, se instancia el modelo de regresión logística con iv. conclusión a lo largo de este documento se profundizó en los fundamentos de la regresión logística, en particular en el uso de la verosimilitud como función de costo y en la aplicación del logaritmoparasimplificarsuderivación.serevisaronejemplos prácticos que ilustran cómo interpretar probabilidades según losvaloresdeentrada,yseabordóelprocesodeactualización de parámetros mediante gradiente descendente. además, el repaso permitió conectar la teoría con la implementación práctica en python, reforzando la comprensión del modelo y su utilidad en la clasificación de datos. con esto, se sientan lasbasesparacontinuarcontécnicasmásavanzadasdeaprendizaje supervisado."}
{"id_doc": "DOC_020", "segmentacion": "A", "chunk_id": "DOC_020_A_004", "idx": 4, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "verosimilitud como función de costo y en la aplicación del logaritmoparasimplificarsuderivación.serevisaronejemplos prácticos que ilustran cómo interpretar probabilidades según losvaloresdeentrada,yseabordóelprocesodeactualización de parámetros mediante gradiente descendente. además, el repaso permitió conectar la teoría con la implementación práctica en python, reforzando la comprensión del modelo y su utilidad en la clasificación de datos. con esto, se sientan lasbasesparacontinuarcontécnicasmásavanzadasdeaprendizaje supervisado."}
{"id_doc": "DOC_021", "segmentacion": "A", "chunk_id": "DOC_021_A_000", "idx": 0, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "apuntes de la clase apuntes semana 6 apuntes del 11 de setiembre de 2025 sahid rojas chacón - 2018319311 curso: inteligencia artificial reds@estudiantec.cr resumen-en este documento, se resume la clase del 11 de caso \"no es naranja\" (clase 0).: si wx+b = 1,458, setiembrede2025,enlacuálserealizóprimeramenteunrepaso entonces σ(1,458)≈0,81; como aquí me interesa que no sea delovistoenlaclaseanterior.demanerageneral,estedocumento naranja (y =0), la probabilidad es 1-σ(1,458)≈0,19. para i recopilainformaciónsobreverosimilitudenlaregresiónlogística, el evento complementario (no naranja como etiqueta positiva lafuncióndecostoeinformaciónsobreunnotebookderegresión logística compartido por el profesor. enesaformulación),sereportó0,81;elpuntoesqueelmismo indexterms-verosimilitud,regresiónlogística,gradientedes- f sirve para ambos casos cambiando y i . cendiente, función sigmoide, derivada. caso \"sí es naranja\" (clase 1).: si wx+b = -1,32, entonces σ(-1,32) ≈ 0,21; la probabilidad de sí ser naranja i. notasobretareai (clase 1) se obtiene con 1-σ(-1,32)≈0,79. estos números ilustran cómo interpretar f(x) en los dos escenarios. serecuerdaotorgarladebidaimportanciaalinformeescrito y a su documentación, por cuanto constituirán la base de la iii. porquémetemoslogaritmos retroalimentación para entregas posteriores y para las etapas multiplicar muchas probabilidades puede complicar la desubsiguientes del proyecto. el informe deberá presentar con rivada y además es numéricamente inestable. usamos identiclaridadlosobjetivos,lametodologíaylosresultados,asegurar dades de logaritmos: la reproducibilidad (datos, código, semillas y versiones), e incluir instrucciones de ejecución suficientes y verificables, ln(an)=nlna, ln(ab)=lna+lnb, manteniendo coherencia entre texto, figuras y conclusiones. para convertir el producto en suma. aplicando ln a (1): ii. verosimilitud:ideayfunciónparaeldataset n (cid:88)(cid:2) (cid:3) ii-a. qué es verosimilitud lnl(w,b)= y lnf (x )+(1-y )ln(1-f (x )) . i w,b i i w,b i verosimilitud es: dado un conjunto de datos y un modelo i=1 (4) conparámetros,¿quétanprobableesobservaresosdatosbajo esosparámetros?enbinario,nuestromodelof (x)devuelve esto se llama log-likelihood y es mucho más amigable para w,b una probabilidad en (0,1) y la verosimilitud del dataset se derivar. construye multiplicando las probabilidades individuales. iv. demaximizaraminimizar:negandoel ii-b. modelo y notación log-likelihood usaré f (x)=σ(w⊤x+b), donde σ es la sigmoide: en entrenamiento solemos minimizar. como maximizar (4) w,b es equivalente a minimizar su opuesto, definimos la pérdida 1 σ(t)= . logística promedio: 1+e-t n ii-c. verosimilitud del conjunto l(w,b)=- 1 (cid:88)(cid:104) y lnf (x )+(1-y )ln (cid:0) 1-f (x ) (cid:1)(cid:105) . n i w,b i i w,b i para datos {(x ,y )}n con y ∈{0,1}: i=1 i i i=1 i (5) nota : esta pérdida es ≥ 0 en práctica; si te da negativa, n l(w,b)= (cid:89) f (x )yi (cid:0) 1-f (x ) (cid:1)1-yi. (1) probablemente hay un bug de signos o promedios. el objetivo w,b i w,b i es empujarla hacia cero. i=1"}
{"id_doc": "DOC_021", "segmentacion": "A", "chunk_id": "DOC_021_A_001", "idx": 1, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "conjunto l(w,b)=- 1 (cid:88)(cid:104) y lnf (x )+(1-y )ln (cid:0) 1-f (x ) (cid:1)(cid:105) . n i w,b i i w,b i para datos {(x ,y )}n con y ∈{0,1}: i=1 i i i=1 i (5) nota : esta pérdida es ≥ 0 en práctica; si te da negativa, n l(w,b)= (cid:89) f (x )yi (cid:0) 1-f (x ) (cid:1)1-yi. (1) probablemente hay un bug de signos o promedios. el objetivo w,b i w,b i es empujarla hacia cero. i=1 ii-d. ejemplo narrativo: calabaza/naranja v. quéparámetrossípodemosactualizar la misma fórmula (1) explica los dos casos: los parámetros libres del modelo son w (vector de pesos) y =1⇒f (x )1(cid:0) 1-f (x ) (cid:1)0 =f (x ), (2) y b (sesgo). todo lo demás depende de ellos. por eso, i w,b i w,b i w,b i necesitamos∂l/∂w y∂l/∂bparapoderaplicardescensopor y =0⇒f (x )0(cid:0) 1-f (x ) (cid:1)1 =1-f (x ). (3) gradiente. i w,b i w,b i w,b i vi. composicióndefuncionesyregladela vii. actualizacióndeparámetros(descensopor cadena gradiente) primero reescribo el modelo de forma explícita como com- con learning rate α>0: posición: n 1 (cid:88)(cid:0) (cid:1) z(x)=w⊤x+b, a(z)=σ(z), f w,b (x)=a(z(x)). w ←w-α n a i -y i x i , (14) i=1 la pérdida por muestra (sin el promedio y con el signo n 1 (cid:88)(cid:0) (cid:1) negativo puesto) queda: b←b-α a -y . (15) n i i (cid:104) (cid:0) (cid:1)(cid:105) i=1 l=- y lna(z(x))+(1-y) ln 1-a(z(x)) . (nota : si la curva de pérdida sube o se vuelve errática, probá reducir α.) usamos regla de la cadena: ∂l = ∂l - ∂a - ∂z , (6) viii. código ∂w ∂a ∂z ∂w ∂l ∂l ∂a ∂z en esta sección explico únicamente el código fuente y = - - . (7) cómo implementa la teoría vista en clase: verosimilitud → ∂b ∂a ∂z ∂b log-likelihood → log-loss, derivadas por regla de la cadena vi-a. paso 1: derivada de l respecto a a y actualización de los parámetros w y b. las figuras 1-3 derivando los logaritmos (regla de la cadena incluida): corresponden a tres capturas del archivo main.py. ∂l (cid:104) 1 1 (cid:105) =- y- -(a)′ + (1-y)- -(1-a)′ viii-a. definiciones del modelo: clase, sigmoide y pérdida ∂a a 1-a (cap01) (cid:104) 1 1 (cid:105) =- y- -1 + (1-y)- -(-1) la figura 1 muestra la clase logisticregressionai. a 1-a y 1-y en el constructor (__init__) se fijan los hiperparámetros =- + . (8) lr"}
{"id_doc": "DOC_021", "segmentacion": "A", "chunk_id": "DOC_021_A_002", "idx": 2, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "los parámetros w y b. las figuras 1-3 derivando los logaritmos (regla de la cadena incluida): corresponden a tres capturas del archivo main.py. ∂l (cid:104) 1 1 (cid:105) =- y- -(a)′ + (1-y)- -(1-a)′ viii-a. definiciones del modelo: clase, sigmoide y pérdida ∂a a 1-a (cap01) (cid:104) 1 1 (cid:105) =- y- -1 + (1-y)- -(-1) la figura 1 muestra la clase logisticregressionai. a 1-a y 1-y en el constructor (__init__) se fijan los hiperparámetros =- + . (8) lr (tasa de aprendizaje) y epochs (épocas de entrenamiena 1-a to), y se inicializan los parámetros entrenables w ∈ rd y vi-b. paso 2: derivada de a respecto a z b∈r, que son los únicos valores que el algoritmo ajusta. la derivada de la sigmoide es la famosa forma cerrada: el método sigmoid implementa la activación logística ∂a =σ(z) (cid:0) 1-σ(z) (cid:1) =a(1-a). (9) σ(z)= 1 , z =w⊤x+b, ∂z 1+e-z vi-c. paso 3: derivadas de z respecto a w y b que mapea la combinación lineal z a una probabilidad a = de z(x)=w⊤x+b: σ(z)∈(0,1). lafunciónbinary_cross_entropy_losscodificala ∂z ∂z log-loss (negativa del log-likelihood promedio): =x, =1. (10) ∂w ∂b n vi-d. juntando todo: primero ∂l/∂z l(w,b)=- 1 (cid:88)(cid:2) y lna +(1-y )ln(1-a ) (cid:3) , n i i i i (16) es útil agrupar ∂l = ∂l - ∂a y luego propagar a w y b: i=1 ∂z ∂a ∂z a =σ (cid:0) w⊤x +b (cid:1) . i i ∂l (cid:16) y 1-y(cid:17) = - + -a(1-a) ∂z a 1-a antesdeaplicarloslogaritmos,elcódigorealizaclippingcon =-y(1-a)+(1-y)a ε=10-15 para evitar log(0) y mejorar la estabilidad numérica.minimizar(16)esequivalenteamaximizarlaverosimilitud =a-y. (11) (cid:81) i ay i i(1-a i )1-yi. vi-e. gradientes finales de w y b viii-b. entrenamiento vectorizado y predicción (cap02) usando (11) y (10): lafigura2concentraelcorazóndelaprendizaje:elmétodo ∂l =(a-y)x, (12) fit (bucle de entrenamiento) y predict (inferencia). ∂w forward (vectorizado).: con x ∈ rn×d y w ∈ rd, se ∂l =(a-y). (13) calcula ∂b z =xw+b1, a=σ(z), promediando sobre el dataset (dividiendo entre n y sumando en i) recuperamos las expresiones habituales de la pérdida usando multiplicación matricial de numpy (np.dot). este promedio (5). paso implementa la composición x→z→a vista en clase. figura 3. cap03: flujo completo: imports, generación del dataset con make_classification, división entrenamiento/prueba, entrenamiento delmodelopropioybloqueanálogoconsklearn. viii-c. bloque principal: imports, dataset y flujo del entrenamiento (cap03) figura1. cap01:claselogisticregressionai.hiperparámetros(lr, epochs),parámetrosw,b,sigmoideypérdida(16). como se muestra en la figura 3, se realiza la importación de librerías necesarias (numpy y módulos de sklearn). luego,"}
{"id_doc": "DOC_021", "segmentacion": "A", "chunk_id": "DOC_021_A_003", "idx": 3, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "dataset (dividiendo entre n y sumando en i) recuperamos las expresiones habituales de la pérdida usando multiplicación matricial de numpy (np.dot). este promedio (5). paso implementa la composición x→z→a vista en clase. figura 3. cap03: flujo completo: imports, generación del dataset con make_classification, división entrenamiento/prueba, entrenamiento delmodelopropioybloqueanálogoconsklearn. viii-c. bloque principal: imports, dataset y flujo del entrenamiento (cap03) figura1. cap01:claselogisticregressionai.hiperparámetros(lr, epochs),parámetrosw,b,sigmoideypérdida(16). como se muestra en la figura 3, se realiza la importación de librerías necesarias (numpy y módulos de sklearn). luego, se crea un dataset de clasificación con make_classification, que genera datos sintéticos controlados para problemas binarios. en este caso, se indica: n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=225. esto produce dos features informativas sin redundancia y un solo clúster por clase, coherente con los ejemplos del curso. a continuación, se realiza la división entrenamiento/prueba con train_test_split, manteniendo la proporción de clases mediante stratify=y y fijando random_state para reproducibilidad. seguidamente, se instancia y entrena el modelo implementado desde cero: modelo_manual = logisticregressionai(lr=0.001, epochs=6000), ejecutando el bucle explicado en la subsección anterior (forward, gradientes y actualización). finalmente, el bloque incluye logisticregression de sklearn para replicar figura 2. cap02: fit (forward z→a, gradientes dw/db, actualización) y el mismo enfoque con la librería estándar, lo que sirve como predict(umbral0,5). referencia y valida que la implementación manual respeta la teoría. gradientes(regladelacadena).: deladerivaciónteórica viii-d. resumen código ↔ teoría se obtiene verosimilitud → log-loss. el código minimiza (16), que es ∂l ∂l 1 ∂l 1 =a-y, = x⊤(a-y), = 1⊤(a-y). -logl; así, \"minimizar la pérdida\" equivale a \"maximizar la ∂z ∂w n ∂b n verosimilitud\". regla de la cadena. la ruta x→z→a→l da ∂l =a-y, en el código aparecen como dw = (1/n_samples) ∂z base de los gradientes vectorizados dw y db. * x.t @ (a - y) y db = (1/n_samples) * parámetros actualizables. solo w y b cambian; el resto (signp.sum(a - y). moide, datos) son transformaciones/entradas fijas conforme a actualización (descenso por gradiente).: con tasa α = la formulación del modelo. lr: ∂l ∂l w ←w-α , b←b-α , ix. conclusiones ∂w ∂b laclaveparanoperderseesmirarlacomposiciónx→z → que en el código se implementa como w = w - lr * dw a→l y empujar las derivadas con la regla de la cadena. el y b = b - lr * db. al disminuir (16) se incrementa la uso de logaritmos cambia productos por sumas y, al negar verosimilitud del modelo. el log-likelihood, pasamos a minimizar una función"}
{"id_doc": "DOC_021", "segmentacion": "A", "chunk_id": "DOC_021_A_004", "idx": 4, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "α = la formulación del modelo. lr: ∂l ∂l w ←w-α , b←b-α , ix. conclusiones ∂w ∂b laclaveparanoperderseesmirarlacomposiciónx→z → que en el código se implementa como w = w - lr * dw a→l y empujar las derivadas con la regla de la cadena. el y b = b - lr * db. al disminuir (16) se incrementa la uso de logaritmos cambia productos por sumas y, al negar verosimilitud del modelo. el log-likelihood, pasamos a minimizar una función estable predicción.: el método predict repite z =xw+b y y derivable. con los gradientes compactos (a-y)x y (aa=σ(z)yumbralizacon0,5paradevolveretiquetasbinarias y), actualizar w y b se vuelve mecánico con descenso por yˆ∈{0,1}, consistente con decidir por la clase más probable. gradiente. apéndice:fórmulasrelevantes sigmoide y derivada: σ(t)= 1 , σ′(t)=σ(t) (cid:0) 1-σ(t) (cid:1) . 1+e-t verosimilitud y log-likelihood: l= (cid:89) f(x )yi (cid:0) 1-f(x ) (cid:1)1-yi, lnl= (cid:88)(cid:2) y lnf(x )+(1-y )ln(1-f(x )) (cid:3) . i i i i i i i i pérdida logística promedio (lo que minimizo): 1 (cid:88)(cid:104) (cid:0) (cid:1)(cid:105) l(w,b)=- y lnf(x )+(1-y )ln 1-f(x ) . n i i i i i gradientes (por muestra): ∂l y 1-y ∂a ∂z ∂z =- + , =a(1-a), =x, =1, ∂a a 1-a ∂z ∂w ∂b ∂l ∂l ∂l =a-y, =(a-y)x, =(a-y). ∂z ∂w ∂b"}
{"id_doc": "DOC_022", "segmentacion": "A", "chunk_id": "DOC_022_A_000", "idx": 0, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "apuntes de la clase del 16 de setiembre de 2025 cursodeinteligenciaartificial nelson rojas obando estudiante ingeniería en computación nelson.rojas@estudiantec.cr resumen-this paper summarizes the main topics discussed la regresión lineal y la regresión logística son técnicas during the artificial intelligence course on september 16, 2025. fundamentales en el aprendizaje supervisado, pero se it covers the quiz about concepts such as linear and logistic aplican a diferentes tipos de problemas: regression, concepts as techniques for handling outliers, and strategies to reduce high bias and high variance in machine regresión lineal: se utiliza cuando la variable delearning models. furthermore, it presents evaluation metrics pendiente es continua. el modelo estima una relación including accuracy, precision, recall, f1-score, confusion matrix, lineal entre las variables independientes y la variable roc curve, and auc, illustrated with practical case studies. dependiente, siguiendo la forma: finally,thepaperhighlightstheimportanceofdatapreprocessing tasks-such as cleaning, integration, reduction, transformation, y =β +β x +---+β x +ε and discretization-as essential steps to improve the quality of 0 1 1 n n datasets and ensure better performance of predictive models. donde y puede tomar cualquier valor real. index terms-inteligencia artificial, machine learning, métricas de evaluación, matriz de confusión, roc, auc, data regresión logística: se emplea cuando la variable preprocessing dependiente es categórica, típicamente binaria (0 o 1). el modelo estima la probabilidad de pertenecer a una i. introducción clase utilizando la función sigmoide: la inteligencia artificial (ia) y el aprendizaje automático 1 requieren no solo del diseño de modelos predictivos, sino p(y =1|x)= 1+e-(β0+β1x1+---+βnxn) también de procesos rigurosos para evaluar su desempeño y garantizar su aplicabilidad en escenarios reales. en este de esta forma, la salida está acotada en el intervalo documento se abordan conceptos fundamentales que permiten [0,1] y se interpreta como probabilidad. comprender la relación entre los modelos, las métricas de 2. describa 3 técnicas para el tratamiento de datos sobresaevaluación y la calidad de los datos utilizados en su entre- lientes. namiento. respueta: enprimerlugar,seestudianmétricasclásicascomolaexac- los datos sobresalientes, también conocidos como titud, precisión, exhaustividad y f1-score, así como métricas outliers, son observaciones que se desvían significativamás avanzadas como la curva roc y el área bajo la curva mente del resto de los datos. su presencia puede afectar (auc), las cuales proporcionan una visión más completa del de manera negativa el rendimiento de los modelos de rendimiento de un clasificador. aprendizaje automático. entre las técnicas más comunes además, se presenta la matriz de confusión como herra- para"}
{"id_doc": "DOC_022", "segmentacion": "A", "chunk_id": "DOC_022_A_001", "idx": 1, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "datos sobresalientes, también conocidos como titud, precisión, exhaustividad y f1-score, así como métricas outliers, son observaciones que se desvían significativamás avanzadas como la curva roc y el área bajo la curva mente del resto de los datos. su presencia puede afectar (auc), las cuales proporcionan una visión más completa del de manera negativa el rendimiento de los modelos de rendimiento de un clasificador. aprendizaje automático. entre las técnicas más comunes además, se presenta la matriz de confusión como herra- para su tratamiento se encuentran: mienta central para interpretar los aciertos y errores de los a) eliminacióndeoutliers:consisteendescartaraquellas modelos, junto con un caso práctico aplicado a la detección observaciones que superan un umbral definido, por de cáncer en pacientes. asimismo, se destacan las principales ejemplo, valores que se encuentran a más de tres tareas del preprocesamiento de datos, entre ellas la limpie- desviaciones estándar de la media. esta técnica es útil za, integración, reducción, transformación y discretización, cuando los outliers provienen de errores de medición esenciales para enfrentar problemas como datos incompletos, o registro. inconsistentes o ruidosos. b) transformacionesdelosdatos:aplicartransformacionesmatemáticas,comolatransformaciónlogarítmicao ii. aspectosadministrativos la raíz cuadrada, puede reducir la influencia de valores ver dos lecturas asociadas con lectura procesamiento de extremos, estabilizando la varianza y mejorando la datos y de redes neuronales además de dos capítulos de un distribución de los datos. libro. c) imputación o sustitución de valores: reemplazar se realizó el quiz #3, donde al finalizar se vieron las los outliers por valores más representativos, como la respuestas correspondientes. el quiz consistió en: media, la mediana o un valor obtenido mediante inter1. mencione la diferencia de regresión lineal y logística. polación.estatécnicaconservaeltamañodelconjunto respuesta: dedatosyesútilcuandolaeliminaciónnoesdeseable. 3. mencione2técnicasparaevitarunaltosesgoy2técnicas para evitar alta varianza. respuesta: en el contexto del aprendizaje automático, el alto sesgo (underfitting) y la alta varianza (overfitting) son problemas comunes. algunas técnicas para mitigarlos son las siguientes: para evitar un alto sesgo: a) aumentar la complejidad del modelo, por ejemplo, utilizando modelos polinómicos en lugar de regresión lineal simple, o redes neuronales más profundas. figura1. entercaption b) reducir la regularización excesiva, ajustando los hiperparámetrosdetécnicascomol1/l2odropout, que en exceso limitan la capacidad de aprendizaje del modelo. para evitar una alta varianza: a) aumentar la cantidad de datos de entrenamiento, mediante recolección adicional o técnicas de data augmentation. b) aplicar regularización, como l1/l2, dropout o early stopping, con el fin de penalizar la complejifigura2. entercaption dad excesiva y mejorar la generalización. 4. anote la derivada de la"}
{"id_doc": "DOC_022", "segmentacion": "A", "chunk_id": "DOC_022_A_002", "idx": 2, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "lugar de regresión lineal simple, o redes neuronales más profundas. figura1. entercaption b) reducir la regularización excesiva, ajustando los hiperparámetrosdetécnicascomol1/l2odropout, que en exceso limitan la capacidad de aprendizaje del modelo. para evitar una alta varianza: a) aumentar la cantidad de datos de entrenamiento, mediante recolección adicional o técnicas de data augmentation. b) aplicar regularización, como l1/l2, dropout o early stopping, con el fin de penalizar la complejifigura2. entercaption dad excesiva y mejorar la generalización. 4. anote la derivada de la función sigmoide 1 elcasodelamujer,siexistelaposibilidadporloquesepodría σ(x)= dar un resultado equivocado, y se conoce como error de tipo 1+e-x 2. respuesta: ejemplos de métricas clásicas: σ′(x)=σ(x) (cid:0) 1-σ(x) (cid:1) accuracy (exactitud): mide la proporción de prediccionescorrectassobreeltotaldeprediccionesrealizadas.es iii. métricas ampliamente usada en problemas de clasificación, como son medidas que se utilizan para indicar el rendimiento en la regresión logística. se define como: de un modelo predictvo. constituyen la forma más objetiva de evaluar y comparar modelos de aprendizaje automático, tp +tn accuracy= permitiendo determinar qué tan bien se ajustan a los datos de tp +tn +fp +fn entrenamiento y, sobre todo, qué tan bien generalizan a datos donde tp (verdaderos positivos), tn (verdaderos negano vistos. tivos), fp (falsos positivos) y fn (falsos negativos). asimismo, se emplean benchmarks, que son conjuntos de este tipo de metrica otorga importancia igual a todas las datos o pruebas estandarizadas utilizadas en la comunidad clases. es importante tomar esto en cuenta si las clases científica para comparar de manera justa el desempeño de no están balanceadas. puede no ser suficiente y da como distintos modelos. resultado un valor porcentual (de 0 a 1). para un modelo iv. matrizdeconfusión bien hecho se esperaría que se acerque bastante a 1. en la matriz de confusión se colocan las clases y se realiza precisión (precision): mide la proporción de predicuna clasificación según su posibilidad y veracidad, como se ciones positivas correctas entre todas las predicciones muestra en la figura 1. de esta forma se obtienen true positivas realizadas, como los errores de tipo 1: positive (verdadero positivo), false positive (falso positivo), tp true negative (verdadero negativo) y false negative (falso precision= tp +fp negativo). esta tabla puede ser n x n clases y al hacer un ploteo de esta tabla se espera que la diagonal esté dando exhaustividad (recall): indica la proporción de verdavalores verdaderos. deros positivos identificados correctamente sobre el total un ejemplo práctico de esto es el caso de"}
{"id_doc": "DOC_022", "segmentacion": "A", "chunk_id": "DOC_022_A_003", "idx": 3, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "esta forma se obtienen true positivas realizadas, como los errores de tipo 1: positive (verdadero positivo), false positive (falso positivo), tp true negative (verdadero negativo) y false negative (falso precision= tp +fp negativo). esta tabla puede ser n x n clases y al hacer un ploteo de esta tabla se espera que la diagonal esté dando exhaustividad (recall): indica la proporción de verdavalores verdaderos. deros positivos identificados correctamente sobre el total un ejemplo práctico de esto es el caso de la figura 2. en el de elementos positivos, y se usa para medir los errores queseevaluánelresultadodeembarazoenhombresymujeres. de tipo 2: claramente un hombre no puede embarazarse por lo que de tp obtener un resultado positivo este sería un error de tipo 1. en recall= tp +fn f1-score:eslamediaarmónicaentreprecisiónyexhaus- 1 tividad, útil cuando existe un desbalance de clases: precision-recall f1=20,8 precision+recall caso de estudio: dado un conjunto de 1000 pacientes se han realizado estudios para determinar la presencia de cancer. 0,6 del total de pacientes, 30 son pacientes con cáncer (clase positiva) y 970 pacientes sin cáncer (clase negativa). matriz de confusión: 0,4 predicción/objetivo cáncer no cáncer cáncer 25(tp) 20(fp) no cáncer 5(fn) 950(tn) 0,2 métricas de evaluación: accuracy: 0 tp +tn 25+950 0 0,2 0,4 0,6 0,8 1 accuracy= = =0,975(97,5%) total 1000 tasa de falsos positivos (fpr) recall (sensibilidad): tp 25 recall= = =0,833(83,3%) tp +fn 25+5 indica la capacidad del modelo para identificar correctamente a los pacientes con cáncer. a pesar del alto accuracy, el recall muestra espacio de mejora en la detección de la clase positiva (cáncer). precisión: tp 25 precisión= = =0,55(55%) tp +fp 25+20 el modelo presenta una baja precisión, lo cual implica que muchas predicciones positivas son en realidad falsos positivos. este valor debe considerarse con cautela, dependiendo del contexto clínico. f1-score: 2-precisión-recall 2-0,55-0,833 f1= = ≈0,662(66,2%) precisión+recall 0,55+0,833 el f1-score refleja un balance bajo entre precisión y sensibilidad, indicando que la capacidad del modelo para clasificar correctamente la clase minoritaria (cáncer) aún no es adecuada. otras métricas no tan básicas: receiver operating characteristic (roc) lacurvarocesunarepresentacióngráficaquemuestra el rendimiento de un clasificador binario a diferentes umbralesdedecisión.enelejexserepresentalatasade falsos positivos (fpr) y en el eje y la tasa de verdaderos positivos (tpr o recall). una curva más cercana a la esquina superior izquierda indica un mejor desempeño del modelo. area under the curve (auc) el auc corresponde al área bajo la curva roc. su valor varía entre 0 y"}
{"id_doc": "DOC_022", "segmentacion": "A", "chunk_id": "DOC_022_A_004", "idx": 4, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "clasificar correctamente la clase minoritaria (cáncer) aún no es adecuada. otras métricas no tan básicas: receiver operating characteristic (roc) lacurvarocesunarepresentacióngráficaquemuestra el rendimiento de un clasificador binario a diferentes umbralesdedecisión.enelejexserepresentalatasade falsos positivos (fpr) y en el eje y la tasa de verdaderos positivos (tpr o recall). una curva más cercana a la esquina superior izquierda indica un mejor desempeño del modelo. area under the curve (auc) el auc corresponde al área bajo la curva roc. su valor varía entre 0 y 1, donde un valor de 0,5 indica un modelosincapacidaddediscriminación(equivalenteaun clasificador aleatorio), mientras que un valor cercano a 1 representa un modelo con alto poder de discriminación. )rpt( sovitisop soredadrev ed asat clasificador aleatorio modelo figura3. ejemplodecurvaroc.unáreabajolacurva(auc)máscercano a1indicamejorrendimiento. de darse un caso en el que la curva, como por ejemplo en la figura3, se acercaramucho a larecta perderíavalor porque estaríadandolosresultadosincorrectos,deunaformacasique aleatoria. el área bajo la curva debe de ser de al menos 0,5. procesamiento de datos problemas encontrados incompletitud: valores faltantes en atributos importantes, ej. si el producto estaba en oferta. inexactitud o ruido: errores y valores atípicos en las transacciones. inconsistencia: discrepancias en los códigos de departamentos o categorías. sediocomocomparaciónelcasodeladiabetesylapresión sanguínea. si se registran mediciones, se espera que tenga un valor sino tiene presión sanguínea entonces no tendría sentido o el sujeto estaría muerto, sería un valor que no aporta pero esimportantenoelimiarelregistroyaquepodríanhaberotras features que si aporten valor. porestarazón,losdatasetsrequirenpreprocesamientoantes de aplicar técnicas de minería o aprendizaje. es un problema del mundo real. porque pueden ser inexactos? instrumentos de recolección de datos defectuosos errores humanos o computacionales en la entrada de datos usuarios que ingresan valores falsos para campos obligatorios (ej. fecha por defecto '1 de enero' para ocultar cumpleaños) - conocido como datos faltantes disfrazados inconsistencias en convenciones de nombres, códigos o formatos (ej. fechas con distintos formatos) tuplas duplicadas que requieren procesos de data clea- ◦ el borde más cercano del bin. ning - ejemplo: valores de salarios. por qué los datos pueden estar incompletos? suavizado de ruido: atributos de interés no siempre están disponibles - ajustar una función matemática a los datos (puede ser noseincluyenporquenoseconsideraronimportantesen lineal o no lineal). el momento de la entrada ◦ ejemplo: ventas mensuales con fluctuaciones; se datos relevantes no se registran por malentendidos o ajusta una regresión lineal para capturar la tendenfallos de equipo cia general y se consideran ruido los valores que datos inconsistentes con otros registros pueden ser elise desvían demasiado. minados"}
{"id_doc": "DOC_022", "segmentacion": "A", "chunk_id": "DOC_022_A_005", "idx": 5, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "los datos pueden estar incompletos? suavizado de ruido: atributos de interés no siempre están disponibles - ajustar una función matemática a los datos (puede ser noseincluyenporquenoseconsideraronimportantesen lineal o no lineal). el momento de la entrada ◦ ejemplo: ventas mensuales con fluctuaciones; se datos relevantes no se registran por malentendidos o ajusta una regresión lineal para capturar la tendenfallos de equipo cia general y se consideran ruido los valores que datos inconsistentes con otros registros pueden ser elise desvían demasiado. minados - aplicar técnicas de filtrado, como la media móvil. historial o modificaciones de datos pasados pueden no haberse registrado ◦ ejemplo: datos diarios de temperatura con ruido; valores faltantes en atributos clave pueden necesitar ser se calcula la media móvil de 7 días. inferidos. data integration: manejo de redundancia: por que los datos pueden ser inconsistentes? lamismainformaciónpuedeestarregistradavariasveces diferencias en convenciones de nombres o códigos usa- o con valores distintos. dos para clasificar elementos - ejemplo: un cliente registrado como \"juan pérez\" y formatos de entrada distintos para un también como \"j. a. pérez\". principales tareas en el preprocesamiento de datos: - direccionesalmacenadascomo\"sanjosé,costarica\" datacleaning(limpiezadedatos):eliminaciónderuido, en una base de datos y como \"sj-cr\" en otra. corrección de inconsistencias y tratamiento de valores uso de pruebas estadísticas para detectar redundancia o faltantes. asociación entre variables: data integration (integración de datos): combinación - prueba de correlación χ2 para datos nominales: de información proveniente de múltiples fuentes hetero- mide la asociación entre variables categóricas. géneas en un repositorio coherente y unificado. - hipótesis de independencia: data reduction (reducción de datos): disminución del h :p(a ∩b )=p(a )p(b ) volumen mediante selección de atributos relevantes, re- 0 i j i j ducción de dimensionalidad o discretización. - las variables se consideran independientes si: datatransformation(transformacióndedatos):incluyenormalización,estandarización,agregaciónyconstrucχ2 calculado ≤χ2 α,df ción de nuevas variables. data discretization (discretización de datos): transformación de atributos continuos en atributos categóricos para facilitar el análisis y la modelización. data cleaning: tratamiento de valores faltantes y ruido: tratamiento de valores faltantes: - ignorar tuplas con valores faltantes (puede llevar a la pérdida de datos). - completar manualmente los valores faltantes (costoso y poco práctico en grandes datasets). - usar un valor global constante (por ejemplo: \"desconocido\", ∞). - rellenar con la media, mediana o moda. también puede hacerse por clase. ◦ ejemplo: en clasificación de clientes por riesgo crediticio, reemplazar con el ingreso promedio de clientes en la misma categoría de riesgo. - inferir valores"}
{"id_doc": "DOC_022", "segmentacion": "A", "chunk_id": "DOC_022_A_006", "idx": 6, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "y ruido: tratamiento de valores faltantes: - ignorar tuplas con valores faltantes (puede llevar a la pérdida de datos). - completar manualmente los valores faltantes (costoso y poco práctico en grandes datasets). - usar un valor global constante (por ejemplo: \"desconocido\", ∞). - rellenar con la media, mediana o moda. también puede hacerse por clase. ◦ ejemplo: en clasificación de clientes por riesgo crediticio, reemplazar con el ingreso promedio de clientes en la misma categoría de riesgo. - inferir valores mediante modelos estadísticos o de aprendizaje automático (regresión, k-nn, árboles de decisión). binning (agrupación en intervalos): - reemplazar cada valor por: ◦ la media del bin. ◦ la mediana del bin."}
{"id_doc": "DOC_023", "segmentacion": "A", "chunk_id": "DOC_023_A_000", "idx": 0, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "apuntes de la clase semana 7 2025 curso de inteligencia artificial rafael vargas solis apuntes del 16 de setiembre de 2025 resumen-este documento presenta un resumen de los temas deestamanera,sedisminuyesuinfluenciaenlavarianza clave abordados en la clase de inteligencia artificial del 16 del modelo y se mejora la distribucio'n de los datos. de setiembre de 2025. comienza describiendo las diferencias - winsorizacio'n (recorte): sustituir los valores at'ıpicos fundamentalesentrelaregresio'nlinealylaregresio'nlog'ıstica,as'ı por valores ma's cercanos dentro de un rango definido, comolaste'cnicascomunesparaelmanejodevaloresat'ıpicosylas estrategiasparaenfrentarelsesgoylavarianzaenlosmodelosde usualmente basado en percentiles (por ejemplo, 1% y aprendizajeautoma'tico.posteriormente,sediscutenlasme'tricas 99%). esta te'cnica conserva la estructura general de los de evaluacio'n tanto para clasificacio'n como para regresio'n, in- datos y evita que los valores extremos distorsionen los cluyendolamatrizdeconfusio'n,precisio'n,exhaustividad(recall), resultados. f1-score, curva roc y el a'rea bajo la curva (auc), apoyadas en ejemplos pra'cticos. finalmente, el documento resalta los 3. mencione dos te'cnicas para evitar un alto sesgo y dos problemas ma's frecuentes en la calidad de los datos -como la incompletitud, la inexactitud y la inconsistencia- y enfatiza para evitar alta varianza. la importancia de las tareas de preprocesamiento, tales como en el aprendizaje automa'tico, es fundamental lograr un limpieza,integracio'n,reduccio'n,transformacio'nydiscretizacio'n, equilibrio entre sesgo y varianza para obtener modelos con para garantizar el desarrollo de modelos predictivos robustos y buena capacidad de generalizacio'n. a continuacio'n, se deconfiables. scriben algunas te'cnicas para abordar ambos problemas: index terms-inteligencia artificial, aprendizaje automa'tico, regresio'n, valores at'ıpicos, sesgo, varianza, me'tricas de eval- para reducir sesgo (underfitting): uacio'n, matriz de confusio'n, roc, auc, preprocesamiento de - incrementar la complejidad del modelo: utilizar moddatos elos ma's sofisticados, como polinomiales en lugar de lineales,redesneuronalesma'sprofundasoalgoritmosno i. preguntasdelquiz lineales, permite capturar relaciones ma's complejas entre 1. ¿cua'l es la diferencia entre regresio'n lineal y re- las variables. gresio'n log'ıstica? - incorporar nuevas variables o caracter'ısticas: mela regresio'n lineal se utiliza para predecir variables con- diante te'cnicas de feature engineering, se pueden intinuas, ajustando una recta que minimiza el error cuadra'tico cluir atributos relevantes que enriquezcan la informacio'n medio. por ejemplo, estimar el precio de una vivienda segu'n disponible, mejorando as'ı la capacidad predictiva del su taman˜o. modelo. en cambio, la regresio'n log'ıstica se aplica a problemas para reducir varianza (overfitting): de clasificacio'n, donde la variable dependiente es catego'rica - aplicar regularizacio'n: me'todos como l1 (lasso) y (binaria en la mayor'ıa de casos). utiliza la funcio'n sigmoide l2 (ridge) an˜aden penalizaciones a los coeficientes del paramapearlosvaloresdeentradaenprobabilidadesentre0y modelo,limitandosumagnitudyevitandoqueelmodelo 1. ejemplo: predecir si un estudiante aprobara' o no"}
{"id_doc": "DOC_023", "segmentacion": "A", "chunk_id": "DOC_023_A_001", "idx": 1, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "la informacio'n medio. por ejemplo, estimar el precio de una vivienda segu'n disponible, mejorando as'ı la capacidad predictiva del su taman˜o. modelo. en cambio, la regresio'n log'ıstica se aplica a problemas para reducir varianza (overfitting): de clasificacio'n, donde la variable dependiente es catego'rica - aplicar regularizacio'n: me'todos como l1 (lasso) y (binaria en la mayor'ıa de casos). utiliza la funcio'n sigmoide l2 (ridge) an˜aden penalizaciones a los coeficientes del paramapearlosvaloresdeentradaenprobabilidadesentre0y modelo,limitandosumagnitudyevitandoqueelmodelo 1. ejemplo: predecir si un estudiante aprobara' o no un curso. se ajuste excesivamente a los datos de entrenamiento. - aumentar los datos o usar te'cnicas de ensamble: 2. describa tres te'cnicas para el tratamiento de datos incrementar el taman˜o del conjunto de entrenamiento o sobresalientes (outliers). aplicarme'todoscomobaggingyrandomforestmejorala los outliers o valores at'ıpicos son observaciones que se estabilidad del modelo y reduce la sensibilidad al ruido alejan significativamente del patro'n general de los datos y de los datos. puedenafectarlaprecisio'ndelosmodelospredictivos.existen diversas estrategias para tratarlos, entre las cuales destacan: 4. ¿cua'l es la derivada de la funcio'n sigmoide σ(x) = - eliminacio'n: consiste en remover los outliers identifi- 1 ? 1+e-x cados cuando se determina que son producto de errores la funcio'n sigmoide se define como: de medicio'n, registros defectuosos o inconsistencias en 1 la recoleccio'n de datos. esta te'cnica debe aplicarse con σ(x)= (1) cautela para no eliminar informacio'n valiosa. 1+e-x - transformacio'n de variables: aplicar funciones su derivada es: matema'ticas como logaritmos, ra'ıces cuadradas o escalados que reduzcan la magnitud de los valores extremos. σ′(x)=σ(x)(1-σ(x)) (2) ii. me'tricas a. accuracy son medidas que se utilizan para indicar el rendimiento tp +tn de un modelo predictivo. constituyen la forma ma's objetiva accuracy = (3) tp +tn +fp +fn de evaluar y comparar modelos de aprendizaje automa'tico, permitiendo determinar que' tan bien se ajustan a los datos de mide la proporcio'n de predicciones correctas. es u'til en datos entrenamiento y, sobre todo, que' tan bien generalizan a datos balanceados, pero engan˜osa en clases desbalanceadas. no vistos. b. precisio'n asimismo, se emplean benchmarks, que son conjuntos de datos o pruebas estandarizadas utilizadas en la comunidad tp cient'ıfica para comparar de manera justa el desempen˜o de precision= (4) distintos modelos. el uso de benchmarks permite establecer tp +fp un esta'ndar de referencia que facilita la reproducibilidad y la indicaque' proporcio'ndeprediccionespositivasfueroncorreccomparacio'n entre diferentes enfoques. tas. relevante cuando los falsos positivos son costosos. en general, las me'tricas pueden dividirse en: c. recall (sensibilidad) - me'tricasdeclasificacio'n:accuracy,precision,recall,f1score,"}
{"id_doc": "DOC_023", "segmentacion": "A", "chunk_id": "DOC_023_A_002", "idx": 2, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "en clases desbalanceadas. no vistos. b. precisio'n asimismo, se emplean benchmarks, que son conjuntos de datos o pruebas estandarizadas utilizadas en la comunidad tp cient'ıfica para comparar de manera justa el desempen˜o de precision= (4) distintos modelos. el uso de benchmarks permite establecer tp +fp un esta'ndar de referencia que facilita la reproducibilidad y la indicaque' proporcio'ndeprediccionespositivasfueroncorreccomparacio'n entre diferentes enfoques. tas. relevante cuando los falsos positivos son costosos. en general, las me'tricas pueden dividirse en: c. recall (sensibilidad) - me'tricasdeclasificacio'n:accuracy,precision,recall,f1score, roc y auc. tp - me'tricas de regresio'n: error cuadra'tico medio (mse), recall= (5) error absoluto medio (mae) o coeficiente de determi- tp +fn nacio'n (r2). mide la capacidad del modelo para identificar correctamente los positivos. importante en contextos donde los falsos negaiii. matrizdeconfusio'n tivos son cr'ıticos. lamatrizdeconfusio'norganizalosresultadosdeunmodelo d. f1-score declasificacio'nenfuncio'ndelasprediccionesrealizadasylas clases reales. se definen cuatro componentes: 2-precision-recall - true positive (tp):positivoscorrectamenteclasificados. f1= precision+recall (6) - false positive (fp): negativos clasificados incorrectaes la media armo'nica entre precisio'n y recall, usada en casos mente como positivos (error tipo i). de desbalance de clases. - true negative (tn): negativos correctamente clasificados. v. casodeestudio - false negative (fn): positivos clasificados incorrectamente como negativos (error tipo ii). se evaluo' un modelo de deteccio'n de ca'ncer con 1000 pacientes. en problemas multiclase, la matriz puede extenderse a n ×n. un clasificador ideal concentra todos los valores en - clase positiva: 30 pacientes con ca'ncer. la diagonal principal. - clase negativa: 970 pacientes sin ca'ncer. matriz de confusio'n: ca'ncer no ca'ncer ca'ncer 25 (tp) 20 (fp) no ca'ncer 5 (fn) 950 (tn) resultados: - accuracy: 25+950 =97.5% 1000 - recall: 25 =83.3% 25+5 - precisio'n: 25 =55% 25+20 - f1-score: 2-0.55-0.833 ≈66.2% 0.55+0.833 a pesar del alto valor de accuracy, las me'tricas muestran limitaciones en la deteccio'n de la clase positiva. vi. me'tricasavanzadas fig.1. ejemplodematrizdeconfusio'nenclasificacio'nbinaria. a. curva roc la curva roc (receiver operating characteristic) muesiv. me'tricascla'sicas tra el desempen˜o de un clasificador binario para distintos a partir de la matriz de confusio'n se derivan las me'tricas umbrales. representa la tasa de verdaderos positivos (tpr) ma's utilizadas: frente a la tasa de falsos positivos (fpr). - inconsistencias en convenciones de nombres, co'digos o formatos. - registros duplicados que requieren procesos de data cleaning. c. causas de incompletitud - atributos de intere's no siempre disponibles o considerados irrelevantes en el momento de captura. - fallos te'cnicos o malentendidos durante la recoleccio'n. - eliminacio'n de registros por inconsistencias. -"}
{"id_doc": "DOC_023", "segmentacion": "A", "chunk_id": "DOC_023_A_003", "idx": 3, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "partir de la matriz de confusio'n se derivan las me'tricas umbrales. representa la tasa de verdaderos positivos (tpr) ma's utilizadas: frente a la tasa de falsos positivos (fpr). - inconsistencias en convenciones de nombres, co'digos o formatos. - registros duplicados que requieren procesos de data cleaning. c. causas de incompletitud - atributos de intere's no siempre disponibles o considerados irrelevantes en el momento de captura. - fallos te'cnicos o malentendidos durante la recoleccio'n. - eliminacio'n de registros por inconsistencias. - ausencia de historial o modificaciones no registradas. - valores faltantes en atributos clave que deben ser inferidos. d. causas de inconsistencias - diferencias en convenciones de nombres o co'digos. - formatos de entrada distintos para un mismo atributo. - conflictos entre bases de datos heteroge'neas. fig.2. ejemplodecurvarocyca'lculodeauc. - errores en la integracio'n de fuentes mu'ltiples. - actualizaciones parciales que dejan registros contradicb. a'rea bajo la curva (auc) torios. el auc mide el a'rea bajo la curva roc: e. principales tareas en el preprocesamiento - auc = 0.5: clasificador aleatorio. - auc cercano a 1: modelo con gran poder de discrimi- - data cleaning: eliminacio'n de ruido, correccio'n de inconsistencias y tratamiento de valores faltantes. nacio'n. - data integration: combinacio'n de datos provenientes de vii. procesamientodedatos mu'ltiples fuentes heteroge'neas en un repositorio unificado. a. problemas encontrados en la calidad de datos - data reduction: reduccio'n de volumen mediante seen escenarios reales, los datos suelen presentar problemas leccio'n de atributos, reduccio'n de dimensionalidad o que afectan directamente la efectividad de los algoritmos de discretizacio'n. miner'ıa y aprendizaje automa'tico. los principales son: - data transformation: normalizacio'n, estandarizacio'n, - incompletitud:valoresfaltantesenatributosimportantes. agregacio'n y construccio'n de nuevas variables. ejemplo: si un producto estaba en oferta y no se registro' - data discretization: transformacio'n de atributos continla variable. uos en categor'ıas o intervalos. - inexactitud o ruido: errores de medicio'n, valores at'ıpicos o entradas ano'malas en transacciones. f. data cleaning: tratamiento de valores faltantes y ruido - inconsistencia: discrepancias en nombres, co'digos 1) valores faltantes: o formatos. ejemplo: fechas almacenadas como - ignorar tuplas con valores faltantes (riesgoso si se pierde dd/mm/aaaa en una base de datos y como mucha informacio'n). mm-dd-yyyy en otra. - completar manualmente (costoso en grandes datasets). uncasoilustrativoeslarecoleccio'ndedatosme'dicos:enla - usar un valor global constante (ej. \"desconocido\"). medicio'n de presio'n sangu'ınea, un valor faltante no implica - rellenar con la media, mediana o moda, tambie'n por que el registro deba eliminarse, ya que otras"}
{"id_doc": "DOC_023", "segmentacion": "A", "chunk_id": "DOC_023_A_004", "idx": 4, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "inconsistencia: discrepancias en nombres, co'digos 1) valores faltantes: o formatos. ejemplo: fechas almacenadas como - ignorar tuplas con valores faltantes (riesgoso si se pierde dd/mm/aaaa en una base de datos y como mucha informacio'n). mm-dd-yyyy en otra. - completar manualmente (costoso en grandes datasets). uncasoilustrativoeslarecoleccio'ndedatosme'dicos:enla - usar un valor global constante (ej. \"desconocido\"). medicio'n de presio'n sangu'ınea, un valor faltante no implica - rellenar con la media, mediana o moda, tambie'n por que el registro deba eliminarse, ya que otras caracter'ısticas clase. (edad, peso, historial cl'ınico) s'ı aportan informacio'n valiosa. - inferir valores mediante modelos estad'ısticos o de ml esto demuestra que los datasets requieren preprocesamiento (regresio'n, k-nn, a'rboles de decisio'n). antes de aplicar te'cnicas de miner'ıa o aprendizaje. 2) binning: binning agrupa valores en intervalos (bins) y b. causas de datos defectuosos reemplaza cada valor por: - instrumentos de recoleccio'n defectuosos. - la media del bin. - errores humanos o computacionales en la entrada de - la mediana del bin. datos. - el borde ma's cercano del bin. - valoresfalsosencamposobligatorios(ejemplo:fechapor ejemplo: salarios ruidosos [2950, 3000, 3020, 8000]. el bin defecto\"1deenero\"paraocultarcumplean˜os),conocidos (2900-3100) se reemplaza por la media (2990), mientras que como datos faltantes disfrazados. 8000 queda como posible outlier. 3) suavizado de ruido: - ajustar una funcio'n matema'tica (lineal o no lineal) para suavizar fluctuaciones. ejemplo: regresio'n lineal en ventas mensuales. - aplicar te'cnicas de filtrado como la media mo'vil: 6 1(cid:88) ma (t)= x 7 7 t-i i=0 donde x es el valor en el d'ıa t. esto genera una curva t suavizada que refleja la tendencia real. g. data integration: manejo de redundancia la misma informacio'n puede estar registrada varias veces o con diferencias. ejemplo: un cliente como \"juan pe'rez\" en una base de datos y \"j. a. pe'rez\" en otra. se aplican pruebas estad'ısticas como la chi-cuadrado (χ2) para detectar redundancia o asociaciones entre variables catego'ricas: h :p(a ∩b )=p(a )p(b ) 0 i j i j siχ2 ≤χ2 ,seaceptalahipo'tesisdeindependencia. calculado α,df references [1] a.burkov,thehundred-pagemachinelearningbook.andriyburkov, 2019.[online].available:https://themlbook.com/"}
{"id_doc": "DOC_023", "segmentacion": "A", "chunk_id": "DOC_023_A_005", "idx": 5, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "siχ2 ≤χ2 ,seaceptalahipo'tesisdeindependencia. calculado α,df references [1] a.burkov,thehundred-pagemachinelearningbook.andriyburkov, 2019.[online].available:https://themlbook.com/"}
{"id_doc": "DOC_024", "segmentacion": "A", "chunk_id": "DOC_024_A_000", "idx": 0, "autor": "Darío Espinoza Aguilar", "fecha": "2025-09-18", "tema": "Repaso de métricas de rendimiento, matriz de confusión, precisión, recall, F1-score y tareas esenciales de preprocesamiento de datos (cleaning, integration, reduction).", "texto": "apuntes semana 7 - 18/09/2025 1st dar'ıo espinoza aguilar 2020109109 computer engenieering darioespinoza477@estudiantec.cr abstract-este documento corresponde a los apuntes de la - tp: true positive clasedel18deseptiembrede2025,dondeserepasanlosconceptos - tn: true negative deme'tricasyfo'rmulasparaevaluarlosmodelos.adema's,sehace - fp: false positive un repaso del proceso de preparacio'n y procesamiento de datos antes de poder entrenar el modelo. - fn: false negative indexterms-me'tricas,datacleaning,procesamientodedatos, a partir de estos se pueden hacer varias combinaciones datasets para calcular ciertas me'tricas i. introduccio'n 1) accuracy: clasificacio'ncorrectaentretodoslosintentos. se menciono' que se iba a dejar la tarea 2 la otra semana la fo'rmula es la siguiente: y el proyecto 1 la semana que le sigue. adema's, nos dio la tp +tn invitacio'n al evento de ingenier'ıa para lo que quiera asistir. accuracy = tp +tn +fp +fn en la parte de noticias, se menciono' que xbox gaming esu'tilcuandoloserroresporclasesonigualdeimportantes. esta desarrollado un ia para desarrollar juegos viejos, ma's otorga importancia igual a todas las claases. que un desarrollador es un porteador de juego viejos para que 2) precision: mide los errores tipo 1 (fp). tasa de predicpuedan ser jugables. ciones positivas correctas entre todas las predicciones positivas. el profe menciona que se lanzo' un protocolo nuevo para tp pago a trave's de agentes. el protocolo es ap2, fue lanzado precision= tp +fp por google y lo que se busca es que los pagos por medios electro'nicos se puedan realizar sin necesidad de intervencio'n 3) recall: mide los errores de tipo 2 (fn). tasa de humana. menciono la posibilidad de hacer pasant'ıa en la predicciones correctas entre todos los ejemplos positivos del empresa donde e'l trabaja. conjunto de datos. tp sehablodelaspro'ximasevaluacionesquevamosatener,el recall= tp +fn profesor menciono que la mayor'ıa de las tareas programadas 4) f1-score: esta es un me'trica que contempla ambos yaseanproyectosotareasvanaserdemodelosclasificatorios errores. comu'nmente utilizada en problemas de clasificacio'n, ya que tiene afinidad por ellos y que se van a tener tareas de especialemnte cuando tenemos desequelibrio de clases. investigacio'n. 2-precision-recall ii. repasodeme'tricas f1= precision+recall son las me'tricas asociadas a un modelo que nos indica el rendimiento de un modelo predictivo. la forma ma's objetiva a. roc (receiver operating characteristic) de evaluar y comparar un modelo. en todos los modelos que ela'reabajolacurvasiempretienequeser>0.5.sies=5 se sacan siempre hay me'tricas o benchmarks que nos indican loquevamosateneresunrandomclasifier,esunclasificador que tan bueno es el modelo. aleatorio. si es cada vez mayor a 0.5 el modelo va siendo mejor. se repaso lo que es la matriz de confusio'n. que de los"}
{"id_doc": "DOC_024", "segmentacion": "A", "chunk_id": "DOC_024_A_001", "idx": 1, "autor": "Darío Espinoza Aguilar", "fecha": "2025-09-18", "tema": "Repaso de métricas de rendimiento, matriz de confusión, precisión, recall, F1-score y tareas esenciales de preprocesamiento de datos (cleaning, integration, reduction).", "texto": "precision+recall son las me'tricas asociadas a un modelo que nos indica el rendimiento de un modelo predictivo. la forma ma's objetiva a. roc (receiver operating characteristic) de evaluar y comparar un modelo. en todos los modelos que ela'reabajolacurvasiempretienequeser>0.5.sies=5 se sacan siempre hay me'tricas o benchmarks que nos indican loquevamosateneresunrandomclasifier,esunclasificador que tan bueno es el modelo. aleatorio. si es cada vez mayor a 0.5 el modelo va siendo mejor. se repaso lo que es la matriz de confusio'n. que de los iii. respasodeprocesamientodedatos algoritmos de clasificacio'n tenemos 2 etiquetas se pueden ver como positivo y negativo, y de esas 2 etiquetas se pueden enlapra'cticapodemostenerciertosproblemasconnuestros tener 4 4 posibles valores que la matriz de confusio'n nos datos: ayuda a visualizar esos valores. en esta matriz se tienen los - incompletitud:valoresfaltantesenatributosimportantes target class que es la etiqueta tenemos en el dataset y el - inexactitud o ruido: errores y valores at'ıpico en las predicted class que es la prediccio'n de nuestro modelo. los transacciones cuatros combinaciones - inconsistencia:discrepanciaenloco'digodedepartamentos o categor'ıa a. ¿por que' los datos pueden ser inexactos? - inferir valores mediante modelos estadistico o de ml (regresio'n, k-nn, a'rboles de decisio'n) - instrumentos de recoleccio'n de datos defectuosos. - errores humanos o computacionales en la entrada de 2) data cleaning (noisy data): datos. - agrupar valores en intervalos(bins). - usuarios que ingresan valores falsos para campos obli- - sepuedeutilizarcondatosmuyruidosos,sereemplazael gatorios. valor por: la media del bin, la mediana del bin, el borde - conocido como datos faltantes disfrazados. ma's cercano del bin. - inconsistencia en convenciones de nombres, co'digos o 3) data integration (redundancy handling): formatos. - seajustaunafuncio'nmatema'tica(linealonolineal)para - tuplas duplicadas que requieren procesos de data clean- suavizar el ruido de los datos. ing. - aplicar te'cnicas de filtrado para suavizar fluctuaciones, se puede utilizar la media mo'vil (utilizar los u'ltimos 7 b. ¿por que' los datos pueden estar incompletos? elementos para ir calculando la media). - atributos de intere's no siempre esta'n disponibles - noseincluyenporquenoseconsideraronimportantesen el momento de la entrada - datos relevantes no se registran por malentendidos o fallos del equipo - datos inconsistentes con otros registros pueden ser eliminados - historial o modificaciones de datos pasados pueden no haberse registrado - valores faltantes en atributos claves pueden necesitar ser inferidos c. ¿por que' los datos pueden ser inconsistentes? - diferencias en convenciones de nombres o co'digos usados para clasificar elementos - formatos de entrada distintos para un mismo atributo"}
{"id_doc": "DOC_024", "segmentacion": "A", "chunk_id": "DOC_024_A_002", "idx": 2, "autor": "Darío Espinoza Aguilar", "fecha": "2025-09-18", "tema": "Repaso de métricas de rendimiento, matriz de confusión, precisión, recall, F1-score y tareas esenciales de preprocesamiento de datos (cleaning, integration, reduction).", "texto": "esta'n disponibles - noseincluyenporquenoseconsideraronimportantesen el momento de la entrada - datos relevantes no se registran por malentendidos o fallos del equipo - datos inconsistentes con otros registros pueden ser eliminados - historial o modificaciones de datos pasados pueden no haberse registrado - valores faltantes en atributos claves pueden necesitar ser inferidos c. ¿por que' los datos pueden ser inconsistentes? - diferencias en convenciones de nombres o co'digos usados para clasificar elementos - formatos de entrada distintos para un mismo atributo - conflictos entre bases de datos o sistemas que manejan el - errores al integrar datos de mu'ltiples fuentes heteroge'neas - actualizaciones parciales o incorrectas que dejan registros contradictorios d. principales tareas en el preprocesamiento de datos - data cleaning eliminacio'n de ruido, correccio'n de inconsistencias, tratamiento de valores faltantes - data integration combinacio'n de datos de mu'ltiples fuentes heteroge'neas en un repositorio coherente - data reduction reduccio'n de volumen mediante seleccio'n de atributos, reduccio'n de dimensionalidad o discretizacio'n - data transformation normalizacio'n, estandarizacio'n, agregacio'n, construccio'n de nuevas variables - data discretization transformacio'n de atributos continuos en atributos catego'ricos 1) data cleaning (missing values): - ignorar tuplas con valores faltantes (riesgo si la perdida de datos es significtiva) - completar manualmente los valores (costoso y poco pra'ctico en grandes datasets) - usar un valor global constante. - rellenar con la media (normal), mediana o moda"}
{"id_doc": "DOC_025", "segmentacion": "A", "chunk_id": "DOC_025_A_000", "idx": 0, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "apuntes de clase: me'tricas y preprocesamiento de datos en ia isaac david brenes torres inteligencia artificial tecnolo'gico de costa rica ibreto@estudiantec.cr 18 de septiembre del 2025 resumen-este documento presenta una formalizacio'n de seguridad y autenticacio'n: utiliza criptograf'ıa avanzaapuntesdeclasecorrespondientesalcursodeinteligenciaartifi- da para verificar la identidad de los agentes y asegurar cial.seabordanconceptosfundamentalesdeme'tricasdeevalualas transacciones. cio'ndemodelosdeclasificacio'n-comoprecisio'n,exhaustividad descentralizacio'n: puede operar en entornos distribui- (recall), f1-score y curvas roc-, as'ı como te'cnicas esenciales de preprocesamiento de datos, incluyendo limpieza, integracio'n, dos, reduciendo puntos u'nicos de fallo. reduccio'n y transformacio'n. adicionalmente, se incluye una escalabilidad: esta' disen˜ado para manejar un alto voluinvestigacio'n sobre el protocolo de pagos para agentes de ia men de micro-transacciones simulta'neas. (agent payments protocol, ap2). interoperabilidad: permite la integracio'n con mu'ltiples sistemas de pago y plataformas blockchain. i. introduccio'n ap2 representa un avance significativo hacia la econom'ıa los sistemas de inteligencia artificial (ia) requieren no auto'noma, donde los agentes de ia pueden participar en solamentedealgoritmosrobustos,sinotambie'ndeunaevalua- mercados digitales sin intervencio'n humana constante. cio'n rigurosa y un preprocesamiento adecuado de los datos. iii. me'tricasdeevaluacio'nparaclasificacio'n lasme'tricasdeevaluacio'npermitencuantificarelrendimiento de un modelo, mientras que las te'cnicas de preprocesamiento iii-a. matriz de confusio'n aseguran la calidad y idoneidad de los datos de entrada. estos lamatrizdeconfusio'nesunaherramientafundamentalpara aspectos son cr'ıticos para el desarrollo de aplicaciones de ia evaluar el rendimiento de un modelo de clasificacio'n binaria. confiables y efectivas. este documento organiza y expande organiza las predicciones del modelo en cuatro categor'ıas los apuntes de clase sobre estos to'picos, integrando adema's (tabla i). unainvestigacio'nsobreelemergenteagentpaymentsprotocol cuadro i: matriz de confusio'n para clasificacio'n binaria. (ap2). valorreal ii. agentesdeiayagentpaymentsprotocol prediccio'n negativo(n) positivo(p) (ap2) positivo(p) fp vp negativo(n) vn fn ii-a. agentes de ia verdadero positivo (vp): el modelo predice la clase losagentesdeiasonentidadesauto'nomasquepercibensu positiva correctamente. entorno mediante sensores y actu'an sobre dicho entorno mefalso positivo (fp): error tipo i. el modelo predice diante actuadores. estos agentes pueden variar desde simples positivo cuando la clase real es negativa. programasreflexivoshastasistemascomplejosqueaprendeny falso negativo (fn): error tipo ii. el modelo predice se adaptan. un componente clave en los agentes modernos es negativo cuando la clase real es positiva. su capacidad para realizar transacciones de manera auto'noma, verdadero negativo (vn): el modelo predice la clase lo cual requiere protocolos seguros y eficientes. negativa correctamente. ii-b. agent payments protocol (ap2) de google cloud iii-b. exactitud (accuracy) el agent payments protocol (ap2) es un framework la exactitud mide la proporcio'n de predicciones correctas desarrollado por google cloud disen˜ado espec'ıficamente para sobre el total de"}
{"id_doc": "DOC_025", "segmentacion": "A", "chunk_id": "DOC_025_A_001", "idx": 1, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "se adaptan. un componente clave en los agentes modernos es negativo cuando la clase real es positiva. su capacidad para realizar transacciones de manera auto'noma, verdadero negativo (vn): el modelo predice la clase lo cual requiere protocolos seguros y eficientes. negativa correctamente. ii-b. agent payments protocol (ap2) de google cloud iii-b. exactitud (accuracy) el agent payments protocol (ap2) es un framework la exactitud mide la proporcio'n de predicciones correctas desarrollado por google cloud disen˜ado espec'ıficamente para sobre el total de instancias. permitirquelosagentesdeiarealicenpagosdeformasegura y automatizada [1]. ap2 facilita las transacciones econo'micas vp +vn exactitud= entre agentes, servicios y usuarios, actuando como una capa vp +vn +fp +fn de confianza y estandarizacio'n. es u'til cuando las clases esta'n balanceadas, pero puede ser caracter'ısticas principales de ap2: engan˜osa en conjuntos de datos desequilibrados. iii-c. precisio'n (precision) iv-a. limpieza de datos (data cleaning) laprecisio'nevalu'alacapacidaddelmodeloparanoetique- valores faltantes: se pueden ignorar tuplas, completar tar como positivo un ejemplo negativo. mide la proporcio'n de manualmente, usar un valor global constante, rellenar verdaderos positivos entre todas las predicciones positivas. con la media/mediana/moda, o inferir valores mediante modelos. vp datos ruidosos: te'cnicas como binning (agrupacio'n en precisio'n= vp +fp intervalos), regresio'n o filtrado (media mo'vil) ayudan a suavizar el ruido. iii-d. exhaustividad (recall o sensibilidad) iv-b. integracio'n y reduccio'n de datos la exhaustividad mide la capacidad del modelo para encontrar todos los ejemplos positivos. calcula la proporcio'n de integracio'n: combina datos de mu'ltiples fuentes, aborverdaderos positivos identificados correctamente. dando problemas de duplicados e inconsistencias (ej. estandarizacio'n de formatos). vp reduccio'n:reduceelvolumendedatosmanteniendola recall= vp +fn informacio'n esencial. incluye reduccio'n de dimensionalidad (pca) y seleccio'n de caracter'ısticas. iii-e. puntuacio'n f1 (f1-score) iv-c. transformacio'n y discretizacio'n el f1-score es la media armo'nica entre la precisio'n y la exhaustividad. es especialmente u'til cuando existe un transformacio'n: normalizacio'n (min-max) y estandadesbalance entre clases. rizacio'n (z-score) para escalar caracter'ısticas. discretizacio'n: convierte variables continuas en ca2×precisio'n×recall tego'ricas mediante binning (igual-ancho o igualf1= precisio'n+recall frecuencia). iii-f. ejemplo de caso: deteccio'n de ca'ncer iv-d. prueba de correlacio'n χ2 para datos nominales considere un conjunto de 1000 pacientes, donde 30 tienen esta prueba estad'ıstica determina si existe una asociacio'n ca'ncer (clase positiva) y 970 no (clase negativa). la matriz de significativa entre dos variables catego'ricas. la hipo'tesis nula confusio'n obtenida es: (h ) supone independencia. se calcula el estad'ıstico χ2 com0 parandolasfrecuenciasobservadas(o )yesperadas(e )en ij ij cuadro ii: matriz de confusio'n para el caso de deteccio'n de una tabla de"}
{"id_doc": "DOC_025", "segmentacion": "A", "chunk_id": "DOC_025_A_002", "idx": 2, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "caso: deteccio'n de ca'ncer iv-d. prueba de correlacio'n χ2 para datos nominales considere un conjunto de 1000 pacientes, donde 30 tienen esta prueba estad'ıstica determina si existe una asociacio'n ca'ncer (clase positiva) y 970 no (clase negativa). la matriz de significativa entre dos variables catego'ricas. la hipo'tesis nula confusio'n obtenida es: (h ) supone independencia. se calcula el estad'ıstico χ2 com0 parandolasfrecuenciasobservadas(o )yesperadas(e )en ij ij cuadro ii: matriz de confusio'n para el caso de deteccio'n de una tabla de contingencia: ca'ncer. prediccio'n/realidad ca'ncer noca'ncer χ2 = (cid:88) r (cid:88) c (o ij -e ij )2 ca'ncer 25(vp) 20(fp) e ij noca'ncer 5(fn) 950(vn) i=1j=1 si χ2 >χ2 , se rechaza h , indicando correlacio'n. calculado α,gl 0 precisio'n = 25 2 + 5 20 =0,555 (55.5%) v. conclusio'n exactitud = 25+950 =0,975 (97.5%) 1000 este documento basado en los apuntes de clase sobre exhaustividad = 25 =0,833 (83.3%) 25+5 me'tricasdeevaluacio'nypreprocesamientodedatosenia.se f1-score = 2×0,555×0,833 ≈0,662 (66.2%) 0,555+0,833 cubrieron las principales me'tricas de clasificacio'n, ilustradas aunque la exactitud es alta (97.5%), la precisio'n y el conunejemplopra'ctico,ysedetallaronlas etapascr'ıticasdel f1-score revelan problemas significativos en la identificacio'n preprocesamiento de datos. adicionalmente, se investigo' el correcta de la clase minoritaria (ca'ncer). agent payments protocol (ap2) como un avance tecnolo'gico relevante para la autonom'ıa de los agentes de ia. la clariiii-g. curva roc y a'rea bajo la curva (auc) ficacio'n y expansio'n de estos conceptos busca servir como la curva caracter'ıstica de operacio'n del receptor (roc) base para futuras investigaciones y aplicaciones pra'cticas en grafica la tasa de verdaderos positivos (exhaustividad) vs. el campo de la inteligencia artificial. la tasa de falsos positivos (1 - especificidad) para distintos referencias umbrales de clasificacio'n. el a'rea bajo la curva (auc) [1] google cloud, \"announcing the agent payments protocol (ap2) cuantificalacapacidaddelmodeloparadistinguirentreclases. for ai commerce,\" google cloud blog, 2024. [en l'ınea]. unauccercanoa1indicaunexcelenteclasificador,mientras disponible: https://cloud.google.com/blog/products/ai-machine-learning/ que un auc de 0.5 corresponde a un clasificador aleatorio. announcing-agents-to-payments-ap2-protocol.[accedido:18-sep-2025]. iv. preprocesamientodedatos la calidad de los datos es crucial para el e'xito de cualquier proyecto de ia. las te'cnicas de preprocesamiento incluyen:"}
{"id_doc": "DOC_025", "segmentacion": "A", "chunk_id": "DOC_025_A_003", "idx": 3, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "los datos es crucial para el e'xito de cualquier proyecto de ia. las te'cnicas de preprocesamiento incluyen:"}
{"id_doc": "DOC_026", "segmentacion": "A", "chunk_id": "DOC_026_A_000", "idx": 0, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "apuntes de clase: redes neuronales brandon emmanuel sa'nchez araya escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica brandon01sanchez@estudiantec.cr 23 setiembre 2025 abstract-este documento presenta una formalizacio'n de apuntes de clase correspondientes al curso de inteligencia artificial. se abordan los conceptos fundamentales de la regresio'n log'ıstica (binaria y multiclase), el uso del dataset mnist y la representacio'n de ima'genes mediante flatten. asimismo, se introduce la codificacio'n one-hot, la formulacio'n matricial con pesosysesgos,ylarelacio'ndeestosmodelosconlaconstruccio'n de redes neuronales. finalmente, se destacan las propiedades esencialesdelasredes,comolanolinealidad,laorganizacio'nen capasysucapacidadpararesolverproblemascomplejosatrave's de la optimizacio'n por gradiente. i. eldatasetmnist eldatasetmnist(modifiednationalinstituteofstandards andtechnology)esunodelosconjuntosdedatosma'sfamosos en el a'rea de aprendizaje automa'tico. fue creado a partir de fig.1. ejemploderepresentacio'ndeund'ıgitoenmnistysusp'ıxelesen la recopilacio'n de miles de d'ıgitos manuscritos provenientes escaladegrises. de estudiantes de secundaria y empleados de la oficina del censodelosestadosunidos.laideaoriginaleradisponerde b. por que' esto es un problema complejo unconjuntoestandarizadoquesirvieraparaprobarycomparar algoritmos de reconocimiento de escritura. aunque un d'ıgito \"5\" tiene una forma reconocible, cada - conjunto de ima'genes:d'ıgitosescritosamano(del0al persona lo escribe distinto. la variacio'n en trazo, grosor, 9). inclinacio'n y ubicacio'n hace que sea dif'ıcil usar reglas fijas; - taman˜o original: 128×128 p'ıxeles. necesitamos un modelo que aprenda a partir de ejemplos. - taman˜o transformado: 28×28 pixeles. - flatten: cada imagen se convierte en un vector de 784 c. clasificacio'n binaria: \"¿es un 5 o no?\" caracter'ısticas. - 1 channel: un solo canal, es decir en blanco y negro. laregresio'nlog'ısticaeslabasedelasredesneuronales.se - cantidad de ejemplos: 60,000 para entrenamiento y usa para clasificar entre dos clases (ej: ¿es un 5 o no lo es?). 10,000 para prueba. 1 en la figura 1 se muestra un ejemplo de co'mo un d'ıgito f (x)= w,b 1+e-(wx+b) manuscritoserepresentaenmnistcomounamatrizde28× 28 p'ıxeles, que luego puede convertirse en un vector de 784 caracter'ısticas (flatten). h(x)=g(f(x)) . ii. ¿co'modisen˜arunprogramaquereconozca todoslosnu'merosquelaspersonaspuedenhacer? g(x)= 1 1+e-x a. p'ıxeles activos e inactivos & formacio'n de la figura en una imagen de mnist, cada p'ıxel tiene una intensidad (0 = \"apagado\", valores altos = \"encendido\"). la figura del f w,b (x)=wx+b d'ıgito se forma por el patro'n de p'ıxeles activos/inactivos. el aprendizaje consiste en ajustar pesos para que ciertas config- enlafigura2seobservaco'molaregresio'nlog'ısticapuede uraciones de p'ıxeles (patrones) produzcan la clase correcta. interpretarse como una red neuronal muy simple. input layer (x∈r784) capa de salida (r10) capa de entrada (r5) f output yˆ∈(0,1) fig. 2. modelo de regresio'n log'ıstica como red neuronal: entradas → combinacio'nlineal→funcio'nsigmoide. d. ¿co'mo alimentar una regresio'n log'ıstica con una matriz? sea x ∈ r28×28 la imagen"}
{"id_doc": "DOC_026", "segmentacion": "A", "chunk_id": "DOC_026_A_001", "idx": 1, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "del f w,b (x)=wx+b d'ıgito se forma por el patro'n de p'ıxeles activos/inactivos. el aprendizaje consiste en ajustar pesos para que ciertas config- enlafigura2seobservaco'molaregresio'nlog'ısticapuede uraciones de p'ıxeles (patrones) produzcan la clase correcta. interpretarse como una red neuronal muy simple. input layer (x∈r784) capa de salida (r10) capa de entrada (r5) f output yˆ∈(0,1) fig. 2. modelo de regresio'n log'ıstica como red neuronal: entradas → combinacio'nlineal→funcio'nsigmoide. d. ¿co'mo alimentar una regresio'n log'ıstica con una matriz? sea x ∈ r28×28 la imagen (matriz de p'ıxeles). se aplana (flatten) en un vector columna: x=vec(x)∈r784. fig.3. regresio'nlog'ısticamultinomial:5entradasconectadasdirectamente con10salidas. taman˜o de entrada (input layer) y conteo de para'metros compactacio'n: de 10 vectores a una sola matriz - input layer: 784 features (un p'ıxel por entrada). en vez de calcular 10 regresiones por separado, apilamos - pesos en binario: 784 pesos en w + 1 bias = 785 sus pesos en una matriz: para'metros en total.   - w 0 ⊤  iii. regresio'nlog'isticamultinomial   -   (experimentoenclase)   -    b 0   w 1 ⊤  b 1 ejercicio del profe: 10 regresiones que responden \"s'ı/no\" ∈r (cid:124) 1 w (cid:123) 0 (cid:122) × (cid:125) 784 =    - . .     , (cid:124) ∈ (cid:123) r b (cid:122) 1 (cid:125) 0 =   . . .    , z =w (cid:124) ∈ x (cid:123) r (cid:122) 1 + 0 (cid:125) b. se eligio' a 10 estudiantes, cada uno \"especialista\" en un   .   b 9  -  d'ıgito (0-9). cada imagen se le pregunta a los especialista   w⊤ uno por uno y ellos respondieron \"s'ı es mi nu'mero\" o \"no 9 - es\". si la respuesta no coincide con la etiqueta verdadera, se hace refuerzo (entrenamiento). como resultado se obtiene: i'ndices: w es el peso que conecta el feature i (p'ıxel i) con j,i la neurona/clase j. - mi w es una matriz w ∈r10×784. one-hot vector - mi b es un vector b∈r10 (un bias por neurona/clase). laetiquetacorrectasecodificacomounvectorconunu'nico ¿que' sucede con el para'metro b? 1 en la posicio'n del d'ıgito correcto: cada neurona/clase tiene su propio sesgo: b = 0 1 2 3 4 5 6 7 8 9 (b 0 ,...,b 9 )⊤. cantidad de neuronas = taman˜o de b. y (one-hot) 0 0 1 0 0 0 0 0"}
{"id_doc": "DOC_026", "segmentacion": "A", "chunk_id": "DOC_026_A_002", "idx": 2, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "con j,i la neurona/clase j. - mi w es una matriz w ∈r10×784. one-hot vector - mi b es un vector b∈r10 (un bias por neurona/clase). laetiquetacorrectasecodificacomounvectorconunu'nico ¿que' sucede con el para'metro b? 1 en la posicio'n del d'ıgito correcto: cada neurona/clase tiene su propio sesgo: b = 0 1 2 3 4 5 6 7 8 9 (b 0 ,...,b 9 )⊤. cantidad de neuronas = taman˜o de b. y (one-hot) 0 0 1 0 0 0 0 0 0 0 iv. ejercicio:devectoramatriz ese 1 marca cua'l estudiante (regresio'n) deber'ıa decir \"s'ı\". 1) una sola regresio'n binaria (vector x) la figura 3 representa la extensio'n al caso multinomial. sea en este modelo, las entradas se conectan directamente con  3   3  mu'ltiples salidas, de manera que cada una corresponde a 4 2 una clase distinta. de esta forma se pueden reconocer si- x= 5   , w = 4   , b=2. multa'neamente los diez d'ıgitos de mnist. 6 5 entonces neurona (para decidir entre dos clases, por ejemplo \"s'ı\"   o \"no\") o varias neuronas (para elegir entre mu'ltiples 3 categor'ıas, como los 10 d'ıgitos en mnist). z =w⊤x+b=[3 2 4 5]   4 +2=67+2=69, yˆ=σ(z). 5 profundidad y complejidad 6 entre ma's capas profundas tenga la red, ma's puede \"des2) varias regresiones a la vez menuzar\" el problema en representaciones intermedias, lo que le permite identificar patrones complejos que una simple ahora dos regresiones (piensa \"dos neuronas de salida\"). apilamos sus pesos en una matriz w y sus sesgos en un regresio'n log'ıstica no podr'ıa capturar. vector b: en la figura 4 se muestra un ejemplo de red neuronal con (cid:20) (cid:21) (cid:20) (cid:21) tres entradas, una capa oculta de cinco neuronas y cuatro 3 2 4 5 2 w = ∈r2×4, b= ∈r2. salidas.esteesquemailustraco'molaintroduccio'ndecapasin4 3 2 1 3 termedias permite transformar las representaciones y capturar con el mismo x de arriba: relaciones no lineales ma's complejas en los datos.   3 (cid:20) 3 2 4 5 (cid:21) 4 (cid:20) 2 (cid:21) (cid:20) 69 (cid:21) capa oculta z =wx+b=  + = . 4 3 2 1 5 3 43 capa de salida 6 capa de entrada v. redneuronal una red neuronal es un modelo matema'tico inspirado en el funcionamiento del cerebro humano. esta' compuesta por unidades llamadas neuronas, organizadas en capas. las capas esta'nconectadasentres'ı,demaneraquelasalidadeunacapa sirve como entrada de"}
{"id_doc": "DOC_026", "segmentacion": "A", "chunk_id": "DOC_026_A_003", "idx": 3, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "de arriba: relaciones no lineales ma's complejas en los datos.   3 (cid:20) 3 2 4 5 (cid:21) 4 (cid:20) 2 (cid:21) (cid:20) 69 (cid:21) capa oculta z =wx+b=  + = . 4 3 2 1 5 3 43 capa de salida 6 capa de entrada v. redneuronal una red neuronal es un modelo matema'tico inspirado en el funcionamiento del cerebro humano. esta' compuesta por unidades llamadas neuronas, organizadas en capas. las capas esta'nconectadasentres'ı,demaneraquelasalidadeunacapa sirve como entrada de la siguiente. propiedades clave - nolinealidad:permiteresolverproblemascomplejosque un modelo lineal no podr'ıa. fig.4. redneuronalpequen˜a:3entradas,1capaocultade5neuronasy4 - capas: la profundidad de la red es un hiperpara'metro salidas. que define su capacidad, y en cada una de las capas hay neuronas. vi. conclusiones - diferenciabilidad: cada capa debe ser diferenciable para que podamos optimizar mediante gradiente descendente. el estudio de la regresio'n log'ıstica permite comprender - optimizacio'n: si puedo derivar, puedo optimizar. los cimientos de las redes neuronales modernas. a partir de problemas de clasificacio'n binaria simples se llega de manera cuando aplicamos una red neuronal despue's de un clasinatural a la extensio'n multiclase, donde se introducen la forficador multinomial, la lo'gica cambia respecto a una clasifimulacio'n matricial y la codificacio'n one-hot. estos elementos cacio'nbinariatradicional.enunaclasificacio'nbinariasimple, muestran co'mo mu'ltiples regresiones pueden integrarse en un larelacio'neslinealentrelasentradas(features)ylasalida.en solo modelo ma's general. una red neuronal, la salida ya no depende directamente de la el concepto de red neuronal surge al conectar varias de imagen original, sino de las activaciones de la capa anterior. estas operaciones en capas sucesivas, incorporando funciones estructura t'ıpica de activacio'n no lineales que ampl'ıan la capacidad de representacio'n. la diferenciabilidad de cada capa asegura la - capadeentrada:eslaquerecibedirectamentelosdatos posibilidad de entrenar el modelo mediante optimizacio'n, del problema. cada neurona de esta capa representa una mientras que la profundidad incrementa su habilidad para caracter'ıstica (feature) de la entrada. por ejemplo, en el capturar patrones complejos. en s'ıntesis, las redes neuronales caso de mnist cada p'ıxel de la imagen se convierte en sonunaevolucio'ndirectadelaregresio'nlog'ıstica,potenciadas una neurona de la capa de entrada. porlaorganizacio'nencapasylaintroduccio'ndenolinealidad. - capas intermedias (ocultas): son las que procesan la informacio'n recibida. aqu'ı la red va combinando y transformando los datos para encontrar patrones ma's abstractos.sellaman\"ocultas\"porquenointeractu'ancon el mundo exterior: solo comunican informacio'n entre la entrada y la salida. - capa de salida: es la que entrega el resultado final del modelo. dependiendo del problema, puede ser una sola"}
{"id_doc": "DOC_026", "segmentacion": "A", "chunk_id": "DOC_026_A_004", "idx": 4, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "la imagen se convierte en sonunaevolucio'ndirectadelaregresio'nlog'ıstica,potenciadas una neurona de la capa de entrada. porlaorganizacio'nencapasylaintroduccio'ndenolinealidad. - capas intermedias (ocultas): son las que procesan la informacio'n recibida. aqu'ı la red va combinando y transformando los datos para encontrar patrones ma's abstractos.sellaman\"ocultas\"porquenointeractu'ancon el mundo exterior: solo comunican informacio'n entre la entrada y la salida. - capa de salida: es la que entrega el resultado final del modelo. dependiendo del problema, puede ser una sola"}
{"id_doc": "DOC_027", "segmentacion": "A", "chunk_id": "DOC_027_A_000", "idx": 0, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "apuntes de clase: redes neuronales con el dataset mnist fabián díaz barboza estudiante ing. computación tecnológico de costa rica cartago, costa rica fdiaz@estudiantec.cr 23/09/2025 1 el dataset mnist y la representación de 1.3 píxeles activos e inactivos: la semántica del características input 1.1 descripción del dataset mnist un píxel con intensidad 0 se considera \"apagado\" y valores altos indican un píxel \"encendido\". imágenes en blanco y negro (1 canal). incluso la regresión logística binaria más simple exige 10 clases (dígitos 0-9). 784 pesos (w )+1 sesgo (b)=785 parámetros, i tamaño estándar: 28×28 píxeles (entrada comúnmente utilizada). lo que muestra la complejidad del espacio de entrada. conjunto: 60000 ejemplos de entrenamiento y 10000 de prueba. 2 la regresión logística binaria: la neurona fundamental 1.2 proceso de aplanamiento (flattening) una imagen de entrada x ∈r28×28 se convierte en un 2.1 clasificación binaria como problema inicial vector columna mediante flatten: la regresión logística estima la probabilidad de que x∈r784, 28×28=784. una entrada pertenezca a la clase positiva; la salida está en (0,1). cada uno de los 784 elementos es una característica (feature) que alimenta el modelo. 2.2 ecuaciones fundamentales de la neurona potencial de activación: z =w⊤x+b. función sigmoide: 1 g(z)= . 1+e-z salida del modelo: yˆ=h(x)=g(w⊤x+b). figura: diagrama esquemático de la neurona (entradas → combinación lineal → activación → salida) figura 1: ejemplo de la representación de un dígito en mnist como matriz 28×28 y su aplanamiento a un figura 2: diagrama esquemático que interpreta la regrevector de 784 características. sión logística como la neurona más simple. 1 3 extensión a la clasificación multinomial y la 4.2 ejemplo numérico de clase: de vector a matriz codificación one-hot v.b.1. cálculo de una sola regresión (vector de 4 features): 3.1 ejemplo de clase: 10 regresiones logísticas, una por alumno     3 3 para manejar las 10 clases se puede entrenar una regre- w =   2 , b=2, x=   4 . sión logística por estudiante (una por clase); la capa de 4 5 5 6 salida tendría 10 neuronas (una por clase). z =w⊤x+b=(3-3)+(2-4)+(4-5)+(5-6)+2=69. 3.2 codificación one-hot de las etiquetas (y) la etiqueta escalar se codifica como un vector one-hot en r10. yˆ=σ(z). v.b.2. cálculo de varias regresiones a la vez (2 clase (dígito) vector one-hot (y ∈r10) esperada neuronas): 0 [1,0,0,0,0,0,0,0,0,0] neurona 0 2 [0,0,1,0,0,0,0,0,0,0] neurona 2  3  9 [0,0,0,0,0,0,0,0,0,1] neurona 9 (cid:20) 3 2"}
{"id_doc": "DOC_027", "segmentacion": "A", "chunk_id": "DOC_027_A_001", "idx": 1, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": " 4 . sión logística por estudiante (una por clase); la capa de 4 5 5 6 salida tendría 10 neuronas (una por clase). z =w⊤x+b=(3-3)+(2-4)+(4-5)+(5-6)+2=69. 3.2 codificación one-hot de las etiquetas (y) la etiqueta escalar se codifica como un vector one-hot en r10. yˆ=σ(z). v.b.2. cálculo de varias regresiones a la vez (2 clase (dígito) vector one-hot (y ∈r10) esperada neuronas): 0 [1,0,0,0,0,0,0,0,0,0] neurona 0 2 [0,0,1,0,0,0,0,0,0,0] neurona 2  3  9 [0,0,0,0,0,0,0,0,0,1] neurona 9 (cid:20) 3 2 4 5 (cid:21) (cid:20) 2 (cid:21) 4 w = , b= , x= . 4 3 2 1 3 5 cuadro 1: codificación one-hot de etiquetas (ejemplos). 6 (cid:20) (cid:21) 69 z =wx+b= . 43 4 compactación por álgebra lineal 5 arquitectura de las redes neuronales profun4.1 formulación matricial de pesos y sesgos das stackeando los vectores de pesos obtenemos la matriz 5.1 definición y estructura típica de pesos y el vector de sesgos: unaredneuronalartificialesunmodelodecómputo w ∈r10×784, b∈r10. inspirado en el cerebro humano, compuesto por unidades llamadas neuronas artificiales. cada neurona recibe la combinación lineal de la capa de salida se escribe un conjunto de entradas x, aplica una combinación lineal como: con sus pesos w y un sesgo b, y luego pasa el resultado por una función de activación g: z =wx+b, z ∈r10. h(x)=g(w⊤x+b). capa de entrada: recibe los 784 píxeles (flatten). elemento símbolo dimensión capas ocultas: transforman la información en reentrada x 784×1 presentaciones abstractas. matriz de pesos w 10×784 sesgos b 10×1 capa de salida: entrega la predicción (10 neuronas potencial de activación z 10×1 para mnist). cuadro 2: dimensiones en la formulación matricial para mnist. figura 3: matriz de pesos w en la capa fully connected: cada fila corresponde a una neurona de salida y cada figura 4: ejemplo esquemático de una red neuronal con columna a un píxel de entrada. capa de entrada, capa(s) oculta(s) y capa de salida. 2 5.2 el rol del sesgo b 5.4 propiedades esenciales de la red retomando, el parámetro b (bias o sesgo) podriamos 1. nolinealidad:lasfuncionesdeactivación(sigmoide, verlo como un desplazamiento en la función de acti- relu, etc.) permiten que la red modelice relaciones vación. sin b, todas las funciones aprendidas por la red no lineales entre entradas y salidas. tenderíanapasarporelorigen,loquelimitalaflexibilidad 2. capasyprofundidad:amayorprofundidad,mayor del modelo. capacidad para representar abstracciones jerárquicas. en el caso de mnist: 3. diferenciabilidad: la diferenciabilidad de las funtenemos 10 regresiones logísticas (una por cada clacionesinternasesrequisitoparaaplicarretropropaga-"}
{"id_doc": "DOC_027", "segmentacion": "A", "chunk_id": "DOC_027_A_002", "idx": 2, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "sesgo b 5.4 propiedades esenciales de la red retomando, el parámetro b (bias o sesgo) podriamos 1. nolinealidad:lasfuncionesdeactivación(sigmoide, verlo como un desplazamiento en la función de acti- relu, etc.) permiten que la red modelice relaciones vación. sin b, todas las funciones aprendidas por la red no lineales entre entradas y salidas. tenderíanapasarporelorigen,loquelimitalaflexibilidad 2. capasyprofundidad:amayorprofundidad,mayor del modelo. capacidad para representar abstracciones jerárquicas. en el caso de mnist: 3. diferenciabilidad: la diferenciabilidad de las funtenemos 10 regresiones logísticas (una por cada clacionesinternasesrequisitoparaaplicarretropropaga- se). ción y optimizar los parámetros mediante gradiente cada regresión tiene un vector de pesos w i ∈r784 y descendente. un sesgo b . i enconjunto,lospesosformanlamatrizw ∈r10×784 6 conclusiones y los sesgos forman un vector b∈r10. enconclusióndelaclase,lasredesneuronalessoncomo es importante corregir una confusión que se habló en una evolución natural de la regresión logística: partienclase: no existe un único b de dimensión 784 por ejemplo. do de la clasificación binaria, pasando por la extensión en cambio, hay un sesgo por neurona de salida. cada multinomial y compactando parámetros mediante álgecomponente b actúa como umbral independiente para la bra lineal, se llega a arquitecturas fully connected que i neuronai,permitiendodesplazarsufuncióndeactivación permiten mayor expresividad y paralelización. la ecuay ajustar su probabilidad de disparo de forma individual. ción z =wx+b nos sintetiza el paso fundamental hacia la representación matricial; pero el verdadero salto en 5.3 fully connected (completamente conectadas) capacidad proviene de combinar esa formulación con funciones de activación no lineales y con múltiples capas las capas fully connected (fc) son aquellas en las diferenciables, lo que habilita la retropropagación y el que cada neurona de una capa se conecta con todas las entrenamiento eficiente de modelos capaces de abstraer neuronas de la capa anterior. características complejas de datos como mnist. en nuestro ejemplo de mnist: cada neurona de salida (de las 10) recibe conexión de los 784 píxeles de entrada. cada conexión tiene su propio peso, y además cada neurona tiene su sesgo b . i esta estructura convierte el modelo en un clasificador mucho más potente que una sola regresión logística binaria, porque permite: 1. aprender múltiples fronteras de decisión en paralelo. 2. combinar la información de todos los píxeles de forma diferenciada para cada clase. 3. ajustar umbrales específicos gracias a los b . i en otras palabras, una red fully connected extiende el poder de una regresión logística binaria: al apilar capas con activaciones no lineales, las salidas de una capa"}
{"id_doc": "DOC_027", "segmentacion": "A", "chunk_id": "DOC_027_A_003", "idx": 3, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": ". i esta estructura convierte el modelo en un clasificador mucho más potente que una sola regresión logística binaria, porque permite: 1. aprender múltiples fronteras de decisión en paralelo. 2. combinar la información de todos los píxeles de forma diferenciada para cada clase. 3. ajustar umbrales específicos gracias a los b . i en otras palabras, una red fully connected extiende el poder de una regresión logística binaria: al apilar capas con activaciones no lineales, las salidas de una capa se convierten en features no lineales que alimentan la siguiente, permitiendo construir clasificadores mucho más expresivos. 5.3.1 de la multiclase al clasificador binario unaarquitecturaútilconsisteenusarprimerolas10regresioneslogísticas(capamulticlase)yluegoaplicarsobre su salida un clasificador binario adicional. por ejemplo, para determinar si la imagen corresponde al dígito \"5\" o no, la decisión puede tomarse a partir de las 10 salidas (o de una combinación entrenada de ellas), en lugar de hacerlo directamente sobre los píxeles. de este modo, las capas previas actúan como extraedores de características no lineales que potencian una decisión binaria final más robusta. 3"}
{"id_doc": "DOC_028", "segmentacion": "A", "chunk_id": "DOC_028_A_000", "idx": 0, "autor": "Gerardo Alberto Gómez Brenes", "fecha": "2025-09-25", "tema": "Conceptos clave de redes neuronales: regresión logística, softmax, activaciones (sigmoide, ReLU, tanh) y retropropagación, con enfoque en estructura, dimensionalidad y costo computacional.", "texto": "apuntes semana 8, clase gerardo alberto gómez brenes - 2022089271 resumen-resumen compacto y organizado de la clase. in- v. one-hot,softmaxydecisión cluye: motivación breve, pautas para la entrega, conceptos clave de redes neuronales y las fórmulas que el profesor mencionó o lasetiquetasmulti-claseserepresentancomovectoresoneutilizó como referencia. hot.paraobtenerunadistribucióndeprobabilidadsobreclases se usa softmax: i. motivaciónynotasgenerales se compartió un ejemplo de robótica donde un modelo softmax(z) = exp(z i ) . adapta el comportamiento del robot ante la pérdida o modi- i (cid:80) exp(z ) j j ficación de una pata. esto ilustra la capacidad de adaptación (aprendizaje por refuerzo y transferencia) y su potencial en la predicción final corresponde al índice con mayor probabiaplicacionescomoprótesis.mensajepráctico:hayáreasdeml lidad. quenosonsolomodelosdelenguaje;robóticaymanipulación son opciones reales. vi. arquitectura:capasyconexiones se enfatizó también la forma correcta de entregar tareas: documentos cortos y autocontenidos, con figuras y tablas una red densa (fully connected / dense) conecta todas las dentro del texto, y máximo unas pocas páginas para esta salidas de una capa con todas las entradas de la siguiente. actividad (usar el grupo para dudas). añadir capas y activaciones no lineales permite resolver relacionesnolinealesqueunperceptrónsimplenopuede(ejemplo ii. pautasparalaentrega clásico: xor). el trabajo debe ser claro y compacto. incluir dentro del documento: vii. funcionesdeactivaciónygradientes resultadosrelevantes(figuras,tablas)ysuinterpretación funciones mencionadas en clase: breve. referencias a notebooks o repositorios solo como com- sigmoide: σ(z) = 1/(1 + e-z). derivada: σ′(z) = plemento, no como sustituto. σ(z)(1-σ(z)). tiene problemas de vanishing gradient selección crítica de gráficos: mostrar los que aporten a en extremos. la conclusión. relu: relu(z) = m'ax(0,z). es eficiente, pero puede generar neuronas \"muertas\" cuando la derivada es cero. iii. entradayprimermodelo:regresión leaky relu: variante con pequeña pendiente negativa logística para evitar neuronas muertas. imágenes 28 × 28 se representan como vectores de 784 tanh(z): acotada en (-1,1), útil en algunos contextos. píxeles. la regresión logística usa la transformación lineal seguida de la sigmoide: viii. forward,pérdidayretropropagación 1 z =wtx+b, σ(z)= . el forward calcula salidas capa a capa. con una función de 1+e-z pérdida l se aplica retropropagación para obtener derivadas para clasificación binaria (ej.: \"¿es 5 o no?\") σ(z) da una parciales ∂l/∂w y actualizar parámetros. regla de actualizaprobabilidad entre 0 y 1. ción (descenso de gradiente): iv. debinarioamulticlaseynotaciónmatricial ∂l w ←w-η , para 10 clases se puede entrenar una regresión por clase o ∂w usar una salida vectorial. notación común: donde η es la tasa de aprendizaje. para el perceptrón se z =xwt +b, mencionó el hinge loss: donde, por ejemplo,"}
{"id_doc": "DOC_028", "segmentacion": "A", "chunk_id": "DOC_028_A_001", "idx": 1, "autor": "Gerardo Alberto Gómez Brenes", "fecha": "2025-09-25", "tema": "Conceptos clave de redes neuronales: regresión logística, softmax, activaciones (sigmoide, ReLU, tanh) y retropropagación, con enfoque en estructura, dimensionalidad y costo computacional.", "texto": "pérdida l se aplica retropropagación para obtener derivadas para clasificación binaria (ej.: \"¿es 5 o no?\") σ(z) da una parciales ∂l/∂w y actualizar parámetros. regla de actualizaprobabilidad entre 0 y 1. ción (descenso de gradiente): iv. debinarioamulticlaseynotaciónmatricial ∂l w ←w-η , para 10 clases se puede entrenar una regresión por clase o ∂w usar una salida vectorial. notación común: donde η es la tasa de aprendizaje. para el perceptrón se z =xwt +b, mencionó el hinge loss: donde, por ejemplo, w puede tener forma 10 × 784 (10 l =m'ax(0, 1-y(wtx+b)). hinge neuronas de salida y 784 entradas). con un batch de tamaño b la entrada x es b×784 y el resultado z es b×10. laretropropagaciónusalaregladelacadenaparapropagar ejemplo numérico: con 10 salidas y 784 entradas hay 10× sensibilidades hacia atrás; por eso es necesario que las capas 784=7840 parámetros sólo en esa capa. sean diferenciables. ix. costoscomputacionalesydimensionalidad aumentar neuronas y capas incrementa parámetros y costo deoptimización.lamaldicióndeladimensionalidadcomplica labúsquedadesolucionesóptimas.ejemplo:siunacapatiene 256 neuronas y la siguiente tiene 10, los pesos entre ellas son 10×256=2560. recomendación práctica: reducir dimensiones innecesarias (filtrado de features, pca) cuando sea posible. x. notassobrerepresentacionesy cnn/embeddings para imágenes, las cnn aplican kernels que extraen patrones locales (bordes, texturas, formas). en lenguaje, embeddings condensan palabras o frases en vectores de dimensión fija;lasimilitudsemánticasemidepordistanciaeneseespacio vectorial."}
{"id_doc": "DOC_029", "segmentacion": "A", "chunk_id": "DOC_029_A_000", "idx": 0, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "1 apuntes de la clase del 25 de setiembre de 2025 kevin carranza jimenez escuela de ingenier'ıa en computacio'n tecnolo'gico de costa rica kcarranza@estudiantec.cr abstract-this document summarizes the lecture held on iii. redesneuronales september 25, 2025, which included the presentation of the una red neuronal artificial (rna) es un modelo comcompanyskild.ia,focusedonapplyingartificialintelligencealgoputacional inspirado en la estructura y funcionamiento del rithmsforrobotcontrol.italsoprovidesareviewoftheprevious lecture,coveringneuralnetworksfromlogisticregressiontotheir cerebro humano, compuesto por nodos (neuronas artificiales) application in binary classifiers using multinomial expressions. organizados en capas y conectados entre s'ı mediante pesos. the session then introduces the perceptron model, defined as a estas redes aprenden patrones complejos a partir de datos linearregressionwithahingelossfunction.itisemphasizedthat de entrada a trave's de un proceso iterativo de ajuste de a single perceptron cannot solve non-linear functions, although pesos, permitiendo resolver tareas de clasificacio'n, prediccio'n multiple perceptrons can be combined to achieve this. finally, themultilayerperceptronisintroducedasaformofdeepneural y reconocimiento en diversos dominios [2]. network with biological inspiration. index terms-skild, regresio'n lineal, multinomial red neu- a. clasificador de mnist ronal, perceptro'n. mnistesundatasetcon60kmuestrasdenu'merosdel0al 9enunsolocanal.enelresumendelaclasesedaaentender que se esta' intentando desarrollar un clasificador utilizando i. introduction este dataset. que cada una de estas imagenes esta' compuesta en el desarrollo del curso, las clases recientes han por un grupo de p'ıxeles. abordado los fundamentos de las redes neuronales y su evolucio'n hacia modelos ma's complejos. la sesio'n del b. regresio'n log'ıstica 25 de septiembre de 2025 incluyo' como tema de intere's un en el resumen de la clase anterior se menciona que para el video de la empresa skild.ia, que tiene como objetivo utilizar clasificador de mnist se comenzaba tratando de hacer una algoritmos de inteligencia artificial para controlar robots y clasificacio'n binaria respecto a la imagen. hasta el momento estos puedan emplear cualquier tarea, trayendo la inteligencia deestaclaseelu'nicoalgoritmoconocidoparadesarrollaresta artificial al mu'ndo f'ısico. tambie'n se incluyo' tanto la clasificacio'n es la regresio'n log'ıstica. para esto se pasan la revisio'n de conceptos previamente estudiados, entre ellos la informacio'n de cada uno de los pixeles de la imagen como regresio'n log'ıstica para desarrollar expresiones multinomiales entrada para le regresio'n log'ıstica. la situacio'n es que el y con multiples capas de estas, desarrollar redes neuronales. problema no puede ser resuelto con una regresio'n log'ıstica tambie'n la introduccio'n del perceptro'n, considerado el punto u'nicamente, si no con una regresio'n log'ıstica multinomial, ya de partida para las redes neuronales profundas. que requerimos 10 clases y la regresio'n log'ıstica solo permite 1. ii. skild.ia c. multinomial skildaiesunastartupemergentededicadaaldesarrollode para esto,"}
{"id_doc": "DOC_029", "segmentacion": "A", "chunk_id": "DOC_029_A_001", "idx": 1, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "de la imagen como regresio'n log'ıstica para desarrollar expresiones multinomiales entrada para le regresio'n log'ıstica. la situacio'n es que el y con multiples capas de estas, desarrollar redes neuronales. problema no puede ser resuelto con una regresio'n log'ıstica tambie'n la introduccio'n del perceptro'n, considerado el punto u'nicamente, si no con una regresio'n log'ıstica multinomial, ya de partida para las redes neuronales profundas. que requerimos 10 clases y la regresio'n log'ıstica solo permite 1. ii. skild.ia c. multinomial skildaiesunastartupemergentededicadaaldesarrollode para esto, al problema requerir 10 clases, se desarrollan una inteligencia artificial de propo'sito general para el control 10 regresiones log'ısticas, una por cada clase y a cada una de robots de mu'ltiples tipos (humanoides, brazos robo'ticos, se le pasa como entrada la informacio'n de los pixeles de plataformas de locomocio'n, etc.). la imagen, por lo que una de las regresiones logisticas dara' lapropuestacentraldeskildaiescrearun\"cerebrorobo'tico mayor probabilidad que las dema's. y en este punto tenemos omni-corporal\" -denominado skild brain- que permita en la figura 1 una arquitectura que ya podr'ıa llamarse red que un mismo modelo de ia controle diferentes cuerpos neuronal, aunque todav'ıa faltar'ıa agregar una siguiente capa robo'ticos sin necesidad de reentrenamientos espec'ıficos para para poder resolver problemas no lineales. cada hardware. tambie'n se menciona en el resumen de la clase anterior un aspecto clave de su disen˜o es la capacidad de adaptacio'n que en lugar de calcular cada regresio'n lineal de forma a fallos o cambios dra'sticos en la morfolog'ıa del robot: vectorial, cambiamos los vectores por matrices para hacer 1 cuando un robot pierde una extremidad o sufre un dan˜o, el sola operacio'n y no n. donde n es el taman˜o de la capa modelo puede reorganizar su control para seguir operando, siguiente utilizando conceptos de a'lgebra lineal. en cada una aprovechando la experiencia aprendida previamente [1]. de las filas sera' representado las neuronas para la siguiente capa y las entradas las columnas. 2 fig. 2: inspiracio'n biolo'gica de la red neuronal. sen˜alaron que este modelo no pod'ıa resolver funciones no linealmente separables, siendo el ejemplo cla'sico la funcio'n lo'gica xor. adema's, advirtieron sobre su limitada expresividad computacional y su escasa capacidad para generalizar en problemas ma's complejos, lo que contradec'ıa las expectativas iniciales de que los perceptrones pudieran resolver tareas fig. 1: primer red neuronal. de visio'n y reconocimiento de patrones. estas observaciones demostraron que, aunque los perceptrones eran u'tiles para ciertos problemas lineales, su aplicacio'n pra'ctica era"}
{"id_doc": "DOC_029", "segmentacion": "A", "chunk_id": "DOC_029_A_002", "idx": 2, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "la red neuronal. sen˜alaron que este modelo no pod'ıa resolver funciones no linealmente separables, siendo el ejemplo cla'sico la funcio'n lo'gica xor. adema's, advirtieron sobre su limitada expresividad computacional y su escasa capacidad para generalizar en problemas ma's complejos, lo que contradec'ıa las expectativas iniciales de que los perceptrones pudieran resolver tareas fig. 1: primer red neuronal. de visio'n y reconocimiento de patrones. estas observaciones demostraron que, aunque los perceptrones eran u'tiles para ciertos problemas lineales, su aplicacio'n pra'ctica era muy el resumen de la clase anterior concluye definiendo algunas limitada. el impacto de estas cr'ıticas fue significativo, concaracteristicas de las redes neuronales, las cuales son que al tribuyendoalprimerinviernodelainteligenciaartificial,hasta no ser lineales nos permite atacar problemas complejos, esta' que el desarrollo del perceptro'n multicapa y el algoritmo de compuesta por capas, estas capas son el hiper para'metro de retropropagacio'n permitieron superar estas restricciones [5]. la red neuronal y es importante que sean diferenciables. si la red neuronal se puede derivar se puede optimizar y que en b. inspiracio'n biolo'gica cada capa hay neuronas. las redes neuronales artificiales se inspiran en el funiv. elperceptro'n cionamiento de las neuronas del cerebro humano, donde cada el perceptro'n es uno de los modelos ma's simples de red neurona recibe sen˜ales de mu'ltiples conexiones sina'pticas, las neuronal artificial, propuesto por frank rosenblatt en 1958. procesa y genera una respuesta que se transmite a otras neuconsiste en una unidad de procesamiento que recibe un con- ronas.demaneraana'loga,enlasredesneuronalesartificiales, juntodeentradasponderadas,lascombinalinealmenteyaplica cadanodoo\"neurona\"recibeentradasponderadas,aplicauna una funcio'n de activacio'n para producir una salida binaria. su funcio'n de activacio'n y transmite su salida a las siguientes objetivoprincipalesclasificarpatroneslinealmenteseparables. capas, reproduciendo de forma simplificada el procesamiento aunquelimitadoparaproblemasnolineales,constituyelabase distribuido y paralelo del sistema nervioso biolo'gico. esta conceptualdearquitecturasma'scomplejascomoelperceptro'n inspiracio'n biolo'gica se ilustra en la figura 2, donde se multicapa y las redes neuronales profundas [3]. muestra la correspondencia entre una neurona biolo'gica y su modelo artificial. a. invierno de la ai el invierno de la inteligencia artificial hace referencia a c. funcio'n de activacio'n per'ıodos histo'ricos en los que las expectativas generadas en regresio'n log'ıstica se llama funcio'n no-lineal (sigmoid). alrededor de la investigacio'n en ia no se cumplieron, provoesta depende de si la sen˜al activa o no la neurona. dependicando una disminucio'n dra'stica en la financiacio'n, el intere's endo de la intensidad de la sen˜al que se haya recibido, esta acade'mico y el desarrollo industrial en este campo. durante dejara' pasar la informacio'n, la bloqueara' o la"}
{"id_doc": "DOC_029", "segmentacion": "A", "chunk_id": "DOC_029_A_003", "idx": 3, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "referencia a c. funcio'n de activacio'n per'ıodos histo'ricos en los que las expectativas generadas en regresio'n log'ıstica se llama funcio'n no-lineal (sigmoid). alrededor de la investigacio'n en ia no se cumplieron, provoesta depende de si la sen˜al activa o no la neurona. dependicando una disminucio'n dra'stica en la financiacio'n, el intere's endo de la intensidad de la sen˜al que se haya recibido, esta acade'mico y el desarrollo industrial en este campo. durante dejara' pasar la informacio'n, la bloqueara' o la transformara' y estos periodos, los avances en ia se ralentizaron debido existen varias funciones de activacio'n. a limitaciones tecnolo'gicas, falta de resultados pra'cticos y cr'ıticas hacia la viabilidad de los enfoques predominantes. se 1) funcio'n sigmoide: la funcio'n sigmoide transforma un reconocen principalmente dos inviernos de la ia: el primero valor de entrada en un rango entre 0 y 1, lo que permite a mediados de los an˜os 1970, y el segundo a finales de los interpretarlacomounaprobabilidad.sudesventajaprincipales an˜os 1980 hasta principios de los 1990 [4]. lasaturacio'ndegradientesenvaloresextremos,loquedificulta en 1969, marvin minsky y seymour papert publicaron el el entrenamiento en redes profundas [6]. libro perceptrons, en el que sen˜alaron limitaciones fundamen1 tales del perceptro'n simple. entre los problemas destacados, σ(x)= 1+e-x 3 2) funcio'n tangente hiperbo'lica (tanh): la tangente 1) pca: el ana'lisis de componentes principales (pca, hiperbo'lica es similar a la sigmoide, pero su rango va de por sus siglas en ingle's: principal component analysis) es un -1 a 1, lo que permite que las salidas este'n centradas en me'todo estad'ıstico ampliamente utilizado para la reduccio'n cero. esto ayuda a mitigar algunos problemas de gradientes de dimensionalidad, que transforma un conjunto de varien comparacio'n con la sigmoide, aunque au'n puede sufrir de ables posiblemente correlacionadas en un nuevo conjunto saturacio'n [6]. de variables no correlacionadas denominadas componentes ex-e-x principales. el procedimiento consiste en centrar los datos, tanh(x)= ex+e-x calcular la matriz de covarianza, obtener sus autovalores y autovectores, y seleccionar los vectores asociados a los 3) funcio'n relu (rectified linear unit): la funcio'n mayoresautovaloresparaproyectarlosdatosenunsubespacio relu es una de las ma's utilizadas en redes neuronales de menor dimensio'n que conserva la mayor varianza posible modernas. define la salida como 0 para valores negativos de la informacio'n original [10], [11], [12]. y como la propia entrada para valores positivos. es computacionalmente eficiente y mitiga en gran parte el problema del desvanecimiento del gradiente, aunque puede presentar el b. comportamiento jera'rquico problema de \"neurona"}
{"id_doc": "DOC_029", "segmentacion": "A", "chunk_id": "DOC_029_A_004", "idx": 4, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "vectores asociados a los 3) funcio'n relu (rectified linear unit): la funcio'n mayoresautovaloresparaproyectarlosdatosenunsubespacio relu es una de las ma's utilizadas en redes neuronales de menor dimensio'n que conserva la mayor varianza posible modernas. define la salida como 0 para valores negativos de la informacio'n original [10], [11], [12]. y como la propia entrada para valores positivos. es computacionalmente eficiente y mitiga en gran parte el problema del desvanecimiento del gradiente, aunque puede presentar el b. comportamiento jera'rquico problema de \"neurona muerta\" [7]. loshumanosaprendencosassimplesparatransformarloen algo ma's complejo, tal es el caso del mlp conformado por f(x)=max(0,x) mu'ltiples regresiones lineales, de lo cual se optienen ganan4) funcio'n leaky relu: la funcio'n leaky relu es una cias exponenciales en algunas funciones, como polinomios, variante de la relu que permite pequen˜os valores negativos la composicio'n de funciones que permite reusar funciones en la salida (usualmente multiplicados por una constante simples otras de orden superior y que mediante una reprepequen˜a, como 0.01). esto evita el problema de neuronas sentacio'ncompacta,enlaquepocospesossepuedenmodelar muertas al asegurar un gradiente no nulo para entradas nega- funcionescomplejas,comoporejemplo,unaredneuronalque tivas [8]. se aproxime a otra. (cid:40) x si x≥0 f(x)= αx si x<0 vi. conclusion laclasepermitio' lacomprensio'ndelosfundamentosdelas 5) funcio'n softmax: la funcio'n softmax convierte un redes neuronales, resaltando su estructura jera'rquica al final vector de valores reales en una distribucio'n de probabilidad, y las motivaciones biolo'gicas que inspiran su arquitectura. dondecadavalorquedaentre0y1ylasumatotalesiguala1. a partir del ana'lisis del perceptro'n y de sus limitaciones, seutilizaprincipalmenteenlacapadesalidadeclasificadores se introdujo la necesidad de arquitecturas ma's complejas, multiclase [6]. como el mlp, que posibilitan la resolucio'n de problemas ezi no lineales. esta sesio'n trato' tanto el potencial como los σ(z) = para i=1,...,k i (cid:80)k ezj desaf'ıos de las redes neuronales, entre ellos la maldicio'n de j=1 la dimensionalidad y la importancia de un disen˜o acorde al problemaencuestio'nente'rminosdecapasyneuronas.as'ı,la v. perceptro'nmulticapa clase proporciono' las bases para comprender las arquitecturas modernas de aprendizaje profundo. el perceptro'n multicapa (mlp, por sus siglas en ingle's) es una arquitectura fundamental dentro de las redes neuronales artificiales. esta' compuesto por una capa de entrada, una o references ma's capas ocultas y una capa de salida. a diferencia del [1] k. wiggers, \"skild ai emerges from stealth with perceptro'n simple, que solo puede resolver problemas lineal- $300m to build a general-purpose ai brain for robots,\" mente separables, el mlp utiliza funciones de activacio'n no techcrunch, sep. 2025, accessed: 2025-10-02. [online]. available: https://techcrunch.com/2025/09/16/skild-ai-emerges-from-stealthlineales en sus neuronas ocultas, lo que"}
{"id_doc": "DOC_029", "segmentacion": "A", "chunk_id": "DOC_029_A_005", "idx": 5, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "es una arquitectura fundamental dentro de las redes neuronales artificiales. esta' compuesto por una capa de entrada, una o references ma's capas ocultas y una capa de salida. a diferencia del [1] k. wiggers, \"skild ai emerges from stealth with perceptro'n simple, que solo puede resolver problemas lineal- $300m to build a general-purpose ai brain for robots,\" mente separables, el mlp utiliza funciones de activacio'n no techcrunch, sep. 2025, accessed: 2025-10-02. [online]. available: https://techcrunch.com/2025/09/16/skild-ai-emerges-from-stealthlineales en sus neuronas ocultas, lo que le permite aproximar with-300m-to-build-a-general-purpose-ai-brain-for-robots/ funciones complejas y resolver problemas no lineales. su [2] s.haykin,neuralnetworksandlearningmachines,3rded. prentice entrenamiento se realiza comu'nmente mediante el algoritmo hall,2009. [3] f. rosenblatt, \"the perceptron: a probabilistic model for information deretropropagacio'n(backpropagation),elcualajustalospesos storage and organization in the brain,\" psychological review, vol. 65, delasconexionesminimizandoelerrorentrelasalidapredicha no.6,pp.386-408,1958. y la deseada. esta arquitectura constituye la base de los [4] s.j.russellandp.norvig,artificialintelligence:amodernapproach, 3rded. prenticehall,2010. modelos modernos de aprendizaje profundo [6], [9]. [5] m. minsky and s. a. papert, perceptrons: an introduction to computationalgeometry. mitpress,1969. [6] i.goodfellow,y.bengio,anda.courville,deeplearning. mitpress, a. maldicio'n de dimensionalidad 2016. [7] v.nairandg.e.hinton,\"rectifiedlinearunitsimproverestrictedboltzamayorcantidaddedimensiones,aumentalacomplejidad, mann machines,\" in proceedings of the 27th international conference a su vez, aumentando la computabilidad y se vuelve ma's onmachinelearning(icml),2010. [8] a. l. maas, a. y. hannun, and a. y. ng, \"rectifier nonlinearities complicado encontrar patrones. para esto existen algoritmos improve neural network acoustic models,\" in proceedings of the 30th de deduccio'n de dimensiones como el pca. internationalconferenceonmachinelearning(icml),2013. 4 [9] d. e. rumelhart, g. e. hinton, and r. j. williams, \"learning representationsbyback-propagatingerrors,\"nature,vol.323,no.6088,pp. 533-536,1986. [10] i.t.jolliffeandj.cadima,\"principalcomponentanalysis:areviewand recent developments,\" philosophical transactions of the royal society a:mathematical,physicalandengineeringsciences,vol.374,no.2065, p.20150202,2016. [11] c. m. bishop, pattern recognition and machine learning. springer, 2006. [12] j.shlens,\"atutorialonprincipalcomponentanalysis,\"arxivpreprint arxiv:1404.1100,2014."}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_000", "idx": 0, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "apuntes de clase inteligencia artificial - semana 9 - 02 de octubre priscilla jime'nez salgado escuela de ingenier'ıa en computacio'n, tecnolo'gico de costa rica cartago, costa rica - 2021022576@estudiantec.cr abstract-este documento hace un repaso general a diferencia del primer sora, que ten'ıa resultados y claro sobre las funciones de activacio'n ma's poco realistas, esta nueva versio'n produce videos utilizadas en las redes neuronales, adema's de ma's naturales y coherentes, adema's de incluir audio explicar conceptos importantes sobre co'mo esta'n gracias a su capacidad multimodal. el profesor disen˜adas y co'mo han evolucionado las redes mostro' un ejemplo hecho con la herramienta y neuronales artificiales. se presentan funciones explico' que incluso podr'ıa usarse para presentacomorelu,sigmoideysoftmax,entreotras,con ciones acade'micas. tambie'n se menciono' la nueva su base matema'tica. tambie'n se repasa el con- aplicacio'n \"sora by openai\", una plataforma donde cepto de perceptro'n y las redes multicapa, y se laspersonaspuedencrearycompartirvideosconincomentan algunos retos cla'sicos en el a'rea, como teligencia artificial a partir de simples descripciones el problema del xor y la llamada \"maldicio'n de o prompts. la dimensionalidad\". i. reviewdelalectura en claseel profesorcomento' deforma muyba'sica la lectura from language to action: a review of large language models as autonomous agents and tool users. sen˜alo' que lo importante para fig. 1: sora by openai el pro'ximo quiz del martes es entender lo esencial: los modelos de lenguaje (llms) ya no solo ii. aspectosadministrativos generan texto, sino que tambie'n funcionan como agentes auto'nomos capaces de razonar, planificar, el profesor compartio' las notas de los trabajos usar memoria e interactuar con herramientas ex- pendientes y brindo' retroalimentacio'n individual a ternas. la lectura diferencia entre sistemas de un cada grupo de trabajo. sin embargo, au'n queda por solo agente y sistemas multi-agente, donde varios entregar la calificacio'n del quiz 4 realizado y de modelos colaboran para resolver problemas ma's la tarea presentada el pasado mie'rcoles, que esta'n complejos. adema's, se destacan sus aplicaciones pendientes de revisio'n. adema's, se indico' que la en investigacio'n, programacio'n, salud, robo'tica y pro'xima semana se asignara' el proyecto del curso. simulaciones, as'ı como los principales retos, entre a. repaso ellos la memoria limitada, la seguridad, la e'tica y la necesidad de mejores evaluaciones. - el perceptro'n: puede entenderse de forma similar a una regresio'n log'ıstica, aunque se diferencia en a. noticias de la semana la funcio'n de pe'rdida que utiliza. durante la historia en clase se hablo' del lanzamiento de sora 2, el de la"}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_001", "idx": 1, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "la en investigacio'n, programacio'n, salud, robo'tica y pro'xima semana se asignara' el proyecto del curso. simulaciones, as'ı como los principales retos, entre a. repaso ellos la memoria limitada, la seguridad, la e'tica y la necesidad de mejores evaluaciones. - el perceptro'n: puede entenderse de forma similar a una regresio'n log'ıstica, aunque se diferencia en a. noticias de la semana la funcio'n de pe'rdida que utiliza. durante la historia en clase se hablo' del lanzamiento de sora 2, el de la inteligencia artificial se produjo el llamado nuevo modelo de generacio'n de video creado por \"invierno de la ia\", en parte debido al problema openai como respuesta al nano banana de google. del xor, ya que este no pod'ıa ser representado adecuadamenteporunmodeloderegresio'nlog'ıstica tienenlacapacidaddeabordarproblemasnolineales. ni por un perceptro'n simple. gracias a ello, se ampl'ıa significativamente el rango deproblemasquepuedenresolverseconesteme'todo. - prediccio'n de compuertas lo'gicas: - inspiracio'n biolo'gica: fig. 2. compuertas lo'gicas en la figura se ilustran las compuertas lo'gicas or y and mediante gra'ficos bidimensionales. - or(𝑋 1 ,𝑋 2):lostria'ngulosindicanlasalida1y fig. 4. inspiracio'n biolo'gica losc'ırculoslasalida0.estacompuertadevuelve 1 siempre que al menos una de las entradas sea lasredesneuronalesseinspiranenco'mofuncionan iguala1. las neuronas en nuestro cerebro. cada neurona esta' - and(𝑋 1 ,𝑋 2): corresponde a una compuerta conectadaconotrasatrave'sdesusdendritas,yenel and donde la primera entrada esta' negada. la nu'cleoesdondeseprocesalainformacio'n. salida es 1 (tria'ngulo) u'nicamente cuando la si lo comparamos con una regresio'n log'ıstica, las primeraentradaes0ylasegundaes1. dendritas ser'ıan como las entradas de datos (inputs), - and(𝑋 1 ,𝑋 2): representa la compuerta and yelnu'cleorepresentar'ıalafuncio'nlinealqueprocesa con la segunda entrada negada. el resultado es esa informacio'n. al final, la neurona decide si deja 1(tria'ngulo)solocuandolaprimeraentradaes1 pasaronoesasen˜al. ylasegundaes0. en cada gra'fico, la l'ınea punteada marca el l'ımite - funciones de activacio'n: en la regresio'n de decisio'n que distingue entre las dos clases de log'ısticaesatransformacio'nseconocecomofuncio'n salida (0 y 1). esta representacio'n facilita la com- nolineal,espec'ıficamentelasigmoide.segu'nlasen˜al prensio'n de co'mo las compuertas lo'gicas realizan la recibida, la neurona se activa o no, permitiendo que clasificacio'n de sus entradas en un espacio bidimen- lainformacio'ncontinu'e,latransformeolabloquee. sional. - problema del xor: fig. 5. funciones de activacio'n - funcio'n relu: la funcio'n 𝑔(𝑥) = max(0,𝑥) esta' limitada por debajo de cero y es estrictafig. 3. problema del xor mente creciente. es muy eficiente en modelos el principal inconveniente es que el problema no de deep learning, pero presenta el problema es linealmente separable, por lo que el algoritmo de las llamadas neuronas muertas, ya que no es del perceptro'n simple"}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_002", "idx": 2, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "sus entradas en un espacio bidimen- lainformacio'ncontinu'e,latransformeolabloquee. sional. - problema del xor: fig. 5. funciones de activacio'n - funcio'n relu: la funcio'n 𝑔(𝑥) = max(0,𝑥) esta' limitada por debajo de cero y es estrictafig. 3. problema del xor mente creciente. es muy eficiente en modelos el principal inconveniente es que el problema no de deep learning, pero presenta el problema es linealmente separable, por lo que el algoritmo de las llamadas neuronas muertas, ya que no es del perceptro'n simple no pod'ıa ofrecer una solucio'n derivableentodoslospuntosy,enalgunoscasos, adecuada. es en este punto donde surgen las redes el gradiente puede llegar a ser cero, impidiendo neuronales o perceptrones multicapa, ya que estos s'ı laactualizacio'ndelospesos. - funciones selu y elu: son de la misma familia.aunquerequierenmayorcostocomputacional,ofrecenunrendimientomuyeficiente. - funcio'n sigmoide: convierte la entrada en un valorentre0y1.esmuyusadacuandosenecesita interpretarlassalidascomoprobabilidades. fig. 6. ejemplo relu - leaky relu: esta funcio'n asigna una pequen˜a - perceptro'n multicapa (mlp): constante al valor m'ınimo permitido, lo que ayuda a evitar el problema de las neuronas muertas.aunquerepresentaunamejorarespecto a la relu original, no se considera la solucio'n definitiva. (cid:40) 0.01𝑥, 𝑥 < 0 𝑔(𝑥) = 𝑥, 𝑥 ≥ 0 fig. 8. perceptro'n (cid:40) 𝜕𝑔(𝑥) 0.01, 𝑥 < 0 elperceptro'nmulticapa(mlp)esunaevolucio'n = 𝜕𝑥 1, 𝑥 ≥ 0 delperceptro'nsimplequepermiteresolverproblemas ma's complejos, especialmente aquellos que no son linealmenteseparables. el profesor lo explico' de manera sencilla con la imagen: en la capa de entrada (input layer) se encuentran los datos originales, representados como fig. 7. ejemplo leaky relu 𝑋 , que no cambian porque son las entradas del - parametric relu (prelu): esta funcio'n per- 𝑖 mite aprender un para'metro que controla si sistema. luego aparecen las capas ocultas (hidden layers), que son las responsables de realizar los la sen˜al continu'a en la parte negativa. dicho ca'lculos, transformaciones y operaciones internas, para'metroseentrenajuntoconelrestodelared, da'ndole a la red la capacidad de aprender relaciones loquebrindamayorflexibilidadalmodelo. ma's complejas. finalmente, esta' la capa de salida (cid:40) (output layer), que entrega el resultado final y 𝑤𝑥, 𝑥 < 0 𝑔(𝑥) = cuyo taman˜o depende del problema que se este' 𝑥, 𝑥 ≥ 0 resolviendo. (cid:40) la gran ventaja del mlp es que, gracias a sus 𝜕𝑔(𝑥) 𝑤, 𝑥 < 0 = mu'ltiples capas y funciones de activacio'n, introduce 𝜕𝑥 1, 𝑥 ≥ 0 no linealidad, lo que le permite resolver problemas - funcio'n tanh: tiene una forma parecida a la que el perceptro'n simple no pod'ıa. adema's, se ensigmoide,perosusalidaesta' acotadaenelrango trenautilizandolapropagacio'n del error(backprop- (-1,1),loquepermitemanejarvalorespositivos agation),queconsisteencalcularcua'ntoseequivoco'"}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_003", "idx": 3, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "𝑤𝑥, 𝑥 < 0 𝑔(𝑥) = cuyo taman˜o depende del problema que se este' 𝑥, 𝑥 ≥ 0 resolviendo. (cid:40) la gran ventaja del mlp es que, gracias a sus 𝜕𝑔(𝑥) 𝑤, 𝑥 < 0 = mu'ltiples capas y funciones de activacio'n, introduce 𝜕𝑥 1, 𝑥 ≥ 0 no linealidad, lo que le permite resolver problemas - funcio'n tanh: tiene una forma parecida a la que el perceptro'n simple no pod'ıa. adema's, se ensigmoide,perosusalidaesta' acotadaenelrango trenautilizandolapropagacio'n del error(backprop- (-1,1),loquepermitemanejarvalorespositivos agation),queconsisteencalcularcua'ntoseequivoco' ynegativos. la red y ajustar los pesos mediante descenso de - binary step function:devuelve1silaentrada gradiente,mejorandoas'ıelrendimientodelmodelo. esmayorqueceroy0siesmenoroigualacero. - funcio'n lineal:ba'sicamentedejapasarlasalida ahora nos preguntamos, ¿co'mo se calcula una sinaplicarningunatransformacio'nadicional. pasadaenlared? el proceso comienza con la expresio'n ℎ(0) = tarea:softmaxsisetratadeunaclasificacio'nmu'ltiple, 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑋𝑊0 + 𝑏0), donde ℎ(0) corresponde a la olinealsiesunproblemaderegresio'n.loimportante primera capa oculta. lo que se hace es calcular esqueseaunafuncio'nnolineal,yaqueesoesloque primero la regresio'n lineal 𝑋𝑊0+ 𝑏0, luego aplicar ledaalaredlacapacidadderesolver lafuncio'nsigmoidealresultado,yconesoseobtiene elvalordelprimerhidden layer. despue's, para la siguiente capa oculta, el procedimiento es pra'cticamente el mismo: ℎ(1) = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(ℎ(0)𝑊1 + 𝑏1). en este caso, el valor de fig. 11. capa de salida ℎ(0) pasaaserlaentradadelasiguientecapa. estemismoprocesoserepitehastallegaralau'ltima - funcio'n costo: esunafuncio'n matema'ticaque capa, que se expresa como ℎ(𝑛) = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(ℎ(𝑛 - calcula el nivel de error del modelo, y cuyo objetivo 1)𝑊𝑛+𝑏𝑛). principalesminimizardichoerrorduranteelproceso en otras palabras, cada capa oculta toma como deentrenamiento. entrada el resultado de la capa anterior, y mediante una combinacio'n lineal ma's la activacio'n, se van construyendo paso a paso los valores hasta la salida finaldelared. -salidaindependienteydistribucio'n: cadasalida puede asociarse a una variable distinta. segu'n el fig. 12. funcio'n de costo caso, la distribucio'n puede ser de tipo catego'rica - maldicio'n de dimensionalidad: pasacuandotra- (como en el uso de softmax) o continua (como en bajamos con datos que tienen much'ısimas variables unaregresio'n). o dimensiones. al ir aumentando esas dimensiones, los datos empiezan a dispersarse y quedan muy separados entre s'ı, lo que hace ma's dif'ıcil encontrar patrones claros. en otras palabras, el modelo tiene que calcular en un espacio cada vez ma's grande y conmenosdensidaddeinformacio'n,loquecomplica fig. 9. salida independiente elaprendizaje. fig. 10. distribucio'n fig. 13. maldicio'n de la dimensionalidad - capa de salida: es la parte final de la red y se - comportamiento jera'rquico: se utiliza este calcula con la fo'rmula ℎ(𝑛) = 𝑔(ℎ(𝑛 -1)𝑊𝑛 + 𝑏𝑛). enfoque porque imita la forma en que los humanos ba'sicamente, lo que hace es tomar la salida de aprenden: comienzancon conceptos simplesy luego la u'ltima capa oculta,"}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_004", "idx": 4, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "calcular en un espacio cada vez ma's grande y conmenosdensidaddeinformacio'n,loquecomplica fig. 9. salida independiente elaprendizaje. fig. 10. distribucio'n fig. 13. maldicio'n de la dimensionalidad - capa de salida: es la parte final de la red y se - comportamiento jera'rquico: se utiliza este calcula con la fo'rmula ℎ(𝑛) = 𝑔(ℎ(𝑛 -1)𝑊𝑛 + 𝑏𝑛). enfoque porque imita la forma en que los humanos ba'sicamente, lo que hace es tomar la salida de aprenden: comienzancon conceptos simplesy luego la u'ltima capa oculta, multiplicarla por los pesos, los combinan para formar ideas ma's complejas. sumarle un sesgo y luego pasarla por una funcio'n esto permite generar mejoras exponenciales en las de activacio'n. esa funcio'n 𝑔(𝑥) no siempre es la funcionesyaprovecharmejorelaprendizaje. sigmoide, puede ser otra dependiendo del tipo de permiteconstruirfuncionespolino'micas. - - utilizalacomposicio'ndefunciones,reutilizando iii. continuacio'ndefuncionesdeactivacio'n funcionessimplesparacrearotrasdemayornivel. las funciones de activacio'n son un elemento ofrece una representacio'n compacta, donde con - fundamentalenlasredesneuronales,yaquepermiten pocos pesos se pueden modelar funciones comintroducirlanolinealidadnecesariapararepresentar plejas. relaciones complejas en los datos. a continuacio'n, - ejemplo:unaredneuronalpuedeaproximarotra sepresentanlasfuncionesma'simportantesjuntocon funcio'n. susprincipalescaracter'ısticasmatema'ticas. fig. 14. comportamiento jera'rquico fig. 17. ejemplos de funciones de activacio'n: a la izquierda la funcio'n lineal y a la derecha la funcio'n tangente hiperbo'lica (tanh), usada en redes neuronales para introducir no linealidad. - mapas de caracter'ısticas en cnn: en una red neuronalconvolucional(cnn),lascapasnotrabajan a. funcio'n lineal solo con los p'ıxeles, sino que van aprendiendo repla funcio'n lineal se define como 𝑓(𝑥) = 𝑥. la resentaciones cada vez ma's complejas de la imagen. derivadaesconstante,porloqueelmodelonopuede al inicio, en las primeras capas, se detectan cosas usar el descenso del gradiente ni aprender de los muy ba'sicas como bordes o l'ıneas. luego, en las datos. capas intermedias, ya aparecen formas un poco ma's clarascomopartesdeojosobocas.finalmente,enlas u'ltimas capas, la red es capaz de reconocer objetos completos,porejemplounrostro. fig. 18. funcio'n lineal fig. 15. extraccio'n progresiva de caracter'ısticas en una cnn - representaciones vectoriales: en procesamiento de lenguaje natural, las palabras se representancomovectoresdealtadimensio'n,estopermite que palabras con funciones similares se agrupen en elespaciovectorial. fig. 19. ejemplo b. sigmoide tiene una activacio'n que var'ıa entre 0 y 1, siempre positiva, acotada y estrictamente creciente. sin embargo,presentaelproblemadequesuderivadase fig. 16. visualizacio'n aproximaaceroenlosextremosdelafuncio'n,loque provoca gradientes muy pequen˜os. esto hace que el entrenamientosevuelvalentoosedetenga,feno'meno conocidocomovanishing gradient. fig. 22. ejemplo softmax - ¿por que' usar 𝑒𝑥 ? porque es una funcio'n estrictamente creciente y evita valores negativos enlasalida. - cross-entropy loss: tambie'n llamada logfig. 20. ejemplo loss o logistic loss, se utiliza como funcio'n de pe'rdida en softmax."}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_005", "idx": 5, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "en elespaciovectorial. fig. 19. ejemplo b. sigmoide tiene una activacio'n que var'ıa entre 0 y 1, siempre positiva, acotada y estrictamente creciente. sin embargo,presentaelproblemadequesuderivadase fig. 16. visualizacio'n aproximaaceroenlosextremosdelafuncio'n,loque provoca gradientes muy pequen˜os. esto hace que el entrenamientosevuelvalentoosedetenga,feno'meno conocidocomovanishing gradient. fig. 22. ejemplo softmax - ¿por que' usar 𝑒𝑥 ? porque es una funcio'n estrictamente creciente y evita valores negativos enlasalida. - cross-entropy loss: tambie'n llamada logfig. 20. ejemplo loss o logistic loss, se utiliza como funcio'n de pe'rdida en softmax. representa probabilidades c. tangente hiperbo'lica enunespaciologar'ıtmicodentrodelrango [0,1] yesnume'ricamenteestable. la funcio'n tanh tiene un rango de valores entre lape'rdidasedefinecomo: -1y1.sucomportamientoessimilaraldelafuncio'n sigmoide, con la diferencia de que esta' centrada en el origen, lo que permite que los valores negativos 𝐿 = log(𝑃(𝑌 = 𝑦 𝑖 | 𝑋 = 𝑥 𝑖 )) tambie'n sean considerados. sin embargo, al igual yenelcasodeclasificacio'nmulticlase: que la sigmoide, presenta el problema del gradiente desvanecido en los extremos, lo que puede dificultar (cid:32) (cid:33) 𝑒𝑠 𝑘 elentrenamientoderedesprofundas. 𝐿 = -log (cid:205)𝐶 𝑒𝑠 𝑗 𝑗=1 fig. 21. ejemplo fig. 23. ejemplo d. funcio'n softmax e. ¿cua'l funcio'n de activacio'n utilizar? la funcio'n softmax convierte la capa de salida la eleccio'n de la funcio'n de activacio'n depende (output layer) en una distribucio'n de probabilidad, del tipo de problema que se este' resolviendo. las yaquenormalizalosvaloresmedianteunasumatoria. funciones sigmoid y tanh suelen presentar el insudefinicio'neslasiguiente: conveniente del vanishing gradient, lo que dificulta el entrenamiento en redes profundas. por ello, se 𝑒𝑥 𝑗 recomienda iniciar con la funcio'n relu, ya que es 𝜎(𝑥) 𝑗 = (cid:205)𝐾 𝑒𝑥 ra'pida de calcular y ampliamente utilizada en deep 𝑘 𝑘=1 learning. en caso de que no funcione adecuadaes comu'nmente utilizada en problemas de clasifimente, se pueden emplear variantes como leaky cacio'n, donde el vector de entrada se conoce como reluoparametric relu,quebuscansuperarestas logits. adema's, se emplea junto con la funcio'n de limitaciones. pe'rdidacross-entropy loss. iv. backpropagation 2) salida: 𝑎𝑙 = 𝑔(𝑧𝑙) donde 𝑔 es nuestra funcio'ndeactivacio'n. permite calcular cua'nto contribuye cada peso al error final de la red, actualizando los para'metros en vamosaactualizarlospara'metrosde𝑧𝑙,queson𝑤𝑙 direccio'n opuesta a la propagacio'n hacia adelante. y 𝑏𝑙. para esto emplearemos la regla de la cadena, este proceso es esencial para que la red aprenda y usando la salida de la activacio'n de la capa anterior. mejoresudesempen˜oduranteelentrenamiento. profundizando a nivel de neurona, se muestra la siguientefigura. a. procesos del entrenamiento - forward propagation: consiste en calcular la salida de la red enviando los datos desde"}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_006", "idx": 6, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "contribuye cada peso al error final de la red, actualizando los para'metros en vamosaactualizarlospara'metrosde𝑧𝑙,queson𝑤𝑙 direccio'n opuesta a la propagacio'n hacia adelante. y 𝑏𝑙. para esto emplearemos la regla de la cadena, este proceso es esencial para que la red aprenda y usando la salida de la activacio'n de la capa anterior. mejoresudesempen˜oduranteelentrenamiento. profundizando a nivel de neurona, se muestra la siguientefigura. a. procesos del entrenamiento - forward propagation: consiste en calcular la salida de la red enviando los datos desde la capa de entrada hacia las capas siguientes, hasta obtenerelresultadofinal. - backpropagation: implica propagar el error desdelacapadesalidahacialascapasanteriores, calculando las derivadas parciales con respecto a los pesos y sesgos para ajustar los para'metros delmodelo. fig. 26. grafo de la capa al y li a detalle fig. 24. forward y back propagation c. vector gradiente b. optimizacio'n del grafo en este ejemplo se considera una red neuronal en el vector gradiente se define como el conjunto la que cada capa contiene u'nicamente una neurona, de derivadas parciales de los para'metros (pesos y suponiendo que la funcio'n de activacio'n utilizada es sesgos) de la red neuronal. al calcularlo, es comu'n lasigmoide,comosemuestraenlafigura??. encontrar operaciones repetidas, lo que se aprovecha en el algoritmo de backpropagation para optimizar losca'lculos. fig. 25. grafo de la red neuronal denominamos a las capas antes de 𝐿 , 𝑎𝑙 hasta - 𝑖 𝑎𝑙-𝑛. definimoselmsecomo: - 𝐿 𝑖 = (𝑎𝑙 - 𝑦 𝑖 ) 2 dividimoslaneuronaen2capas: - 1) entrada: 𝑧𝑙 = 𝑤𝑙𝑎𝑙-1 + 𝑏𝑙 donde 𝑎𝑙-1 correspondealosinputs𝑥. fig. 27. vector gradiente d. mu'ltiples neuronas e. ca'lculo de funcio'n de perdida paraestaseccio'n,ellossglobalseobtienesumando las diferencias entre la salida de cada neurona en la capa de activacio'n 𝑗 y su valor esperado 𝑦 , 𝑗 recorriendotodaslasneuronasdelacapa 𝑙. 𝑛𝑙 𝐿 𝑖 = ∑︁ (𝑎( 𝑗 𝑙) - 𝑦 𝑗 ) 2 𝑗=1 en la siguiente figura se muestra un ejemplo donde se evalu'a la salida de una capa de activacio'n fig. 28. grafo con mayor dimensionaldad utilizandoestafuncio'ndepe'rdida. - super'ındice: sen˜ala la capa a la que pertenece unavariable.ejemplo:𝑎(𝑙) correspondealacapa 𝑙. - sub'ındice: identifica el nu'mero de neurona dentro de una capa espec'ıfica. ejemplo: 𝑎(𝑙) se 𝑗 refiereala 𝑗-e'simaneuronaenlacapa 𝑙. fig. 30. ejemplo - pesos: se representan con dos sub'ındices: el cambios a la regla de la cadena primero indica la neurona destino y el segundo laneuronadeorigen.ejemplo:𝑤(𝑙) representael como las funciones 𝐿 𝑖 , 𝑧( 𝑗 𝑙) y 𝑎( 𝑗 𝑙) han sido mod𝑗,𝑘 ificadas, es necesario"}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_007", "idx": 7, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "dimensionaldad utilizandoestafuncio'ndepe'rdida. - super'ındice: sen˜ala la capa a la que pertenece unavariable.ejemplo:𝑎(𝑙) correspondealacapa 𝑙. - sub'ındice: identifica el nu'mero de neurona dentro de una capa espec'ıfica. ejemplo: 𝑎(𝑙) se 𝑗 refiereala 𝑗-e'simaneuronaenlacapa 𝑙. fig. 30. ejemplo - pesos: se representan con dos sub'ındices: el cambios a la regla de la cadena primero indica la neurona destino y el segundo laneuronadeorigen.ejemplo:𝑤(𝑙) representael como las funciones 𝐿 𝑖 , 𝑧( 𝑗 𝑙) y 𝑎( 𝑗 𝑙) han sido mod𝑗,𝑘 ificadas, es necesario plantear nuevas derivadas que pesoqueconectalaneurona𝑎(𝑙-1) conlaneurona 𝑘 permitan actualizar los para'metros de cada neurona 𝑎(𝑙). 𝑗 𝑗. a continuacio'n, en la siguiente figura se ilustra en la ecuacio'n se observa que, para ajustar un co'mounaneuronadelacapa 𝑙 recibeentradasdesde pesoespec'ıfico𝑤(𝑙),debemoscalcularsusderivadas 𝑗,𝑘 varias neuronas de la capa anterior (𝑙 - 1). este parciales.sinembargo,graciasalconceptodecache', procesosepuededividirendospasos: la u'nica derivada que cambia al actualizar un peso - preactivacio'n: diferente es 𝛿𝑧( 𝑗 𝑙) , mientras que el resto permanece 𝑛𝑙-1 𝛿𝑤( 𝑗 𝑙 , ) 𝑘 𝑧(𝑙) = 𝑏(𝑙) + ∑︁ 𝑤(𝑙)𝑎(𝑙-1) constanteparatodalacapa. 𝑗 𝑗 𝑗,𝑘 𝑘 lasderivadasseexpresandelasiguienteforma: 𝑘=1 donde 𝑏(𝑙) es el sesgo de la neurona y 𝑤(𝑙) los 𝑗 𝑗,𝑘 𝛿𝐿 pesosdeconexio'n. 𝛿𝑎𝑙 𝑖 = ((𝑎 1 𝑙 - 𝑦 1 ) 2 + (𝑎 2 𝑙 - 𝑦 2 ) 2 +---+ (𝑎𝑙 𝑛 - 𝑦 𝑛 ) 2 ) - activacio'n: 𝑗 𝑎( 𝑗 𝑙) = 𝑔(𝑧( 𝑗 𝑙) ) 𝛿 𝛿 𝑎 𝐿 𝑙 𝑖 = 2(𝑎𝑙 𝑗 - 𝑦 𝑗 ) donde 𝑔 representa la funcio'n de activacio'n 𝑗 aplicada. 𝛿𝑎(𝑙) 𝑗 = 𝑔(𝑧(𝑙) )(1-𝑔(𝑧(𝑙) )) 𝛿𝑧(𝑙) 𝑗 𝑗 𝑗 𝛿𝑧(𝑙) 𝑗 = 𝑎(𝑙-1) 𝛿𝑤(𝑙) 𝑘 fig. 29. ejemplo 𝑗,𝑘 con esto se logra actualizar los pesos de la capa 𝑙, aunquelasderivadasnocambian,s'ıdebenmanejarse ma's'ındicesamedidaquelaredcreceencomplejidad. capa 𝑙-1 cuando el ca'lculo debe extenderse hacia una capa anterior,comolacapa𝑙-1,elprocedimientosevuelve ma's complejo. esto ocurre porque, segu'n el taman˜o de la siguiente capa, el algoritmo requiere combinar ma's conexiones y para'metros, lo que incrementa la dificultaddelca'lculo. 𝑛𝑙 𝛿𝑧(𝑙) 𝛿𝑎(𝑙) 𝛿𝐿 𝛿𝐿 𝑖 ∑︁ 𝑗 𝑗 𝑖 = 𝛿𝑎(𝑙-1) 𝛿𝑎(𝑙-1) 𝛿𝑧(𝑙) 𝛿𝑎(𝑙) 𝑘 𝑗=1 𝑘 𝑗 𝑗"}
{"id_doc": "DOC_030", "segmentacion": "A", "chunk_id": "DOC_030_A_008", "idx": 8, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "𝛿𝑧(𝑙) 𝛿𝑎(𝑙) 𝛿𝐿 𝛿𝐿 𝑖 ∑︁ 𝑗 𝑗 𝑖 = 𝛿𝑎(𝑙-1) 𝛿𝑎(𝑙-1) 𝛿𝑧(𝑙) 𝛿𝑎(𝑙) 𝑘 𝑗=1 𝑘 𝑗 𝑗"}
{"id_doc": "DOC_031", "segmentacion": "A", "chunk_id": "DOC_031_A_000", "idx": 0, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "apuntes inteligencia artificial, clase 02 de octubre javier alonso rojas rojas escuela de ingenier'ıa en computacio'n instituto tecnolo'gico de costa rica cartago, costa rica javrojas@estudiantec.cr abstract-estos apuntes reflejan lo conversado en la clase del 02 de octubre donde se mencionaron temas como. el funcionamiento de los agentes basados en modelos de lenguaje de granescala(llm)ysupapelenlainteligenciaartificialmoderna. semencionaronlasprincipalesherramientasyframeworkspara lacreacio'ndeagentes,juntoconlasdiferenciasentresistemasde unsoloagenteymultiagente.adema's,seexaminaelcasodesora de openai como ejemplo de modelo multimodal de generacio'n devideoyaudio,considerandotambie'nlosretose'ticosasociados. finalmente, se incluye un repaso de los fundamentos de las redes neuronales, sus funciones de activacio'n y el proceso de entrenamientomediantebackpropagation,comobaseconceptual de los llm actuales. fig.1. sora2 i. introduction la inteligencia artificial (ia) ha avanzado ra'pidamente gracias a los modelos de lenguaje de gran escala (llms) y al iii. sora2byopenai desarrollo de agentes inteligentes capaces de actuar y razonar en distintos contextos. estos sistemas han transformado la sora 2 es la nueva versio'n del modelo multimodal de generacio'ndetextoenunprocesodeplanificacio'nyejecucio'n openai, capaz de generar video y audio sincronizados a ma's complejo, permitiendo la creacio'n de agentes auto'nomos partir de texto. presenta notables mejoras en realismo f'ısico, que integran herramientas y colaboran entre s'ı. coherencia visual y control creativo. su funcio'n de cameos los apuntes abordan los fundamentos y la arquitectura de permite insertar la imagen y voz del usuario, bajo consenlos agentes basados en llm, distinguiendo entre sistemas timiento, ampliando las posibilidades narrativas y expresivas. individualesymultiagente,ydestacandoelpapeldelchainof adema's, han desarrollado un tipo de red social donde se thought (cot) como mecanismo clave para el razonamiento pueden compartir los videos creados con el modelo. estructurado. adema's, se incluye el caso de estudio sora de el modelo ofrece mayor precisio'n en iluminacio'n, openai y un repaso de las bases neuronales del aprendizaje movimientoysonido,adema'sdeopcionesdecontrolestil'ıstico profundo,comolasfuncionesdeactivacio'nyelentrenamiento mediante steerability. openai ha incorporado medidas e'ticas por backpropagation. para evitar la reproduccio'n de personas reales o la generacio'n de contenido sensible, lanza'ndolo de forma gradual mediante ii. mencio'ndelecturadeagentes la sora app y futuras apis. sora 2 representa un avance sigse hizo un repaso general de la lectura \"from language to nificativoenlageneracio'naudiovisualconia,aunqueplantea action:areviewoflargelanguagemodelsasautonomous retos importantes en materia de privacidad y autenticidad agents and tool users\". explico' que lo fundamental para digital. el pro'ximo quiz del martes es comprender lo esencial: los modelos de lenguaje (llms) han pasado de ser simples iv. repasoderedesneuronales generadores de texto a actuar como agentes auto'nomos con capacidad para razonar, planificar, usar memoria e interactuar a. el perceptro'n y su evolucio'n con herramientas externas. la lectura tambie'n distingue entre sistemas"}
{"id_doc": "DOC_031", "segmentacion": "A", "chunk_id": "DOC_031_A_001", "idx": 1, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "general de la lectura \"from language to nificativoenlageneracio'naudiovisualconia,aunqueplantea action:areviewoflargelanguagemodelsasautonomous retos importantes en materia de privacidad y autenticidad agents and tool users\". explico' que lo fundamental para digital. el pro'ximo quiz del martes es comprender lo esencial: los modelos de lenguaje (llms) han pasado de ser simples iv. repasoderedesneuronales generadores de texto a actuar como agentes auto'nomos con capacidad para razonar, planificar, usar memoria e interactuar a. el perceptro'n y su evolucio'n con herramientas externas. la lectura tambie'n distingue entre sistemas de un solo agente y sistemas multiagente, en los el perceptro'n puede entenderse de forma similar a una que varios modelos cooperan para resolver tareas ma's com- regresio'n log'ıstica, aunque se diferencia en la funcio'n de plejas. adema's, se analizan sus aplicaciones en a'reas como pe'rdida que utiliza. durante la historia de la inteligencia la investigacio'n, la programacio'n, la salud, la robo'tica y las artificialsurgio' elllamado\"inviernodelaia\",enpartedebido simulaciones,juntoconlosprincipalesdesaf'ıosqueenfrentan: al problema del xor, ya que este no pod'ıa ser representado la memoria limitada, la seguridad, la e'tica y la necesidad de adecuadamente por un modelo lineal ni por un perceptro'n mejores me'todos de evaluacio'n. simple. fig.3. comportamientojera'rquico fig.2. funcionesdeactivacio'n b. el problema del xor el principal inconveniente del perceptro'n simple es que el problema xor no es linealmente separable, por lo que este modelo no puede ofrecer una solucio'n adecuada. esto dio fig.4. funcionamientodelascnn origen a las redes neuronales multicapa (mlp), capaces de resolver problemas no lineales y ampliar significativamente el rango de aplicaciones posibles. cada capa se calcula de la siguiente forma: h(0) =σ(xw +b ) (1) 0 0 c. inspiracio'n biolo'gica h(1) =σ(h(0)w +b ) (2) 1 1 las redes neuronales artificiales se inspiran en el funh(n) =g(h(n-1)w +b ) (3) cionamientodelcerebrohumano.cadaneuronarecibesen˜ales n n a trave's de sus dendritas (entradas), las procesa en el nu'cleo f. capas de salida y distribucio'n mediante una combinacio'n lineal, y decide si transmite o no las salidas pueden ser catego'ricas o continuas: la sen˜al segu'n una funcio'n de activacio'n. - en clasificacio'n, se usa softmax como funcio'n de salida. - en regresio'n, se emplea una funcio'n lineal. d. funciones de activacio'n en todos los casos, la activacio'n final g(x) debe ser no lineal para permitir un aprendizaje ma's expresivo. las funciones de activacio'n introducen no linealidad en el modelo, permitiendo que la red aprenda relaciones complejas: g. maldicio'n de la dimensionalidad - relu: g(x) = max(0,x); eficiente, pero puede generar cuandosetrabajacondatosdemuchasvariables,lospuntos \"neuronas muertas\" cuando"}
{"id_doc": "DOC_031", "segmentacion": "A", "chunk_id": "DOC_031_A_002", "idx": 2, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "segu'n una funcio'n de activacio'n. - en clasificacio'n, se usa softmax como funcio'n de salida. - en regresio'n, se emplea una funcio'n lineal. d. funciones de activacio'n en todos los casos, la activacio'n final g(x) debe ser no lineal para permitir un aprendizaje ma's expresivo. las funciones de activacio'n introducen no linealidad en el modelo, permitiendo que la red aprenda relaciones complejas: g. maldicio'n de la dimensionalidad - relu: g(x) = max(0,x); eficiente, pero puede generar cuandosetrabajacondatosdemuchasvariables,lospuntos \"neuronas muertas\" cuando el gradiente es cero. se dispersan en un espacio de alta dimensio'n, reduciendo su - leaky relu: introduce una pequen˜a pendiente en la densidad y dificultando el hallazgo de patrones significativos. parte negativa para evitar neuronas inactivas. h. comportamiento jera'rquico - tanh: produce salidas en el rango (-1,1), u'til para manejar valores positivos y negativos. como vemos en la figura 3, las redes neuronales apren- - sigmoide: transforma la entrada en valores entre 0 y 1, den de forma jera'rquica, combinando funciones simples para comu'n en tareas de clasificacio'n binaria. formarotrasma'scomplejas.estopermiteconstruirrepresentaciones compactas y eficientes, en las que un nu'mero reducido de pesos puede modelar funciones avanzadas. e. perceptro'n multicapa (mlp) i. cnn elmultilayerperceptron(mlp)extiendeelperceptro'nsimple an˜adiendo capas ocultas que permiten resolver problemas en las redes convolucionales (cnn), las primeras capas no lineales. su estructura general incluye: detectan bordes o patrones ba'sicos, las intermedias aprenden estructurasma'sdefinidasylasu'ltimascapasreconocenobjetos - capa de entrada: recibe los datos originales x i . completos, como rostros o figuras, esto representado en la - capas ocultas: realizan transformaciones y ca'lculos infigura 4. ternos. - capa de salida: entrega el resultado final, cuyo taman˜o j. representaciones vectoriales depende del tipo de problema. enelprocesamientodelenguajenatural(nlp),laspalabras el entrenamiento se realiza mediante backpropagation, que serepresentancomovectoresenunespaciodealtadimensio'n, calcula el error del modelo y ajusta los pesos utilizando de modo que las palabras con significados o funciones simidescenso de gradiente. lares se ubican pro'ximas entre s'ı en dicho espacio. d. parametric relu (prelu) la funcio'n parametric relu (prelu) es una variante de la funcio'n relu tradicional, como se muestra en la figura ??. a diferencia de la relu esta'ndar, esta introduce un para'metro α que se aprende durante el entrenamiento y controla la pendiente en la regio'n negativa. de esta manera, el modelo puede ajustar automa'ticamente el grado de \"fuga\" en los valores menores que cero, evitando el problema de las neuronas muertas. fig.5. tangentehiperbo'lica su definicio'n matema'tica es la siguiente: (cid:40) αx, si x<0 v. funcionesdeactivacio'n g(x)="}
{"id_doc": "DOC_031", "segmentacion": "A", "chunk_id": "DOC_031_A_003", "idx": 3, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "es una variante de la funcio'n relu tradicional, como se muestra en la figura ??. a diferencia de la relu esta'ndar, esta introduce un para'metro α que se aprende durante el entrenamiento y controla la pendiente en la regio'n negativa. de esta manera, el modelo puede ajustar automa'ticamente el grado de \"fuga\" en los valores menores que cero, evitando el problema de las neuronas muertas. fig.5. tangentehiperbo'lica su definicio'n matema'tica es la siguiente: (cid:40) αx, si x<0 v. funcionesdeactivacio'n g(x)= x, si x≥0 las funciones de activacio'n son un componente esencial la derivada correspondiente es: en las redes neuronales, ya que permiten introducir la no (cid:40) dg(x) α, si x<0 linealidad necesaria para modelar relaciones complejas entre = dx 1, si x≥0 los datos. a continuacio'n, se describen las funciones ma's relevantes junto con sus principales caracter'ısticas matema'ticas. elpara'metroαseentrenajuntoconelrestodelospesosde lared,loqueotorgaalmodelomayorflexibilidadycapacidad de adaptacio'n frente a distintas distribuciones de datos. por a. lineal esta razo'n, la prelu suele ofrecer un mejor desempen˜o en la funcio'n lineal se define como: arquitecturasprofundasdondelareluesta'ndarpodr'ıaperder gradiente. f(x)=ax e. softmax su derivada es constante (f′(x) = a), por lo que el modelo la funcio'n softmax transforma las salidas de la capa final no puede aprovechar el descenso del gradiente para aprender en una distribucio'n de probabilidad, como vemos en la figura patrones complejos. debido a su cara'cter estrictamente lineal, 6. su expresio'n se define como: no introduce capacidad de generalizacio'n ni no linealidad en exj σ(x) = la red. j (cid:80)k exk k=1 dondecadavalorx sedenominalogit.estafuncio'nseutiliza j b. sigmoide principalmente en problemas de clasificacio'n multiclase, ya que garantiza que todas las salidas sean positivas y sumen 1. lafuncio'nsigmoideproducesalidasentre0y1,essiempre positiva, acotada y estrictamente creciente: - el uso de ex asegura una funcio'n estrictamente creciente y evita valores negativos. 1 - se emplea junto con la funcio'n de pe'rdida crossσ(x)= 1+e-x entropy loss, tambie'n llamada log-loss. la pe'rdida se define como: a pesar de su utilidad inicial, presenta el problema del l=-logp(y =y |x =x ) vanishing gradient: la derivada tiende a cero en los extremos i i de la funcio'n, lo que hace que el aprendizaje sea lento o y, en el caso multiclase: incluso se detenga en redes profundas. esk l=-log (cid:80)c esj j=1 c. tangente hiperbo'lica (tanh) f. seleccio'n de la funcio'n de activacio'n la funcio'n tangente hiperbo'lica tiene un rango de salida la eleccio'n de la funcio'n de activacio'n depende del tipo de entre(-1,1)comovemosenlafigura5ysuformaessimilar problemaylaarquitecturadelared.lasfuncionessigmoidy a la"}
{"id_doc": "DOC_031", "segmentacion": "A", "chunk_id": "DOC_031_A_004", "idx": 4, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "l=-logp(y =y |x =x ) vanishing gradient: la derivada tiende a cero en los extremos i i de la funcio'n, lo que hace que el aprendizaje sea lento o y, en el caso multiclase: incluso se detenga en redes profundas. esk l=-log (cid:80)c esj j=1 c. tangente hiperbo'lica (tanh) f. seleccio'n de la funcio'n de activacio'n la funcio'n tangente hiperbo'lica tiene un rango de salida la eleccio'n de la funcio'n de activacio'n depende del tipo de entre(-1,1)comovemosenlafigura5ysuformaessimilar problemaylaarquitecturadelared.lasfuncionessigmoidy a la sigmoide, pero centrada en el origen: tanh tienden a sufrir el problema del gradiente desvanecido, ex-e-x por lo que no son recomendadas para redes profundas. en tanh(x)= la pra'ctica, se suele comenzar con la funcio'n relu por ex+e-x su eficiencia computacional y buen rendimiento en modelos esto permite representar tanto valores positivos como nega- de deep learning. si esta presenta problemas (por ejemplo, tivos,loquefacilitalaconvergenciadelmodelo.sinembargo, neuronas muertas), se pueden utilizar variantes como leaky al igual que la sigmoide, tambie'n sufre del problema del relu o parametric relu, que permiten mantener un flujo gradiente desvanecido en los extremos. de gradiente estable incluso en valores negativos. fig.6. usodesoftmax fig.8. redneuronalsimple fig.9. redneuronalmascompleja donde g representa la funcio'n de activacio'n. fig.7. forwardpropagationybackpropagation lospara'metrosw(l) yb(l) seactualizanutilizandolaregla delacadena,derivandoelerrorconrespectoacadapara'metro vi. backpropagation y aprovechando la salida de la capa anterior. el algoritmo de backpropagation permite calcular cua'nto d. vector gradiente contribuye cada peso al error final de la red, actualizando los el vector gradiente esta' formado por todas las derivadas para'metros en la direccio'n opuesta a la propagacio'n hacia parcialesdelospara'metros(pesosysesgos)delared.durante adelante. este proceso es esencial para que la red neuronal el ca'lculo del gradiente se identifican operaciones repetidas, aprendaymejoresudesempen˜oduranteelentrenamiento.esto loquepermiteoptimizarlosca'lculosenelalgoritmodebackrepresentado en la figura 7. propagation mediante reutilizacio'n de resultados intermedios a. forward propagation (cache'). consiste en calcular la salida de la red, enviando los datos e. redes con mu'ltiples neuronas desde la capa de entrada hacia las capas ocultas hasta obtener en redes con mayor dimensionalidad como la de la figura la salida final. 9, se introducen notaciones adicionales: b. backpropagation - super'ındice: indica la capa. ejemplo: a(l) representa la activacio'n en la capa l. implica propagar el error desde la capa de salida hacia las - sub'ındice: indica la neurona dentro de una capa. ejemcapas anteriores, calculando las derivadas parciales respecto a plo: a(l) es la j-e'sima neurona en la capa l. los pesos y sesgos para ajustar los para'metros del modelo. j - pesos: se representan"}
{"id_doc": "DOC_031", "segmentacion": "A", "chunk_id": "DOC_031_A_005", "idx": 5, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "de la figura la salida final. 9, se introducen notaciones adicionales: b. backpropagation - super'ındice: indica la capa. ejemplo: a(l) representa la activacio'n en la capa l. implica propagar el error desde la capa de salida hacia las - sub'ındice: indica la neurona dentro de una capa. ejemcapas anteriores, calculando las derivadas parciales respecto a plo: a(l) es la j-e'sima neurona en la capa l. los pesos y sesgos para ajustar los para'metros del modelo. j - pesos: se representan como w(l), que conecta la neurona j,k c. optimizacio'n del grafo computacional a(l-1) con a(l), aca j seria el destino y k el origen. k j consideremos una red neuronal como la de la figura 8, cada neurona de la capa l recibe entradas desde todas las donde cada capa contiene una u'nica neurona y la funcio'n de neuronas de la capa anterior (l-1), siguiendo los pasos: activacio'nutilizadaeslasigmoide.elca'lculosepuededividir - preactivacio'n: en las siguientes partes: - funcio'n de pe'rdida (mse): z(l) =b(l)+ n (cid:88)l-1 w(l)a(l-1) j j j,k k l =(a(l)-y )2 k=1 i i - activacio'n: donde a(l) es la salida de la red y y i el valor esperado. a(l) =g(z(l)) j j - entrada: z(l) =w(l)a(l-1)+b(l) para obtener la activacio'n de una neurona destino, se calculan las contribuciones de todas las neuronas de la capa - salida: anterior,multiplicandolospesosdeconexio'ncorrespondientes a(l) =g(z(l)) por la activacio'n de cada neurona origen. posteriormente, se sumanestosproductosjuntoconelsesgoasociado,repitiendo el proceso para cada neurona de la capa. f. funcio'n de pe'rdida global la funcio'n de pe'rdida global se obtiene sumando las diferenciasentrelasalidadecadaneuronaenlacapadeactivacio'n l y su valor esperado y : j l = (cid:88) nl (a(l)-y )2 i j j j=1 g. aplicacio'n de la regla de la cadena dadoquelasfuncionesl ,z(l) ya(l) esta'nencadenadas,es i j j necesario aplicar la regla de la cadena para derivar cada peso w(l) ysesgob(l).sololaderivada ∂z j (l) cambiaconcadapeso j,k j ∂w(l) j,k actualizado, mientras que las dema's se mantienen constantes dentro de la capa. las derivadas parciales relevantes son: ∂l i =2(a(l)-y ) ∂a(l) j j j ∂a(l) j =g(z(l))(1-g(z(l))) ∂z(l) j j j ∂z(l) j =a(l-1) ∂w(l) k j,k apartirdeestasderivadas,lospesosysesgosseactualizan siguiendo el descenso del gradiente: ∂l ∂l w(l) ←w(l) -η i , b(l) ←b(l)-η i j,k j,k ∂w(l) j j ∂b(l) j,k j donde η representa la tasa de aprendizaje. en redes ma's profundas, al extender el ca'lculo hacia capas anteriores (l-1), el nu'mero de para'metros y combinaciones a derivar aumenta considerablemente,"}
{"id_doc": "DOC_031", "segmentacion": "A", "chunk_id": "DOC_031_A_006", "idx": 6, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "capa. las derivadas parciales relevantes son: ∂l i =2(a(l)-y ) ∂a(l) j j j ∂a(l) j =g(z(l))(1-g(z(l))) ∂z(l) j j j ∂z(l) j =a(l-1) ∂w(l) k j,k apartirdeestasderivadas,lospesosysesgosseactualizan siguiendo el descenso del gradiente: ∂l ∂l w(l) ←w(l) -η i , b(l) ←b(l)-η i j,k j,k ∂w(l) j j ∂b(l) j,k j donde η representa la tasa de aprendizaje. en redes ma's profundas, al extender el ca'lculo hacia capas anteriores (l-1), el nu'mero de para'metros y combinaciones a derivar aumenta considerablemente, incrementando la complejidad computacional del algoritmo."}
