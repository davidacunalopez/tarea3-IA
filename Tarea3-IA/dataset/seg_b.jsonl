{"id_doc": "DOC_033", "segmentacion": "B", "chunk_id": "DOC_033_B_000", "idx": 0, "autor": "María José Chacón Rodríguez", "fecha": "2025-10-07", "tema": "Conceptos avanzados de redes neuronales convolucionales (CNN), con énfasis en arquitecturas modernas como AlexNet, VGG y ResNet, aplicadas al procesamiento de imágenes.", "texto": "apuntes ia clase 7/10\ngianmarco oporta pe'rez\ningenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\nsan jose', costa rica\ngooporta@estudiantec.cr\n\nabstract-el presente documento recopila los apuntes de la mu'ltiples agentes que colaboran o compiten entre s'ı\nclase del 7 de octubre, correspondientes al curso de inteligencia y con su entorno para alcanzar objetivos comunes o\nartificial. se abordan los lineamientos del proyecto 1, centrado\nindividuales.\n\nen redes neuronales convolucionales aplicadas al reconocimiento\nde voz mediante espectrogramas, as'ı como los fundamentos\nteo'ricos de las redes convolucionales y su aplicacio'n en tareas\n\nii. descripcio'ndelproyecto\nde clasificacio'n y segmentacio'n de ima'genes.\nel primer proyecto tiene como objetivo aplicar redes neuindex terms-inteligencia artificial, redes neuronales conronales convolucionales (cnn) en la tarea de reconocimiento\nvolucionales, clasificacio'n de audio, pytorch, espectrogramas\nde voz a partir de espectrogramas. su propo'sito es desarrollar\nun modelo de clasificacio'n multiclase capaz de identificar"}
{"id_doc": "DOC_033", "segmentacion": "B", "chunk_id": "DOC_033_B_001", "idx": 1, "autor": "María José Chacón Rodríguez", "fecha": "2025-10-07", "tema": "Conceptos avanzados de redes neuronales convolucionales (CNN), con énfasis en arquitecturas modernas como AlexNet, VGG y ResNet, aplicadas al procesamiento de imágenes.", "texto": "i. resultadosdelquiz5 distintos tipos de sonidos, tales como vocalizaciones humanas\nse realizo' el quiz 5 durante el inicio de la sesio'n, la cual y animales, instrumentos musicales o ruidos ambientales.\ncontiene las siguientes preguntas: para lograrlo, se utiliza un conjunto de datos pu'blico orientado a la clasificacio'n de audio, en el cual cada muestra\n- pregunta:describaque' esunaredtotalmenteconectada.\ncorresponde a una grabacio'n corta etiquetada con su clase\nr/ es una red neuronal en la cual cada neurona esta'\ncorrespondiente.antesdelentrenamiento,lassen˜alesacu'sticas\nconectada con todas las neuronas de la capa siguiente,\n\nson transformadas en representaciones visuales denominadas\nde principio a fin.\nespectrogramas, las cuales se emplean como entrada a las\n- pregunta: mencione tres funciones de activacio'n no\nredes convolucionales.\nlineales.\nr/ relu, sigmoide y tanh. elproyectorequierelaconstruccio'nmanualdedosmodelos\n- pregunta: describa los cuatro componentes principales empleando pytorch sin librer'ıas de alto nivel:\nde un agente llm. - modelo a: variante cla'sica de lenet-5 adaptada al\nr/ reconocimiento de audio mediante espectrogramas.\n- perfil:definelaidentidadopersonalidaddelagente, - modelo b: arquitectura alternativa fundamentada en\ndeterminando su tono, estilo de comunicacio'n y literatura acade'mica o en un disen˜o propio justificado\ncomportamiento general. teo'ricamente.\n- memoria: permite conservar informacio'n relevante se deben generar dos versiones del conjunto de datos: una\nde interacciones anteriores o resultados previos, fa- con los audios transformados a ima'genes (base) y otra con\ncilitando la continuidad y el contexto en tareas versionesaumentadasmediantete'cnicasdedataaugmentation\nextensas. orientadasaldominiodelaudio.esteprocesobuscaincremen-\n- herramientas:correspondenarecursosofunciones tar la robustez del modelo y su capacidad de generalizacio'n.\nexternas que el agente puede invocar para realizar durante la fase de entrenamiento se construyen cuoperacionesespec'ıficasoaccederainformacio'nadi- atro combinaciones principales: modelo a/base, modelo\ncional. a/aumentado,modelob/baseymodelob/aumentado.cada\n- planificacio'n o razonamiento: consiste en la ca- modelo se entrena con diferentes hiperpara'metros, evaluando\npacidad del agente para interpretar las instrucciones sudesempen˜oconme'tricascomoprecisio'n,pe'rdida,f1-score\ndel usuario, elaborar estrategias y seleccionar la y matriz de confusio'n. la herramienta weights & biases se\naccio'n ma's apropiada segu'n el objetivo planteado. utiliza para monitorear y visualizar los resultados durante el\n- pregunta:describaladiferenciaentresistemasdeagente entrenamiento.\nu'nico y sistemas multiagentes. finalmente, los modelos seleccionados se comparan para\nr/ un agente u'nico percibe su entorno, toma decisiones determinar el de mejor rendimiento general. el informe debe\ny ejecuta acciones de manera independiente, mientras presentarse en formato ieee, acompan˜ado del co'digo fuente\nque los sistemas multiagentes esta'n conformados por y el cuaderno en jupyter notebook."}
{"id_doc": "DOC_033", "segmentacion": "B", "chunk_id": "DOC_033_B_002", "idx": 2, "autor": "María José Chacón Rodríguez", "fecha": "2025-10-07", "tema": "Conceptos avanzados de redes neuronales convolucionales (CNN), con énfasis en arquitecturas modernas como AlexNet, VGG y ResNet, aplicadas al procesamiento de imágenes.", "texto": "iii. aspectospra'cticosdelproyecto\nel desarrollo debe realizarse en pytorch, construyendo\nmanualmente cada capa de la red. es necesario registrar\ntodas las me'tricas relevantes utilizadas en clases anteriores,\nincluyendo la pe'rdida, precisio'n y f1-score. dado que los\nespectrogramas generados pueden ser pesados, se recomienda\nreducir su resolucio'n a 224 p'ıxeles por lado.\n\nla fecha tentativa de entrega fue establecida para el jueves\n30deoctubre.seesperaquelosmodelosimplementadossean\nfig.1. redesconvolucionales\n\ncompletamente reproducibles y que incluyan mecanismos de\ncontrol de aleatoriedad y registro de resultados.\nc. aplicaciones comunes\n\niv. fundamentosderedesneuronales\nconvolucionales lasredesconvolucionalesseaplicanampliamenteendiversas tareas de visio'n artificial, entre las que destacan:\nlas redes neuronales convolucionales (cnn, por sus siglas\nen ingle's) representan una evolucio'n de las redes neuronales - clasificacio'n de ima'genes.\ntradicionales orientadas al procesamiento de datos estruc- - segmentacio'n de objetos.\nturados espacialmente, como ima'genes o espectrogramas. a - segmentacio'n de instancias.\ndiferencia de las redes completamente conectadas, las cnn - procesamiento general de ima'genes.\naprendenpatronesespacialesyjera'rquicosdemaneraeficiente. estas arquitecturas han demostrado una gran eficacia en\nhasta este punto del curso, se ha trabajado con redes problemas de reconocimiento visual, deteccio'n de patrones y\nque reciben un vector de caracter'ısticas como entrada, lo procesamiento de sen˜ales en el dominio de la visio'n.\ntransforman mediante capas ocultas y producen una salida."}
{"id_doc": "DOC_033", "segmentacion": "B", "chunk_id": "DOC_033_B_003", "idx": 3, "autor": "María José Chacón Rodríguez", "fecha": "2025-10-07", "tema": "Conceptos avanzados de redes neuronales convolucionales (CNN), con énfasis en arquitecturas modernas como AlexNet, VGG y ResNet, aplicadas al procesamiento de imágenes.", "texto": "references\nsin embargo, este enfoque no considera la estructura espacial\nde los datos, lo cual puede generar errores al mover, rotar o [1] s.pacheco,\"convolutionalneuralnetworks\"presentacio'n,\nescalar los objetos dentro de una imagen.\na. ejemplo: dataset cifar-10\n\nel conjunto de datos cifar-10 se utiliza frecuentemente\npara experimentacio'n en visio'n por computadora. contiene\nima'genes a color de taman˜o 32×32×3, representando diez\nclases diferentes.\naunquecadaimagenesrelativamentepequen˜a,unaversio'n\nde mayor resolucio'n, por ejemplo 200×200×3, aumentar'ıa\ndra'sticamente el nu'mero de para'metros de entrada, lo que\ndificultar'ıa la escalabilidad del modelo.\nb. estructura de una red convolucional\nen una red convolucional, las neuronas se organizan en\ntres dimensiones: ancho, alto y profundidad. cada neurona\nesta' conectada u'nicamente a una pequen˜a regio'n de la capa\nanterior, en lugar de estar completamente conectada, lo cual\nreduce la complejidad y mejora la eficiencia computacional.\nlas capas principales que componen una cnn son:\n- capa convolucional: calcula las salidas de las neuronas\nconectadas a regiones locales de la capa anterior.\n- capa de agrupamiento (pooling): reduce el taman˜o\nespacial de las representaciones, manteniendo las caracter'ısticas ma's relevantes.\n- capa completamente conectada (fully connected):\ntransforma la representacio'n final en probabilidades de\npertenencia a una clase espec'ıfica.\na medida que las ima'genes avanzan a trave's de las capas\nconvolucionales y de agrupamiento, se reduce su taman˜o\nespacial, pero aumenta la abstraccio'n de las caracter'ısticas"}
{"id_doc": "DOC_033", "segmentacion": "B", "chunk_id": "DOC_033_B_004", "idx": 4, "autor": "María José Chacón Rodríguez", "fecha": "2025-10-07", "tema": "Conceptos avanzados de redes neuronales convolucionales (CNN), con énfasis en arquitecturas modernas como AlexNet, VGG y ResNet, aplicadas al procesamiento de imágenes.", "texto": "aprendidas."}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_000", "idx": 0, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "redes neuronales convolucionales y\n\nbackpropagation\n\napuntesdeclases\nrodolfo david acun˜a lo'pez\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nrodolfoide69@estudiantec.cr\n\nabstract-en este documento podra' encontrar informacio'n a. respuesta del quiz\nsobre la semana 10 de clases de ia, donde se comparten las\nse realizo' el quiz 5 donde se establecieron las respuestas\nrespuestas del quiz 5, se comentan detalles sobre el primer\nde este. las preguntas con sus respuestas son las siguientes:\nproyecto, un pequen˜o resumen de la clase anterior sobre back\npropagationysehablasobreuntemanuevodondepodemosver pregunta: describa que' es una red totalmente conectada\ntemas como convnet o arquitectura de red convolucional. (fully connected) respuesta: es un tipo de red neuronal en\nindex terms-redes neuronales convolucionales, backprop- la que cada neurona esta' conectada con todas las neuronas de\nagation, cnn, reconocimiento de patrones, procesamiento de la capa siguiente.\nima'genes, deep learning\npregunta: mencione 3 funciones de activacio'n no-lineales.\nrespuesta: relu, sigmoide y tanh"}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_001", "idx": 1, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "i. introduccio'n pregunta: describa los 4 componentes principales de un\nagente llm. respuesta:\nlas redes neuronales han revolucionado el campo de la in- - perfil: puede tener su propia personalidad\nteligenciaartificial,especialmenteentareasdereconocimiento - memoria: permite que el agente recuerde informacio'n\ndepatronesyprocesamientodesen˜ales.enestedocumentose pasada o resultados previos para mantener contexto en\nabordan dos conceptos fundamentales: el algoritmo de back- tareas largas\npropagation, que permite el entrenamiento eficiente de redes - herramientas: son funciones externas que el agente\nneuronales profundas, y las redes neuronales convolucionales puede usar para ejecutar acciones\n(cnn),quehandemostradoserparticularmenteefectivaspara - planificacio'n o razonamiento: decide que' hacer, interel procesamiento de ima'genes y sen˜ales. pretandolasinstruccionesdelusuarioyeligiendolamejor\nel backpropagation es un algoritmo de optimizacio'n que accio'n\ncalcula los gradientes de la funcio'n de pe'rdida con respecto pregunta: describa la diferencia entre sistemas de agente\na los para'metros de la red, permitiendo su ajuste mediante u'nico y sistemas multiagentes. respuesta: un agente u'nico\ndescenso de gradiente. por otro lado, las cnn introducen percibe su entorno, toma decisiones y actu'a por s'ı mismo,\nconceptos como convolucio'n y pooling que aprovechan la mientrasquelossistemasmultiagentessonvariosagentesque\nestructura espacial de los datos, reduciendo significativamente interactu'an entre s'ı y con el entorno.\nel nu'mero de para'metros necesarios comparado con redes\nb. explicacio'n del proyecto\ntotalmente conectadas."}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_002", "idx": 2, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "paraesteproyectonecesitamosaplicarredesneuronalespara\neste documento se estructura de la siguiente manera:\nel reconocimiento de voz a partir de espectrogramas, es decir,\n\nprimero se presentan aspectos administrativos del curso inreconocimiento de patrones en voz donde utilizaremos una\ncluyendo respuestas del quiz y detalles del proyecto, luego se\n\narquitectura que se llama redes neuronales convolucionales\n\nrevisa el algoritmo de backpropagation con sus fundamentos\n(cnn) la cual nos sirve para el procesamiento de ima'genes.\nmatema'ticos, y finalmente se introduce el concepto de redes\n\nelretodeesteproyectoesanalizarunaserietemporalenun\nconvolucionales con la arquitectura alexnet como ejemplo.\naudiodondeanalizaremoslasen˜alquevienealsegundodonde\nestemos procesando, por ejemplo, si estamos procesando un\n\nii. aspectosadministrativos 5t, hay que procesar un 5t+1, 5t+2, y as'ı sucesivamente. se\npodr'ıan resolver con redes recurrentes.\ndebido a que no hubo noticias previas a la clase, se inicio' otra forma de hacer esto es convertir esa voz en espectrocon una breve explicacio'n sobre el primer proyecto de redes gramas, la cual es un diagrama de tiempo y las frecuencias\nneuronales. queproducelasen˜aldeaudio.conestoseproduceunpatro'n."}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_003", "idx": 3, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "una herramienta mencionada por el profesor es weights\nand biases, la cual es una herramienta de seguimiento y\nvisualizacio'n de experimentos de machine learning donde\n\nnosotrosejecutamosunentrenamientoyvemosentiemporeal\ndesde cualquier computador co'mo se esta' comportando un\nmodelo. la ventaja es que podemos ver el comportamiento\npor lo que podemos detenerlo si no vemos buenos resultados.\nel procesamiento de ima'genes puede ser algo pesado por\nlo que debemos reducir el taman˜o de estas a un taman˜o de\n224x224. esto debido a que computacionalmente se vuelve\ncostoso.\nfig.1. backandforwardpropagation.\nnosepuedenutilizarlibrer'ıascomoresnetquesirvenpara\nabstraer la definicio'n de capas ma's alla' de torch.nn.\ntenemos solo una neurona. cada una de esas neuronas esta'n\n\nel profesor nos recomienda buscar una herramienta en\ncompuestas por una funcio'n no lineal que tiene como entrada"}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_004", "idx": 4, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "redes neuronales que nos pueda hacer toda la arquitectura\nuna funcio'n lineal. para optimizar los pesos en esa funcio'n\ndel modelo. incluso esta se puede hacer en pytorch, queda\ndebemos hacer derivadas parciales. al final esa derivada va\na nuestra disposicio'n.\n\na ser el activador de la capa anterior por lo que no necesito\ntenemos dos modelos:\nconocerco'mofuecomputadacadacapaanterior.solonecesito\n- lenet-5 cla'sico: este es la arquitectura ma's ba'sica\n\nelresultadoyyaconesopuedocalcularladerivadaqueyovoy\n(como el profesor lo menciona) para el procesamiento\na necesitar. si yo ocupo calcular la derivada parcial, respecto\nde ima'genes el cual fue creado por yann lecun.\na la funcio'n de pe'rdida con mi para'metro w, lo que tengo que\n- arquitectura alternativa: esta esta' basada en literatura\nhacer es aplicar la regla de la cadena para llegar al para'metro\nla cual implementa cualquier arquitectura distinta.\ndemifuncio'n.hayca'lculosquesiempresevanarepetirporlo\npodemosescogerdiferentesespectrogramascomoporejemque podremos guardar esos ca'lculos para evitar recalcularlos\nplo, el data augmentation el cual trata de aumentar los datos\nde nuevo para cada uno de los para'metros.\ndeentrenamientoparamejorarlageneralizacio'ndemimodelo\ncon la finalidad de obtener mejores patrones. ∂l ∂zl ∂al∂l\ni = i,\nen el paper specaugment, que sale en la bibliograf'ıa del ∂wl ∂wl ∂zl ∂al\nproyecto, propone 3 tipos de te'cnicas: ∂l ∂zl∂al∂l\ni = i.\n- time masking: donde tomo una frecuencia del 1 al 1.5 ∂bl ∂bl ∂zl ∂al\ndonde hago una ma'scara para cancelar el ruido\nesto nos da como resultado un vector gradiente, la cual\n- time warping: para estirar o encoger podemos ver en la fig. 2, que tiene el ca'lculo de todos los\n- frequencymasking:queaplicama'scarassimilarespero gradientes por todos los para'metros en la red.\nen el eje de la frecuencia, lo que simula la pe'rdida o"}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_005", "idx": 5, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "interferencia de ciertas bandas del espectro de audio\n\nel entrenamiento se debe realizar varias veces por lo que\n\nse debe dejar todo montado y conectado la herramienta\nde weights and biases. esto porque el entrenamiento con\nima'genespuedeserpesado,requieredegpu.elprofesormenciona que podemos usar google colab pero que es limitado,\npor lo que debemos aprovechar los recursos.\nla extensio'n de la documentacio'n debe ser de ma'ximo 10\npa'ginas. los apuntes anteriores son los ma's relevantes sobre\nla explicacio'n. fig.2. vectorgradiente.\nc. repaso de back propagation si tenemos mu'ltiples neuronas, tenemos que utilizar super'ındices o sub'ındices, podemos ver un ejemplo en la fig.\neste nos permite determinar cua'nto aporta cada peso al\n\n3. el primero me indica la capa en la que me encuentro, en\nerror total de la red, ajustando los para'metros en la direccio'n\neste caso el l. el segundo me indica cua'l neurona es para\ncontraria al proceso de propagacio'n hacia adelante, as'ı como\nidentificar cada una de ellas. los pesos esta'n asociados a las\nlo podemos observar en la fig. 1.\ncapasmevanaindicarhaciado'ndevoyydedo'ndeprovengo."}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_006", "idx": 6, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "vamosaverlasoperacionescomografosdondevanaserun\npodemos tener dos funciones, las cuales son:\ntipo de operaciones donde vamos a ponerle un sobrenombre.\n- preactivacio'n\nel sobrenombre nos puede ayudar con las derivadas parciales.\ncuando tratamos de optimizar un grafo, contamos con dos z(l) =b(l)+\nn (cid:88)l-1\nw(l) a(l-1)\netapas. la de salida la cual le llamamos activacio'n l donde j j j,k k\nk=1\nde error mayor. ba'sicamente las neuronas se desconectan de\n\nsusentradasoriginalesyrecibenotrasparalascualesnofueron\nentrenadas.\n\nhayundatasetquesellamacifar-10elcualson10clases\ncon taman˜o pequen˜o 32x32 pero son a color, con 3 canales.\npor tanto, si tuviera que hacer una red neuronal para conectar\ncada p'ıxel a una neurona, tendre' una entrada de 3072 pesos.\ncon esto entramos a un problema de dimensionalidad. esto\nse ve bien pero las ima'genes son pequen˜as, ¿que' pasa si\nse vuelven ma's grandes? para resolver este problema, entra\nfig.3. grafodimensional. en juego el convnet, donde vamos a tener 3 dimensiones,\ndonde vamos a tener filtros. cada filtro se encargara' de\nreconocer patrones en una imagen. esos filtros pueden ser\n- activacio'n (cid:16) (cid:17) reconocimientos de l'ıneas verticales, horizontales, diagonales,\na(l) =g z(l)\nj j entreotras,queseespecializanenextraerinformacio'ndeesas\nla funcio'n de activacio'n se aplica a toda la capa. toda la ima'genes.\ncapasecomputaconsigmoide.tenemosfuncionesdepe'rdida,\ndonde podemos tener una pe'rdida total dada por:\nl = (cid:88)\nnl\n(cid:0) a(l)-y (cid:1)2"}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_007", "idx": 7, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "i j j\nj=1\nel resultado de la derivada de pe'rdida con respecto a la\nactivacio'n es la siguiente: fig.4. redtotalmenteconectadavsconvnet\nmi salida se va a reducir, es decir, si ten'ıa 224x224, mi\n∂ ∂ l a i l = (cid:0) (al 1 -y 1 )2+(al 2 -y 2 )2+---+(al n -y n )2(cid:1)′ salidapuedeser212x212,ysieran3canales,puedequeahora\nj tengan 64 canales. esa cantidad de canales van a representar\n∂l i =2 (cid:0) al -y (cid:1) la cantidad de filtros que yo tuve que calcular. esos filtros\n∂al j j pueden representar colores, l'ıneas, nu'meros, entre otros.\nj\nla arquitectura esta' compuesta por 3 etapas:\nel resultado de la derivada de activacio'n con respecto a la\nde reactivacio'n es la siguiente: a. convolutional layer\nenestasetomaelfiltro,sedeslizaporlaimagenparahacer\n∂a(\nj\nl)\n=g (cid:16) z(l) (cid:17) (cid:0) 1-g (cid:16) z(l) (cid:17)(cid:1) el ca'lculo de los features. tenemos como entrada el largo,\n∂z(l) j j anchoycanalesquevoyaprocesar.estacomputalasalidade\nj\nneurona que se encuentran conectadas a las regiones locales.\nen la siguiente figura yo puedo hacer varios ca'lculos de\nporlotanto,sisequiereusarunacantidaddexfiltros,lasalida\nderivadas, donde a m'ı no me interesa co'mo llego' el valor\nde esta va a ser el ancho, largo y x. despue's de los filtros se\nz ya que al final es un valor que me llego' a la funcio'n, donde\naplica una funcio'n de activacio'n. esos filtros se calculan por\nse aplicaron varias derivadas para llegar a un valor. al final,\ncada canal.\ncada neurona que compute, no me interesa co'mo me llego' la\ninformacio'n desde la funcio'n de pe'rdida ya que a partir de b. pooling layer\ncierto punto. con el valor resultante, puedo sacar la derivada estaseencargadereducirlasdimensionesenlargoyancho,\ncon respecto a x y con respecto a y almacenadas, y tener las en otras palabras, reducir la imagen. esta aplica la operacio'n\nderivadas desde mi funcio'n de pe'rdida con mis entradas. en de downsampling a lo largo de dimensiones espaciales como"}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_008", "idx": 8, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "resumen, la propagacio'n hacia adelante es la capa de entrada elanchoylargo.sisuentradaesde32x32x12,susalidapuede\npor la de salida y la propagacio'n hacia atra's es desde la capa llegar a ser de 16x16x16. este no tiene para'metros y no me\nde salida hacia la de entrada donde se calcula la gradiente del afecta la profundidad.\nerror con respecto a los pesos de cada capa.\nc. fully-connected\n\niii. convnet\nen esta es ma's fa'cil reducir la informacio'n de una imagen\nhasta ahora hemos trabajado con una red fully connected a un vector ma's pequen˜o que el que yo ten'ıa en la entrada,\ncon entradas y salidas. el problema es que en el modelo entonces, a partir de ese momento hago mi clasificador. en\nanterior usado para mnist, nos puede dar varios errores otras palabras, se calcula la probabilidad de que pertenezca\ncomo si muevo las ima'genes de un centro, y las neuronas que a una clase y as'ı convertir una imagen de p'ıxeles a una\nse activaban estaban fijas, entonces podemos tener un margen probabilidad de pertenecer a una clase.\nd. alexnet\nesta es una arquitectura que salio' para la revolucio'n de\nconvoluciones y el deep learning en todos sus aspectos.\nfig.5. arquitecturaalexnet.\nen la fig. 5 podemos apreciar el proceso. los cubitos de\nadentro representan el taman˜o de los filtros.\nde esta manera podemos tratar un problema de clasificacio'n en ima'genes. lo primero que se hace es una convolucio'n donde extraemos ciertas caracter'ısticas. para reducir\nlas ima'genes se realiza un pooling, donde se pierden p'ıxeles.\nal final llegamos a un resumen de la imagen anterior. solo\nquedar'ıa hacer una fully connected."}
{"id_doc": "DOC_032", "segmentacion": "B", "chunk_id": "DOC_032_B_009", "idx": 9, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-07", "tema": "Profundización en redes neuronales convolucionales (ConvNet), reconocimiento de patrones visuales, reducción de dimensionalidad y arquitectura AlexNet aplicada al procesamiento de imágenes.", "texto": "fig.6. reduccio'ndeimagen."}
{"id_doc": "DOC_034", "segmentacion": "B", "chunk_id": "DOC_034_B_000", "idx": 0, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "apuntes de clase #2\nluis felipe calderón pérez\nescuela de ingeniería en computación\ntecnológico de costa rica\ncartago, costa rica\n2021048663\n9-10-2025\n\nresumen-este documento presenta los apuntes de la decima\nsemana del curso de inteligencia artificial. se realizó un repaso\nde las redes neuronales convolucionales (cnn), partiendo de las\n\nlimitacionesdelasredesfullyconnectedylanecesidaddeextraer\ninformación espacial de las imágenes. se estudió el funcionamiento de la estructura de las capas convolucionales, pooling\ny fully-connected y feature map. finalmente, se discutieron los figura1. arquitecturacnnvsnn[3]\nprincipios para diseñar arquitecturas convolucionales eficientes,\nconsiderando tamaños de filtro, stride, padding y reducción de mismas estarán conectadas a pequeñas regiones de la capa\nparámetros.\nanterior, y esto reduce el tamaño de la imagen a un vector.\nindex terms-cnn, capas convolucionales, transfer learning,"}
{"id_doc": "DOC_034", "segmentacion": "B", "chunk_id": "DOC_034_B_001", "idx": 1, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "en cada cara del cubo de las neuronas se tienen n filtros\narquitecturas convolucionales.\nde tamaño acorde a las imágenes de entrada y obtenemos un\n\ni. breverepasodelaclaseanterior\nfeaturemap,queeselresultadodeaplicarelfiltroalaimagen\ni-a. redes convolucionales anterior.\nrecibimosuninputdecaracterísticas,elcualtransformamos\ni-b. arquitectura de cnn\ncon una serie de capas ocultas.\n\nrequiere de 3 capas principales\nse había visto hasta el momento las redes fully connected.\n\n1. convolutional layer\nteníamos el problema de que aprendemos una secuencia de\ncomputa el filtro contra una imagen.\npíxeles, lo que nos lleva a errores si movemos los objetos de\nrecibe de entrada el ancho, largo y canales.\nlugar o si aplicamos rotación a los objetos. lo correcto sería\ntiene n filtros que extraen características de las imásacar la información de la imagen para tomar una decisión.\ngenes.\ntambién se habló del dataset de cifar-10, donde hay\naplica una capa de activación\nimágenes con 3 canales de 32x32. y nos damos cuenta de\ntiene como parámetros (wx)n+b\nque no es escalable, ya que se tendrían 120,000 parámetros\nque ajustar únicamente en la entrada. 2. pooling layer\npor ello se buscan métodos más eficientes; entonces lle- reduce de la imagen en ancho y largo.\ngamos a las convnet, en donde las neuronas se organizan se encarga de aplicar el downsampling a lo largo del\nen 3 dimensiones: largo, ancho y profundidad (canales). las ancho y largo.\nno tiene parámetros.\nse introduce periódicamente en medio de capas convolucionales.\nlo más usual es usar max pooling.\nfórmula dimensionalidad\n- entrada wxhxd.\n- k, tamaño de kernel.\n- s, stride.\nfigura3. funcionamientokernelcapadeconvolución[3]\n- d, mantiene la profundidad.\nw 2 = w- s k +1 ii-a. filtro o kernel"}
{"id_doc": "DOC_034", "segmentacion": "B", "chunk_id": "DOC_034_B_002", "idx": 2, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "3. fully-connected\nes una matriz bidimensional de números, que transforma\nesta parte clasifica, ya que calcula la probabilidad de\nuna imagen en el momento en que deslice ese filtro, produpertenecer a una clase; transformando una imagen de\nciendo una imagen como salida. para aplicar los filtros se va\npíxeles a probabilidad de pertenecer a una clase.\n\na tener algo similar a un caso donde una imagen de entrada\n\nlas cnns nos permiten resolver\ntiene mucho ruido y se le aplica un gaussian kernel (tiene\nclasificación de imágenes. una campana de gauss) y da una imagen resultante como si\nsegmentación de objetos. tuvieraunblur.dependiendodelfiltrousado,obtenemosotras\nsegmentación de instancias. formas como resultado e influimos en el procesamiento.\nprocesamiento de imágenes.\nii-b. local receptive fields"}
{"id_doc": "DOC_034", "segmentacion": "B", "chunk_id": "DOC_034_B_003", "idx": 3, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "ii. capadeconvolución\nuna neurona esta conectada a un campo en específico del\ntiene varias características, están compuestas por varios input. esto es muy eficiente, ya que podría haber filtros que\nfiltros (w), va a tener un comportamiento local, lo deslizamos extraigan líneas verticales, otros horizontales y así.\n\ny va a seguir extrayendo las mismas features alrededor de la\nii-b1. campo receptivo: es un filtro de nxn, donde cada\nimagen. esto permite que los pesos se ajusten para que sirvan\nneurona estará enfocada en un solo campo receptivo.\nen una posición como en otra.\nii-b2. stride: es la forma clásica de deslizar el filtro,\nsegún una cantidad de pasos que realiza el filtro sobre la\nimagen durante la convolución.\nii-b3. padding: técnica para agregar pixeles alrededor\ndel borde de la imagen, permitiendo controlar el tamaño de\nsalida de la convolución. se recomienda que el padding se\nllene con 0, ya que si se usa 1 podría generar ruido o mala\nfigura2. arquitecturaalexnet[2] data. y su fórmula es k-1, k = tamaño del filtro.\n2\nii-b4. cálculo de dimensiones: papers la cantidad de convoluciones y relu se usa la fórmula\nm, cantidad de pixeles en fila/columna. de 3≥n≥0.\nk, tamaño del kernel.\niii-a. ¿que arquitectura preferimos?\np, tamaño del padding.\nse prefiere a las arquitecturas con convoluciones pequeñas,\ns, cantidad de pasos."}
{"id_doc": "DOC_034", "segmentacion": "B", "chunk_id": "DOC_034_B_004", "idx": 4, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "yaquelasconvolucionesgrandesnosllevanaquelasneuronas\nm-k+2p +1= dimension resultante\n\ns se computen de forma lineal y que la cantidad de pesos sea\nii-c. pesos mayor.\nsi se tuviera una imagen de 224x224x3, con un tamaño\niii-b. algunas \"reglas\"\nde kernel 11, stride de 4 y padding de 0, y le aplicamos la\nel tamaño de la imagen debería ser divisible por 2.\nfórmula anterior da 55. y con la arquitectura de alexnet 2\nlas convoluciones deben usar campos receptivos pequeobtenemos que aprendimos 96 de profundidad.\nños 3x3, con un stride de 1.\nii-d. pesos compartidos para pooling layer es común max pooling de f=2, s=3.\nsi ya tenemos un filtro que extrae cierta caracteristica, y s, cantidad de pasos.\nsirve para una posición; también sirve para otra posición. por\niii-c. menciones finales\nlo que, vamos a usar el mismo filtro para toda la imagen."}
{"id_doc": "DOC_034", "segmentacion": "B", "chunk_id": "DOC_034_B_005", "idx": 5, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "al final se mencionan arquitecturas similares a lenet para\nii-e. transfer learning tomarencuentaparaelproyecto,talescomo,alexnet,afnet,\nsemencionaqueenelpaperdealexnetdespuésdeaplicar googlenet (reduce parámetros), vgg16 y resnet.\nsu arquitectura y lo referente a ella. y se dan cuenta que en nota: embedding, información distribuida en espacio veclas primeras capas hay figuras o información similar. por lo torial que retorna mi nn.\nque se introduce el término de transfer learning, que consiste\n\nreferencias\nen pasar el peso de las primeras capas a otra red, para ahorrar\n[1] \"resnet, alexnet, vggnet, inception: understanding various architiempo de entrenamiento.\ntectures of convolutional networks,\" cv-tricks.com, aug. 01, 2022.\nhttps://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/\n\niii. arquitecturasconvolucionales\n[2] r.r.abril,\"redesconvolucionales,\"lamáquinaoráculo,jul.2025,\nse componen de convolutional layer, pooling layer y [online]. available: https://lamaquinaoraculo.com/deep-learning/redesneuronales-convolucionales/\nde dense layer. se deben tomar desiciones sobre nuestra\n[3] s. a. p. portuguez, \"apuntes de la clase de inteligencia artificial,\"\narquitectura, por ejemplo, si la convolución reduce el input\ncartago,costarica,agosto2025,clasedel9deoctubredel2025.\ndebo decidir si hago o no el pooling. y estas desiciones\ndeterminan el comportamiento del tamaño de la imagen, pero\nsi la imagen es muy reducida, le llega poca información a\nla fully connected. se introdujó el término de stack, que es,\ninput → [[conv → relu]∗n → pool?]∗m → [fc →"}
{"id_doc": "DOC_034", "segmentacion": "B", "chunk_id": "DOC_034_B_006", "idx": 6, "autor": "Brandon Rodríguez Campos", "fecha": "2025-10-09", "tema": "Implementación de CNN y transfer learning en PyTorch; análisis de overfitting, regularización, dropout y normalización en redes profundas.", "texto": "relu] ∗ k → fc,m ≥ 0,k ≥ 0, y se menciona que en"}
{"id_doc": "DOC_038", "segmentacion": "B", "chunk_id": "DOC_038_B_000", "idx": 0, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "resumen sobre autoencoders, segmentación y\nrags: conceptos y arquitecturas\nandrés sánchez rojas\nescuela de ingeniería en computación\ninstituto tecnológico de costa rica\n16/10/2025\n\nresumen-estossonlosapuntesdelasegundaclasedesemana patrones relevantes y reconstruir las entradas con alta fidelidad.\n11 del curso de ia. los autoencoders son modelos de aprendizaje estaversatilidadlosconvierteenherramientasvaliosastantoen\nno supervisado que aprenden a reconstruir sus entradas a través\naplicaciones de seguridad como en procesamiento de imágenes\ndeunarepresentacióncomprimida(espaciolatente).enestedocuy señales.\nmento se resumen conceptos clave: arquitectura encoder/decoder,\nvariational autoencoders (vae) y la reparametrización, funciones\nde pérdida típicas, y aplicaciones como detección de anomalías\nii-b. encoder y decoder\ny denoising. también se introduce la segmentación de imagen\ny la arquitectura u-net, y se discuten conceptos relacionados\n\nel encoder es un conjunto de bloques convolucionales que\ncon rags (retrieval augmented generation), agentes basados en\nextraen la información más relevante de la entrada y descartan\nllm,tokenizaciónyembeddings.elobjetivoesofrecerunavisión\ncompacta y legible para un lector que busca una introducción loquenoaporta,comprimiendolosdatosatravésdeun\"cuello\ntécnica y aplicada. de botella\" para eliminar ruido y características innecesarias;\nindex terms-autoencoders, variational autoencoders, u-net, la salida de ese proceso es el vector o espacio latente, una\nsegmentación de imagen, rag, tokenización, embeddings\nrepresentacióndebajadimensionalidadqueconservalosrasgos\nútilesparadiferenciarpatrones.eldecodereslapartequetoma"}
{"id_doc": "DOC_038", "segmentacion": "B", "chunk_id": "DOC_038_B_001", "idx": 1, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "i. introducción\nel espacio latente y reconstruye la señal o imagen original,\n\neste documento sintetiza los principios y aplicaciones\nexpandiendo la información comprimida para producir una\nprácticas de los autoencoders, describiendo su entrenamiento\nsalida lo más fiel posible a la entrada; su objetivo es invertir\nsin etiquetas por reconstrucción, sus usos en reducción de dila codificación del encoder y permitir tareas como denoising,\nmensionalidad, compresión, detección de anomalías, denoising\nupscaling o detección de anomalías mediante la comparación\ny superresolución, y la extensión hacia variantes relevantes\nentre entrada y reconstrucción.\ncomolosvariationalautoencoders;ademáspresentalaconexión\ncon tareas de visión por computador (p. ej., segmentación y\nii-c. aplicaciones\narquitecturas tipo u-net) y la extensión a representaciones\npara texto mediante tokenización y embeddings, así como su\nentre las aplicaciones prácticas destacan:\npapel en sistemas más amplios como rags y agentes basados\nen llm. el texto ofrece una guía práctica con definiciones, detección de anomalías y fraude .\nfórmulas y recomendaciones operativas para implementar eliminación de ruido (denoising).\nexperimentos en imágenes y texto. aumento de resolución (upscaling).\nreconocimiento facial y compresión de imágenes."}
{"id_doc": "DOC_038", "segmentacion": "B", "chunk_id": "DOC_038_B_002", "idx": 2, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "ii. autoencoders\nii-a. definición y propósito\n\niii. variationalautoencoder\nlosautoencoderssonunaarquitecturanovedosaenelámbito\ndel aprendizaje automático que se caracteriza por comparar son una variante probabilística de los autoencoders que\nsus salidas con las mismas entradas, lo que permite entrenarlos generan una representación latente continua modelada como\nsin necesidad de etiquetas, clasificándolos como modelos no una distribución. en lugar de devolver un único vector latente\nsupervisados. su principal utilidad radica en la reducción de determinista, el encoder estima parámetros de una distribución:\ndimensionalidad,ofreciendorepresentacionesmáspotentesque la media µ(x) y la log-varianza logσ2(x).\ntécnicas clásicas como el análisis de componentes principales\n(pca).estacapacidaddecompresiónyreconstrucciónloshace\niii-a. reparametrización\nespecialmente útiles en tareas como la detección de anomalías,\nla identificación de transacciones fraudulentas, la eliminación la reparametrización permite que la aleatoriedad se aísle\nde ruido en datos, el aumento de resolución (upscaling) y el re- en una variable independiente, de forma que los gradientes\nconocimientofacial.enesencia,losautoencodersaprendenuna puedan fluir hacia los parámetros que predicen la media y la\ncodificación eficiente de los datos, lo que les permite capturar varianza.\niii-b. funciones de pérdida vi. tokenizaciónyembeddings\nla pérdida de un variational autoencoder combina dos vi-a. tokenización\ntérminos: la tokenización convierte texto en secuencias de identifica1. reconstruction loss: mide la discrepancia entre la dores. estrategias comunes de tokenización son: por palabra,\nentrada y la reconstrucción producida por el decoder. por subword, por caracter, por bytes. cada estrategia tiene"}
{"id_doc": "DOC_038", "segmentacion": "B", "chunk_id": "DOC_038_B_003", "idx": 3, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "2. kl divergence: compara qué tanto se parecen dos trade-offs en cobertura, eficiencia y manejo de formas raras.\ndistribuciones.\nvi-b. embeddings\nla pérdida total habitual es la suma: reconstruction loss + kl\n\nlos embeddings son vectores densos que representan tokens\ndivergence.\n\nosecuenciasenunespaciodondelaproximidadindicasimilitud\nsemántica. al agregar embeddings de tokens (por ejemplo\niii-c. espacio latente y generación\n\nmediante promedio o modelos que producen representaciones\nun beneficio importante de los variational autoencoders es de secuencia) se obtienen vectores de frases/consultas útiles\nque el espacio latente resultante es continuo: puntos cercanos para búsquedas semánticas y recuperación en rags, y como\nen el espacio latente generan observaciones similares, lo entrada para razonamiento en agentes.\nque permite interpolación y generación de nuevas muestras\n\nvii. conclusiones\nmediante muestreo.\nlos puntos vistos en esta clase y resumidos en este docuiv. segmentacióndeimagen mento ofrecen una síntesis compacta de conceptos relevantes\nen autoencoders, variational autoencoders, segmentación de\nlasegmentaciónconsisteenlocalizaryetiquetarpíxelesque imágenes con arquitecturas como u-net, y rags y agentes\npertenecenaobjetosdeinterésdentrodeunaimagen.devuelve basados en llms. estos avances han permitido la expansión\nun mapa donde cada píxel tiene una etiqueta, siendo útil en del uso de la inteligencia artificial en ambientes en los que\naplicaciones que requieren alta resolución espacial como el antesnosehubieraconsideradoútil.sinembargo,debemosser\nanálisis médico o el conteo de células. responsablesaldecidirquétareasrealmenterequierenunagente\no pueden usar un sistema más ligero de machine learning.\niv-a. arquitectura u-net\nu-net es una arquitectura en forma de ü\"similar a un"}
{"id_doc": "DOC_038", "segmentacion": "B", "chunk_id": "DOC_038_B_004", "idx": 4, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "autoencoder pero con skip connections entre las capas de\nencoder y decoder. estas conexiones permiten conservar información durante el upsampling, mejorando significativamente la\nprecisión de los mapas de segmentación. u-net ha demostrado\nser especialmente útil en tareas médicas como la identificación\nde células cancerígenas.\n\nv. ragsyagentes\nv-a. rag: retrieval augmented generation\nlos rags combinan recuperaciónde documentos relevantes\ncon generación de lenguaje. el flujo general es:\n\n1. convertir la consulta y fragmentos de texto en embeddings.\n\n2. buscar textos relevantes en una base de conocimiento\nmediante medidas de similitud en el espacio de embeddings.\n\n3. pasar los fragmentos recuperados como contexto a\nun modelo de lenguaje para generar respuestas más\nfundamentadas.\nv-b. agentes basados en llm\nlos agentes usan un llm como núcleo de decisión para\norquestarpasos(consultarfuentes,ejecutarapis,leermemoria).\nun agente integra recuperación, gestión del contexto y conectores a herramientas externas para resolver tareas complejas"}
{"id_doc": "DOC_038", "segmentacion": "B", "chunk_id": "DOC_038_B_005", "idx": 5, "autor": "Andrés Sánchez Rojas", "fecha": "2025-10-16", "tema": "Resumen sobre autoencoders, variational autoencoders, segmentación de imágenes con U-Net y conceptos de RAG y agentes basados en LLM.", "texto": "de forma autónoma."}
{"id_doc": "DOC_035", "segmentacion": "B", "chunk_id": "DOC_035_B_000", "idx": 0, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "apuntes ia clase 14/10/2025\njuan jime'nez valverde\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\njuand0908@estudiantec.cr\n\nabstract-estedocumentoresumelosconceptosclavevistosen c. stride, padding y ca'lculo de dimensiones\nlaclasedeinteligenciaartificialsobreredesneuronalesconvolucionales(cnn)yautoencoders.seabordantemasfundamentales - stride: indica cua'ntos pasos da el filtro al desplazarse\ncomolosfiltros,camposreceptivos,stride,paddingylascapasde sobre la imagen. un stride mayor reduce el taman˜o de la\npooling, as'ı como las arquitecturas cla'sicas de cnn, entre ellas salida.\nlenet,alexnet,zfnet,googlenet,vgg16,resnetydensenet. - padding:agregap'ıxeles(usualmenteceros)alrededorde\nadema's, se presentan consideraciones pra'cticas para el ana'lisis\nla imagen de entrada para controlar el taman˜o de salida\nde modelos de aprendizaje profundo, como la visualizacio'n de\nactivaciones, los embeddings de caracter'ısticas y la estructura y preservar dimensiones. el padding sime'trico t'ıpico se\nde los autoencoders, incluyendo sus aplicaciones en reduccio'n calcula como:\nde dimensionalidad, deteccio'n de anomal'ıas y procesamiento (k-1)\nde ima'genes. el objetivo es ofrecer una visio'n general clara y p=\n2\nconcisa,u'tiltantoparaelestudioteo'ricocomoparalaaplicacio'n\npra'ctica. donde k es el taman˜o del kernel.\nindex terms-redes neuronales convolucionales, autoencoders, visualizacio'n de activaciones, embeddings, pooling, ar- el taman˜o de salida se calcula con:"}
{"id_doc": "DOC_035", "segmentacion": "B", "chunk_id": "DOC_035_B_001", "idx": 1, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "quitecturas de aprendizaje profundo\n(m-k+2p)\ndimensio'n de salida= +1\n\ni. introduccio'n s\nlas redes neuronales convolucionales (cnn) se han condonde: m es el taman˜o de la entrada, k el taman˜o del kernel,\nvertido en un pilar fundamental de la visio'n por computadora\np el padding aplicado y s el stride.\nmoderna, ya que permiten extraer automa'ticamente caracter'ısticas jera'rquicas a partir de ima'genes. comprender sus\nd. pesos y arquitectura de alexnet\nmecanismosinternos,incluyendolosfiltros,loscamposreceptivos,elstride,elpaddingylasoperacionesdepooling,resulta\nen redes convolucionales se utilizan pesos compartidos,\nesencial para disen˜ar arquitecturas eficientes e interpretar el\nlo que reduce dra'sticamente el nu'mero de para'metros, ya que\ncomportamiento de los modelos.\n\nel mismo conjunto de filtros se aplica en todas las posiciones\npor su parte, los autoencoders complementan el uso de las\nespaciales.enlaarquitecturadealexnet,porejemplo,seemcnn al aprender representaciones compactas de los datos sin\nplean96filtrosenlaprimeracapaconvolucional,permitiendo\nrequeriretiquetas,loqueloshaceidealesparatareasdeaprenextraer mu'ltiples caracter'ısticas visuales de forma eficiente.\ndizaje no supervisado como la reduccio'n de dimensionalidad,\nla deteccio'n de anomal'ıas y la reconstruccio'n de ima'genes.\ne. pooling layer\neste documento sintetiza los principales conceptos y arquitecturas revisados en clase, sirviendo como material de despue's de las convoluciones, se aplica la capa de pooling,\nreferencia para la comprensio'n y aplicacio'n pra'ctica de estos que resume la informacio'n espacial (alto y ancho) sin alterar\nmodelos en inteligencia artificial. la cantidad de canales. existen dos tipos principales:"}
{"id_doc": "DOC_035", "segmentacion": "B", "chunk_id": "DOC_035_B_002", "idx": 2, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "ii. repasodelaclaseanterior - max pooling: conserva el valor ma'ximo de cada regio'n.\na. filtros o kernels - average pooling: calcula el promedio de los valores en\nla regio'n.\nsonmatrices(porejemplo,de3×3o5×5)quesedeslizan\nsobre la imagen para aplicar convoluciones. el gaussian dada una entrada de taman˜o w ×h ×d, el pooling reduce\nkernel se utiliza para suavizar la imagen y eliminar ruido. w y h, manteniendo d.\nb. campo receptivo\nf. fully-connected layer\nes la regio'n local de la imagen a la que una neurona esta'\nconectada. por ejemplo, para una entrada de 32×32×3 y un finalmente, las caracter'ısticas extra'ıdas se transforman en\ncampo receptivo de 5×5, cada neurona tendra' 5×5×3=75 un u'nico vector, conectando todas las neuronas entre s'ı. esta\npesos. capa permite realizar la clasificacio'n final del modelo.\ng. arquitecturas convolucionales\nuna red convolucional combina secuencias de convolucio'n\n→activacio'n(relu)→pooling.estepatro'nserepitevarias\nveces para extraer informacio'n progresivamente ma's abstracta\nde la imagen. generalmente, se prefieren filtros pequen˜os\n(como 3 × 3) para capturar detalles locales de forma ma's\neficiente.\nel convolutional stack se forma al aplicar mu'ltiples capas\nde convolucio'n consecutivas. por ejemplo, en una imagen de\n5×5,unfiltro3×3puededesplazarseparagenerarunasalida\nde 3×3.\nregla pra'ctica: las dimensiones de las ima'genes deben\nser divisibles entre 2, lo cual facilita la reduccio'n progresiva\nmediante pooling.\nh. principales arquitecturas\n1) lenet: disen˜ada por yann lecun et al. (1998), fue una\nfig.1. representacio'ndeembeddingsmediantet-sne.\nde las primeras redes convolucionales exitosas. cuenta con 5\ncapas: dos convolucionales, dos de pooling y una totalmente\nconectada [1]. su estructura sirvio' de base para las redes\nprofundas. los features aprendidos por las capas internas\nmodernas.\nsuelen ser dif'ıciles de entender por los humanos, lo que\n2) alexnet: propuesta por krizhevsky, sutskever y hinton\ncomplica saber que' esta' \"viendo\" realmente el modelo.\n(2012), marco' un hito en la visio'n por computadora. procesa\n2) visualizacio'n y ana'lisis de activaciones: una forma de\nima'genes de 224×224 con filtros grandes (11×11, 5×5,\nentender mejor el funcionamiento interno es observar:\n3×3) y cinco capas convolucionales. popularizo' el uso de\nrelu, dropout y la utilizacio'n de mu'ltiples gpus para el - visualizacio'n de activaciones: muestra que' regiones de\nla imagen activan ciertas neuronas.\nentrenamiento [2].\n3) zfnet: basada en alexnet, reduce la profundidad y - visualizacio'n de filtros: permite observar los pesos\nde los kernels. en las primeras capas, estos muestran\ntaman˜o de los filtros para analizar co'mo afectan las capas\npatronesreconocibles(bordes,colores,texturas),mientras\na la representacio'n interna. sirvio' como experimento para\nque en capas profundas se vuelven ma's abstractos.\nvisualizar activaciones intermedias y optimizar arquitecturas.\n4) googlenet (inception): reduciendo los ma's de 60 mil- estosme'todosayudanadetectarsielmodeloesta' aprendiendo\nlonesdepara'metrosdealexnetaunos4millones,googlenet caracter'ısticas relevantes o solo ruido.\nintrodujo los mo'dulos inception [3]. cada mo'dulo combina 3) embeddings y reduccio'n de dimensionalidad: las reconvoluciones de diferentes taman˜os (1 × 1, 3 × 3, 5 × 5) des pueden transformar ima'genes en representaciones vectorijunto con max pooling, concatenando sus resultados. al final, ales llamadas embeddings. estas representaciones condensan\nla salida (7×7×1024) se aplana y se pasa a un average la informacio'n relevante de una imagen, permitiendo separar\npooling de 1×1×1024. clases en el espacio de caracter'ısticas. al reducir la dimen5) vgg16: caracterizada por su simplicidad, utiliza sionalidad (manteniendo las distancias relativas), podemos\nu'nicamente filtrosde 3×3 yaumenta la profundidad hasta16 visualizar las relaciones entre clases.\ncapas.estaarquitecturademostro' queaumentarlaprofundidad una te'cnica comu'n para ello es t-sne, que proyecta estos\nmejora el rendimiento si se mantienen filtros pequen˜os y vectores a dos dimensiones preservando la estructura del\nconsistentes. espacio original (fig. 1).\n6) resnet: introduce las conexiones residuales, que per- 4) mapasdeactivacio'n: adema'sdelasvisualizacionesde\nmiten el paso de informacio'n entre capas no adyacentes. esto filtros, es posible generar mapas de activacio'n o heatmaps\nevita la degradacio'n del gradiente en redes muy profundas y que muestran que' regiones espec'ıficas de la imagen influyen\nmejora la capacidad de entrenamiento. ma's en la decisio'n del modelo. estas te'cnicas son u'tiles,\n7) densenet: conecta cada capa con todas las anteriores, por ejemplo, en aplicaciones me'dicas para resaltar fracturas\nfavoreciendo la reutilizacio'n de caracter'ısticas y reduciendo o anomal'ıas en radiograf'ıas.\nla cantidad de para'metros necesarios. este enfoque mejora la\nb. autoencoders\neficiencia y el flujo de informacio'n a lo largo de la red.\naunqueutilizanarquitecturassimilaresalasredesconvoluiii. materiadeclase\ncionales, los autoencoders trabajan sin etiquetas expl'ıcitas,\na. problemas en las redes neuronales convolucionales\nporloqueseconsideranme'todosnosupervisados.suobjetivo\n1) explicabilidad del modelo: uno de los principales de- es reconstruir la entrada original, aprendiendo una represaf'ıos actuales es la falta de interpretabilidad en las redes sentacio'n interna comprimida."}
{"id_doc": "DOC_035", "segmentacion": "B", "chunk_id": "DOC_035_B_003", "idx": 3, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "iv. partesdelautoencoder\nlos autoencoders se componen de tres partes principales:\nel encoder, el cuello de botella y el decoder. cada una\ncumple una funcio'n espec'ıfica en el proceso de codificacio'n\ny reconstruccio'n de los datos.\na. encoder\nel encoder esta' formado por un conjunto de bloques convolucionales seguidos de mo'dulos de pooling. su funcio'n\nprincipal es extraer las caracter'ısticas ma's relevantes de la\nimagendeentradaycomprimirlainformacio'n.laexpectativa\ndel encoder es aprender informacio'n importante de la entrada\nmediante un proceso de downsampling, reduciendo la dimensionalidad y conservando los rasgos esenciales.\nb. cuello de botella\nel cuello de botella constituye la parte ma's importante y\nfig.2. estructuraba'sicadeunautoencoder. pequen˜a del modelo. representa la informacio'n comprimida\nen un espacio latente, donde se encuentran codificadas las\ncaracter'ısticas ma's significativas. esta capa restringe el flujo\nel proceso consta de tres partes, como se muestra en la de informacio'n proveniente del encoder al decoder, limitando\nfig. 2: la cantidad de datos que pueden ser reconstruidos.\n1) encoder: reduce la imagen a un vector compacto.\nc. decoder\n2) espacio latente: contiene la representacio'n esencial o\nel decoder esta' compuesto por una serie de convoluciones\ncodificada de la entrada."}
{"id_doc": "DOC_035", "segmentacion": "B", "chunk_id": "DOC_035_B_004", "idx": 4, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "que realizan upsampling para reconstruir la imagen original a\n3) decoder: reconstruye la imagen original a partir del\npartir del vector latente. en pytorch, esta tarea suele implevector latente.\nmentarse mediante capas convtranspose2d. el objetivo\n1) tareas comunes de un autoencoder:\ndel decoder es generar una salida lo ma's fiel posible a la\n- reduccio'n de dimensionalidad: genera una repre- entrada original.\nsentacio'n ma's compacta y poderosa que pca, conserd. hiperpara'metros a considerar\nvando la informacio'n esencial.\nel desempen˜o del autoencoder depende en gran medida de\n- deteccio'n de anomal'ıas: se entrena para reconstruir\nlos hiperpara'metros seleccionados, entre los que destacan:\ndatos de una tarea tomando en cuenta u'nicamente ejemplos positivos o normales. por ejemplo: - taman˜o de la codificacio'n (vector latente): determina\nel nivel de compresio'n de los datos. un taman˜o menor\n- transferencias bancarias correctas.\nimplica mayor compresio'n, pero puede perderse infor-\n- audio o ima'genes de alta fidelidad sin defectos.\nmacio'n relevante.\nel modelo aprende la representacio'n latente de estos ca-\n- nu'mero de capas: define la profundidad del encoder\nsosy,alpresentarleejemplosano'malos,sureconstruccio'n\ny del decoder. un nu'mero mayor de capas genera un\nfalla, evidenciando la anomal'ıa.\nmodelo ma's complejo y con mayor capacidad de repre-\n- procesamiento de ima'genes (fig. 3): permite tareas sentacio'n, mientras que un nu'mero menor lo hace ma's\ncomo compresio'n, eliminacio'n de ruido o incluso super\nra'pido pero menos preciso.\nresolucio'n, es decir, generar ima'genes de alta resolucio'n\na partir de versiones borrosas o pequen˜as. v. conclusiones"}
{"id_doc": "DOC_035", "segmentacion": "B", "chunk_id": "DOC_035_B_005", "idx": 5, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "durante la clase se destacaron los componentes esenciales\nestos principios sientan las bases de los algoritmos generay las arquitecturas principales de las redes neuronales contivosmodernos,dondeelmodeloaprendeareconstruirocrear\nvolucionales, explicando co'mo las capas, filtros y operaciones\ncontenido visual de forma auto'noma.\nde pooling trabajan en conjunto para extraer informacio'n\nrelevante de las ima'genes. la visualizacio'n de activaciones\n\ny embeddings permite comprender mejor el funcionamiento\ninterno de los modelos profundos, mejorando su interpretabilidad y facilitando el diagno'stico de su desempen˜o.\nasimismo, los autoencoders se presentaron como herramientas potentes dentro del aprendizaje no supervisado, capacesdecomprimirinformacio'n,detectaranomal'ıasymejorar\nfig.3. procesamientodeima'genes la calidad de las ima'genes mediante su reconstruccio'n.\ndominar estos conceptos proporciona una base teo'rica y\npra'cticaso'lidaparaeldisen˜o,ana'lisisyaplicacio'nefectivade\nmodelos de aprendizaje profundo en distintos contextos.\n\nreferences\n[1] y.lecun,l.bottou,y.bengio,andp.haffner,\"gradient-basedlearning\nappliedtodocumentrecognition,\"proceedingsoftheieee,vol.86,no.\n11,pp.2278-2324,1998.\n[2] a. krizhevsky, i. sutskever, and g. hinton, \"imagenet classification\nwithdeepconvolutionalneuralnetworks,\"advancesinneuralinformationprocessingsystems(nips),pp.1097-1105,2012.\n[3] c.szegedyetal.,\"goingdeeperwithconvolutions,\"proceedingsofthe\nieeeconferenceoncomputervisionandpatternrecognition(cvpr),"}
{"id_doc": "DOC_035", "segmentacion": "B", "chunk_id": "DOC_035_B_006", "idx": 6, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-10-14", "tema": "Fundamentos y arquitectura de redes neuronales convolucionales (LeNet, AlexNet, GoogleNet, VGG, ResNet, DenseNet) y autoencoders aplicados a reducción de dimensionalidad y reconstrucción de imágenes.", "texto": "pp.1-9,2015."}
{"id_doc": "DOC_036", "segmentacion": "B", "chunk_id": "DOC_036_B_000", "idx": 0, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "inteligencia artificial\napuntes semana 11, clase #1\n\nluis fernando benavides villegas\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nlubenavides@estudiantec.cr\n\nabstract-este documento recopila los apuntes de la clase del de aplicacio'n del filtro, por lo que reduce el taman˜o del mapa\nmartes 14 de octubre de 2025 para el curso de inteligencia ar- de salida.\ntificial. se repasan conceptos como los fundamentos de las redes\n2) padding: agrega p'ıxeles de relleno alrededor de la\nneuronales convolucionales (cnn), abarcando el uso de filtros,\nimagendeentradaparacontrolareltaman˜odelmapadesalida\nstride,paddingypoolingparalaextraccio'ndecaracter'ısticasen\nima'genes.seanalizanarquitecturasrepresentativascomolenet, y preservar las dimensiones espaciales. un padding sime'trico\nalexnet, googlenet, vgg, resnet y densenet, destacando su evita que la convolucio'n reduzca el taman˜o de la imagen:\nevolucio'n y aportes al aprendizaje profundo. adema's, se introk-1\nducenlosconceptosdeembeddingsyvisualizacio'ndeactivaciones\np= ,\npara interpretar el comportamiento de los modelos, junto con el 2\nestudio de los autoencoders y su aplicacio'n en reconstruccio'n de\ndonde k es el taman˜o del filtro.\nima'genes,reduccio'ndedimensionalidad,deteccio'ndeanomal'ıas\ny generacio'n de datos. 3) dimensio'ndesalida: dadaunaimagendeentradayun\nindex terms-inteligencia artificial, redes neuronales con- kernel, el taman˜o de salida se calcula como:\nvolucionales, pooling, embeddings, visualizacio'n, autoencoder,\n(m+2p -k)\ndeep learning. +1,\ns"}
{"id_doc": "DOC_036", "segmentacion": "B", "chunk_id": "DOC_036_B_001", "idx": 1, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "i. repasodelaclaseanterior donde m es el taman˜o de la entrada, k el taman˜o del filtro,\np el padding y s el stride.\na. convoluciones y filtros\nc. comparticio'n de pesos\nuna convolucio'n consiste en aplicar un filtro (kernel) sobre\nuna imagen para extraer informacio'n relevante. el filtro es en una red convolucional, los mismos pesos que se caluna matriz de nu'meros que se entrena junto con la red. al cularon para una regio'n espec'ıfica se reutilizan en todas las\ndesplazarseporlaimagen,calculaunvalorporcadaposicio'n, dema's posiciones donde el filtro se deslice. por ejemplo,\ngenerandounanuevaimagenllamadamapadecaracter'ısticas si un filtro aprende a detectar l'ıneas verticales, esa misma\n(feature map o activation map). configuracio'n de pesos servira' para reconocerlas sin importar\n1) filtrogaussiano: produceunaimagenconunefectode en que' parte de la imagen aparezcan. de esta manera, se\ndesenfoque (blur), eliminando el ruido y dejando solo la parte reducesignificativamentelacantidaddepara'metrosaentrenar\ndel contorno. y mejora la eficiencia del modelo.\n2) redes neuronales: en redes convencionales, todos los en arquitecturas como alexnet, permite que las primeras\np'ıxeles estan conectados a todas las neuronas de la siguiente capas aprendan caracter'ısticas generales como bordes y colcapa. en las convoluciones, solo una porcio'n de los p'ıxeles ores, mientras que las capas ma's profundas combinan esa\nesta' conectada, observando solo una parte espec'ıfica de la informacio'n para reconocer formas y objetos ma's complejos.\nimagen.\nd. capa de pooling\n3) campo receptivo: es la regio'n de la imagen que una\nneurona observa para generar su salida. depende tanto del despue'sdeaplicarlasconvoluciones,seutilizaunacapade\ntaman˜o de la entrada como del filtro aplicado. por ejemplo, si pooling para reducir el taman˜o espacial de la imagen y manlaimagendeentradaesde32×32×3,lareddebeprocesarlos tener solo la informacio'n ma's relevante. esta operacio'n toma\ntres canales de color. si el filtro tiene taman˜o 5×5, entonces bloqueslocalesyrealizaunaoperacio'nestad'ısticasobreellos,\nel campo receptivo resultante sera' un cubo de 5×5×3, es como el ma'ximo o el promedio, para resumir su contenido.\ndecir, todas las neuronas que intervienen en esa regio'n. estos - max pooling: selecciona el valor ma'ximo de cada\ncampos extraen caracter'ısticas necesarias para el clasificador. bloque. es el me'todo ma's utilizado.\n- average pooling: calcula el promedio de los valores de\nb. para'metros de la convolucio'n\ncada bloque.\n1) stride: define cua'nto se deslizan los filtros sobre la - l2 pooling: aplica una norma cuadra'tica sobre los valimagendeentrada.unstridemayorprovocamenosposiciones ores.\nel pooling reduce el ancho y el alto de la imagen, pero c) zfnet: creada en base a alexnet, ajusta el taman˜o\nconserva la cantidad de canales, por lo que con entrada de de los filtros y la profundidad para estudiar co'mo cada capa\ntaman˜o w ×h×d, el pooling reduce w y h, manteniendo transforma la informacio'n. introdujo te'cnicas para visualizar\nd.estoevitaqueelmodelocrezcaencantidaddepara'metros activacionesintermedias,ayudandoacomprenderydepurarel\ny mantiene la informacio'n esencial para las siguientes capas. comportamiento interno de las cnn.\nd) googlenet (inception): presentada por google en\ne. capa fully-connected\n2014, redujo de 60 a 4 millones de para'metros mediante los\ntras las etapas de convolucio'n y pooling, la red produce mo'dulos inception, que combinan convoluciones de distintos\nun vector que resume las caracter'ısticas ma's relevantes de la taman˜os (1×1, 3×3, 5×5) y max pooling en paralelo. en\nimagen.estevectorseconectaaunaovariascapastotalmente la etapa final, un average pooling global transforma el tensor\nconectadas. cada neurona de estas capas esta' conectada con de 7×7×1024 en un vector 1×1×1024, reemplazando las\ntodas las salidas anteriores, permitiendo combinar las carac- capas densas y mejorando la eficiencia [2].\nter'ısticas extra'ıdas para realizar la clasificacio'n final.\ne) vgg16: simplifica el disen˜o utilizando solo filtros\nel perceptro'n multicapa (mlp) se encarga de transformar pequen˜os de 3×3 y bloques repetidos de convolucio'n y pooleste vector en una prediccio'n, como la probabilidad de perte- ing. aumenta la profundidad hasta 16 o 19 capas, mostrando\nnencia a una clase espec'ıfica. que ma's capas con filtros simples mejoran el rendimiento\ngeneral.\nf. arquitecturas convolucionales\nf) resnet: introduce las conexiones residuales, que per1) estructura general: una arquitectura convolucional se\nmiten que la informacio'n fluya entre capas no consecutivas.\ncompone de bloques repetidos de:"}
{"id_doc": "DOC_036", "segmentacion": "B", "chunk_id": "DOC_036_B_002", "idx": 2, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "estas conexiones evitan el desvanecimiento del gradiente y\nconvolucio'n→activacio'n→pooling posibilitanentrenarredesextremadamenteprofundasdeforma\nestable.\nestosbloquesserepitenvariasvecesparaextraerinformacio'n\ng) densenet: conectacadacapacontodaslasanteriores\nprogresivamentema'sabstracta.posteriormente,elresultadose\ndentro de un bloque, promoviendo la reutilizacio'n de caracaplana(flatten)yseconectaaunaoma'scapasfullyconnected\nter'ısticas y reduciendo la redundancia. esta estructura densa\npara la clasificacio'n.\nmejoralapropagacio'ndelgradiente,optimizalaeficienciadel\nserecomiendaelusodefiltrospequen˜os(porejemplo,3×3\nmodelo y mantiene un nu'mero reducido de para'metros.\no 5×5) ya que permiten:\n- reducir la cantidad de para'metros a aprender. ii. problemasconlasredesneuronales\n- capturar relaciones no lineales al encadenar mu'ltiples convolucionales\ncapas.\na pesar de su alto desempen˜o, las redes convolucionales se\nfiltros grandes (7×7 o ma's) capturan ma's informacio'n en comportan como una \"caja negra\", ya que resulta dif'ıcil comuna sola capa, pero aumentan excesivamente el nu'mero de prender que' tipo de informacio'n esta'n utilizando para tomar\npara'metros y reducen la no linealidad.\nsus decisiones. las representaciones internas que generan son\n2) reglas pra'cticas: altamente abstractas, lo que plantea un reto importante de\n- es preferible que las dimensiones de las ima'genes sean interpretabilidad.\ndivisibles entre 2 para facilitar las reducciones con max uno de los principales desaf'ıos actuales es entender que'\npooling. es lo que realmente afecta a la red durante el proceso de\n- en general, se utiliza stride de 2 y padding de 1 para clasificacio'n.lascapasinternasaprendencaracter'ısticascommantener dimensiones manejables. plejas que no siempre son comprensibles para los humanos.\n- el pooling de 2 × 2 es el ma's comu'n, reduciendo la este problema de explicabilidad motiva el uso de te'cnicas\nimagen a la mitad en cada dimensio'n. de visualizacio'n que permitan analizar la respuesta de las\n3) principales arquitecturas: neuronas ante diferentes est'ımulos visuales.\na) lenet-5: propuesta por yann lecun en 1998, fue\na. visualizacio'n y ana'lisis de activaciones\nuna de las primeras redes convolucionales aplicadas al reconocimiento de d'ıgitos escritos a mano [1]. su estructura una forma pra'ctica de estudiar el comportamiento interno\nincluye dos capas convolucionales, dos de pooling y una de las cnn es observar los feature maps generados por cada\ntotalmente conectada, estableciendo la base para las redes capa.\nmodernas de visio'n por computadora. en estos mapas se puede identificar que' regiones de la\nb) alexnet: desarrollada por krizhevsky, sutskever y imagen activan ciertas neuronas y, por lo tanto, cua'les son\nhinton en el 2012, marco' el inicio del deep learning mod- los elementos visuales que el modelo considera relevantes.\nerno. procesa ima'genes de 224 × 224 con filtros grandes en las primeras capas, las activaciones suelen asemejarse\n(11×11, 5×5, 3×3), emplea activaciones relu, dropout todav'ıaalaimagenoriginal,peroconformeseprofundiza,las\ny entrenamiento distribuido en mu'ltiples gpus, logrando un representacionessevuelvencadavezma'sabstractasydif'ıciles\nsalto significativo en precisio'n sobre el conjunto imagenet. de interpretar.\nfig.2. diagramadelfuncionamientodeunautoencoder\nproporcionandotransparenciaalprocesodedecisio'ndelmodelo.\nfig.1. representacio'ndeclasescont-sne."}
{"id_doc": "DOC_036", "segmentacion": "B", "chunk_id": "DOC_036_B_003", "idx": 3, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "iii. autoencoders\nes una red neuronal disen˜ada para aprender una repreadema's, la inspeccio'n de los filtros aprendidos ayuda a\nsentacio'n comprimida de sus datos de entrada. a diferencia\nverificar que' patrones esta' capturando la red. los filtros\nde las redes supervisadas, no necesita etiquetas externas, ya\ninicialestiendenamostrartexturas,bordesocolores,mientras\nque su objetivo es reconstruir la entrada en la salida. durante\nque los u'ltimos codifican composiciones ma's complejas. este\nel entrenamiento, el modelo aprende a capturar los patrones\ntipo de ana'lisis permite detectar si la red esta' aprendiendo\nma's relevantes de los datos, filtrando el ruido y conservando\ncaracter'ısticas significativas o simplemente ruido del conjunto\nla informacio'n esencial.\nde entrenamiento.\nla estructura ba'sica se compone de tres partes (fig. 2):\nb. reduccio'n de dimensionalidad encoder→espacio latente→decoder\npara comprender las redes, se puede hacer el estudio de el aprendizaje del autoencoder consiste en minimizar el\nsus embeddings. al final de la red, cada imagen puede error de reconstruccio'n entre la entrada original y la salida\nrepresentarse mediante un vector nume'rico que resume su reconstruida. aunque no haya etiquetas externas, el entreinformacio'n sema'ntica. ima'genes similares quedan pro'ximas namiento es parcialmente supervisado, ya que la salida se\nentre s'ı en este espacio vectorial, mientras que las de distintas compara directamente con la entrada.\nclases se separan claramente. estas representaciones pueden\nvisualizarse mediante algoritmos de reduccio'n de dimension- a. encoder\nalidad, como: consiste en una serie de bloques convolucionales seguidos\n- t-sne: proyecta los vectores a dos o tres dimensiones, de operaciones de pooling, con el objetivo de extraer las\npreservando la estructura de las distancias originales, caracter'ısticas ma's relevantes de la entrada y comprimir la\ncomo se puede ver en la fig. 1. informacio'n a trave's de un proceso de reduccio'n espacial o\n- pca:alternativama'ssimple,aunquemenosefectivapara downsampling. cada bloque convolucional aprende distintos\nrelaciones no lineales. niveles de representacio'n, pasando de detalles simples como\nbordes y texturas a rasgos ma's abstractos. de esta manera, el\ncuando la separacio'n entre clases es clara en el espacio\nencoder transforma los datos originales en una versio'n ma's\nreducido, se considera que el modelo ha aprendido una repcompacta, conservando u'nicamente la informacio'n esencial\nresentacio'n adecuada. por el contrario, si las clases aparecen\npara la reconstruccio'n posterior.\nmezcladas, indica que la red no ha logrado distinguir correctamente las caracter'ısticas de cada una. b. espacio latente o cuello de botella\nen esta etapa se almacena la informacio'n esencial en\nc. mapas de activacio'n\nun vector de baja dimensionalidad, conocido como espacio\nse pueden generar heatmaps o mapas de activacio'n que latente. este vector contiene la codificacio'n interna de la\ndestacanlaszonasespec'ıficasdeunaimagenqueinfluyenma's entrada, capturando u'nicamente los rasgos ma's significativos.\nenladecisio'ndelmodelo.estosmapassonu'tilesparaverificar debidoasutaman˜olimitado,restringeelflujodeinformacio'n\nsilaredesta' enfoca'ndoseenlasregionescorrectasdelobjeto. hacia el decoder, lo que obliga al modelo a conservar solo\npor ejemplo, estos me'todos permiten justificar predicciones, lo ma's relevante para lograr una reconstruccio'n efectiva. en\ncomo localizar una fractura o anomal'ıa en una radiograf'ıa, este espacio, muestras similares tienden a ubicarse pro'ximas"}
{"id_doc": "DOC_036", "segmentacion": "B", "chunk_id": "DOC_036_B_004", "idx": 4, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "iv. conclusiones\n\nlas redes convolucionales permiten extraer\nautoma'ticamente caracter'ısticas jera'rquicas de las ima'genes,\nimpulsando el desarrollo de arquitecturas cada vez ma's\nprofundas y eficientes. a pesar de su potencia, siguen siendo\npoco interpretables, por lo que se recurre a te'cnicas de\nvisualizacio'n y ana'lisis de activaciones. finalmente, los\nautoencoders ampl'ıan estos conceptos al aprendizaje no\nfig.3. ejemplodesuper-resolucio'nconautoencoder.\nsupervisado, permitiendo la compresio'n, reconstruccio'n y\ngeneracio'n de datos a partir de representaciones latentes.\nentre s'ı, formando agrupamientos que reflejan la estructura\n\nreferencias\nsema'ntica de los datos.\n[1] y.lecun,l.bottou,y.bengio,andp.haffner,\"gradient-basedlearning\nc. decoder y reconstruccio'n appliedtodocumentrecognition,\"proceedingsoftheieee,vol.86,no.\n11,pp.2278-2324,1998.\na partir del vector del espacio latente, utiliza capas de [2] c.szegedyetal.,\"goingdeeperwithconvolutions,\"proceedingsofthe\nupsampling o convoluciones transpuestas para expandir pro- ieeeconferenceoncomputervisionandpatternrecognition(cvpr),\npp.1-9,2015.\ngresivamente la representacio'n comprimida hasta recuperar la\nforma original. durante este proceso, el modelo aprende a\nreconstruir los detalles perdidos, generando una salida que se\nasemeje lo ma's posible a la entrada inicial.\nd. aplicaciones de los autoencoders\n- reduccio'n de dimensionalidad: obtener representaciones ma's compactas que las de pca.\n- deteccio'n de anomal'ıas: los ejemplos normales se reconstruyen bien, mientras que los at'ıpicos muestran un\nerrordereconstruccio'nelevado.sepuedefijarunumbral\npara decidir cua'ndo un dato es ano'malo.\n- eliminacio'nderuido:aprenderareconstruirunaimagen\nlimpia a partir de una ruidosa.\n- edicio'n y generacio'n de ima'genes: al modificar el vector latente se pueden crear variantes o nuevas ima'genes,\npor ejemplo, para comprimirlas.\n- super-resolucio'n: generar versiones de alta resolucio'n\na partir de ima'genes pequen˜as (fig. 3).\ne. hiperpara'metros relevantes\n- taman˜o del vector latente: define la cantidad de informacio'n que el modelo puede retener en el espacio\ncomprimido. un vector ma's pequen˜o produce un modelo\nma'seficienteenco'mputo,peroconmenorcapacidadpara\ncapturardetallesdelaimagen.encambio,unvectorma's\ngrande permite representar ma's caracter'ısticas, aunque\nincrementaelcostodeentrenamientoydeprocesamiento.\n- nu'mero de capas: tanto el encoder como el decoder\npuedenvariarenprofundidad.unmayornu'merodecapas\npermite modelar relaciones ma's complejas, pero tambie'n\nhace el entrenamiento ma's pesado y sensible al ajuste de\npara'metros.\n- funcio'n de pe'rdida: para tareas de reconstruccio'n de\nima'genes se utiliza comu'nmente el mean squared error\n(mse). esta funcio'n compara cada p'ıxel de la imagen\noriginal con el de la reconstruccio'n, midiendo su diferencia. un error cercano a cero indica que el modelo ha"}
{"id_doc": "DOC_036", "segmentacion": "B", "chunk_id": "DOC_036_B_005", "idx": 5, "autor": "Juan Jiménez Valverde", "fecha": "2025-10-14", "tema": "Análisis de redes convolucionales y autoencoders: filtros, pooling, embeddings, explicabilidad, arquitecturas clásicas (AlexNet, VGG, ResNet, DenseNet) y visualización de activaciones.", "texto": "logrado reproducir correctamente la entrada."}
{"id_doc": "DOC_037", "segmentacion": "B", "chunk_id": "DOC_037_B_000", "idx": 0, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "apuntes semana 11 clase #1\n14/10/2025\nalex steven naranjo mas'ıs\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nemail: alnaranjo@estudiantec.cr\n\nresumen-este documento recopila los apuntes de la clase b. para'metrosdelaconvolucio'n:stride,paddingytaman˜o\n\ndel martes 14 de octubre de 2025 para el curso de inteligencia de salida\nartificial.seabordaronlosfundamentosdelasredesneuronales\npara una entrada 1d de longitud m, kernel k, padding p y\nconvolucionales (cnn), explicando el funcionamiento de los\nfiltros, el campo receptivo, el stride, el padding y las capas stride s, la salida es:\nde pooling para la extraccio'n de caracter'ısticas en ima'genes. (cid:22) (cid:23)\nm+2p-k\nadema's, se estudiaron arquitecturas cla'sicas como lenet, alex- out= +1.\ns\nnet,googlenet/inception,vgg16,resnetydensenet.finalmente, se introdujeron los conceptos de embeddings, visualizacio'n de en 2d se aplica por dimensio'n (alto y ancho). el padding\nactivaciones y autoencoders, analizando sus aplicaciones en resime'trico t'ıpico para \"conservacio'n de taman˜o\" con s=1 es\nduccio'ndedimensionalidad,deteccio'ndeanomal'ıas,eliminacio'n\np = k-1 (si k es impar). el stride > 1 reduce la resolucio'n\nde ruido y super-resolucio'n, junto con consideraciones pra'cticas 2\nde entrenamiento y seleccio'n de hiperpara'metros. espacial.\nindex terms-redes neuronales convolucionales, pooling,\nc. pesos compartidos y eficiencia\nembeddings, visualizacio'n, autoencoder, deep learning\nla comparticio'n de pesos aplica el mismo kernel en todas"}
{"id_doc": "DOC_037", "segmentacion": "B", "chunk_id": "DOC_037_B_001", "idx": 1, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "i. introduccio'n lasposicionesespaciales,reduciendopara'metrosfrenteacapas\ndensas. en primeras capas, la red aprende bordes y texturas;\nlasredesneuronalesconvolucionales(cnn)sonunpilar en capas profundas, patrones sema'nticos ma's abstractos.\nen la visio'n por computadora moderna, pues permiten extraer\nd. capa de pooling\nautoma'ticamente caracter'ısticas jera'rquicas de las ima'genes.\ncomprendersuscomponentescomofiltros,camposreceptivos, reduce la resolucio'n espacial conservando canales:\nstride,paddingypoolingesesencialparadisen˜ararquitecturas max pooling: retiene el valor ma'ximo de cada ventana.\neficientes. por su parte, los autoencoders complementan este average pooling: promedia los valores.\naprendizaje al representar la informacio'n de forma comprimi- regla pra'ctica: pooling 2×2 con stride 2 para reduccio'n a la\nda, sin necesidad de etiquetas externas, y habilitan tareas de mitad. mantiene d =c y reduce h,w.\nin\naprendizaje no supervisado/semisupervisado.\ne. activaciones, normalizacio'n y regularizacio'n\n\nii. fundamentosderedesneuronales activacio'n: relu es esta'ndar en cnn modernas (evita\nconvolucionales(cnns) saturacio'n y acelera entrenamiento). tanh/sigmoid pueden usarse en salidas espec'ıficas.\na. filtros (kernels) y campos receptivos\nbatch normalization (bn): estabiliza la distribucio'n\nun filtro 2d de taman˜o k×k se desliza sobre la imagen (o de activaciones, permite mayores tasas de aprendizaje y\nmapa de activacio'n) para producir un feature map. para una acelera la convergencia.\nentrada rgb h ×w ×c y c filtros, cada filtro tiene regularizacio'n: dropout (t'ıpico en capas densas), l2"}
{"id_doc": "DOC_037", "segmentacion": "B", "chunk_id": "DOC_037_B_002", "idx": 2, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "in out\ntaman˜o k×k×c y produce un canal en la salida. (weightdecay)ydataaugmentationreducensobreajuste.\nin\nfiltro gaussiano: suaviza la imagen (blur) y reduce f. capa fully-connected (mlp) y clasificacio'n\nruido; resalta contornos al combinarse con operadores\ntras extraer mapas de activacio'n, se aplica flatten (o global\nde gradiente.\naveragepooling)ycapasdensasparaclasificacio'n.enproblecampo receptivo (rf): regio'n de la entrada que \"ve\"\nmas multi-clase se usa softmax y pe'rdida de entrop'ıa cruzada.\nuna neurona de una capa dada. aumenta con la profundidad. si encadenamos capas con kernel k y stride s , iii. arquitecturasconvolucionales\ni i\nel rf efectivo crece de forma acumulativa. a. lenet-5\npara'metros y costo: el nu'mero de para'metros en una capa pionera (lecun, 1998) para d'ıgitos manuscritos (mnist).\nconv es k2 - c - c + c (sesgo). la complejidad dos bloques conv+pooling y capas densas. introdujo la viabiin out out\ncomputacional se aproxima por h -w -k2-c -c . lidad pra'ctica de cnns."}
{"id_doc": "DOC_037", "segmentacion": "B", "chunk_id": "DOC_037_B_003", "idx": 3, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "out out in out\nb. alexnet (2012)\nkrizhevskyetal.popularizanrelu,dropout,entrenamiento\nen mu'ltiples gpus y kernels grandes (11×11, 5×5, 3×3)\nen entradas 224×224. disparo' la adopcio'n de deep learning\na gran escala.\nc. zfnet y visualizacio'n intermedia\najusta taman˜os de kernel/stride y estudia feature maps\ninternos para entender que' aprende cada capa, motivando\npra'cticas de disen˜o y depuracio'n.\nd. googlenet / inception\nmo'dulos con ramas paralelas (1×1, 3×3, 5×5 + max\npooling);reducepara'metros(de∼60ma∼4m)usandocuellos\n1×1 y global average pooling al final.\ne. vgg-16\nfilosof'ıa de simplicidad: solo 3×3 + profundidad (16/19\ncapas).apesardemuchospara'metros,esunbaselinedida'ctico muy usado.\nfigura1. representacio'ndeembeddingsmediantet-sne.\nf. resnet (redes residuales)\nskip connections (y = f(x)+x) permiten entrenar redes\nmuy profundas mitigando vanishing gradient. bloques basic/bottleneck se apilan eficientemente.\ng. densenet\nconexiones densas \"todas con todas\" dentro del bloque;\nfomenta reutilizacio'n de caracter'ısticas, mejora el flujo de\ngradiente y reduce para'metros a igual rendimiento."}
{"id_doc": "DOC_037", "segmentacion": "B", "chunk_id": "DOC_037_B_004", "idx": 4, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "iv. explicabilidaddelmodeloyembeddings\na. visualizacio'n de activaciones y filtros\nobservar feature maps muestra que' regiones activan cafigura2. estructuraba'sicadeunautoencoder.\nda neurona. en capas iniciales, activaciones recuerdan bordes/colores; en capas profundas, part'ıculas sema'nticas ma's\ncomplejas.\n\nv. autoencoders(codificadoresautoma'ticos)\nb. embeddings y reduccio'n de dimensionalidad a. estructura general y objetivo\nlosembeddingssonvectoresenrdquecapturansema'ntica.\nvectores de clases similares tienden a agruparse en el espacio encoder→espacio latente→decoder\nlatente.\naprenden a reconstruir la entrada. aunque la sen˜al de entret-sne:proyeccio'nnolineala2d/3dpreservandovecinnamientoesauto-supervisada(salida=entrada),seconsideran\ndarios locales.\nt'ıpicamenteme'todosnosupervisadospornorequeriretiquetas\npca: proyeccio'n lineal; u'til como baseline o preproceexternas.\nsamiento.\nb. componentes y variantes\nc. mapas de activacio'n (heatmaps)\nencoder: reduce espacialidad y comprime informacio'n\nheatmaps sen˜alan zonas que ma's influyen en la prediccio'n (conv + downsampling).\n(u'til en aplicaciones me'dicas/industriales para justificar deci- latente: vector/tensor compacto; su taman˜o controla capacisiones). dad vs. compresio'n.\nb. optimizacio'n y regularizacio'n\noptimizadores: sgd+momentum (control fino), adam\n(ra'pida convergencia).\nlr scheduling: step/cosine/plateau.\nregularizacio'n: l2 (weight decay), dropout (sobre todo\nen densas), early stopping.\nc. reglas pra'cticas de arquitectura\ndimensiones divisibles entre 2 para facilitar pooling.\npreferir kernels pequen˜os (3 × 3 / 5 × 5) y apilar\nfigura3. ejemploconceptualdesuper-resolucio'nconautoencoder.\nprofundidad para mayor no linealidad."}
{"id_doc": "DOC_037", "segmentacion": "B", "chunk_id": "DOC_037_B_005", "idx": 5, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "usarglobalaveragepoolingantesdedensasparareducir\npara'metros.\ndecoder: reconstruye con upsampling o convoluciones transinsertar bn despue's de conv y antes de relu para\npuestas.\nestabilidad.\nvariantes: denoising (entrenar con entrada ruidosa y salida\nlimpia),sparse(regularizalatente),under/overcomplete.(no- d. notas de implementacio'n\nta: vaes y gans exceden el alcance de esta clase, pero se\nen frameworks como pytorch, la reconstruccio'n en decorelacionan con lo generativo.)\nders suele emplear convtranspose2d o upsample+1×\n1 conv; para clasificacio'n, crossentropyloss (con\nc. funciones de pe'rdida comunes logsoftmax interno) es esta'ndar.\nmse(meansquarederror):reconstruccio'np'ıxelap'ıxel vii. conclusiones\n(continuo).\nlas cnn han transformado la visio'n por computadora\nmae: ma's robusto a outliers.\nal extraer jerarqu'ıas de caracter'ısticas de manera automa'tica\nbce/bcewithlogits: para ima'genes normalizay eficiente. no obstante, su interpretabilidad sigue siendo\ndas/binarizadas.\nun reto; te'cnicas de visualizacio'n, embeddings y heatmaps\nperceptual/ssim (opcional): mejor correlacio'n percepayudan a entender y validar decisiones. los autoencoders\ntual que mse.\nextiendenestosconceptoshacialacompresio'n,reconstruccio'n\nygeneracio'ndedatos,habilitandoaplicacionespra'cticascomo\nd. aplicaciones reduccio'ndedimensionalidad,deteccio'ndeanomal'ıasysuperresolucio'n. una ingenier'ıa cuidadosa de arquitectura, data y\nreduccio'n de dimensionalidad y almacenamiento efientrenamiento es clave para un desempen˜o robusto.\nciente en bbdd vectoriales.\ndeteccio'ndeanomal'ıas:entrenarcondatos\"normales\"; referencias\naltas pe'rdidas de reconstruccio'n sugieren anomal'ıas.\n[1] y.lecun,l.bottou,y.bengio,andp.haffner,\"gradient-basedlearning\neliminacio'n de ruido (denoising). appliedtodocumentrecognition,\"proceedingsoftheieee,vol.86,no.\nsuper-resolucio'n: reconstruir versiones de mayor reso- 11,pp.2278-2324,1998.\nlucio'n. [2] c.szegedyetal.,\"goingdeeperwithconvolutions,\"cvpr,2015.\ne. hiperpara'metros relevantes\ntaman˜o del latente: ma's pequen˜o = mayor compresio'n/menor fidelidad; ma's grande = mayor capacidad/costo.\nprofundidaddelencoder/decoderytipodeupsampling\n(nearest/bilinear vs. convtranspose2d).\npe'rdida de reconstruccio'n (mse/mae/bce/ssim)\nsegu'n dominio."}
{"id_doc": "DOC_037", "segmentacion": "B", "chunk_id": "DOC_037_B_006", "idx": 6, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-10-14", "tema": "Fundamentos de redes neuronales convolucionales y autoencoders, incluyendo embeddings, visualización de activaciones y buenas prácticas de diseño en CNNs.", "texto": "vi. buenaspra'cticasdeentrenamientoydisen˜o\na. preprocesamiento y aumento de datos\nnormalizacio'n por canal (media/desviacio'n del dataset).\ndataaugmentationmoderado:flips,crops,ligerosjitters;\n\nevita overfitting."}
{"id_doc": "DOC_039", "segmentacion": "B", "chunk_id": "DOC_039_B_000", "idx": 0, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "apuntes semana 11 clase #2\n\neder vega suazo\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\n\nic-6200 - inteligencia artificial gr2\n\nresumen-este documento condensa la segunda leccio'n de la el profesor destaco' adema's las aplicaciones pra'cticas resemana11centradaenautoencodersysuaplicacio'naima'genesy visadas: la reduccio'n de dimensionalidad como alternativa a\ntexto.seexplicanlaestructuraencoder-espaciolatente-decoder,\nme'todos tradicionales, la deteccio'n de anomal'ıas mediante\nvariantes pra'cticas (denoising, vae, under/overcomplete) y arel ana'lisis del error de reconstruccio'n, y la restauracio'n de\nquitecturas relacionadas (u-net, skip-connections). se discuten\ntareas y aplicaciones: reduccio'n de dimensionalidad, deteccio'n ima'genes afectadas por ruido o baja resolucio'n. finalmente,\nde anomal'ıas, super-resolucio'n y segmentacio'n, adema's de la serepaso' elconceptodeespaciolatentecontinuo,introducido\ntransicio'n a representaciones de texto (tokenizacio'n y embed- en los autoencoders variacionales (vae), el cual permite gedings)ymodelosdelenguaje.elapunteincluyerecomendaciones\nnerar nuevas muestras mediante la interpolacio'n entre puntos\nexperimentales y criterios de evaluacio'n pra'cticos orientados a\ndel espacio latente, estableciendo as'ı la base para los modelos\nla implementacio'n de proyectos y a la replicacio'n de resultados.\ngenerativos que se profundizar'ıan en la sesio'n actual.\nindex terms-autoencoder, vae, denoising, reduccio'n de"}
{"id_doc": "DOC_039", "segmentacion": "B", "chunk_id": "DOC_039_B_001", "idx": 1, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "iii. apuntesdeclase\ndimensionalidad,tokenizacio'n,embeddings,u-net,deteccio'nde\nanomal'ıas. iii-a. organizacio'n y avisos\nlas revisiones del proyecto sera'n presenciales, el proi. introduccio'n fesor aviso' a los apuntadores faltantes que lo tomen en\ncuenta ya que una semana se debera' de apartar para la\n\neste documento sintetiza los conceptos trabajados en la\nrevisio'n del proyecto.\nsesio'n sobre arquitecturas basadas en redes convolucionales\npro'ximamentehabra' dosentregablesprincipales:unejeraplicadas a autoencoders y la extensio'n hacia representaciocicio pra'ctico con autoencoders (ima'genes) y una tarea\nnes para texto. el documento ofrece una gu'ıa pra'ctica con\nma's compleja sobre texto y agentes, esta puede que\ndefiniciones, fo'rmulas y recomendaciones operativas para la\nvalga ma's porcentaje. ya que esta implica planificar\nimplementacio'n de experimentos en ima'genes y texto. se suexperimentos y validaciones con tiempo para revisiones\ngiere acompan˜ar este documento con las figuras referenciadas\nen laboratorio.\nparafacilitarlacomprensio'ndearquitecturasyvisualizaciones\nnota: zoom limita la validez de los enlaces; el profesor\nde espacios latentes.\ngenero' un link que ya esta' en el grupo de telegram para\nlas lecciones y la duracio'n t'ıpica de la sesio'n es ∼40"}
{"id_doc": "DOC_039", "segmentacion": "B", "chunk_id": "DOC_039_B_002", "idx": 2, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "ii. repasodelaclase\nmin, cuando acabe hay que ingresar nuevamente en el\nla sesio'n inicio' con un repaso de los temas vistos ante- mismo link.\nriormente, en los que se introdujeron los fundamentos de los\niii-b. autoencoders: idea y componentes\nautoencoders y su relacio'n con las redes convolucionales. se\nrecordo' que estas arquitecturas son una aplicacio'n directa de un autoencoder aprende una funcio'n f : x (cid:55)→ xˆ donde xˆ\nlas cnn en un contexto no supervisado, donde el objetivo intenta aproximarse a x. internamente:\nprincipalesreconstruirlaentradaoriginalapartirdeunarepre- encoder: transforma x en z =g (x).\nθ\nsentacio'n comprimida. el profesor enfatizo' que, a diferencia espacio latente: z es un vector de baja dimensio'n que\nde los modelos de clasificacio'n, los autoencoders no utilizan condensa caracter'ısticas relevantes.\netiquetas externas, sino que aprenden de los propios datos, decoder: reconstruye xˆ=h (z).\nϕ\npermitiendo capturar patrones y regularidades internas. en ima'genes el encoder usa convoluciones y pooling para\ndurante el repaso, se analizo' la estructura general de un reducir resolucio'n y aumentar canales. el decoder usa operaautoencoder compuesta por un encoder, un espacio latente cionesdeupsamplingoconvolucio'ntranspuestapararecuperar\ny un decoder. el encoder transforma la entrada en una la forma espacial. ver figura 1 para un esquema general de\nrepresentacio'ndemenordimensionalidadqueconcentralain- encoder/decoder.\nformacio'nesencial;eldecoder,asuvez,reconstruyelaimagen\niii-c. entrenamiento y funciones de pe'rdida\na partir de esa representacio'n. este proceso de codificacio'n\ny decodificacio'n se comparo' con una forma de \"compresio'n el objetivo del entrenamiento es reducir la diferencia entre\naprendida\"dondeelmodelodecideque' informacio'nconservar laentradaoriginalylareconstruccio'nqueproduceelmodelo.\ny cua'l descartar. los elementos que generan las sen˜ales evaluadas por la\nlaeleccio'nentrecomparacio'np'ıxelap'ıxelyunape'rdida\nparadatosbinariosdependedelrangoylainterpretacio'n\nde los p'ıxeles.\ncuando la calidad visual importa, adema's de la pe'rdida\nde entrenamiento suele evaluarse la reconstruccio'n con\nme'tricas perceptuales (p. ej. ssim) para complementar\nla evaluacio'n nume'rica.\niii-d. variantes y su propo'sito\niii-d0a. denoising: entrenar con x y objetivo"}
{"id_doc": "DOC_039", "segmentacion": "B", "chunk_id": "DOC_039_B_003", "idx": 3, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "ruidosa\nx limpio. el modelo aprende a eliminar ruido espec'ıfico (ej.\nsalt-and-pepper).\nfigura 1: esquema general de encoder, espacio latente y\niii-d0b. under-/overcomplete: latente ma's pequen˜o\ndecoder.\nobligaacomprimir;latentemayorpuedememorizarenexceso.\niii-d0c. vae: permite muestrear y hacer interpolacio'n\nen un espacio continuo u'til para generacio'n. la combinacio'n\nfuncio'n de pe'rdida son el encoder -que produce el vector\nde reconstruccio'n y kl genera latentes con estructura eslatente o, en variantes probabil'ısticas, los para'metros de una\ntad'ıstica. ver figura 2 para ilustracio'n de muestreo y espacio\ndistribucio'n- y el decoder -que genera la reconstruccio'n xˆ\nlatente continuo.\na partir de ese latente.\niii-c0a. pe'rdidadereconstruccio'n.: eslamedidaprincipal que compara la entrada y la salida del autoencoder.\nsu funcio'n es indicar cua'nto error comete el modelo al\nreconstruir.segu'neltipodedatosysunormalizacio'nseelige\nla forma pra'ctica de esta pe'rdida:\npara ima'genes normalizadas en [0,1] es comu'n usar una\npe'rdida basada en la comparacio'n p'ıxel a p'ıxel (mencionada en clase como la opcio'n directa). el profesor\ncomparo' esta te'cnica con el enfoque que usa'bamos en\nregresio'n para penalizar diferencias entre valores.\npara ima'genes con valores binarios o interpretadas como probabilidades se emplea una pe'rdida adecuada a\nese caso (la alternativa binaria que se menciono' en la\npresentacio'n).\nlo que se obtiene con esta pe'rdida es un indicador directo\nde calidad de reconstruccio'n. en aplicaciones como deteccio'n\nde anomal'ıas se usa ese error (o una me'trica derivada) para\ndecidir si una muestra es at'ıpica.\niii-c0b. regularizacio'n en vae (pe'rdida adicional).:"}
{"id_doc": "DOC_039", "segmentacion": "B", "chunk_id": "DOC_039_B_004", "idx": 4, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "en la variante variacional el encoder no entrega un vector\ndeterministasinopara'metrosdeunadistribucio'nenelespacio\nlatente. adema's de la pe'rdida de reconstruccio'n, se incorpora\nfigura 2: representacio'n del muestreo en vae\nunte'rminoqueobligaaqueladistribucio'nlatentesigaunareferencia(elprofesorlodescribio' comoforzarunadistribucio'n\ncontinua,porejemplo,normal).esete'rminoderegularizacio'n:\niii-e. aplicaciones pra'cticas\nproviene directamente de los para'metros que calcula el\nencoder (media y dispersio'n en la clase). iii-e0a. reduccio'n de dimensionalidad: guardar z en\nsu propo'sito es estructurar el espacio latente para que una base de datos vectorial. comparar vectores con similitud\nsea continuo y muestreable, permitiendo interpolacio'n y de coseno:\ngeneracio'n controlada. u-v\nsim(u,v)= .\n∥u∥∥v∥\niii-c0c. notas pra'cticas (mencionadas en clase).:\nnormalizar los p'ıxeles al rango adecuado facilita la usos: bu'squeda por similitud, indexacio'n y como entrada\neleccio'n de la pe'rdida. comprimida para clasificadores simples (knn).\niii-e0b. deteccio'n de anomal'ıas (ejemplo bancario):\nentrenarcontransaccionesva'lidas.paraunatransaccio'nnueva\nx: calcular err = l (x,xˆ). si el error err es mayor a un\nrec\numbral, lo marca como posible fraude. seleccio'n del umbral\nτ medianterocovalidacio'nmanual.importanteevaluartasa\nde falsos positivos y costo operativo.\niii-e0c. denoise y super-resolution: para superresolution el objetivo puede ser una imagen de alta resolucio'n\nx y la entrada x . arquitecturas con skip-connections"}
{"id_doc": "DOC_039", "segmentacion": "B", "chunk_id": "DOC_039_B_005", "idx": 5, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "hr lr\n(u-net style) mejoran la preservacio'n de detalles. se\nrecomienda usar una figura comparativa de entrada/resultado\nen el informe experimental (ver figura 3).\nfigura 3: ejemplo sugerido: comparacio'n entrada ruidosa /\nsalida reconstruida / referencia.\nfigura5:ejemplodevisualizacio'nt-snedevectoreslatentes.\nnota: aplicaciones forenses (p. ej. mejora de ca'maras) un\ncompan˜ero plantea las consideraciones legales sobre manipulacio'n de evidencia. iii-g. transicio'n a nlp: tokenizacio'n y embeddings\niii-e0d. segmentacio'n (u-net): u-net concatena mapas de caracter'ısticas del encoder en el decoder. esto restaura iii-g0a. tokenizacio'n: estrategias: palabra completa,\ninformacio'n espacial perdida por pooling y mejora mapeo de subword (bpe), cara'cter, bytes. subword reduce oov y\nma'scarasparasegmentacio'ndeobjetos.(sesugiereincluiruna controla longitud de secuencia. ver figura 6 para un esquema\nfigura de arquitectura u-net y un ejemplo de ma'scara en la de tokenizacio'n subword.\nentrega.) iii-g0b. embeddings: cadatokensemapeaaunvector\ne∈rd mediantelacapaembedding.pararepresentarfrases\nse puede usar promedio de embeddings o agregadores ma's\ncomplejos.\niii-g0c. modelosdelenguaje: evolucio'n:rnn/lstm\n→ transformers con self-attention. la self-attention permite\ncapturar dependencias largas y producir embeddings contextuales;esosembeddingssirvenpararecuperacio'n,clasificacio'n\ny agentes.\nfigura 4: representacio'n de u-net\niii-f. espacios latentes: visualizacio'n y utilidad (ampliado)\nvisualizarz cont-sne/umapfacilitaverseparabilidadpor\nclases. cuando los clusters son n'ıtidos un clasificador simple\nsobre z funcionara' bien. en vae la continuidad del espacio\nfigura 6: esquema ilustrativo de tokenizacio'n subword y\npermite interpolar entre muestras y generar ima'genes plausimapeo a ids.\nbles no vistas. ver figura 5 para un ejemplo de visualizacio'n\nt-sne.\niii-h. recomendaciones operativas para la tarea\nprobar al menos dos configuraciones: (1) denoising autoencoder, (2) vae con latente de prueba (p. ej. 32, 64)\ndependiendo de la gpu.\nguardar checkpoints y curvas de pe'rdida. evaluar mse\ny ssim.\npara anomal'ıas, definir umbral con validacio'n y reportar\nprecisio'n/recall.\nparatexto,experimentartokenizacio'nsubwordyentrenar\nun embedding ba'sico antes de usar modelos preentrenados."}
{"id_doc": "DOC_039", "segmentacion": "B", "chunk_id": "DOC_039_B_006", "idx": 6, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "iv. conclusiones\n\nlos autoencoders representan una herramienta fundamental\ndentro del aprendizaje profundo no supervisado, al permitir\n\nque un modelo aprenda representaciones compactas de los\ndatos sin depender de etiquetas externas. durante la sesio'n\nsedestaco' co'molaarquitecturaencoder-decoderconstituyela\nbaseparamu'ltiplesaplicaciones,desdelareduccio'ndedimensionalidad hasta la generacio'n y reconstruccio'n de ima'genes.\nlacomprensio'ndelespaciolatenteresultaesencial,yaqueen\ne'l se concentra la informacio'n ma's relevante de las entradas\ny se posibilita la deteccio'n de patrones, la identificacio'n de\nanomal'ıas o la generacio'n de nuevos ejemplos a partir de\ndistribuciones continuas como en los vae.\nasimismo, se vio la importancia de seleccionar correctamente las funciones de pe'rdida y de interpretar el error\nde reconstruccio'n segu'n el contexto de aplicacio'n. en tareas\nvisuales, arquitecturas como u-net o las variantes con skipconnections ampl'ıan el potencial del modelo, mientras que\nen procesamiento de texto la nocio'n de codificacio'n latente se\ntrasladaalosembeddingsyalatokenizacio'ncomopasospreviosalosmodelosdelenguaje.enconjunto,losautoencoders\nofrecen una base conceptual y pra'ctica para desarrollar soluciones que integren visio'n e informacio'n textual, avanzando\nhacia sistemas ma's auto'nomos e interpretativos."}
{"id_doc": "DOC_039", "segmentacion": "B", "chunk_id": "DOC_039_B_007", "idx": 7, "autor": "Eder Vega Suazo", "fecha": "2025-10-16", "tema": "Síntesis práctica sobre autoencoders, VAE, U-Net, segmentación, embeddings y tokenización, con enfoque en implementación y evaluación experimental.", "texto": "referencias\n[1] stevenpachecop,\"autoencoder\"2025.\n[2] stevenpachecop,\"ragsyagentesusandollms\"2025.\n\n[3] compan˜erosd.clase,\"11 semana ai 20251014 (1,2,3).,\"2025."}
{"id_doc": "DOC_040", "segmentacion": "B", "chunk_id": "DOC_040_B_000", "idx": 0, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "apuntes semana 12\n\napuntes del 10 de octubre de 2025\nandrey uren˜a bermu'dez - 2022017442\n\ninteligencia artificial\nescuela de computacio'n, instituto tecnolo'gico de costa rica\ncorreo: andurena@estudiantec.cr\n\nabstract-estos apuntes corresponden a la semana 12 del\ncurso de inteligencia artificial, impartido por el profesor steven\npacheco portugue's en el instituto tecnolo'gico de costa rica.\n\nse abordan los temas relacionados con los modelos de lenguaje\nde gran escala (llm), la tokenizacio'n, embeddings, y la introduccio'nalparadigmaderetrieval-augmentedgeneration(rag)\ny agentes inteligentes. adema's, se presentan los anuncios del\ncurso y el cronograma restante del semestre.\n\ni. introduccio'n\ndurante esta sesio'n, se revisaron aspectos fundamentales\nde los modelos de lenguaje modernos y su relacio'n con las figura1. ejemplodeunmodeloderedneuronalpreentrenado.\narquitecturas de inteligencia artificial actuales. tambie'n se\nanalizaron conceptos claves para comprender co'mo los llm\nprocesantexto,transformaninformacio'nenvectores,yaplican\n- semana 17: semana colcho'n (sin actividades prote'cnicas de recuperacio'n de conocimiento externo mediante\ngramadas).\nrag. finalmente, se discutieron las implicaciones e'ticas y el\n- semana 18:\nuso responsable de estos sistemas.\n∗ martes 2 de diciembre: examen i."}
{"id_doc": "DOC_040", "segmentacion": "B", "chunk_id": "DOC_040_B_001", "idx": 1, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "ii. anunciosdelcurso ∗ jueves 4 de diciembre: entrega del proyecto ii.\n- seasigno' latarea04sobreagentes,confechadeentrega\nel 6 de noviembre. la revisio'n sera' presencial y consiste\nen la creacio'n de un agente funcional.\n- se presento' el cronograma para el cierre del semestre,\n\niii. repasodeconceptos\norganizado por semanas:\n- semana 13:\na. modelos de lenguaje de gran escala (llm)\n∗ martes 28 de octubre: quiz 6 y tema quantization - unsupervised.\n\nlosllmsehanconvertidoenlabasedelossistemasmod-\n∗ jueves30deoctubre:temaunsupervised-pca\nernos de inteligencia artificial. permiten generar, comprender\ny entrega del proyecto i.\ny razonar sobre texto, co'digo, ima'genes y audio.\n- semana 14:\ncada entrada (input) es representada mediante valores\n∗ martes 4 de noviembre: revisio'n presencial del\nnume'ricos en punto flotante que describen caracter'ısticas. el\nproyecto i.\ntratamiento var'ıa segu'n si la entrada corresponde a texto,\n∗ jueves 6 de noviembre: revisio'n presencial del\nnu'meros o s'ımbolos.\nproyecto i y entrega de la tarea 04: agentes.\n- semana 15:\n∗ martes 11 de noviembre: clase virtual sobre\nunsupervised - pca, asignacio'n del proyecto ii b. tokenizacio'n\ny la tarea 05: autoencoder - quantization.\n∗ jueves 13 de noviembre: revisio'n virtual de la la tokenizacio'n convierte las palabras, signos o s'ımbolos\ntarea de agentes. en representaciones nume'ricas llamadas tokens. estos tokens\n- semana 16: permiten al modelo procesar texto de manera eficiente.\n∗ martes 18 de noviembre: tema riesgos de la existen varios tipos de tokenizacio'n, resumidos en la\ninteligencia artificial. tabla i.\ntablai iv. materianueva:retrieval-augmented\ntiposcomunesdetokenizacio'nysusprincipalesventajas. generation(rag)\nventaja princi- un sistema rag conecta un llm con un mo'dulo"}
{"id_doc": "DOC_040", "segmentacion": "B", "chunk_id": "DOC_040_B_002", "idx": 2, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "tipo ejemplo\npal de recuperacio'n de informacio'n (retriever) para incorporar\npalabra \"losmedios\" simplificada conocimiento externo relevante durante la generacio'n de recara'cter \"l\",\"o\",\"s\" sinoovs\nspuestas.\nsubpalabra(bpe, equilibra vocab-\n\"super\"+\"vivencia\"\nwordpiece) ulario/contexto\nsoportacualquier a. chunks\n\nbyte-level bytesutf-8\ns'ımbolo\nespacioenblanco \"hola\",\"mundo\" ra'pidoysimple el texto se divide en fragmentos denominados chunks, que\nsuelen contener entre 200 y 500 tokens. cada fragmento se\ntransforma en un vector mediante un modelo de embeddings,\ntras la tokenizacio'n, los tokens se representan como veccapturando su significado sema'ntico.\ntores en un espacio continuo. esto permite medir similitud\nsema'ntica entre palabras.\nb. consulta o recuperacio'n\nc. me'tricas de similitud\ndada una consulta, el sistema convierte la pregunta en\nlas me'tricas ma's utilizadas incluyen: un embedding y calcula la similitud con los embeddings\nindexados, devolviendo los ma's cercanos sema'nticamente.\n- distancia euclidiana: mide que' tan separados esta'n dos\npuntos en el espacio vectorial.\nc. aumento y generacio'n\n- similitud del coseno:\na-b los fragmentos recuperados se integran en el prompt envisim(a,b)= ado al llm, proporcionando contexto adicional que gu'ıa la\n||a||||b||\nrespuesta hacia informacio'n verificada y relevante.\nevalu'a el a'ngulo entre los vectores; un a'ngulo menor\nimplica mayor similitud. d. ventajas principales\n- reduccio'n de alucinaciones.\ntablaii - actualizacio'n continua del conocimiento.\nt e r j a e n m s p f l o o r s m i a m n pl e i n fic to a k d e o n d s e c t o o n k i e d n e i n z t a i c f i i o c 'n a : d l o a r s es pa n l u a m b e' r r a i s co se s. - eficiencia de costos en entrenamiento.\n- aplicabilidad en dominios especializados.\npalabra token idnume'rico - asistentes empresariales enriquecidos.\nlos los 105 - soporte a la investigacio'n y atencio'n al cliente."}
{"id_doc": "DOC_040", "segmentacion": "B", "chunk_id": "DOC_040_B_003", "idx": 3, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "llm llm 2124\naprenden aprenden 893 v. llmtradicionalvsagenteinteligente\n\npatrones patrones 5749\nun llm tradicional puede ofrecer informacio'n general,\npero carece de personalizacio'n y accio'n. por ejemplo, si se le\nd. embeddings\nconsulta\"¿cua'ntosd'ıasdevacacionesmequedan?\",nopodra'\nlosembeddingssonrepresentacionesnume'ricasdensasque\nresponder con precisio'n al no tener acceso a datos personales.\n\nasignan a cada token un vector en un espacio continuo de\nen cambio, un agente inteligente integra:\nalta dimensio'n. capturan significado sema'ntico y relaciones\ncontextualesentrepalabrasuoracionescompletas,permitiendo - memoria: recuerda preferencias y contextos previos.\ncomparaciones ma's profundas entre ideas o documentos. - herramientas: accede a apis externas (clima, vuelos,\ncalendario).\ne. capacidades de los llm - planificacio'n: organiza y ejecuta tareas en funcio'n de\nobjetivos.\n\ndebido a su entrenamiento a gran escala y arquitecturas\n- accio'n: transforma planes en resultados concretos.\nbasadas en transformers, los llm presentan capacidades\neste paradigma refleja la evolucio'n hacia sistemas que\nemergentes:\nrazonan y actu'an, ma's alla' de solo responder texto.\n- comprensio'n contextual.\n- generacio'n coherente de texto. vi. escalamientoresponsable\n- razonamiento y planificacio'n ba'sica.\n- aprendizaje en el prompt (in-context learning). es fundamental evaluar cua'ndo realmente se requiere es-\n- multitarea sin reentrenamiento. calar de un modelo llm a un sistema de agentes o mul-\n- conocimiento esta'tico derivado de los datos de entre- tiagentes. esto implica garantizar seguridad, privacidad y el\nnamiento. uso e'tico de los datos. los agentes deben ser disen˜ados bajo\n- costos computacionales elevados. principios de transparencia y responsabilidad."}
{"id_doc": "DOC_040", "segmentacion": "B", "chunk_id": "DOC_040_B_004", "idx": 4, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-10-21", "tema": "Introducción a modelos de lenguaje (LLM), tokenización, embeddings y sistemas de recuperación aumentada (RAG), con enfoque en agentes inteligentes y ética de la IA.", "texto": "vii. conclusio'n\nlos temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan\ninformacio'n y co'mo se esta'n extendiendo hacia arquitecturas\nma's complejas y u'tiles, como los sistemas rag y los agentes\ninteligentes. estas herramientas representan un paso clave\nhacia una inteligencia artificial ma's contextual, adaptable y\nresponsable.\n\nreferencia\npacheco portuguez, s. (2025). presentacio'n del curso de\n\ninteligencia artificial. instituto tecnolo'gico de costa rica."}
{"id_doc": "DOC_042", "segmentacion": "B", "chunk_id": "DOC_042_B_000", "idx": 0, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "apuntes semana 12 - modelos de lenguaje\n\nextensos y sistemas avanzados\n(llms, rag y agentes inteligentes)\n\nfernando daniel brenes reyes\nescuela de ingeniería en computación\ninstituto tecnológico de costa rica\ncartago, costa rica\n21 de octubre\n2020097446@estudiantec.cr\n\nresumen-el presente documento contiene un repaso y am- ii-b. embeddings y espacios vectoriales\npliación de los conceptos fundamentales de los modelos de\nlenguaje extensos (llms), su representación del conocimiento una vez tokenizados, los ids numéricos se convierten en\nmediante la tokenización y los embeddings en espacios vec- embeddings, que son representaciones numéricas densas en\ntoriales. se detalla la evolución del llm tradicional hacia\nun espacio continuo de alta dimensión.\n\narquitecturas avanzadas como retrieval-augmented generation\n(rag), que resuelve las limitaciones de conocimiento estático, y captura semántica: los embeddings capturan el siglosagentesinteligentes,queintegranmemoria,planificaciónyla"}
{"id_doc": "DOC_042", "segmentacion": "B", "chunk_id": "DOC_042_B_001", "idx": 1, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "nificado y las relaciones contextuales entre palabras u\ncapacidad de ejecutar acciones autónomas, reflejando el estado\noraciones completas.\ndel arte en la inteligencia artificial contextual y adaptable.\nindexterms-llm,rag,agentesinteligentes,tokenización, proximidad: las palabras con significados similares se\nembeddings, aprendizaje contextual. ubican próximas en el espacio vectorial.\noperaciones: este espacio permite realizar operacioi. introducción nes semánticas, como analogías (por ejemplo, rey -\nlos modelos de lenguaje extensos (llms) se han hombre+mujer ≈reina).\nconsolidado como la base de los sistemas modernos de inte- para medir la similitud entre dos vectores a y b en rn, la\nligencia artificial generativa (iag). estos modelos no solo similitud del coseno es la métrica más utilizada:\ngeneran texto, sino que también permiten la comprensión y el\nrazonamientosobretexto,códigoyotrainformacióncompleja. a-b\nsim(a,b)= (1)\naunque son potentes, los llms poseen un conocimiento ||a||||b||\nlimitado a sus datos de entrenamiento (estático) y pueden\nincurrir en alucinaciones. para superar estas barreras, se han"}
{"id_doc": "DOC_042", "segmentacion": "B", "chunk_id": "DOC_042_B_002", "idx": 2, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "desarrollado enfoques como retrieval-augmented generation\n(rag) y los agentes inteligentes.\n\nii. fundamentosdellmsyrepresentación\nii-a. tokenización: de la palabra al número\npara que los llms puedan computar con el lenguaje,\nel texto de entrada debe convertirse en una representación\nnumérica. el proceso de tokenización transforma palabras,\nsignos o símbolos en unidades mínimas llamadas tokens,\nasignando a cada una un id numérico único.\nexisten múltiples estrategias de tokenización, cada una\noptimizada para un objetivo distinto:\npor palabra: ofrece simplicidad.\nporcarácter:permitemanejarsímbolosopalabrasfuera\ndel vocabulario (oov).\nsubpalabra (bpe, wordpiece): logra un equilibrio\nóptimoentreeltamañodelvocabularioylapreservación figura1. representacióntridimensionaldetokens(realeza).\ndel contexto.\nii-c. capacidades emergentes\n\nel entrenamiento masivo de los llms les confiere capacidades avanzadas que emergen sin haber sido entrenados\ndirectamente para ellas:\nrazonamiento y planificación.\naprendizajeenelprompt(in-contextlearning):adaptan el comportamiento a partir de ejemplos dados en la\nentrada.\nmultitarea: realizan traducción, clasificación y codificación sin reentrenamiento."}
{"id_doc": "DOC_042", "segmentacion": "B", "chunk_id": "DOC_042_B_003", "idx": 3, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "iii. retrieval-augmentedgeneration(rag)\nrag es un paradigma que conecta un llm con un mó- figura3. agenteinteligente.\ndulo de recuperación (retriever) para inyectar conocimiento\nexterno, actualizado y verificable durante la generación de\nrespuestas. iv. dellmaagenteinteligente\n\nlos agentes inteligentes basados en llms superan la\niii-a. proceso y flujo de rag\npasividaddelossistemasrag.estosagentespuedenrazonar,\n\n1. preparación (chunking): los documentos se dividen planificar y actuar de manera autónoma, interactuando con\nen fragmentos (chunks), que suelen contener entre 200 el mundo real mediante herramientas externas.\ny 500 tokens, a menudo con overlap para preservar el\niv-a. componentes clave del agente\ncontexto.\n\n2. indexación: cada chunk se convierte en un embedding 1. memoria: permite mantener coherencia y contexto a lo\ny se almacena en una base de datos vectorial (por largo del tiempo.\nejemplo, faiss, qdrant, pinecone). corto plazo: ventana de contexto del modelo."}
{"id_doc": "DOC_042", "segmentacion": "B", "chunk_id": "DOC_042_B_004", "idx": 4, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "3. consulta y recuperación: la pregunta del usuario se largo plazo: bases de datos externas, incluyendo\ntransforma en un embedding, se calcula la similitud sistemas rag para la recuperación contextual.\ncon los vectores indexados y se seleccionan los top-k 2. planificación:permitedescomponerproblemascomplechunks más cercanos semánticamente. jos en pasos y razonar sobre ellos.\n\n4. aumento y generación: los chunks recuperados se\nchains of thought (cot): razonamiento secuenintegran en una plantilla estructurada (prompt) como\ncial.\ncontexto adicional, asegurando que la respuesta del\ntrees of thought (tot):exploracióndemúltiples\nllm sea precisa y fundamentada.\ncaminos de razonamiento antes de decidir.\niii-b. ventajas y limitaciones 3. acción: capacidad de ejecutar tareas concretas mediante herramientas externas (apis, buscadores, sistemas\nrag ofrece la reducción de alucinaciones, la actuarag). por ejemplo, un agente puede acceder a un\nlización continua del conocimiento y la aplicabilidad en\nsistemaderecursoshumanospararesponder:\"¿cuántos\ndominios especializados. no obstante, los sistemas rag sidías de vacaciones me quedan?\".\nguen siendo pasivos; su función se limita a complementar la\nrespuesta del llm con datos recuperados. iv-b. escalamiento responsable\nla implementación de agentes requiere evaluar cuándo es\nnecesarialacomplejidaddeunsistemamultiagente.escrucial\ngarantizar la seguridad, privacidad y el uso ético de los\ndatos, diseñando los agentes bajo principios de transparencia\ny responsabilidad."}
{"id_doc": "DOC_042", "segmentacion": "B", "chunk_id": "DOC_042_B_005", "idx": 5, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-10-21", "tema": "Repaso integral sobre tokenización, embeddings y sistemas avanzados basados en LLMs, con énfasis en RAG y agentes inteligentes para razonamiento, planificación y acción autónoma.", "texto": "referencias\n[1] pacheco portuguez, s. (2025). presentación del curso de inteligencia\nartificial.institutotecnológicodecostarica.\nfigura2. diagramadelflujodeunsistemarag,desdelaindexaciónhasta\n\nlageneracióndelarespuesta."}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_000", "idx": 0, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "1\napuntes semana 12, martes 21 de octubre\ncarranza jiménez kevin\ninstituto tecnológico de costa rica\ncorreo electrónico: kcarranza@estudiantec.cr\n\nresumen-el siguiente documento presenta el resumen de la modelos de lenguaje de gran escala (llm). los rag combiclasedeldíamartes21deoctubre,impartidaporelprofesorste- nan la capacidad generativa de los llm con mecanismos de\nvenpachecoportugüezenelinstitutotecnológicodecostarica.\nrecuperación de información externa, permitiendo respuestas\nla clase presenta el cronograma restante del curso, un resumen\nmás precisas y actualizadas basadas en conocimiento relevande la clase anterior en el que se repasan los temas de modelos\nde lenguaje a gran escala (llm), tokenización, embeddings y la te [1]. por su parte, los agentes inteligentes extienden este\nintrodución del paradigma de retrieval-augmented generation enfoque al incorporar razonamiento, planificación y toma de\n(rag) y agentes inteligentes. en la presente clase se profundiza decisiones, posibilitando sistemas que no solo generan texto,\nen el tema de llm, rags y agentes introducidos en la clase\nsino que también actúan de manera autónoma en función de\nanterior.\nobjetivos específicos [2].\nindex terms-llm, rag, embedding, agente\niii-a. ¿por qué los llm son tan utilizados?"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_001", "idx": 1, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "i. introducción los modelos de lenguaje de gran escala (llm) se han\nla sesión inició con una revisión del cronograma restante convertido en la base de numerosos sistemas modernos\ndel curso. posteriormente, se realizó un repaso de la clase de inteligencia artificial, impulsando avances significativos\nanterior, en la cual se introdujeron conceptos fundamentales en tareas de generación, comprensión y razonamiento\nsobrelosmodelosdelenguajedegranescala(largelanguage sobre texto, código e incluso modalidades más complejas\nmodels,llm),elprocesodetokenizaciónylarepresentación como imágenes y audio. estos modelos son capaces\nsemántica mediante embeddings. de representar conocimiento a gran escala mediante el\na partir deeste punto, la clase se centró endos temas prin- aprendizaje de patrones lingüísticos y semánticos a partir de\ncipales: la integración de modelos mediante esquemas de re- enormes volúmenes de datos, lo que explica la sorprendente\ncuperación aumentada de generación (retrieval-augmented coherencia y versatilidad de sus resultados [3]. comprender\ngeneration, rag) y la noción de agentes inteligentes. los mecanismos internos que permiten estas representaciones,\nasí como sus limitaciones y potencial de generalización,\nresultaesencialparaeldesarrollodeaplicacionesmásseguras"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_002", "idx": 2, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "ii. aspectosadministrativosdelaclase\ny efectivas basadas en inteligencia artificial generativa.\nse presentó el calendario en el cual se muestrán las próximas actividades y evaluaciones que restan del curso. en la\n\nla figura 1 ilustra la arquitectura de un modelo de red\ntabla i.\nneuronal preentrenado diseñado para la clasificación de eventos de colisión. este modelo recibe como entrada un conjunto\n\niii. ragsyagentesutilizandollms de características o features que incluyen la velocidad del\nvehículo,lacalidaddelterreno,elgradodevisióndisponibley\nlos esquemas de recuperación aumentada de generación\nlaexperienciatotaldelconductor.apartirdeestosparámetros,\n(retrieval-augmented generation, rag) y los agentes intelila red aprende a identificar patrones que permiten estimar la\ngentesrepresentanunaevoluciónsignificativaenelusodelos\nprobabilidad de que ocurra una colisión bajo determinadas\n\ncuadroi\n\ncronogramadeclasesyactividades\n\nsemana martes jueves\n12 claseagentes-llmyasig- clasequantization\nnacióndetarea04agentes\n13 aplicarquiz6yclasequan- clase unsupervised - pca y"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_003", "idx": 3, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "tization-unsupervised entregadeproyectoi\n14 evaluación presencial proyec- entrega tarea 04 agentes y\ntoi. evaluaciónpresencialproyectoi\n15 clase virtual unsupervised - revisión virtual de tarea 04\npca, asignación de proyecto agentes\nii y asignación de tarea 05\n\nautoencoder-quantization\n16 clasesesgosdeai\n17 semanacolchón semanacolchón figura 1. modelo de red neuronal preentrenado para la clasificación de\n18 exameni entregaproyectoii eventosdecolisión.\n2\n\ncuadroii\nejemplosimplificadodetokenización\npalabra token idnumérico\n\nlos los 105\n\nllm llm 2124\n\naprenden aprenden 893\n\npatrones patrones 5749\ncondiciones.elusodemodelospreentrenadosenestecontexto\nfacilita una generalización más robusta y una convergencia\nmás rápida durante el proceso de entrenamiento, lo cual\n\nresulta ventajoso en escenarios donde los datos etiquetados\nson limitados [4].\niii-b. tokenización\nfigura2. 3dsemanticfeaturespace\nen el procesamiento del lenguaje natural, cada palabra,\nsigno o símbolo debe transformarse en una representación\nnumérica para que pueda ser comprendida y procesada por iii-c. representación de tokens en un espacio vectorial\nlos modelos de lenguaje. este proceso se conoce como tokeuna vez que el texto ha sido tokenizado, cada token se\nnización, y consiste en dividir el texto en unidades mínimas\nconvierte en un número que sirve únicamente como idendenominadas tokens, que pueden corresponder a palabras,\ntificador dentro del vocabulario del modelo. sin embargo,\nsubpalabrasoinclusocaracteresindividuales.acadatokense\nestos valores numéricos carecen de significado semántico por\nleasignaunidentificadornuméricoúnicodentrodeunvocabusí mismos, ya que no reflejan las relaciones o similitudes\nlario previamente definido, lo que permite representar oracioentre las palabras. para que un modelo pueda comprender el\nnes completas como secuencias de números. existen diversas\ncontextoyelsignificadodellenguaje,esnecesariotransformar\nestrategias de tokenización, como la basada en subpalabras"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_004", "idx": 4, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "dichos identificadores en representaciones continuas que cap-\n(byte pair encoding o wordpiece), que buscan equilibrar la\nturen las propiedades semánticas y sintácticas de las palabras\n\neficiencia del vocabulario con la capacidad del modelo para\ndentro del texto. este proceso se logra mediante el uso de\nmanejar palabras desconocidas o de diferentes idiomas [5].\nembeddings, los cuales permiten a los modelos de lenguaje\n\nlatablaiimuestraunejemplosimplificadodelprocesode\n\naprenderrepresentacionesvectorialesquepreservanrelaciones\ntokenización,enelcualcadapalabradeltextoesdescompuesta\nde significado y proximidad contextual [6].\n\nen su correspondiente token y asociada a un identificador\n\nla figura 2 representa un espacio vectorial tridimensional\nnumérico dentro del vocabulario del modelo. este procedien el que las palabras se distribuyen según tres dimensiones\nmiento permite representar de forma estructurada los elemensemánticas: edad, género y realeza. cada punto del espacio\ntos lingüísticos, facilitando que el modelo procese el texto\ncorresponde a la proyección de una palabra en función de"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_005", "idx": 5, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "como una secuencia de valores discretos que posteriormente\nsus características aprendidas por el modelo, lo que permite\nserán transformados en vectores continuos mediante técnicas\nobservar relaciones de similitud y diferencia entre concepde embedding.\ntos. por ejemplo, términos como \"rey\" y \"reina\" se ubican\nla tabla iii resume algunos de los tipos más comunes de\npróximos entre sí en la dimensión de realeza, pero difieren\ntokenizaciónutilizadosenmodelosdelenguaje.cadaenfoque\nen la dimensión de género, ilustrando cómo los embeddings\ndifiere en el nivel de granularidad con que divide el texto:\ncapturanrelacionessemánticascomplejasdentrodeunespacio\ndesde unidades completas como palabras, hasta fragmentos\ncontinuo [6].\nmás pequeños como subpalabras, caracteres o incluso bytes\nindividuales. esta diversidad de métodos permite adaptar la\nrepresentación del texto según las necesidades del modelo, iii-d. similaridad entre vectores"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_006", "idx": 6, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "equilibrando la complejidad del vocabulario con la capacidad unavezquelaspalabrashansidotransformadasenvectores\npara manejar palabras desconocidas o símbolos especiales. dentro de un espacio continuo, es posible cuantificar su grado\nde similitud midiendo la distancia o el ángulo entre dichos\nvectores. en este contexto, dos vectores próximos representan\ncuadroiii palabras con significados semánticamente similares, mientras\ntiposcomunesdetokenización\n\nque aquellos que se encuentran alejados reflejan conceptos\ntipo ejemplo ventajaprincipal distintos o no relacionados. esta propiedad permite a los mopalabra \"losmodelos\" simplicidad\ndelos de lenguaje capturar relaciones latentes como analogías\ncaracter \"l\",.o\",\"s\" sinoov*\nsubpalabra .aprend-ïendo\" equilibrio vocabula- o asociaciones conceptuales, lo que ha sido fundamental para\nrio/contexto\ntareas como la búsqueda semántica, la traducción automática\nbyte-level bytesutf-8 soportacualquiersímbolo\nespacioenblanco \"hola\",\"mundo\" rápidoysimple y la inferencia contextual [7].\n3\niii-e. métricas más comunes -como el razonamiento contextual, la inferencia lógica o la\nadaptación a tareas no vistas durante el entrenamiento- no\nlas métricas más comunes para calcular similitud entre\nfueron programadas de forma directa, sino que surgen como\nvectores son:"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_007", "idx": 7, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "resultado del aprendizaje de patrones complejos a partir de\ndistancia euclidiana:\nenormes volúmenes de datos textuales y contextuales. este\n(cid:115)\nd(a,b)= (cid:88) (a -b )2 (1) fenómenohasidoobjetodecrecienteinterés,yaqueevidencia\ni i\ncómo la escala y la estructura de los modelos pueden dar\ni\n\nlugar a comportamientos no lineales y sofisticados en el\nmide que tan lejos están los puntos.\nprocesamiento del lenguaje natural [9].\n\nsimilitud del coseno\na-b\nsim(a,b)= (2) iv-b. capacidades de modelos de lenguaje\n∥a∥∥b∥\ncomprensión textual: interpretan el significado de pamideelánguloentrevectores:cuantomáspequeño,más labras y frases según el entorno en el que aparecen.\nsimilares. generación coherente de texto: pueden redactar, tradula más usada en modelos de lenguaje es la similitud de cir o resumir información manteniendo estilo y consiscoseno, ya que se enfoca en la dirección del vector más que tencia.\nen su magnitud. razonamiento y planificación: resulven problemas, explican pasos y trazan estrategias."}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_008", "idx": 8, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "iv. embeddings aprendizaje de prompt: adaptan su comportamiento a\npartir de ejemplos dados en la misma conversación (inlosembeddingssonrepresentacionesnuméricasdensasque\ncontext learning).\nasignanacadatoken-yaseaunapalabra,subpalabraoinclumultitarea: realizan traducción, clasificación, codificasounafrase-unvectorenunespaciocontinuodealtadimención, análisis o dialogo sin requerir reentrenamiento.\nsión. estas representaciones permiten capturar el significado\nsemántico y las relaciones contextuales entre los términos, de\nmodoquepalabrasconsentidossimilaresseubiquenpróximas iv-c. limitación de los modelos de lenguaje\nentre sí dentro del espacio vectorial. además, los modelos alucinaciones: generan respuestas convincentes pero\nmodernos son capaces de generar embeddings a nivel de frase incorrectas o inventadas.\no enunciado (sentence embeddings), los cuales condensan el memoria limitada: no recuerdan interacciones pasadas\nsignificado global de un texto. este tipo de representación más allá de su ventana de contexto.\nposibilita comparar oraciones, ideas o documentos en función conocimiento estático: su información proviene de los\ndesucontenidosemántico,enlugardebasarseúnicamenteen datos de entrenamiento.\ncoincidencias literales de palabras [8]. costos computacionales: requieren grandes recursos\nla figura 3 representa un ejemplo conceptual de embed- para entrenamiento e inferencia."}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_009", "idx": 9, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "dingsparafrasessimilaresenelquesemuestranlasdiferentes\nfases de forma general y simplificada, pasando des de la\n\nv. retrival-augmentedgeneration(rag)\npalabra,documentouoración,hastaelespaciodelembedding.\nel enfoque de recuperación aumentada de generación\n(retrieval-augmented generation, rag) combina la potencia\niv-a. capacidad de los modelos de lenguaje\ngenerativa de los modelos de lenguaje de gran escala (llm)\ngracias a su entrenamiento a gran escala y al uso de conunmóduloderecuperacióndeinformaciónexterna,conoarquitecturas basadas en transformers, los modelos de len- cido como retriever. este componente permite inyectar conoguaje de gran escala (llm) han desarrollado un conjunto de cimientorelevanteprovenientedebasesdedatosocolecciones\ncapacidades emergentes que trascienden las funciones para de documentos en el momento de la consulta, ampliando así\nlas que fueron diseñados explícitamente. estas habilidades lacapacidaddelmodeloparagenerarrespuestasmásprecisas,\nactualizadas y fundamentadas en evidencia. de esta manera,\nel sistema integra razonamiento generativo con recuperación\ninformativa,superandolaslimitacionesdelosllmentrenados\núnicamente con conocimiento estático [1].\nv-a. ingesta y chunking\nelprimerpasoenlaconstruccióndeunsistemaderecuperación aumentada de generación (rag) consiste en preparar\nlos documentos que servirán como fuente de información.\npara ello, el texto se segmenta en fragmentos manejables\ndenominados chunks, que suelen tener una longitud entre 200\ny 500 tokens, con el fin de preservar la coherencia semántica\nfigura3. ejemploconceptualdeembeddingsdefrasessimilares y facilitar la recuperación eficiente de información relevante.\n4\nposteriormente, cada fragmento se transforma en un vector filtrado híbrido; pinecone, un servicio en la nube que ofrece\nmediante un módulo de embeddings, el cual codifica su signi- indexaciónvectorialescalableymantenimientoautomáticode\nficado semántico en un espacio de alta dimensión. esta repre- índices;yfaiss(facebookaisimilaritysearch),unabibliosentaciónvectorialpermitemedirsimilitudesentreconsultasy teca desarrollada por meta que permite búsquedas eficientes\nfragmentosdetexto,habilitandolabúsquedacontextualbasada en grandes volúmenes de vectores mediante técnicas de cuanen significado y no en coincidencias literales [10]. tización y optimización de memoria. estas herramientas son\nv-a1. chunking tamaño fijo: el proceso de chunking de esencialesparaelfuncionamientodesistemasragmodernos,\ntamaño fijo consiste en dividir los documentos en segmentos alpermitirunarecuperaciónrápidayprecisadelcontextomás\ndetextodelongitudpredefinida,conelobjetivodeestandarizar relevante [14]-[16].\nlas unidades de información utilizadas en los sistemas de\nrecuperación. esta técnica permite equilibrar la granularidad\nv-c. consulta o recuperación\ndelcontenido:fragmentosdemasiadopequeñospuedenperder\nuna vez construida la base vectorial, el siguiente paso\ncontextosemántico,mientrasquefragmentosdemasiadogranconsisteenrealizarlarecuperaciónsemánticadeinformación.\ndesdificultanlabúsquedaeficienteyaumentanlaambigüedad\ndada una consulta o pregunta formulada por el usuario, esta\nen la recuperación. al mantener un tamaño constante, los"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_010", "idx": 10, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "se transforma en un embedding que captura su significado\nchunks facilitan la indexación vectorial y mejoran la precien un espacio de alta dimensión. posteriormente, se calcula\nsión de los modelos que emplean embeddings para comparar\nla similitud -comúnmente mediante la métrica del cosenoconsultas y pasajes de texto [11].\nentre este vector de consulta y todos los embeddings previav-a2. chunking recursivo: el chunking recursivo es una\nmente indexados. finalmente, el sistema devuelve los top-k\ntécnica avanzada utilizada para segmentar texto de manera\nfragmentos más cercanos, es decir, aquellos cuya representajerárquica y adaptativa, en lugar de emplear longitudes fijas.\nción vectorial es más similar a la de la consulta. este proceso\neste método divide los documentos siguiendo la estructura\npermiterealizarbúsquedasbasadasenelsignificadosemántico\nlingüística del contenido, como párrafos, oraciones o secciodel texto, en lugar de depender de coincidencias literales o\nnes,yaplicafragmentacionesadicionalescuandounsegmento\npalabrasexactas,loquemejorasignificativamentelaprecisión\nexcedeunlímitedetokensdefinido.deestemodo,sepreserva"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_011", "idx": 11, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "contextual en aplicaciones basadas en retrieval-augmented\nel contexto semántico relevante en cada fragmento, evitando\ngeneration (rag) [1].\ncortesarbitrariosquepodríanafectarlacoherenciadeltexto.el\nenfoquerecursivoresultaespecialmenteútilentareasderecuperaciónaumentadadegeneración(rag),dondemantenerla v-d. augmentación y generación (inyección de contexto)\nintegridad semántica de los chunks mejora significativamente elpasofinalenunsistemaretrieval-augmentedgeneration\nla precisión de la recuperación contextual [12]. (rag) consiste en integrar la información recuperada dentro\nv-a3. chunkingsimilaridadsemántica: elchunkingbasa- delprompt queseenviaráalmodelodelenguaje.paraello,se\ndo en similitud semántica emplea medidas vectoriales -prin- construye una plantilla o estructura de entrada que combina\ncipalmente la similitud del coseno- para dividir un texto la pregunta del usuario con los fragmentos de texto más\nen fragmentos coherentes según su significado, en lugar de relevantes obtenidos en la fase de recuperación. este contexto\nhacerlo por longitud o estructura gramatical. en este enfoque, adicional actúa como una fuente de conocimiento explícita\nsegeneranembeddingsdeoracionesopárrafosconsecutivos,y que guía al llm, permitiéndole generar una respuesta más\nse calcula la similitud coseno entre ellos. cuando la similitud precisa, coherente y sustentada en la evidencia. de esta\ncae por debajo de un umbral predefinido, se considera que el\ncontextocambiasignificativamente,estableciendoasíunnuevo\nlímitedechunk.estemétodoproducedivisionesmásnaturales\ndesde el punto de vista semántico, preservando la coherencia\ntemática y mejorando la recuperación contextual en sistemas\nbasados en retrieval-augmented generation (rag) [13].\nv-b. indexación\nla indexación vectorial es un proceso fundamental en\nlossistemasderecuperaciónaumentada(retrieval-augmented\ngeneration, rag), que permite almacenar y buscar eficientemente representaciones numéricas de documentos o fragmentos de texto en un espacio vectorial de alta dimensión.\nsu propósito es facilitar la recuperación de información semánticamentesimilaraunaconsultamediantelacomparación\nde vectores utilizando métricas como la similitud coseno o\nla distancia euclidiana. entre las soluciones más utilizadas\nse encuentran qdrant, un motor de búsqueda vectorial de\ncódigo abierto optimizado para búsquedas por similitud y figura4. diagramadelprocesodelrag\n5\nmanera,elmodelonodependeúnicamentedesuconocimiento referencias\npreentrenado, sino que se apoya en información actualizada y\n[1] p. lewis, e. perez, a. piktus, f. petroni, v. karpukhin, n. goyal,\nespecífica al dominio, lo cual mejora la fiabilidad y reduce la h. küttler, m. lewis, w. tau yih, t. rocktäschel, s. riedel, and\nalucinación de respuestas [17]. d.kiela,\"retrieval-augmentedgenerationforknowledge-intensivenlp\ntasks,\"advancesinneuralinformationprocessingsystems(neurips),"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_012", "idx": 12, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "la figura 4 ilustra de forma general el funcionamiento\n2020.\ndel proceso retrieval-augmented generation (rag). este [2] s. wang, y. qin, w. chen, z. wu, z. xi, y. xu, t. gui, x. qiu, and\nenfoque combina la recuperación de información relevante z.zhang,\"asurveyonlargelanguagemodelbasedautonomousagents,\"\narxivpreprintarxiv:2401.03428,2024.\ndesde una base vectorial con la generación de texto asistida\n[3] openai, \"gpt-4 technical report,\" arxiv preprint arxiv:2303.08774,\npor un modelo de lenguaje. a partir de una consulta del 2023.\nusuario, el sistema identifica los fragmentos más relacionados [4] y.lecun,y.bengio,andg.hinton,\"deeplearning,\"nature,vol.521,\npp.436-444,2015.\nsemánticamente, los integra dentro del prompt y genera una\n[5] r. sennrich, b. haddow, and a. birch, \"neural machine translation\nrespuestafundamentadaendichasevidencias.deestamanera, of rare words with subword units,\" in proceedings of the 54th annual\nel modelo puede ofrecer respuestas más precisas, actualizadas meetingoftheassociationforcomputationallinguistics(acl),2016,\npp.1715-1725.\ny contextualizadas que las obtenidas únicamente a partir del\n[6] t.mikolov,k.chen,g.corrado,andj.dean,\"efficientestimationof\nconocimiento interno del llm [1]. wordrepresentationsinvectorspace,\"inproceedingsoftheinternationalconferenceonlearningrepresentations(iclr),2013.\n[7] j. pennington, r. socher, and c. d. manning, \"glove: global vectors"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_013", "idx": 13, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "vi. beneficios for word representation,\" in proceedings of the 2014 conference on\nempirical methods in natural language processing (emnlp), 2014,\nel uso de arquitecturas basadas en retrieval-augmented pp.1532-1543.\ngeneration (rag) ofrece múltiples beneficios frente al uso [8] n.reimersandi.gurevych,\"sentence-bert:sentenceembeddingsusing\nsiamese bert-networks,\" in proceedings of the 2019 conference on\nde modelos de lenguaje puros. en primer lugar, permite\nempirical methods in natural language processing (emnlp), 2019,\nuna significativa reducción de las alucinaciones, ya que pp.3982-3992.\nel modelo genera sus respuestas apoyándose en evidencia [9] j. wei, y. tay, r. bommasani, c. raffel, b. zoph, s. borgeaud,\nd. yogatama, m. bosma, d. zhou, d. metzler, e. chi, t. hashimoto,\ndocumental verificable en lugar de depender únicamente de\no.vinyals,p.liang,j.dean,andw.fedus,\"emergentabilitiesoflarge\nsuconocimientoimplícito.además,posibilitalaactualización languagemodels,\"arxivpreprintarxiv:2206.07682,2022.\ncontinuadelconocimiento,dadoquelabasederecuperación [10] v.karpukhin,b.oguz,s.min,p.lewis,l.wu,s.edunov,d.chen,\nandw.tauyih,\"densepassageretrievalforopen-domainquestionanspuede ser renovada con información reciente sin necesidad\nwering,\"inproceedingsofthe2020conferenceonempiricalmethods\nde reentrenar el modelo. este enfoque también contribuye a innaturallanguageprocessing(emnlp),2020,pp.6769-6781.\nuna mayor eficiencia de costos, al disminuir la necesidad de [11] g. izacard and e. grave, \"leveraging passage retrieval with generative models for open domain question answering,\" in proceedings of"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_014", "idx": 14, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "entrenamientosextensivosyaprovecharmodelospreexistentes\n\nthe 16th conference of the european chapter of the association for\ncombinados con fuentes dinámicas de datos. finalmente, el computationallinguistics(eacl),2021,pp.874-880.\nparadigma rag favorece la aplicabilidad en dominios espe- [12] l. gilardi and d. steiner, \"recursive chunking strategies for improved context retrieval in large language models,\" arxiv preprint arcializados,permitiendoadaptarelcomportamientodelsistema\nxiv:2309.02706,2023.\na contextos como medicina, derecho o ingeniería mediante la [13] y. liu, s. kumar, and p. gupta, \"semantic chunking with cosine\nincorporación de bases de conocimiento específicas [18]. similarity for enhanced context preservation in rag systems,\" arxiv\npreprintarxiv:2403.11892,2024.\n[14] j. johnson, m. douze, and h. jégou, \"billion-scale similarity search\n\nvii. casosdeuso withgpus,\"ieeetransactionsonbigdata,vol.7,no.3,pp.535-547,\n2019.\nlos sistemas basados en retrieval-augmented generation [15] p.s.inc.,\"pineconedocumentation,\"https://docs.pinecone.io/,2024.\n(rag)presentanaplicacionesprácticasenmúltiplesdominios. [16] q.team,\"qdrant:vectordatabasedocumentation,\"https://qdrant.tech/\ndocumentation/,2024.\npor ejemplo, en el ámbito corporativo, los asistentes empre-\n[17] g. izacard, p. lewis, m. lomeli, l. hosseini, f. petroni, t. schick,\nsariales enriquecidos pueden ofrecer información precisa y s. riedel, and d. kiela, \"atlas: few-shot learning with retrieval augcontextualizada a partir de bases de conocimiento internas, mentedlanguagemodels,\"arxivpreprintarxiv:2208.03299,2022.\n[18] y.gao,s.li,j.lin,j.callanetal.,\"retrieval-augmentedgeneration\nmejorando la productividad y la toma de decisiones. en el\nforlargelanguagemodels:asurvey,\"arxivpreprintarxiv:2312.10997,\ncampo de la investigación, los rag facilitan la recuperación 2023.\nde literatura relevante y la síntesis de información compleja,\nacelerandoelanálisisdegrandesvolúmenesdedatostextuales.\nasimismo, en el área de soporte al cliente, estos sistemas"}
{"id_doc": "DOC_043", "segmentacion": "B", "chunk_id": "DOC_043_B_015", "idx": 15, "autor": "Kevin Carranza Jiménez", "fecha": "2025-10-21", "tema": "Profundización en LLM, RAG y agentes inteligentes; análisis de tokenización, embeddings, chunking y aplicaciones prácticas en recuperación aumentada de generación.", "texto": "permiten generar respuestas fundamentadas y coherentes a\nconsultas de usuarios, reduciendo errores y mejorando la\nexperiencia de atención mediante información verificada y\nactualizada [18].\n\nviii. tarea4:agenteconversacional\nal final de la clase se presentó la asignación y revisión del\nenunciadodelatarea4,centradaeneldesarrollodeunagente\nconversacional. la fecha de entrega se ha establecido para el\n\njueves 6 de noviembre."}
{"id_doc": "DOC_044", "segmentacion": "B", "chunk_id": "DOC_044_B_000", "idx": 0, "autor": "Nelson Rojas Obando", "fecha": "2025-10-23", "tema": "Introducción al aprendizaje no supervisado y quantization como técnica de optimización de modelos de deep learning, reduciendo tamaño y consumo computacional sin pérdida significativa de precisión.", "texto": "apuntes de la clase del 23 de octubre de 2025\n\ncursodeinteligenciaartificial\n\nnelson rojas obando\nestudiante ingeniería en computación\nnelson.rojas@estudiantec.cr\n\nresumen-este informe presenta una síntesis de los temas iii-a. ejemplo contextual\nabordadosenlasesióndel23deoctubredelcursodeinteligencia\nun modelo como llama 2 posee más de 70 mil millones\nartificial, centrada en el cierre del aprendizaje supervisado y\nla introducción al proceso de quantization como técnica de deparámetros,loqueequivaleaaproximadamente28gbsolo\noptimización de modelos de aprendizaje profundo. para almacenarlos en disco. cargar ese modelo en memoria\nindex terms-inteligencia artificial, quantization sería inviable sin una gpu especializada, por lo que la\n\nquantization se convierte en una alternativa para reducir el\n\ni. introducción tamaño y mantener la funcionalidad.\ndurante la clase se abordaron temas de actualidad relaiv. representaciónnumérica\ncionados con la evolución de los modelos de lenguaje y su\nimpacto en el futuro del internet. con esta sesión concluye iv-a. números enteros\nla sección del curso dedicada al aprendizaje supervisado. a los computadores representan los números utilizando separtir de este punto, los contenidos se centran en métodos de cuencias de bits. con n bits se pueden representar 2n valores\naprendizajenosupervisado,esdecir,aquellosquenodependen distintos.\nde etiquetas o resultados predeterminados para evaluar la por ejemplo, con 3 bits se pueden representar los números\ncalidad del aprendizaje del modelo. del 0 al 7.\nel formato más común para representar números enteros"}
{"id_doc": "DOC_044", "segmentacion": "B", "chunk_id": "DOC_044_B_001", "idx": 1, "autor": "Nelson Rojas Obando", "fecha": "2025-10-23", "tema": "Introducción al aprendizaje no supervisado y quantization como técnica de optimización de modelos de deep learning, reduciendo tamaño y consumo computacional sin pérdida significativa de precisión.", "texto": "ii. aspectosadministrativos\ncon signo en cpus es el complemento a dos, donde:\nsemencionólaintegracióndeherramientascomochatgpt el bit más significativo indica el signo (0 = positivo, 1 =\natlas para chrome, que reflejan cómo las empresas están negativo). los demás bits representan el valor absoluto.\norientando sus estrategias hacia la adopción de modelos de\nlenguaje extensos (llms) como núcleo de sus servicios 4.2. números de punto flotante\ndigitales. los números de punto flotante se utilizan para representar\nasimismo, se compartieron noticias institucionales sobre valores reales que no pueden expresarse de manera exacta\nla rama ieee del tecnológico de costa rica, que organiza con enteros. en la norma ieee 754, un número flotante se\nreunionesperiódicasentredistintasuniversidades.elpropósito representa mediante tres componentes principales: el signo, el\nprincipal es identificar fuentes de financiamiento para eventos exponente y la mantissa (también conocida como fracción o\ntecnológicos, especialmente aquellos destinados a llevar co- significando).\nnocimiento a zonas rurales o con menor acceso. también se\nanunciólarealizacióndeuntallerdeteambuildingeldomingo parte descripción bitstípicos(float32)\nsigno(s) indicasielnúmeroespositivoonegativo 1\n9 de noviembre, con un costo de $20, que incluye almuerzo\nexponente(e) determinalaescalaorangodelnúmero 8\ny transporte. mantissa(m) definelaprecisiónopartefraccionaria 23"}
{"id_doc": "DOC_044", "segmentacion": "B", "chunk_id": "DOC_044_B_002", "idx": 2, "autor": "Nelson Rojas Obando", "fecha": "2025-10-23", "tema": "Introducción al aprendizaje no supervisado y quantization como técnica de optimización de modelos de deep learning, reduciendo tamaño y consumo computacional sin pérdida significativa de precisión.", "texto": "cuadroi\n\niii. temaprincipal:quantization estructuradelformatoieee754de32bits.\nquantization es una técnica de optimización de modelos de\naprendizaje profundo que busca reducir el tamaño y el con- el valor real que representa el número en punto flotante se\nsumo de recursos computacionales de un modelo sin compro- calcula mediante la siguiente ecuación:\nmeter significativamente su precisión. la idea es convertir los\nparámetros del modelo (usualmente almacenados en formato x=(-1)s×(1+m)×2(e-127)\nde punto flotante de 32 bits, float32) a representaciones de\nmenor precisión, como int8, int4 o incluso int1, dependiendo donde:\ndel nivel de compresión deseado. s es el bit de signo.\nesto permite ejecutar modelos de gran tamaño en hardware m es la fracción o mantissa normalizada.\ncon recursos limitados (por ejemplo, dispositivos móviles o e es el exponente con un sesgo de 127 (en el caso de\nmicrocontroladores). float32).\neste formato permite representar números muy grandes o ix. conclusiones\nmuy pequeños, aunque implica un mayor uso de memoria y\nel estudio del quantization permite comprender cómo los\nrecursos computacionales en comparación con representaciomodelos de inteligencia artificial pueden adaptarse a las limines de menor precisión."}
{"id_doc": "DOC_044", "segmentacion": "B", "chunk_id": "DOC_044_B_003", "idx": 3, "autor": "Nelson Rojas Obando", "fecha": "2025-10-23", "tema": "Introducción al aprendizaje no supervisado y quantization como técnica de optimización de modelos de deep learning, reduciendo tamaño y consumo computacional sin pérdida significativa de precisión.", "texto": "taciones del hardware sin comprometer significativamente su\ndesempeño.estatécnicarepresentaunpuntodeconexiónentre\n\nv. quantizationderedesneuronales\nel desarrollo teórico de los algoritmos y su aplicación real en\nen redes neuronales, las matrices de pesos y sesgos están\nsistemas de producción, donde los recursos computacionales,\nrepresentadascomoflotantes.elprocesodequantizationbusca\nlaenergíayeltiempodeinferenciasonfactoresdeterminantes.\n\nconvertir esos valores a enteros para reducir memoria y\nacelerar la inferencia.\nv-a. etapas del proceso\nquantize: los valores en punto flotante se transforman\na enteros.\ninferencia el modelo realiza sus cálculos con aritmética\nentera.\n\ndequantize los resultados se transforman nuevamente a\nflotantes para la siguiente capa.\nel desafío está en mantener la precisión del modelo. los\nhardware modernos (gpu, tpu, cpu vectoriales) incluyen\nsoporte para operaciones de baja precisión (por ejemplo, int8)\npara facilitar este proceso.\n\nvi. tiposdequantization\nvi-a. quantization simétrica\nusa un rango centrado en cero:\nvi-b. quantization asimétrica\nutiliza un rango desplazado [α,β]:"}
{"id_doc": "DOC_044", "segmentacion": "B", "chunk_id": "DOC_044_B_004", "idx": 4, "autor": "Nelson Rojas Obando", "fecha": "2025-10-23", "tema": "Introducción al aprendizaje no supervisado y quantization como técnica de optimización de modelos de deep learning, reduciendo tamaño y consumo computacional sin pérdida significativa de precisión.", "texto": "vii. estrategiasyvariantes\nvii-a. dynamic quantization\nlaescalayelrangosecalculanentiempodeinferencia.se\naplican factores estadísticos derivados del conjunto de datos\nde prueba (\"calibration set\").\nvii-b. post-training quantization (ptq)\ndespués del entrenamiento, se insertan observadores (observers) en el modelo para analizar las salidas de cada capa\ny determinar los mejores parámetros de escala y punto cero.\neste proceso no requiere reentrenamiento y es rápido, aunque\npuede perder algo de precisión.\nvii-c. quantization-aware training (qat)\nsimula la quantization durante el entrenamiento. el modelo\naprendeacompensarloserroresintroducidosporlareducción\ndeprecisión,porloquemantieneunrendimientosuperiortras\nel proceso.\n\nviii. ventajasdelquantization\nmenor consumo de memoria: los modelos comprimidos\nse cargan más rápido.\nmenor tiempo de inferencia: cálculos más simples.\nmenor consumo energético: ideal para dispositivos embebidos o móviles.\nportabilidad: permite ejecutar modelos complejos en\n\nhardware limitado."}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_000", "idx": 0, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "notas de clase\n\ninteligenciaartificial-23deoctubre-semana12\nluis alfredo gonza'lez sa'nchez\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\n2021024482 gonzal3z.luis@estudiantec.cr\n\nabstract-neural network quantization is a vital technique in modelos de aprendizaje automa'tico desarrollados en distinaithatreducesmodelsizeandcomputationalcostbyconverting tos frameworks como pytorch o tensorflow en una repreweights and activations from floating-point to lower-precision\nsentacio'n intermedia esta'ndar y eficiente. esta representacio'n\nformats, such as integers. this process enables deployment on\n\nfacilita la interoperabilidad y el despliegue de modelos en\nresource-constrained devices, like mobile or embedded systems,\nwhile maintaining high accuracy. different methods include diferentes plataformas y hardware mediante optimizaciones\nsymmetricandasymmetricquantization,withstrategiestailored en c++ u otros lenguajes, asegurando que el mismo modelo\nto specific data distributions and hardware constraints. dy- pueda ejecutarse con alto rendimiento en entornos variados.\nnamic, granular, and post-training quantization further refine\nconsiderandoloanterior,lasplataformasposeendiversaslimthis approach by adjusting intervals per layer, per sample, or\nitacionesyrendimientotantoensoftwarecomoenhardware,si\nafter training, respectively. these techniques involve calculating\nscaling factors and zero points to effectively map high-precision se entrenan modelos grandes, posiblemente un celular no este\nvaluestolower-bitrepresentations,introducingminimalaccuracy adaptado para soportar dicho modelo, para ello se observara\nloss. overall, quantization enhances efficiency, reduces power el concepto de quantization.\nconsumption, and facilitates real-time ai applications, making\nit a cornerstone of practical deep learning deployment. iii. quantization\nindex terms-quantization in neural networks,model com- suponga que se tiene un modelo de deep learning con\npression,qat quantization techniques,integer representation\nmuchas capas, por ejemplo , llama 2 , con 70 mil millones\nde parametros aproximadamente, si cada parametro es de 32"}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_001", "idx": 1, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "i. introduction bits, se obtiene un taman˜o aproximado de 28 gb para solo\nalmacenar el modelo, ¿co'mo podr'ıamos cargarlo a memoria?\nlacuantizacio'nenredesneuronalesesunate'cnicaesencial\nuna alternativa es comprar una gpu con dicho taman˜o para\npara mejorar la eficiencia del co'mputo y reducir el taman˜o\nel procesamiento del modelo, pero gpus que soporten esos\nde los modelos, principalmente transformando los datos de\ntaman˜os son costosas , lo que se busca es reducir el taman˜o\npunto flotante a formatos de menor precisio'n, como enteros.\ndel modelo, una de sus te'cnicas es quantization\nesta transformacio'n permite que los modelos se ejecuten de\nmanera ma's ra'pida y con menor consumo de memoria, lo a. definicion\ncual es fundamental para desplegar inteligencia artificial en quantization es una te'cnica de compresio'n de modelos de\ndispositivos con recursos limitados, como mo'viles y sistemas aprendizajeautoma'ticoquereduceelnumerodebitsutilizados\nembebidos. en el presente documento, se busca resumir la para representar los para'metros del modelo, transformando\ninformacio'n vista en la clase del 23 de octubre,donde se ha los valores de punto flotante a representaciones de menor\nrevisado co'mo diferentes me'todos de cuantizacio'n -desde la precisio'n, generalmente enteros de 8, 5, 2 o incluso 1 bit.\nsime'trica y asime'trica hasta la dina'mica, granulada y post- lo que se busca es disminuir el taman˜o y la complejidad\nentrenamiento-manejanlaconversio'ndepesos,activaciones computacionaldelmodelo,manteniendounaprecisio'ncercana\ny sesgos, optimizando el balance entre precisio'n y eficiencia. al original. no se debe de confundir como una te'cnica de\nadema's, se menciona co'mo te'cnicas como la cuantizacio'n redondear pesos, sino de convertir y ajustar los tipos de datos\nconscienteduranteelentrenamiento(qat)ayudanamantener paraoptimizarelbalanceentretaman˜o,velocidaddeinferencia\nla precisio'n del modelo al considerar el efecto de la cuanti- yprecisio'n.sebuscauntradeoffoptimoentrecapacidadesdel\nzacio'n desde el inicio del aprendizaje. modelo vs rendimiento.\nb. ventajas"}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_002", "idx": 2, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "ii. brevedefinicio'ndeonnix\n- menor consumo de memoria al cargar los modelos en\n\npara continuar el tema de quantization en supervised learn- memoria\ning , es importante entender la herramienta onnix, suponga - permite insertar el modelo en sistemas con recursos\nun modelo llm ya entrenado¿co'mo empieza a funcionar el limitados / con propo'sito especifico, como celulares o\nproductoosistema?laherramientaonnixpermiterepresentar embebidos\n\nasignan rangos de valores flotantes a niveles discretos\nenteros.\n- cuantizacio'n de entradas: las entradas a cada capa\ntambie'nseconviertenaenterosparamantenerlacoherencia en la representacio'n y facilitar operaciones eficientes.\nfig.1. partesdeunnu'meropuntoflotante - cuantizacio'n del sesgo (bias): los te'rminos de sesgo,\nque son sumados en cada neurona, se transforman de\nfloat.\n- genera un menor tiempo para hacer las inferencias, sus - normalizacio'ndelrango:sedefinenvaloresma'ximosy\ndatos son ma's simples\nm'ınimosparapesos,entradasysesgos,quecorresponden\n- menor consumo energ'ıa debido a menor complejidad de a los valores l'ımite de la representacio'n entera (por\ncomputacio'n\nejemplo, el rango de int8). esto asegura que los valores\ncuantizados este'n dentro de rangos representables."}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_003", "idx": 3, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "iv. breverepasoalasoperacionesconbits\n- ca'lculo en espacio entero: las operaciones de la capa\nse dara' un breve repaso a la manipulacio'n de bits en (multiplicacio'nysuma)serealizanenenteros,generando\nsistemas computacionales para entender mejor el proceso de un vector cuantizado.\nquantization - des-cuantizacio'n: despue's de la capa, los valores encon2n bitssepuedenrepresentar2n valoresdistintos.esto teros se convierten nuevamente a punto flotante para\nsignifica que, por ejemplo, con 3 bits es posible representar continuar con el procesamiento de modo que las capas\n23 =8 nu'meros diferentes. siguientes no requieren conocer el esquema de cuantiun ejemplo ba'sico que se vio' en clase es de conversio'n de zacio'n aplicado.\nbinario a decimal es el nu'mero 6, que en binario se escribe\ndurante la dequantization es donde puede ocurrir pe'rdida de\ncomo 110. la conversio'n se realiza sumando las potencias de\nprecisio'n, ya que la conversio'n entre representaciones intro2 correspondientes a los bits activos (1) segu'n su posicio'n:\nduceaproximaciones.sinembargo,elobjetivoesquelasalida"}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_004", "idx": 4, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "cuantizadasealosuficientementecercanaalaoriginalparano\n6=1×22+1×21+0×20 =4+2+0=6 afectarelrendimientodelmodelo.esteprocesoesbeneficioso\n\nya que permite que modelos originalmente pesados funcionen\ncada d'ıgito binario representa una potencia de 2, comen- eficientemente con menor consumo de memoria y tiempo\nzando desde la derecha con la potencia 0. de co'mputo, esencial sistemas embebidos o con capacidad\nlos nu'meros enteros en sistemas digitales se representan limitada. es importante aclara que las capas que siguen a una\nnormalmente en complemento a 2, donde el bit ma's significa- capa cuantizada generalmente no requieren modificaciones ni\ntivo indica el signo: 0 para positivo y 1 para negativo. esto conocen directamente la cuantizacio'n aplicada, manteniendo\nfacilitarealizaroperacionesaritme'ticasconnu'merosnegativos transparencia en la mayor'ıa de frameworks.\nusando operaciones binarias esta'ndar.\n\nvi. tiposdequantization\npara nu'meros en punto flotante, la representacio'n se divide\nen tres partes: signo, exponente y mantisa (fraccio'n). el valor los tipos de consonantizan son los siguientes:\ndecimal se calcula aproximadamente como: - quantizationsime'trica:mapeavalorespositivosynegativos de un rango m'ınimo a ma'ximo que incluye el cero.\naqu'ı, el valor cero real se mapea exactamente a cero\nvalor=(-1)signo×(1+mantisa)×2exponente-bias\nentero. esto simplifica el manejo de pesos y activaciones\nesta te'cnica de representacio'n permite expresar un amplio con signo, aplicando la misma escala en ambos lados del\nrangodenu'merosrealesconprecisio'nlimitadayeficienciaen cero.\nalmacenamiento mediante manipulacio'n de bits. - quantization asime'trica: mapea valores entre 0 y un\nobserve la figura 1 donde se puede observar las partes valor ma'ximo entero, pero el valor m'ınimo real no se\ndel numero flotante de 32 bits ahora bien , considerando las mapea a cero, sino a un valor entero llamado zero point\npartes del numero punto flotante, es importante detallar que (z), que representa el valor neutro o \"offset\" de la\nla precisio'n dada por la mantiza se va a disminuir con cuantizacio'n.estopermiterepresentarvaloresconundequantization . splazamiento, u'til cuando los valores no esta'n centrados\nen cero."}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_005", "idx": 5, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "v. procedimientodequantizationenmodelosde las fo'rmulas para la cuantizacio'n simetrica y asime'tricas se\nredesneuronales describen a continuacio'n : se toma un rango [b,a] y se mapea\npasos generales del procedimiento: a un rango de salida, se calcula el para'metro de escalado y\npor ultimo se calcula el nu'mero neutro del mapeo.\n- transformacio'n de pesos:lospesosdelared,originalmente en formato de punto flotante (float), se convierten - x q =clamp (cid:0)(cid:4)x s f (cid:5) +z; 0; 2n-1 (cid:1)\na valores enteros mediante mapas de cuantizacio'n que - x =valor flotante\nf\n- las convoluciones se realizan con mu'ltiples filtros que\naprenden valores y distribuciones distintas.\n- cada filtro detecta diferentes caracter'ısticas (features) de\nla imagen.\n- no es posible aplicar el mismo intervalo a,b para todos los filtros, por lo que se calcula un intervalo a,b\nespec'ıfico para cada filtro respetando su distribucio'n.\nquantization post-training:\n- se realiza despue's del entrenamiento, utilizando datos\nfig.2. tiposdequantizaton:sime'tricavsasime'trica\nnunca antes vistos por el modelo.\n- introduce un componente llamado observer, que obtiene\n- para'metro de escalado s: estad'ısticas de cada capa para calibrar las salidas y\ncalcular los para'metros de cuantizacio'n como la escala\nα-β\ns= (s) y punto cero (z).\n2n-1\n- permite cuantizar el modelo sin necesidad de re2n-1=el rango de salida\nentrenamiento completo.\n- para'metro neutro z:\n(cid:22) (cid:23) quantization aware training (qat):\nβ\nz = -1- s - me'todoavanzadodondelacuantizacio'nsesimuladurante\nel entrenamiento.\n- n es el nu'mero de bits. - el modelo aprende a compensar la pe'rdida de precisio'n\ny su respectiva des-cuantizacio'n : por la cuantizacio'n al utilizar la funcio'n de perdida para"}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_006", "idx": 6, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "actualizar los pesos que constantemente sufren de este\nx =s×(x -z)\nf q efecto.\npara la cuantizacio'n sime'trica : - mejora el rendimiento en modelos cuantizados para en-\n- x q =clamp (cid:0)(cid:4)x s f (cid:5) ; - (cid:0) 2n-1-1 (cid:1) ; 2n-1-1 (cid:1) tornos de baja precisio'n.\n- para'metro de escalado s: viii. conclusio'n\nabs(α)\ns= la informacio'n presentada demostro' la importancia para\n2n-1-1\nla optimizacio'n del uso de recursos en modelos de redes\n- n es el nu'mero de bits. neuronales,especialmentesisedeseaimplementarensistemas\ny su respectiva descuantizacio'n se brinda por la siguiente embebidos o con recursos limitados, en esta clase se aprendio\nformula que : tex\nx f =s×x q - la cuantizacio'n consiste en transformar pesos, activaciones y sesgos de punto flotante a representaciones de"}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_007", "idx": 7, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "vii. otrostiposdecuantizaciones\nmenor precisio'n, principalmente enteros, con el fin de\ncuantizacio'n dina'mica: la cuantizacio'n dina'mica se en- reducir taman˜o y acelerar la inferencia.\nfoca en cuantizar las activaciones de las neuronas seleccio- - existen distintos tipos de cuantizacio'n: sime'trica,\nnandoapropiadamentelosvaloresm'ınimos(a)yma'ximos(b) asime'trica, dina'mica, granulada y postpara el mapeo de cuantizacio'n de cada tensor. la estrategia entrenamiento,cadaunaconestrategiasespec'ıficaspara\nde seleccio'n del intervalo a,b es la siguiente: mapear y convertir datos.\n- para cuantizacio'n asime'trica, se seleccionan los valores\nextremos reales del tensor, es decir, b y a corresponden\nal ma'ximo y m'ınimo del tensor respectivamente.\n- para cuantizacio'n sime'trica, se toma el mayor valor en\nte'rminos absolutos y se define el intervalo como [-a,a],\ncentrado en cero.\n- esta te'cnica puede inducir un mayor error debido a la\nsensibilidad a valores at'ıpicos (outliers). una solucio'n es\nutilizar percentiles basados en la distribucio'n del tensor,\nexcluyendo los outliers y reduciendo el error cuadra'tico\nmedio (mse)."}
{"id_doc": "DOC_046", "segmentacion": "B", "chunk_id": "DOC_046_B_008", "idx": 8, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-10-23", "tema": "Quantization en redes neuronales: métodos simétricos, asimétricos, dinámicos y post-entrenamiento, con aplicación en modelos grandes como LLaMA 2 y despliegue en sistemas embebidos.", "texto": "cuantizacio'n granulada en convoluciones:"}
{"id_doc": "DOC_041", "segmentacion": "B", "chunk_id": "DOC_041_B_000", "idx": 0, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "inteligencia artificial\n\napuntesdeclase-21deoctubrede2025\n1st kendall rodr'ıguez camacho\nescuela de ingenieria en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nkenrodriguez@estudiantec.cr\n\nabstract-el presente documento contiene los apuntes de la tablei\nclase del martes 21 de octubre de 2025, que cubren conceptos calendarioprevistoparaelrestodelcurso\nclave sobre los modelos de lenguaje extensos (llms). los\napuntes explican co'mo los llms representan el conocimiento semana martes jueves\nmedianteembeddingsyespaciosvectoriales,eintroducente'cnicas 12 asignacio'ndetarea04(agentes) clasedequantization\ncomo retrieval-augmented generation (rag) y agentes in- 13 quiz 6, terminar tema quantiza- claseunsupervisedlearningyenteligentes que ampl'ıan las capacidades de los llms con infor- tion,empezarunsupervisedlearn- tregaproyecto01\nmacio'n externa y acciones auto'nomas. ingypca\n14 revisio'ndeproyecto01deforma entrega de tarea 04 (agentes) y\npresencial (se sacara' cita en un continuarconrevisio'ndeproyectos\n\ni. introduccio'n forms)\n15 clasevirtual,seasignaproyecto02 revisio'n tarea 04 (agentes) de"}
{"id_doc": "DOC_041", "segmentacion": "B", "chunk_id": "DOC_041_B_001", "idx": 1, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "ytarea05sobrequantization formavirtual\nlos modelos de lenguaje extensos (llms) han transfor16 claseriesgosdeia visitaamicrosoft\nmado la interaccio'n con la inteligencia artificial gracias a\n17 - -\nsu capacidad para generar texto coherente, traducir idiomas,\n18 examenpresencial entregadeproyecto02\nredactar co'digo y analizar informacio'n compleja. su funcionamientosebasaenlarepresentacio'nnume'ricadepalabras\ny frases en espacios vectoriales, donde los embeddings cap- b. asignacio'n de tarea 04\nturan relaciones sema'nticas y contextuales. se asigna la tarea 04, la cual consiste en desarrollar un\nsi bien los llms ofrecen capacidades sorprendentes, su asistente conversacional que se desempen˜e ante diferentes\nconocimiento es limitado a los datos de entrenamiento y preguntas basadas en una base de documentos (apuntes de\ncarecen de habilidades para actuar o buscar informacio'n acti- clase realizados por los estudiantes hasta la fecha).\nvamente. para superar estas limitaciones, se han desarrollado se requiere implementar te'cnicas de recuperacio'n y auenfoques como retrieval-augmented generation (rag), que mento de contexto (rag) y comparar emp'ıricamente los\nenriquece las respuestas con informacio'n externa relevante en resultados con distintos esquemas de segmentacio'n del texto.\ntiempo real, y agentes inteligentes basados en llms, capaces la fecha de entrega esta' prevista para el jueves 6 de\nde razonar, planificar y ejecutar tareas auto'nomas. noviembre.\neste documento explora estos me'todos y su evolucio'n,"}
{"id_doc": "DOC_041", "segmentacion": "B", "chunk_id": "DOC_041_B_002", "idx": 2, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "iii. fundamentosdelosllms\nmostrandoco'mosepuedepasardemodelospasivosasistemas\nque no solo comprenden el lenguaje, sino que tambie'n inter- a. funcionamiento general\nactu'an con el entorno, toman decisiones informadas y aplican los llms procesan los datos de entrada transforma'ndolos\nconocimiento actualizado. en representaciones nume'ricas que describen caracter'ısticas\nsema'nticas. cada palabra, s'ımbolo o cara'cter se convierte en\nuna secuencia de valores nume'ricos mediante la tokenizacio'n,\n\nii. aspectosdelcurso\n\npara luego ser procesados en redes neuronales profundas con\na. calendario previsto para el resto del curso millones o miles de millones de para'metros.\nla siguiente tabla i muestra la planificacio'n de las ac- b. del lenguaje al nu'mero\ntividades restantes del curso, indicando las fechas y tareas eltextodebeconvertirseenrepresentacionesnume'ricaspara\ncorrespondientes para cada semana. ser interpretado por el modelo. el proceso de tokenizacio'n\nnota: en caso de que no se realice la visita a microsoft, la divideeltextoenunidadesm'ınimasllamadastokens(palabras,\nclase\"riesgosdeia\"setrasladar'ıaaljuevesdelasemana15, subpalabras o caracteres), asignando a cada una un identifiy el examen se aplicar'ıa el jueves 20 de noviembre (semana cador u'nico. estos identificadores se transforman en vectores\n16). que los modelos utilizan como entrada."}
{"id_doc": "DOC_041", "segmentacion": "B", "chunk_id": "DOC_041_B_003", "idx": 3, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "tableii\ntiposdetokenizacio'n\n\ntipo ejemplo ventajaprincipal\nporpalabra \"losmodelos\" simplicidad\nporcara'cter \"h\",\"o\",\"l\",\"a\" sin palabras fuera del\nvocabulario(oov)\nsubpalabra(bpe,wordpiece) \"compu\",\"tadora\" equilibrioentrevocabularioycontexto\nbyte-level co'digoasciioutf-8 soporta cualquier fig. 2. proceso de generacio'n de embeddings desde palabra, oracio'n o\ns'ımbolooidioma documentohastaelespaciovectorial.\nespaciosenblanco \"hola\",\"mundo\" ra'pidoysimple\nc. fo'rmulas de similitud entre vectores\nparacompararlasimilitudodistanciaentrevectores,seutilizan diversas fo'rmulas matema'ticas, entre las ma's comunes:\n- distancia euclidiana: mide la separacio'n entre puntos en\nel espacio. para dos vectores a,b∈rn:\n(cid:118)\n(cid:117) n\n(cid:117)(cid:88)\nd(a,b)=(cid:116) (a\ni\n-b\ni\n)2\ni=1\n- similitud del coseno: mide el a'ngulo entre vectores y su\norientacio'nenelespacio,siendolama'susadaenmodelos\nde lenguaje:\na-b\nsim(a,b)=\nfig.1. representacio'ndetokensenunespaciobidimensionalyejemplode\n∥a∥∥b∥\noperacionessema'nticas.\nd. embeddings\nlosembeddingssonrepresentacionesnume'ricasdensasque\n\niv. representacio'ndelconocimiento\ncapturanelsignificadosema'nticoylasrelacionescontextuales\na. tokenizacio'n depalabras,oracionesodocumentoscompletos.estosvectores\npermiten comparar ideas, medir similitud y realizar operaenlosllmsseutilizandistintosenfoquesdetokenizacio'n,\nciones sema'nticas en un espacio continuo de alta dimensio'n.\ncada uno con caracter'ısticas particulares que afectan el\nel proceso de generacio'n de embeddings se puede resumir\nrendimiento del modelo: la tabla ii resume estos enfoques,\nen los siguientes pasos:\nsus ejemplos y ventajas principales.\n- entrada textual: la unidad de texto que se quiere repb. representacio'n en espacios vectoriales resentar, que puede ser una palabra, una oracio'n o un\ndocumento completo.\nuna vez tokenizado el texto, los identificadores se trans-\n- modelo de embeddings: un modelo que transforma la"}
{"id_doc": "DOC_041", "segmentacion": "B", "chunk_id": "DOC_041_B_004", "idx": 4, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "forman en vectores dentro de un espacio continuo de alta\nentrada en un vector nume'rico denso, capturando su\ndimensio'n. las palabras con significados similares se ubican\nsignificado y contexto.\npro'ximas entre s'ı, mientras que las palabras con significados\n- embedding resultante: el vector que representa la endistintos aparecen ma's alejadas.\ntrada en el espacio continuo. vectores cercanos indican\nesto permite medir similitud sema'ntica y realizar operaconceptos sema'nticamente similares.\nciones vectoriales, como analog'ıas entre conceptos, suma o\n- espacio de embeddings: el espacio vectorial donde cada\nresta de vectores.\nembedding ocupa una posicio'n. este espacio permite\ntalcomosemuestraenlafigura1,losvectoresrepresentan\nmedir similitudes y realizar bu'squedas por proximidad.\n\ntokens proyectados en un espacio bidimensional para facilitar\ncomo se ilustra en la figura 2, la figura representa el flujo\nla comprensio'n, ilustrando relaciones sema'nticas entre ellos.\nde generacio'n de embeddings: desde palabras, oraciones o\npor ejemplo, la conocida analog'ıa:\ndocumentosdeentrada,pasandoporelmodelodeembeddings,\nrey-hombre+mujer≈reina. hasta el vector resultante y su posicio'n en el espacio de\nembeddings.\nen la pra'ctica, estos vectores existen en un espacio de alta adema's, la figura 3 muestra un ejemplo simplificado de\ndimensio'n(ndimensiones),loquepermitecapturardemanera sentence embeddings proyectados en un plano bidimensional,\nma's precisa la informacio'n sema'ntica y contextual de las donde frases con significado similar aparecen pro'ximas entre\npalabras. s'ı.\nque inyecta conocimiento externo relevante en tiempo real.\nesto permite generar respuestas ma's precisas y coherentes,\naccediendo a informacio'n actualizada y fundamentada.\na. preparacio'n de los documentos\nlos documentos que se desean utilizar para la recuperacio'n\nse dividen en fragmentos llamados chunks, normalmente de\nentre 200 y 500 tokens. para evitar pe'rdida de informacio'n,\nlos chunks suelen tener un overlap entre s'ı.\n1) chunkingdetaman˜ofijo: sesegmentanlosdocumentos\nen trozos de longitud fija, respetando en la medida de lo\nposible los l'ımites de las frases, y se mantiene un overlap\npara preservar contexto.\n2) chunking recursivo: en este enfoque, los chunks no\nfig.3. ejemplodesentenceembeddingsenunespaciobidimensional se cortan estrictamente segu'n el taman˜o ma'ximo, sino que\nse ajustan para mantener la sema'ntica de las oraciones. se\ncomparan oraciones con la similitud del coseno y, si son"}
{"id_doc": "DOC_041", "segmentacion": "B", "chunk_id": "DOC_041_B_005", "idx": 5, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "v. capacidadesylimitaciones\nsuficientementesimilaressegu'nunumbral,secombinanenun\na. capacidades emergentes\nchunkma'sgrande,lograndounalmacenamientoma'seficiente\ngraciasasuentrenamientomasivoyalusodearquitecturas y contextual.\nbasadas en transformers, los llms presentan capacidades\nb. transformacio'n en embeddings\ncomo:\n- compresio'ntextual:interpretanelsignificadodepalabras cada chunk se convierte en un vector mediante un modelo\ny frases segu'n el entorno en el que aparecen. deembeddings.estosvectoresseutilizanparamedirsimilitud\n- generacio'n coherente de texto: pueden redactar, traducir sema'ntica y permitir la recuperacio'n eficiente de fragmentos\noresumirinformacio'nmanteniendoestiloyconsistencia. relevantes durante la consulta del usuario.\n- razonamiento y planificacio'n: resuelven problemas, ex- c. indexacio'n\nplican pasos y trazan estrategias simples.\n\nlos vectores resultantes se almacenan en bases vectoriales\n- aprendizaje en el prompt: adaptan su comportamiento a\nespecializadas, que pueden residir en memoria ram o disco:\npartir de ejemplos dados en la misma conversacio'n (incontext learning). - faiss: principalmente en ram, ra'pido para bu'squedas.\n- multitarea: realizan traduccio'n, clasificacio'n, codifi- - qdrant: almacenamiento en disco con soporte de\ncacio'n, ana'lisis o dia'logo sin requerir entrenamiento bu'squeda vectorial.\nadicional. - pinecone: almacenamiento en disco y nube, escalable.\nse almacena adema's la metadata asociada, como el texto\nb. limitaciones\noriginal del chunk, para permitir una recuperacio'n eficiente.\na pesar de sus capacidades, los llms presentan limitad. consulta o recuperacio'n\nciones importantes:\n- alucinaciones: pueden generar respuestas incorrectas o cuando llega una pregunta del usuario, el proceso consiste\ninventadas, especialmente cuando la informacio'n de en- en:\ntrada es ambigua o insuficiente. 1) transformar la consulta en un embedding.\n- memorialimitada:olvidaninformacio'nqueseencuentra 2) calcular la similitud con todos los embeddings indexafuera del contexto actual, no recordando interacciones dos.\nprevias a menos que se almacenen externamente. 3) seleccionar los top-k chunks ma's cercanos\n- conocimiento esta'tico: no tienen acceso a informacio'n sema'nticamente.\nposteriorasufechadecortedeentrenamiento,porloque\ne. augmentacio'n y generacio'n\nno esta'n actualizados en tiempo real.\n- altos costos computacionales: requieren hardware espe- paraenriquecerelpromptdelllm,loschunksrecuperados\ncializado y significativos recursos para entrenamiento e se organizan en una plantilla estructurada, que combina el\ninferencia eficiente, lo que puede limitar su uso pra'ctico. contextextra'ıdodelosdocumentosconlaquestiondelusuario.\nestaplantillaaseguraqueelmodelorecibatodalainformacio'n"}
{"id_doc": "DOC_041", "segmentacion": "B", "chunk_id": "DOC_041_B_006", "idx": 6, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "vi. retrieval-augmentedgeneration(rag)\nrelevantedemaneracoherente,permitie'ndolegenerarrespuesdadas las limitaciones de los llms tradicionales, los en- tas precisas y fundamentadas.\nfoques de retrieval-augmented generation (rag) entran en a modo de ejemplo, la figura 4 muestra la estructura\nescena. el enfoque rag potencia los llms conecta'ndolos de la plantilla, donde se pueden observar sus componentes\ncon un mo'dulo de recuperacio'n de informacio'n (retriever) principales: prompt, context y question.\nplanificar y actuar de manera auto'noma, ejecutando tareas en\nel mundo real, como consultar apis, buscar informacio'n o\ntomar decisiones basadas en conocimiento externo.\nesta capacidad se estructura en tres componentes principales:\na. memoria\nla memoria permite al agente mantener coherencia y contexto a lo largo de la interaccio'n:\n- corto plazo: ventana de contexto del modelo.\n- largoplazo:basesdedatosexternas,incluyendosistemas\nrag donde la informacio'n se divide en chunks y se\nfig.4. estructuradeundocumentorag\nrepresentan como embeddings para su recuperacio'n.\nb. planificacio'n\nla planificacio'n dota al agente de la habilidad de descomponer problemas complejos y razonar sobre mu'ltiples pasos:\n- chains of thought (cot): razonamiento secuencial paso\na paso.\n- treesofthought(tot):exploracio'ndemu'ltiplesposibles\ncaminos de razonamiento antes de tomar decisiones.\nc. accio'n\nfig.5. diagramadelflujoderagmostrandolapreparacio'ndedocumentos,\ngeneracio'ndeembeddings,indexacio'n,recuperacio'nygeneracio'nderespues- finalmente, la accio'n permite al agente ejecutar tareas\ntas. concretas y aplicar su razonamiento en el mundo real:\n- utilizacio'n de herramientas externas, como buscadores,\napis o sistemas rag.\nf. beneficios de rag\n- enriquecimiento de respuestas con informacio'n recuper-\n- reduccio'n de alucinaciones. ada en tiempo real, basada en chunks de documentos\n- actualizacio'n continua con informacio'n reciente. relevantes.\n- eficiencia en costos y tiempo de respuesta.\n- aplicabilidad en dominios especializados con infor- viii. conclusio'n\nmacio'n privada. los modelos de lenguaje extensos (llms) han revolucionado el procesamiento del lenguaje natural, permitiendo\ng. aplicaciones de rag\ntareas complejas como la generacio'n de texto coherente, el\n- asistentes empresariales enriquecidos: pueden consultar razonamiento contextual y la ejecucio'n multitarea sin necesidocumentacio'n interna y responder de forma precisa. dad de reentrenamiento.\n- investigacio'n: lectura automa'tica de papers, resu'menes y el uso de embeddings y espacios vectoriales permite que\ncitas. los llms comprendan relaciones sema'nticas profundas. adi-\n- soportealcliente:ana'lisisdeticketspreviosparagenerar cionalmente, te'cnicas como retrieval-augmented generation\nrespuestas coherentes y ra'pidas. (rag) mejoran su precisio'n y acceso a informacio'n actualla figura 5 ilustra el flujo general de un sistema rag, izada, mientras que los agentes inteligentes basados en llms\ndonde se integran la creacio'n de embeddings, la indexacio'n y les permiten actuar de manera auto'noma, planificar y utilizar\nlabu'squedavectorialparaenriquecerlasrespuestasgeneradas herramientas externas, superando la pasividad de los modelos\npor el llm. tradicionales.\nsi bien los rag mejoran significativamente el rendimiento a pesar de estas mejoras, los llms y sus extensiones ende un llm tradicional al proporcionarle informacio'n externa frentan limitaciones importantes, como memoria finita, costos\nyactualizada,estossistemassiguensiendopasivos:nopueden computacionales elevados y riesgo de alucinaciones. por ello,\nbuscaractivamenteinformacio'nenlawebnitomardecisiones su implementacio'n requiere un disen˜o cuidadoso y un uso\nauto'nomas. su funcio'n se limita a complementar la respuesta responsable, asegurando que sus capacidades se aprovechen\ndel llm con los datos recuperados. de manera eficiente y confiable."}
{"id_doc": "DOC_041", "segmentacion": "B", "chunk_id": "DOC_041_B_007", "idx": 7, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-10-21", "tema": "Fundamentos de los Modelos de Lenguaje Extensos (LLMs), representación mediante embeddings y espacios vectoriales, introducción a RAG y agentes inteligentes con aplicaciones prácticas.", "texto": "vii. dellmsaagentesinteligentes\nlosagentesbasadosenllmsrepresentanunpasoma'salla'\n\ndelosrags.mientrasquelosragssoloenriquecenrespuestas con informacio'n recuperada, los agentes pueden razonar,"}
{"id_doc": "DOC_045", "segmentacion": "B", "chunk_id": "DOC_045_B_000", "idx": 0, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-10-23", "tema": "Técnicas de cuantización en redes neuronales: reducción de parámetros, uso de enteros, métodos simétricos y asimétricos, y estrategias como QAT y post-training quantization para optimización de modelos.", "texto": "apuntes semana 12\n\napuntesdel23deoctubre\njuan pablo rodr'ıguez cano\n\nic-6200 inteligencia artificial\ntecnolo'gico de costa rica\njp99@estudiantec.cr\n\nabstract-la cuantizacio'n en una te'cnica en redes neuronales y hasta 1 bit. la cuantizacio'n resulta en un menor tiempo de\npara reducir el taman˜o de los para'metros de los modelos, inferencia y menor consumo de energ'ıa, adema's de facilitar\n\nprincipalmente transformando los datos de punto flotante a\nlaopcio'ndecorrerestosmodelosensistemaspequen˜oscomo\nenteros, lo cual adema's reduce el tiempo de computacio'n de\ndispositivos mo'viles o sistemas embebidos.\noperaciones. esta te'cnica es esencial para distribuir modelos en\n\nsistemas comerciales y ampliar la cantidad de plataformas que\na. representacio'n de nu'meros\npuedan correr estos modelos.\nindex terms-cuantizacio'n, punto flotante, reduccio'n de\nse suelen utilizar nu'meros en bloques de 8 bits para los\npara'mtetros.\nenteros, para representar nu'meros negativos se utiliza el complementoa2enloscomputadores.encontraste,paralospunto"}
{"id_doc": "DOC_045", "segmentacion": "B", "chunk_id": "DOC_045_B_001", "idx": 1, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-10-23", "tema": "Técnicas de cuantización en redes neuronales: reducción de parámetros, uso de enteros, métodos simétricos y asimétricos, y estrategias como QAT y post-training quantization para optimización de modelos.", "texto": "i. actividaddeieee\nflotantesseutilizaelieee-754,cuyotaman˜oderepresentacio'n\nes un evento anual que se dara' esta vez en noviembre es de 32 bits, se utiliza la siguiente fo'rmula.\nen la sabana. es una oportunidad para conocer sobre temas\ninnovadores en inteligencia artificial y biolog'ıa molecular. es 23\n(cid:88)\nuna oportunidad para crear contactos dentro de la industria v =(-1)sign×2e-127×(1+ b 23-i 2-i)\nya que los presentadores suelen ser receptivos al pu'blico y i=1\ndisponen de tiempo para hablar.\npara no perder tanta informacio'n se tiene el siguiente\nmecanismo:\n\nii. quantization\n1) antes de que las entradas lleguen a la siguiente capa se\nuna vez que entrenado un modelo de redes neuronales, se\n\ncuantizan los pesos\ndebe colocar en un sistema para la distribucio'n de este. para\n2) estos pesos se limitan a ciertos rangos, dependiendo de\nesto existen varias te'cnicas, entre ellas, una opcio'n comu'n es\nla cantidad de bits de la cuantizacio'n. lo que se quiere\nutilizar el framework onnx, que toma modelos escritos en\nes que la distribucio'n sea equivalente.\ndiferentes lenguajes y bibliotecas y se crea una versio'n que\n3) se hacen las operaciones con los datos de tipo entero.\nmaximiza la eficiencia de recursos y computacio'n utilizando\n4) alsalirdelacapa,sede-cuantizanlospesosparaquelas\nc++.\nsiguientes capas operen con nu'meros de punto flotante,\nelmecanismoporelcualsedisminuyeeslacuantizacio'ny\nsin \"saber\" que fueron cuantizados.\nse enfoca en el hecho que los para'metros de los modelos son\nrepresentados con tipos de datos de punto flotante, se reducen iii. tiposdecuantizacio'n\npara hacer los modelos ma's densos con te'cnicas especiales\n1) asime'trica → el valor de 0 corresponde al valor menor\npara no afectar mucho la precisio'n de la inferencia. aunque\ny el ma'ximo es el peso ma'ximo\nno es posible no introducir error, es necesario asumir esta\n2) sime'trica → el cero es el peso 0, el valor absoluto\ndesventaja para desplegar los modelos.\nma'ximo de los pesos se mapea a un extremo, si es"}
{"id_doc": "DOC_045", "segmentacion": "B", "chunk_id": "DOC_045_B_002", "idx": 2, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-10-23", "tema": "Técnicas de cuantización en redes neuronales: reducción de parámetros, uso de enteros, métodos simétricos y asimétricos, y estrategias como QAT y post-training quantization para optimización de modelos.", "texto": "llama 2 es un modelo muy popular y notorio por tener\nnegativo se mapea al valor ma's negativo dentro de los\nun taman˜o muy grande, tiene 70 mil millones de para'metros,\n\nvalores posibles con los bits\ncada uno esta' representado por un punto flotante de 32 bits,\nlo que resulta en 28gb que deber'ıan estar en memoria si\na. cuantizacio'n asime'trica\nse quisiera utilizar en una ma'quina local. esto claramente\nno es viable porque la mayor'ıa de ma'quinas comerciales x q =clamp(x s f +z;0;2n-1)\ncuentan con una capacidad menor a eso. adema's, las opera- x f =valorflotante\nciones que se se hacen con datos de punto flotante son muy z =-1× β\ns\nlentas en comparacio'n a datos representados por enteros. la *s es el para'metro de escalado\ncuantizacio'n hace una reduccio'n de los bits requeridos para s= α-β\n2b-1\nrepresentar cada para'metro y lo convierte a enteros, que se x =s(x -z) → permite volver al valor original con un\nf q\npueden representar en las siguientes configuraciones: 8, 5, 2 grado de error\nb. cuantizacio'n sime'trica\nrango: [-(2n-1),(2n-1)]\ns= abs(α)\n2n-1-1\nx =sx\nf q"}
{"id_doc": "DOC_045", "segmentacion": "B", "chunk_id": "DOC_045_B_003", "idx": 3, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-10-23", "tema": "Técnicas de cuantización en redes neuronales: reducción de parámetros, uso de enteros, métodos simétricos y asimétricos, y estrategias como QAT y post-training quantization para optimización de modelos.", "texto": "iv. estrategiasdeseleccio'ndelrango\n- cuantizacio'n dina'mica\n- ca'lculo estad'ıstico de cua'l sera' el valor de esa capa\n- se utiliza en la etapa post-training quantization\n- post training quantization\n- hay que tratar los pesos at'ıpicos porque puede confinar los dema's pesos en un rango muy pequen˜o e\nintroduce ma's error\n- se puede utilizar el percentil en vez del min y max\n- agregamos observers que se encargan de hacer la\nestad'ısticas, calibran todas las salidas de la capa\n- se hace con los datos de prueba\n- quantization aware training (qat)\n- insertarmo'dulosirrealesenlacomputacio'ndegrafo\ndel modelo para similar el efector de cuantizacio'n\ndurante el entrenamiento.\n- la funcio'n de perdida es usada para actualizar los\npesos que constantemente sufren.\n\n-"}
{"id_doc": "DOC_001", "segmentacion": "B", "chunk_id": "DOC_001_B_000", "idx": 0, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "inteligencia artificial\n\napuntesdeclases\nrodolfo david acun˜a lo'pez\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nrodolfoide69@estudiantec.cr\n\nabstract-en este documento podra' encontrar informacio'n a. yann lecun\n\nsobre la primera clase de ia donde se presentan distintas\n\nyann lecun es un investigador muy reconocido en el\nperspectivas sobre esta. desde la perspectiva de la ia podremos\n\ncampo de la inteligencia artificial y es considerado uno de\nencontrar conceptos como autonom'ıa y adaptabilidad. tambie'n\nintroducenlosprincipiosba'sicosdelaprendizajesupervisadoyno los tres padrinos del deep learning. es el fundador de las\nsupervisado, destacando su aplicacio'n para resolver problemas convolutionalneuralnetworks,untipoderedneuronalcapaz\ncomplejos. de procesar ima'genes de manera eficiente.\nindex terms-principios de la ia, conceptos de ia\nanteriormente, los costos computacionales para procesar\n\ni. introduccio'n ima'genes eran muy altos lo que limitaba su uso. por esta\nrazo'n, e'l disen˜o' una arquitectura que permit'ıa realizar este"}
{"id_doc": "DOC_001", "segmentacion": "B", "chunk_id": "DOC_001_B_001", "idx": 1, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "lainteligenciaartificialpuedeserunconceptomuyvariado\nprocesamiento con un costo computacional mucho menor, un\ndesde diferentes perspectivas. en el a'rea de la computacio'n\nmodelo ma's liviano y efectivo para tareas visuales.\npueden ser sistemas que puedan tener y mostrar comporactualmenteyannlecunesdirectordeproyectosenmeta,\ntamientos de forma \"inteligente\". en la primer clase pudimos\ndonde dirige todas lo relacionado con inteligencia artificial.\nver conceptos que nos permiten comprender las bases de esta,\nsu impacto en el a'rea fue reconocido con el premio turing,\nincluyendo distintas perspectivas sobre que' es la ia y co'mo\n\notorgado por sus contribuciones en el desarrollo y avance de\nse puede definir desde puntos de vista.\nlas redes neuronales.\n\nii. noticias\nb. ¿que' es la inteligencia?\na. sobre la ia\nnoexisteunconsensoclaroquedefinaque'eslainteligencia\n\nahora la ia ha llegado para facilitar el trabajo de los\nartificial. este concepto ha sido estudiado desde diversas\nprogramadores, por ejemplo, cursor es una herramienta que\ndisciplinas como la psicolog'ıa y la filosof'ıa, pero podemos\nayuda mucho en el area de la programacion."}
{"id_doc": "DOC_001", "segmentacion": "B", "chunk_id": "DOC_001_B_002", "idx": 2, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "decir de forma abstracta que la inteligencia puede verse de\nb. benchmark varias maneras, como un grupo de animales que trabajan\nswe-lancer es un benchmark el cual fue desarrollado para realizar una tarea espec'ıfica, o como la manera en que\npor openai y servia como herramienta para evaluar los los humanos piensan y se comportan. adema's estos sistemas\nmodelosdelenguajeentareasfreelanceextra'ıdasdediferentes pueden ser auto'nomos y adaptativos capaces de aprender y\nplataformas. este inclu'ıa ma's de 1400 tareas reales. ajustarse a nuevas situaciones.\n\niii. introduccio'nalainteligenciaartificial c. auto'nomo y adaptativo\n¿que' piensas cuando escuchas hablar de ia? ¿co'mo crees auto'nomo significa que no necesitamos realizar instrucque funciona? al inicio la inteligencia artificial pod'ıa basarse ciones para funcionar ya que es capaz de tomar decisiones\nen una lo'gica el cual estaba llena de instrucciones condi- y ejecutar acciones por s'ı mismo.\ncionales, donde se procesaban mu'ltiples datos para llegar a adaptativo implica que un sistema puede modificar su\nuna conclusio'n. con el tiempo se llego a utilizar el modelo comportamiento si el entorno o el espacio del problema\nde machine learning, que ba'sicamente a partir de datos, cambian.porejemplo,enclasesemostro' unaimagendeunas\ncomienza a identificar y resolver patrones. cuando hablamos hormigas que se apilaban unas sobre otras para alcanzar una\nde ia hay diferentes perspectivas de lo que es. la sociedad hoja. este comportamiento demuestra co'mo pueden adaptarse\npodr'ıa pensar que puede ser como un terminator que quiere a las condiciones de su ambiente para cumplir un objetivo.\ndominar al mundo, los amigos de otras carreras no relad. ¿que' puede ser la ia?"}
{"id_doc": "DOC_001", "segmentacion": "B", "chunk_id": "DOC_001_B_003", "idx": 3, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "cionasdasconcomputacionpuedenpensarqueeresunamente\nbrillantehaciendoalgunatareacontecnolog'ıaosilepreguntas un sistema que muestra un comportamiento \"inteligente\".\na otro de computacion puede que te vea como un vago gando porejemplo,laautomatizacio'ndetareasoalgoquenosayuda\nmucho dinero, y as'ı sucesivamente con diferentes analog'ıas. a resolver problemas complejos. tambie'n puede ser como ver\nunrobotquemueveunacajadeunlugaraaunlugarb.puede 1970s\nestar relacionado con simulacio'n, como la visio'n, escucha o - se popularizan los algoritmos evolutivos.\nsentidosnaturales.sonsolucionesquesoncapacesdeaprender - el robot freddy puede usar la percepcio'n visual.\nauto'nomamentemientrasseadaptananuevosdatos.entonces, - se inventa el lenguaje de programacio'n prolog para el\nen otras palabras ma's resumidas, puede ser: uso de reglas.\n- sistema mostrando comportamiento \"inteligente\" 1980s\n- algo \"inteligente\" que resuelva tareas complejas - esta'n las ma'quinas lisp para sistemas expertos.\n- simulacio'n de visio'n, escucha o sentidos naturales - redes neuronales a trave's de la introduccio'n de la retro-\n- soluciones adaptativas que aprenden de forma auto'noma propagacio'n.\nun ejemplo de lo que puede ser ia es observar carros de - se populariza la inteligencia de enjambre.\nuber que utilizan varias capacidades, como computer vision,\n1990s\nla cual sirve para reconocer diversos para'metros, tales como\nveh'ıculos,personas,sen˜alesdetra'nsito,distancias,entreotros. - td-gammon muestra el poder del aprendizaje por refuerzo que es como optimizaje a prueba y error.\notro ejemplo es la capacidad estad'ıstica que la ia puede\nemplear para extraer las caracter'ısticas de una imagen. - experimentos con coches auto'nomos.\n- auge de los bots de internet y la bu'squeda.\ne. ¿que' puede ser el corazo'n de la ia? 2000s\ncomobiensabemoslaiapuedeseralimentadademultiples - reconocimiento facial con redes neuronales.\nmaneras. sin embargo, llamamos como corazo'n de la ia a: - watson de ibm gana en jeopardy.\n- funcio'n matema'ticas - deteccio'n de movimiento avanzada de xbox kinect.\n- datos - alphago de google se convierte en campeo'n de go.\napesardequeesta'nestasdos,puedenserma'simportantelos - agentes para realizar tareas especificas.\ndatosyaquesinestos,nohayia.ademaslosdatospresentes"}
{"id_doc": "DOC_001", "segmentacion": "B", "chunk_id": "DOC_001_B_004", "idx": 4, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "v. machinelearning\ndeben ser buenos datos ya que si son erro'neos, las funciones\narthur samuel fue la persona en acun˜ar este termino en\nmatema'ticas no van a hacer nada.\n1959 el cual lo hace un juego llamado checkers. este hombre\nf. datos trabajo' para ibm y tenia que encontrar una manera de vender\ncomo mencionamos anteriormente, podemos decir que un el producto\nalgoritmo va a ser malo si tenemos datos que son incorrectos entonces el machine learning cuenta con dos aristas, las\no hay datos faltantes. un ejemplo puede ser el prono'stico cuales son ciencia y ingenier'ıa. la idea es que se construyen\n\ndel tiempo en cartago erroneos ya que hacen faltan datos maquinas que puedan hacer tareas solo infiriendo entre los\nestad'ısticos, por lo que genera una incorrecta prediccio'n. hay datos existentes, aproximamos una funcio'n. este sigue un envarios tipos de datos pero principlamente tenemos: foque estad'ıstico para entrenar modelos. tengamos en cuenta\nque las aproximaciones son u'tiles pero no perfectas.\n- los datos cuantitativos o sin sesgos que puede tomar\nuna cantidad exacta para que represente un para'metro"}
{"id_doc": "DOC_001", "segmentacion": "B", "chunk_id": "DOC_001_B_005", "idx": 5, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "vi. tiposdeaprendizajes\nmedible.\nhay varios tipos de aprendizajes, sin embargo solo men-\n- losdatoscualitativoscomoelolordeunafloroelsabor\ncionare' las dos vistas en clases.\nde una comida. estos son datos que no son sencillos de\nmedir y que puede variar entre los humanos. a. supervisado\n\niv. l'ineadeltiempo enestevoyatenerunconjuntosdedatosconcaracter'ısticas.\npor ejemplo, una casa con las variables x1, x2 y x3, que van\na continuacio'n se muestran algunos eventos a travez de los\na ser el numero de cuartos, metros cuadrado y cantidad de\nan˜os los cuales fueron mostrados en una imagen en clases.\nrobos. con esto podemos tener una tabla que me va a dar la\n1950s\nmuestra y me etiqueta.\n- se presenta el nombre de ia\n- se introduce el concepto de la red neuronal artificial.\n- seinventaelmodelodelperceptro'nysecreequevaaser\nel mejor. sin embargo, este no poda resolver problemas\nsencillos. resolv'ıa solo problemas lineales.\n1960s\n- se introducen modelos de machine learning.\n- el robot unimate trabaja en una l'ınea de montaje de\nautomo'viles.\n- el robot shakey tiene movimiento natural y habilidades\npara resolver problemas. fig.1. ejemplodeaprendizajesupervisado.\nla muestra o conjunto de datos tiene caracter'ısticas y mi\netiqueta que me asocia el conjunto de caracter'ısticas a un\nvalor.entonces,mimodeloesdetiposupervisadoporquehago\nuna aproximacio'n. podemos verlo como f(x) aproximado a y.\nsi el y esta' muy lejos, penalizo mi modelo para que en ese\npuntosemejoreelentrenamiento.estosmodelosquetratande\naproximar un valor, se llaman modelos de regresio'n. tambie'n\nhay modelos de clasificacio'n.\nb. no supervisado\nen estas a diferencia de las supervisadas, no tenemos\netiquetas. en otras palabras, no tengo quien me supervise el\naprendizaje. como por ejemplo, los algoritmos de clustering.\neste algoritmo lo que hace es que agrupa datos segu'n las caracter'ısticas que comparte, que generalmente usan algoritmos\nde distancia."}
{"id_doc": "DOC_001", "segmentacion": "B", "chunk_id": "DOC_001_B_006", "idx": 6, "autor": "Rodolfo David Acuña López", "fecha": "2025-08-07", "tema": "Principios fundamentales de la inteligencia artificial, autonomía, adaptabilidad y tipos de aprendizaje supervisado y no supervisado.", "texto": "references\n[1] apuntes de la clase de inteligencia artificial, profesor steven andrey\n\npachecoportuguez,institutotecnolo'gicodecostarica,2025."}
{"id_doc": "DOC_002", "segmentacion": "B", "chunk_id": "DOC_002_B_000", "idx": 0, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "inteligencia artificial\napuntesdelaclase07/08/2025\n\nfernando daniel brenes reyes\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\n2020097446@estudiantec.cr\n\nabstract-estos apuntes organizan y ampl'ıan el material b. impacto en el empleo y en programadores\nintroductoriodelaclasesobreinteligenciaartificial.seincluyen\nlaautomatizacio'nconiaesta' permitiendoaorganizaciones\nnoticias recientes , fundamentos te'cnicos , cuestiones pra'cticas,\nunal'ıneadetiempohisto'rica,yunadescripcio'ndelasprincipales reducir tiempo en tareas repetitivas (redaccio'n de correos,\nramasdelaprendizajeautoma'tico.eldocumentoesta' preparado generacio'n de reportes, tests ba'sicos de software). para proen formato ieee, con lugares marcados para figuras y sugeren- gramadores esto significa:\ncias bibliogra'ficas.\nindexterms-inteligenciaartificial,gpt-5,autosauto'nomos, - aumento de productividad: asistentes que generan esdatos, aprendizaje supervisado, aprendizaje no supervisado. queleto de co'digo y pruebas unitarias.\n- cambio en habilidades requeridas: mayor e'nfasis\nen disen˜o, validacio'n, e'tica, pruebas adversariales y\n\ni. introduccio'n\norquestacio'n.\nla inteligencia artificial (ia) es un campo multidisci- - riesgos: tareas de bajo nivel y rutinas repetitivas pueden\nverse desplazadas; se recomienda desarrollar habilidades\nplinario que combina informa'tica, estad'ıstica, matema'tica\nde alto valor (arquitectura, ingenier'ıa de datos, devops,\ny aspectos del dominio de aplicacio'n para crear sistemas\nml ops).\nque pueden percibir, razonar, aprender y actuar. en esta\ncompilacio'n ampliada se cubren conceptos teo'ricos, avances\nc. autos auto'nomos en california\nrecientes y aplicaciones pra'cticas relevantes para un curso\nintroductorio. california es uno de los centros donde empresas realizan\npruebas y despliegues de veh'ıculos auto'nomos; estos sistemas"}
{"id_doc": "DOC_002", "segmentacion": "B", "chunk_id": "DOC_002_B_001", "idx": 1, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "ii. noticiasycontextoreciente integran lidar, ca'maras, mapas hd y planificacio'n en\ntiempo real. las a'reas de intere's incluyen:\na. gpt-5 y modelos de lenguaje avanzados\n- sensores y fusio'n: lidar + ca'maras + radar + gps.\na mediados de 2025 emergieron nuevas generaciones de - percepcio'n: deteccio'n y clasificacio'n de peatones,\ngrandes modelos de lenguaje con capacidades multimodales, veh'ıculos y sen˜ales.\nmejor manejo del contexto y mejoras en razonamiento. estos - planificacio'n: toma de decisiones en entornos urbanos\nmodelos impactan fuertemente en herramientas de productivi- complejos.\ndad(resu'menes,generacio'ndeco'digo,asistenciadeescritura) - protocolos de emergencia: procedimientos para fallas\ny han generado debate acerca de su adopcio'n responsable y del sistema, intervencio'n humana y registro de eventos.\nefectos laborales.\nfig. 1. l'ınea simplificada deevolucio'n de modelos delenguaje: gpt-1 →\ngpt-2→gpt-3→gpt-4 fig.2. veh'ıculoauto'nomo:sensores,percepcio'n,planificacio'nycontrol."}
{"id_doc": "DOC_002", "segmentacion": "B", "chunk_id": "DOC_002_B_002", "idx": 2, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "iii. definicionesyconceptosba'sicos\na. ¿que' es inteligencia?\nnoexisteunadefinicio'nu'nicaaceptada.eniaoperativase\n\nsueleentendercomolacapacidaddeunsistemaparapercibir\nsu entorno, razonar, tomar decisiones auto'nomas y adaptarse\na cambios. distintos campos (psicolog'ıa, filosof'ıa, ciencias\nde la computacio'n) aportan matices: aprendizaje, simbolismo,\nfig.3. esquemadeunaredconvolucional\nrazonamiento probabil'ıstico, entre otros.\nb. auto'nomo vs. adaptativo v. datos:elcorazo'ndelaia\na. calidad y preprocesamiento\nauto'nomo: actu'a sin intervencio'n humana constante.\nadaptativo:modificasucomportamientobasa'ndoseennueva los datos deben ser:\ninformacio'n o retroalimentacio'n. - representativos del problema real.\nun sistema puede ser auto'nomo pero no adaptativo (p. ej. un - limpios: sin errores obvios (p. ej. mezcla celrobot con una ruta fija) o adaptativo pero no completamente sius/fahrenheit).\nauto'nomo (p. ej. un asistente que sugiere cambios que un - balanceados o bien tratados para evitar sesgos.\nhumano valida). - steven pacheco 2025 - si tenemos mal los datos, mala\nes la salida de nuestra funcio'n\nc. capacidad de generalizacio'n te'cnicas comunes: normalizacio'n, imputacio'n\n(media/mediana), deteccio'n y tratamiento de outliers,\nla generalizacio'n es la habilidad de un modelo de deingenier'ıa de caracter'ısticas y enriquecimiento.\nsempen˜arse bien sobre datos no vistos durante el entrenamiento. es el objetivo central al medir la utilidad pra'ctica b. sesgos y equidad\nde un modelo.\nlos datasets reflejan las desigualdades del mundo real."}
{"id_doc": "DOC_002", "segmentacion": "B", "chunk_id": "DOC_002_B_003", "idx": 3, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "un modelo entrenado con datos sesgados puede perpetuar\n\niv. deeplearningyredesneuronales discriminaciones. ejemplos pra'cticos:\na. perceptro'n y or'ıgenes - reconocimiento facial con peor desempen˜o en ciertos\ngrupos demogra'ficos.\nel perceptro'n (de'cada de 1950) es un modelo de unidad - modelos de cre'dito que penalizan poblaciones subreprede decisio'n lineal que calcula una combinacio'n ponderada de sentadas.\nentradas y aplica una funcio'n de activacio'n. funciona bien buenas pra'cticas: auditor'ıas de sesgo, conjuntos de prueba\nparaproblemaslinealmenteseparables,peronopuederesolver estratificados, transparencia en datos y procesos.\nproblemas no lineales (ej. xor).\nc. ejemplo del profe: celsius vs fahrenheit\nb. redes profundas y arquitecturas comunes un error cla'sico en datasets es mezclar unidades. si un\n\ncampo de temperatura contiene valores en ambas escalas sin\nel deep learning usa redes con muchas capas: perceptrones\netiqueta, el modelo puede aprender patrones erro'neos. es\nmulticapa (mlp), redes convolucionales (cnn) y redes reesencial normalizar unidades y validar rangos.\ncurrentes (rnn/lstm/transformer). cada arquitectura esta'\norientada a distintos tipos de datos: vi. brevehistoriadelaia(l'ineadeltiempo)\n- cnn: ima'genes y datos con estructura espacial. - 1950s: perceptro'n y primeras investigaciones (rosen-\n- rnn / lstm: secuencias temporales y texto blatt).\n(histo'ricamente). - 1960s: nacimiento temprano del machine learning y\n- transformers: atencio'n y modelado de dependencias a primeros sistemas simbo'licos.\nlarga distancia (estado del arte en nlu y nlg). - 1970s: lenguajes lo'gicos (prolog), algoritmos cla'sicos\n(dijkstra).\nc. yann lecun y las cnn - 1980s:iniciodelaexperimentacio'nconautosauto'nomos.\n- 1990s: resurgimiento con redes multicapa y aprendizaje\nyann lecun fue pionero en redes convolucionales (lenet) por refuerzo.\ny en su aplicacio'n al reconocimiento de d'ıgitos. las cnn - 2000s: auge del reconocimiento facial y visio'n por\naprenden filtros que detectan caracter'ısticas locales (bordes, computadora.\ntexturas) y luego construyen representaciones de alto nivel - 2010s-2020s: deep learning, grandes modelos de\nmediante capas sucesivas. lenguaje, despliegues comerciales."}
{"id_doc": "DOC_002", "segmentacion": "B", "chunk_id": "DOC_002_B_004", "idx": 4, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "vii. ramasdelaprendizajeautoma'tico b. aprendizaje no supervisado\na. aprendizaje supervisado - definicio'n: modelo identifica patrones en datos sin etiquetas.\nconsiste en aprender una funcio'n f : x → y a partir\n- ejemplo: agrupamiento de clientes por ha'bitos de comde ejemplos etiquetados (x ,y ). te'cnicas: regresio'n lineal,\ni i pra.\nsvm,a'rboles,redesneuronales.seevalu'aconme'tricascomo\n- te'cnicas:\nrmse, accuracy, f1.\n- clustering (ej. k-means).\nb. aprendizaje no supervisado - reduccio'n de dimensionalidad (ej. pca).\nno hay etiquetas; el objetivo es encontrar estructura. c. comparacio'n\nte'cnicas: clustering (k-means, dbscan), reduccio'n de di- aspecto supervisado no supervisado\nmensionalidad (pca, t-sne, umap). datos etiquetados sin etiquetas\nobjetivo prediccio'n descubrir patrones\nc. aprendizaje por refuerzo\nejemplos regresio'n, clasificacio'n clustering\nagentes aprenden interactuando con un entorno y recibiendo recompensas. aplicaciones: juegos (atari, go), control\nrobo'tico. referencia cla'sica: sutton & barto.\nfig.5. ejemplodeaprendizajeporrefuerzo\nfig.4. ejemplodeaprendizajeporrefuerzo\nconclusio'n"}
{"id_doc": "DOC_002", "segmentacion": "B", "chunk_id": "DOC_002_B_005", "idx": 5, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "viii. cient'ificovsingenieroenia la ia es un campo en ra'pida evolucio'n: combina teor'ıa\nmatema'tica, ingenier'ıa de software y consideraciones e'ticas.\ncient'ıfico de ia: foco en investigacio'n, nuevos modelos,\ncomprender fundamentos, cuidar la calidad de los datos y\nexperimentos.\nadquirir habilidades pra'cticas (keras, mlops, validacio'n) son\ningeniero de ia / ml engineer: foco en produccio'n,\nclaves para trabajar efectivamente en este a'mbito.\nrendimiento, escalabilidad, mlops y despliegue.\nambos roles se complementan; en proyectos reales conviven\ny colaboran.\n\nix. tiposdeaprendizajeenia\na. aprendizaje supervisado\n- definicio'n: modelo que aprende a partir de datos etiquetados (caracter'ısticas + valores conocidos).\n- ejemplo: predecir el precio de una casa usando sus\ncaracter'ısticas.\n- proceso:\n- divisio'n: muestra (caracter'ısticas) + etiqueta (valor\nobjetivo).\n- aproximacio'n: minimiza el error mediante una\nfuncio'n de pe'rdida (l).\n- te'cnicas:\n- regresio'n (valores continuos)."}
{"id_doc": "DOC_002", "segmentacion": "B", "chunk_id": "DOC_002_B_006", "idx": 6, "autor": "Fernando Daniel Brenes Reyes", "fecha": "2025-08-07", "tema": "Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos autónomos, con énfasis en el aprendizaje supervisado y no supervisado basado en datos.", "texto": "- clasificacio'n (categor'ıas discretas)."}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_000", "idx": 0, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "apuntes de clase\n\ninteligencia artificial - semana 2 - 12 de agosto\npriscilla jime'nez salgado\nescuela de ingenier'ıa en computacio'n, tecnolo'gico de costa rica\ncartago, costa rica - 2021022576@estudiantec.cr\n\nabstract-el presente documento recopila los - medicina:usodemodelosdemachine learning\napuntes de la segunda semana del curso de in- para analizar ima'genes me'dicas, como escaneos\nteligencia artificial. se introducen conceptos de cerebrales, y detectar patrones o anomal'ıas que\nmachine learning y deep learning, los principales ayudeneneldiagno'stico.\ntipos de aprendizaje, el rol de la calidad de los\ndatos y el ciclo completo de desarrollo, validacio'n\ny despliegue de modelos.\n\ni. introduccio'nalainteligenciaartificial\n\nse hace un repaso de los fundamentos de la\nfig. 1. ana'lisis de ima'genes me'dicas con algoritmos de aprendizaje auinteligencia artificial (ia), sus caracter'ısticas de toma'tico\nautonom'ıa y adaptabilidad, y su capacidad para\n-\nagricultura:ana'lisisdedatossobreclima,tipo\nresolver problemas complejos. se destacan ejemplos\ndeplanta,composicio'nyhumedaddelsuelopara\npra'cticos y analog'ıas para comprender el comporoptimizar que' cultivar, cua'ndo sembrar, cua'ndo\ntamiento de sistemas inteligentes.\nregaryque' fertilizantesusar.\na. definicio'n de la inteligencia artificial\nla inteligencia artificial (ia) es, en pocas palabras, un conjunto de tecnolog'ıas capaces de realizar tareas que requieren inteligencia humana,\ncomo ver, escuchar, aprender y adaptarse. su objetivo es resolver problemas complejos de forma\neficaz y generar valor.\nb. ¿que' significaserauto'nomoyadaptativoenesta\na'rea?\nfig.2. aplicacio'ndedatosparaperfeccionarlaspra'cticasdecultivo\nen ia, un sistema puede considerarse inteligente\ncuando combina dos capacidades clave: la au-\n-\nvideojuegos:enlaimagensemuestraunejemtonom'ıaparaactuarsininstruccionesconstantesyla plo donde la ia analiza el entorno de un videoadaptabilidad para modificar su comportamiento en juegotipoplataformas,identificandoobsta'culos,\nrespuestaacambiosdelentornoodelascondiciones enemigos y recompensas, para decidir que' acque se presenten. ciones ejecutar mediante diferentes entradas y\nsalidasdeunmodeloderedneuronal.\nc. usos de la inteligencia artificial"}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_001", "idx": 1, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "la inteligencia artificialtiene aplicaciones en una\ngran variedad de a'reas. algunos ejemplos destacados son:\nen resumen, se le entrega u'nicamente la entrada y\nsu etiqueta, y el modelo debe inferir o aproximar el\nresultadoconlamayorprecisio'nposible.\nen este contexto, se cuenta con dos perspectivas:\ncienciaeingenier'ıa.\nmachine learning: ciencia\n- generar conocimiento: se refiere a investigar y desarrollar nuevas ideas para mejorar\nel aprendizaje automa'tico. esto puede incluir\ncrear modelos desde cero, encontrar formas ma's\nfig.3. iaidentificandoaccioneso'ptimasenunentornodeplataformas\n\neficientes de optimizar funciones o aprovechar\ndatossinetiquetarparaentrenarsistemas.\nd. modelos deterministas y estoca'sticos\nen inteligencia artificial y machine learning, un - me'tricas: se emplean me'tricas para determinar\nmodelo puede clasificarse segu'n co'mo responde a que' modelo ofrece un mejor desempen˜o, evalunaentrada: uando su rendimiento en funcio'n del problema\nespec'ıficoquesebuscaresolver.\n-\ndeterminista:paraunaentradaespec'ıfica,siempredevuelvelamismasalida.ejemplo:determinarsihayluzded'ıaalas12p.m. - data scientist: se encarga de trabajar con los\ndatos desde su recoleccio'n hasta su preparacio'n\nfinal, aplicando te'cnicas de ana'lisis y manipu-\n- estoca'stico: para una entrada espec'ıfica, puede\nlacio'nparaconvertirloseninformacio'ncomprendevolver diferentes salidas de un conjunto de\nsibleyaplicable.\nposibilidades, incorporando aleatoriedad. ejemplo:predecirelclimaexactoalas12p.m.\n- research scientist: centra su labor en investigar y proponer nuevos modelos, algoritmos y\nenfoques teo'ricos que impulsen el avance del"}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_002", "idx": 2, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "ii. machinelearning\nmachine learning. esto incluye experimentar\nconaprendizajenosupervisado.\n\nel concepto de machine learning consiste en\ndisen˜ar ma'quinas capaces de realizar tareas sin estar\nprogramadasdeformaexpl'ıcita,extrayendolalo'gica\ndirectamentedelosdatos.\npor ejemplo, no se le indicara' a la computadora\nque' es un perro ni las reglas que lo definen (cola, fig.4. descripcio'ngeneraldelusodemachinelearningenlaciencia.\nojos, raza, etc.); en su lugar, se le proporcionara' una\nimagen y retroalimentacio'n para que aprenda por s'ı en esta misma l'ınea, google publico' un art'ıculo\nmisma. as'ı, si se dispone de ima'genes etiquetadas titulado towards an ai co-scientist, en el que\ncomo x e y, el modelo recibe cada imagen junto presenta una inteligencia artificial capaz de generar\nconsuetiquetaydebe,demaneraimpl'ıcita,aprender hipo'tesiscient'ıficasapartirdelana'lisisdedatos.esta\na distinguir entre ambas, sin que se le explique el tecnolog'ıa se probo' en el a'mbito me'dico y permitio'\nprocedimiento. obteneravancessignificativosenesaa'rea.\nmachine learning: ingenier'ıa ahorabien,consideremoselsiguientegra'fico:\nen la etapa de puesta en produccio'n de un\nmodelo, lo primero que se debe considerar es la\nnecesidad de refactorizar el co'digo, ya que sera'\nnecesario seguir monitoreando el modelo e, incluso,\nvolveraentrenarlopasadociertotiempo.estoimplica"}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_003", "idx": 3, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "modificar su estructura para que pueda ofrecer la\nmisma funcionalidad, pero de manera ma's eficiente.\nporejemplo,sisedisponedeunmodelomuypesado,\n\nes fundamental transformar el modelo para que\npuedaserservidoaunclientedeformapra'ctica.\npara lograrlo, existen diversas herramientas u'tiles.\nuna de ellas es onnx, que permite tomar un\nmodelo desarrollado en un framework espec'ıfico\ny optimizarlo, generando una versio'n ma's ligera y\ncompacta. tambie'n pueden aplicarse te'cnicas como\nfig.6. s'ıntesisdelmachinelearningenelcampodelaingenier'ıa.\nelmodel distillation,dondeunmodeloma'spequen˜o\n\naprendeaimitarelcomportamientodeunodemayor\ntaman˜o, logrando realizar las mismas tareas con elprocesocomienzaconlarecoleccio'n de datos.\na partir de ellos, se genera un conjunto estructurado\nmenor peso. otra estrategia es la que reduce la\ncantidad de bits en los para'metros del modelo. estos\n(dataset) que se dividira' en dos partes: una para\npara'metros,normalmenteenpuntoflotante,seajustan el entrenamiento y otra para las pruebas. esta\ninformacio'nsealmacenaparasuusoposterior.\npara mejorarel rendimiento, reduciendo eltaman˜o y\nacelerandosuejecucio'n. en la fase de entrenamiento, el modelo aprende\na partir del conjunto de datos de entrenamiento,\nen el a'mbito del mlops, el objetivo es gestionar\najustando sus para'metros segu'n el feno'meno que se\nelmodelodeformama'sdetallada.unconceptoclave\ndesea modelar. durante esta etapa tambie'n se define\nes el data shift, que ocurre cuando el modelo fue\nque' tipo de modelo utilizar, justificando la eleccio'n\nentrenado con una distribucio'n de datos espec'ıfica,\nenfuncio'ndelasnecesidadesdelproblema.\npero con el tiempo los patrones cambian, afectando\nsuprecisio'n.poreso,unadelasresponsabilidadesdel luego se pasa a la validacio'n, donde se utilizan"}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_004", "idx": 4, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "equipodemlopsesmonitorearelcomportamiento datos que el modelo no ha visto antes para evaluar\ndel modelo y, si detectan un cambio en los datos, su rendimiento en condiciones similares a las de\nvolver a entrenarlo. este monitoreo continuo no solo produccio'n. esto permite identificar que' modelo\nincluye el rendimiento del modelo, sino tambie'n la obtiene los mejores resultados segu'n los criterios de\ncalidad y actualidad de los datos, la infraestructura evaluacio'nestablecidos.\n\nde despliegue y la trazabilidad de las versiones\nel modelo seleccionado se guarda en un repositoutilizadas.\nrio o almacenamiento de modelos, desde el cual\nse prepara para su implementacio'n en el entorno\nelegido.\nporu'ltimo,sellevaacaboeldespliegueoservicio\nde predicciones, donde el modelo ya entrenado y\nfig.5. descripcio'ngeneraldelusodemachinelearningenlaingenier'ıa. validadocomienzaaprocesardatosrealesyagenerar\nresultados. todo este flujo puede representarse me- continuacio'n.\ndiante un diagrama que describe cada paso, desde la\n- supervisado: el modelo aprende a partir de\nobtencio'n inicial de los datos hasta la puesta en\ndatos que incluyen etiquetas, las cuales sirven\nmarcha del sistema en produccio'n.\ncomo referencia durante el entrenamiento. un\nejemplocomu'neslaclasificacio'ndeima'genes."}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_005", "idx": 5, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "iii. jerarqu'ıadeconceptosenia\n- no supervisado:elmodelotrabajacondatossin\n\netiquetas y se encarga de encontrar patrones en\nlos datos ocultos. un ejemplo claro de esto son\nlosclusters.\n- semi-supervisado: combina datos etiquetados\ny no etiquetados, u'til cuando el proceso de\netiquetadoescostosoodif'ıcilderealizar.\n- auto-supervisado: el propio dato de entrada\nsirve como etiqueta. se emplea en autoencoders\ny modelos de representacio'n, principalmente\nfig.7. relacio'nentreia,machinelearningydeeplearning\npara reducir el taman˜o de vectores o extraer\n-\nmachinelearning:utilizadatosparaqueunsis- caracter'ısticasrelevantes.\ntemapuedaentrenarseymejorarsurendimiento.\n- aprendizaje por refuerzo:elmodelo,llamado\npara lograrlo, emplea me'todos como algoritmos\nagente, aprende mediante un sistema de recomestad'ısticos, regresio'n lineal, regresio'n log'ıstica\npensas, mejorando su desempen˜o a trave's de la\no a'rboles de decisio'n. ma's alla' de la te'cnica\ninteraccio'nconunentorno.\nusada, su base siempre es la misma: aprender\napartirdelosdatosdisponibles. - few-shot learning: el modelo necesita solo"}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_006", "idx": 6, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "unospocosejemplosparaaprenderarealizaruna\n- deep learning: es una especialidad dentro del tareaespec'ıfica.\n\nmachine learning que utiliza redes neuronales\nprofundas,formadasporvariascapasconectadas. - one-shot learning: basta con mostrarle una\nu'nica vez co'mo realizar la tarea para que el\n\nesteenfoqueesmuyefectivopararesolvertareas\nmodelopuedareproducirla.\ncomplejas, aunque requiere grandes volu'menes\ndedatosparafuncionarcorrectamente. - zero-shot learning: el modelo es capaz de\nrealizar tareas sin haber sido entrenado previaenresumen,lasdiferenciasesquelainteligencia\nmenteparaellasdeformaespec'ıfica.\nartificialeselconceptoma'samplio,queengloba\ntodo lo relacionado con lograr que una ma'quina v. pipelinedemachinelearning\nactu'edeforma\"inteligente\".elmachinelearnel desarrollo de un modelo de machine learning\ning agrupa algoritmos que aprenden con datos,\npasaporvariasetapasclave:\ny el deep learning es una te'cnica dentro de\neste que esta' disen˜ada para trabajar con canti1) data adquisition: este es muy importante\ndades masivas de informacio'n y resolver tareas\n\nya que se necesitan obtener datos de calidad y\nespec'ıficascongranprecisio'n.\nrepresentativos, evitando errores como valores"}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_007", "idx": 7, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "iv. tiposdeaprendizajeenmachinelearning faltantesyduplicados.\nsehizounrepasodelaclaseanterioryseretomaron 2) data preparation:paraestaetapasetieneque\nlos dema's tipos de aprendizaje y se describen a preparar los datos que se tienen para tener un\ndataset de calidad, ya sea, eliminando datos raciones hasta encontrar la que produce el mejor\nduplicados o se descartan datos que no tienen rendimientodelmodelo.\nutilidad.\n\nvii. paradigmasderesolucio'ndeproblemasen\n3) feature engineering: esta fase consiste en machinelearning\ncrear nuevas variables u'tiles a partir de los\ndatos que se encuentran disponibles o eliminar 1) agrupamiento: busca patrones o relaciones\naquellas que no aporten informacio'n relevante, ocultas en los datos para formar grupos, u'til\nespecialmenteenelcasodedatostabulares. para descubrir conexiones que no se hab'ıan\nconsiderado.\n4) modelo selection: elegir el modelo ma's adecuadosegu'nelproblemayrecursosdisponibles,\npuedeirdesdeopcionessimplescomoregresio'n\nlog'ıstica hasta redes neuronales profundas para\ncasoscomplejos.\n5) model training: esta fase de entrenamiento\ndelprocesodelmodelo,sedividenlosdatosen\ntraining set y validation set y se ajustan hiperpara'metrosconme'todoscomogridsearch.\n6) model deployment: en este proceso de deployment se implementa el modelo en profig.8. ejemplodelprocesodeagrupamiento\nduccio'n y se supervisa para poder garantizar\nusubuenrendimiento.\n2) prediccio'n y clasificacio'n:"}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_008", "idx": 8, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "vi. hiperpara'metrosypara'metros\nes importante diferenciar entre para'metros e - prediccio'n: estima un valor nume'rico\nhiperpara'metros: basa'ndoseenpatronesdelosdatos.\n-\nclasificacio'n:asignadatosaunacategor'ıa\n- para'metros: son los valores internos que un segu'nsuscaracter'ısticas.\nmodelo aprende automa'ticamente a partir de los\ndatosduranteelentrenamiento.estosvaloresson\n\nlos que el modelo ajusta para minimizar el error\nymejorarsucapacidaddeprediccio'n.\n- hiperpara'metros: son valores definidos manualmente antes de iniciar el entrenamiento, y\ncontrolan el comportamiento del algoritmo. un\nejemplo es el batch size (por ejemplo, 32 muestras por iteracio'n). estos no se aprenden del\nconjunto de datos, sino que se configuran para\nguiarelprocesodeentrenamiento.\nel ajuste de hiperpara'metros requiere un proceso fig.9. ejemplodelprocesodeprediccio'nyclasificacio'n\nde experimentacio'n, probando distintas configu3) optimizacio'n: encuentra la mejor solucio'n viii. notaimportante\nentremuchasposibles.\n- enunciado tarea moral mencionar un aporte\n-\nlocal:lamejorenuna'reaconcreta. decadaunodeellos:\n- global: la mejor en todo el espacio de -yannlecun\nbu'squeda. -yoshuabengio\n-samaltman\n-geoffreyhinton\n-timnitgebru\n-iangoodfellow"}
{"id_doc": "DOC_003", "segmentacion": "B", "chunk_id": "DOC_003_B_009", "idx": 9, "autor": "Priscilla Jiménez Salgado", "fecha": "2025-08-12", "tema": "Introducción a machine learning y deep learning, tipos de aprendizaje, calidad de datos y ciclo de desarrollo y validación de modelos.", "texto": "incluya un resumen del funcionamiento de las\nsiguientesherramientas/conceptos:\n-onnx\n-mlflow\n-vertex\n-langchain\n-huggingface\n-ollama\n-chain-of-thought\nfecha de entrega: 19 de agosto, pero esta es\nmoral.\n\nix. aspectosadministrativos\nla pro'xima seccio'n corresponde a a'lgebra linfig.10. ejemplodelprocesodeoptimizacio'n\neal, la cual es fundamental para los temas que se\nabordara'nma'sadelante.\n4) bu'squeda: encuentra el camino ma's eficiente\nhacia una solucio'n, representando la mejor la pro'xima semana la modalidad sera' virtual.\nopcio'ncomoelcaminodemenorcosto. hasta el martes 26 de agosto las clases sera'n\npresencialesyesemarteshabra' quiz acumulativo.\n\nfig.11. ejemplodelprocesodebu'squeda"}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_000", "idx": 0, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "notas de clase\n\ninteligenciaartificial-12deagosto-semana2\nluis alfredo gonza'lez sa'nchez\nescuela de ingenier'ıa en computadores\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\n2021024482 gonzal3z.luis@estudiantec.cr\n\nabstract-this document provides a concise summary of the c. la jerarqu'ıa de conceptos en ia\n\nkey concepts and examples covered in week two of the artificial\nintelligence course. it begins with an overview of general ai observe la figura 1.\nconcepts, followed by an explanation of the types and main\napproaches of machine learning. the document also explores\n\nvarious problem-solving paradigms and concludes with a brief\n\nreview of the essential components of the machine learning\npipeline. this summary aims to enhance understanding and\nreinforce the material presented in class.\nindex terms-ai, machine learning, pipeline.\n\ni. introduction\n\nel presente documento busca brindar un breve resumen a"}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_001", "idx": 1, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "los conceptos y ejemplos vistos en la clase de la semana 2\ndel curso de inteligencia artificial , comenzando por un breve\nrecorrido a determinados conceptos generales de la ia, luego\nse procedera' a explicar los tipos de aprendizaje en machine fig.1. jerarquiadeconceptosia\nlearning, sus principales enfoques , algunos paradigmas de\nresolucio'n de problemas y por u'ltimo un breve repaso a los\nconceptos del pipeline de machine learning.\nd. ¿que' es machine learning ?\n\nii. conceptosgenerales\nconsiste en generar ma'quinas o programas a partir de\nse presentara' un breve repaso a conceptos vistos en clase. algoritmos que sean capaces de resolver un problema a partir\ndeinferencias.enestecontextosebuscaque,apartirdedatos\na. definicio'n de modelos\ndeentrada(yenocasionesconsusresultados)dondeelsistema\n- modelo determinista : es aquel modelo que, bajo una sea capaz de resolver problemas en base a los datos que ya\nmisma entrada se produce los mismos resultados. no inposeesinqueseleexpliquecomoresolverlo.considerandolo\ncorpora aleatoriedad, por lo que los resultados obtenidos\nanterior , se describe a continuacio'n los tipos de aprendizaje\nse vuelven predecibles , por ejemplo, bajo caracter'ısticas\nen machine learning.\nme'dicas se puede determinar una enfermedad asociada.\n- modelo estoca'stico : es aquel modelo en que , bajo una\nentradadeterminadalassalidassondiferentesdebidoaun iii. tiposdeaprendizaje"}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_002", "idx": 2, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "conjuntodeposiblesresultadosasociadosadichaentrada\na. aprendizaje supervisado\n, existe control respecto al grado de aleatoriedad, por\nejemplo, consultar el clima a la misma hora todos los\n\ntipodeaprendizajequeinvolucraunconjuntodedatoscon\nd'ıas.\ncaracter'ısticas y una respectiva etiqueta, esa etiqueta asocia\nb. definicio'n de para'metros dichas caracter'ısticas con un valor. la etiqueta supervisa y\ncorrigelasaproximacionesdadasporlafuncio'nplanteadapara\n- para'metros : son los datos que se le configuran al\nobtener mejores resultados, una forma de visualizarlo es de la\nmodelo, permiten controlar el comportamiento de sus\nsiguiente manera :\n\nalgoritmos\n- hiperpara'metros : son todos los valores que el mod- x={x i ,y i }\nelo aprende a partir de los datos de entrenamiento,\nle permiten al modelo identificar errores y ajustar sus donde x representa mi caracteristica, xi representa la etiqueta\nalgoritmos. y yi, representa el resultado \"supervisado\" por la etiqueta.\nb. aprendizaje no supervisado f. otros tipos de aprendizaje\neste tipo de aprendizaje es aquel que se entrena con datos se detallan a continuacio'n algunos tipos de aprendizaje\nsin etiquetas, es decir, sin informacio'n previa que le permita adicionales\ndetermina cua'l es el \"resultado correcto\".a diferencia del - fev-shot : me'todo en el cual el modelo aprende a\naprendizaje supervisado, donde las etiquetas sirven como una partir de breves ejemplos. una posible aplicacio'n es con\n\"supervisio'n\" o gu'ıa para corregir y entrenar el modelo, en respecto a los llms ,si el llms no sabe como realizar\nel aprendizaje no supervisado no hay una referencia expl'ıcita latarea,selebrindaejemplosyapartirdeesosejemplos\nparaevaluarocorregir.unejemplomencionadoenlaclase,son es capaz de resolver la tarea.\nlos algoritmos de clu'ster , donde analiza en base a sus - one shot:me'todosimilaralanterior,soloquereducela\ncaracter'ısticas si pertenece a un grupo u otro, sin hacer uso candidaddeejemplosa1,yapartirdeeseu'nicoejemplo\nde etiquetas. logra resolver.\nc. aprendizaje semi-supervisado - zero shot : en este me'todo el sistema resuelve en base a\nlo que \"sabe\" , no existen ejemplos del que pueda tomar\nen palabras sencillas es una combinacio'n de los me'todos\npara aprender.\ndeaprendizajeanteriores,enestetipodealgoritmo,seutilizan\ntanto datos etiquetados como datos no etiquetados para entre- iv. enfoquesdelmachinelearning\nnar el modelo, es decir no siempre va a existir una etiqueta\nexisten dos principales enfoques, ciencia e ingenieria:\nasociada a las caracter'ısticas, es principalmente u'til cuando\nse posee etiquetas en solo en una parte de los datos para a. machine learning: ciencia\nentrenar,yendatosdondelasetiquetassondif'ıcilesdeobtener\n- se genera conocimiento : es la parte acade'mica que se\n,aprovechalainformacio'ndedatossinetiquetasparamejorar"}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_003", "idx": 3, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "dedica a investigar y desarrollar nuevas ideas respecto a\nel rendimiento del modelo.\naprendizaje automa'tico, incluye la elaboracio'n de modd. aprendizaje auto-supervisado elos desde cero y optimizar funciones, por lo general,\nesta' en manos de investigadores con un fuerte trasfondo\npara este tipo de aprendizaje, el modelo no depende de\nmatema'tico que les permite conocer a gran detalle los\ndatos con etiquetas \"pre- insertadas\" para el entrenamiento,\n\nprocesos que suceden durante el desarrollo de dichos\nen lugar de eso, genera automa'ticamente etiquetas o sen˜ales\nmodelos o algoritmos.\nde supervisio'n a partir de los propios datos. un ejemplo visto\nen clase es el uso en modelos de lenguaje, donde el sistema - me'tricas : a partir de los datos recolectados, los data\nscientist generan me'tricas que permiten evaluar el depredice a partir de palabras previas cual es la palabra que\nsempen˜o del modelo o algoritmo.\nsigue a dicha palabra, puede visualizarse la palabra previa\ncomodato,peroalavezcomounamismaetiquetaparapoder - data scientist : son las personas encargadas de realizar\nlarecoleccio'nylapreparacio'ndelosdatos,detalmanera\ndefinir la palabra siguiente."}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_004", "idx": 4, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "queseanaplicablesparaelentrenamientodelosmodelos\ne. aprendizaje por refuerzo\nu algoritmos.\nel aprendizaje por refuerzo consiste en un aprendizaje con - research scientist : a diferencia de los data scientist ,\n\"premiacio'n\",existeunagenteydichoagenteaprendeatomar losresearchscientistseencargandeinvestigaryproponer\ndecisionesatrave'sdelainteraccio'nconunentorno,buscando proponer nuevos modelos, algoritmos o modelos.\nmaximizarunarecompensa,sepuedenpenalizarporacciones\nb. machine learning: ingenieria\nque llevaran a un resultadoinsatisfactorio.ejemplo, observe la\nfigura 2. en el videojuego representado, se puede entrenar al - puesta en produccio'n del modelo : es necesario la\ncontinua monitorizacio'n del modelo antes de la puesta a\nproduccio'n y pasado un tiempo, se busca realizar correcciones, monitorizar si los resultados o salidas mantienen\nunadistribucio'nnormalprobabil'ısticaycomoelprogreso\ndeentrenamientoluegodelapuestaenproduccio'nmueve\ndichadistribucio'n,esdecir,darseguimientoalassalidas.\n- transformar el modelo :hace referencia a todas las\n\nestrategias y cambios requeridos para adaptar el modelo\nsegu'n los requerimientos especificados , por ejemplo,\nsi se dispone de un modelo en python, cua'les son las\nestrategias para adaptarlo en otras plataformas como c++\nfig.2. videojuegodeejemplo\no mo'vil , una de dichas estrate'gias es la reduccion del\npersonaje (agente) para que cruce al otro lado basa'ndose en peso del modelo.\nsi el salto se realiza correctamente (accio'n) se recompensa, y - onnx:esunaherramientaquefacilitalaportabilidadde\nsi la accio'n es insatisfactoria (caer al vac'ıo y perder una vida) modelos entre plataformas y dispositivos, optimizando y\nse castiga por dicha accio'n. reduciendo el taman˜o del modelo.\n- mlops : los equipos de mlops tiene la finalidad de"}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_005", "idx": 5, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "monitorear y gestionar el modelo de manera detallada\n,tanto con los datos como la calidad de los mismos,\npor ejemplo, en el seguimiento que se da en puesta\nen produccio'n estan encargados de monitorear el comportamiento del modelo, en casos donde suceda algun\ncambio en los datos, deben de entrenarlo nuevamente.\n\nv. paradigmasderesolucio'ndeproblemas\nse detallan a continuacio'n ciertos paradigmas o estrategias\npara resolver problemas en el a'mbito de la inteligencia artififig.4. ejemplodegra'ficaparavisualizarproblemasdeprediccio'n\ncial:\na. problemas de bu'squeda\nd. problemas de agrupamiento\n: son todos aquellos algoritmos que permiten encontrar la\n\npara los problemas de agrupamiento no existen etiquetas\nmejor solucio'n, representada por el camino ma's barato o la\ncon las cuales entrenar, se busca encontrar pertenencia a un\nruta ma's corta, se detallan algoritmos como dijkstra , dfs,\ngrupodadassimilitudesacaracter'ısticas,lassolucionesaestos\nbfs.\nproblemas permite encontrar relaciones a preguntas que quiza'\nno se ten'ıan previamente.\nfig.3. ejemplodealgoritmosdebu'squeda\nb. problemas de optimizacio'n\nson todas aquellas soluciones que pretenden encontrar la fig.5. ejemplodeproblemasdeagrupamiento\nsolucio'nma'soptimadentrodeungrupodesoluciones.existen\n2 tipos:"}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_006", "idx": 6, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "vi. pipelinedelmachinelearning\n- solucio'n local : es aquella seleccionada dentro de un\ngrupo o a'rea especifica, un ejemplo es el visto en clase - data adquisition :una de las partes ma's importantes\nrespecto a bares, el mejor bar cercano al tec de cartago del piplene de machine learning, es todo el proceso de\nes la nave, pero esto es por que en el a'rea cercana al tec recoleccio'ndedatosrelevantesdelproblemaaresolver,\nes la mejor o la u'nica opcion. se deben de resolver preguntas como ¿cua'l es la calidad\n- solucio'nlocal :serefierealamejorsolucio'nencontrada de los datos que estoy tomando? ¿de do'nde los saco?\nentre todas las posibles soluciones, tomando el ejemplo ¿es mi fuente libre de sesgo?.\ndel bar, si se evalu'a a nivel provincia , probablemente se - data preparation : la etapa de la preparacio'n de\nencuentre un mejor bar que la nave. los datos, considerando que el computador trabaja con\nnu'meros, se debe de resolver las pregunta ¿co'mo voy\nc. problemas de prediccio'n y clasificacio'n\na representar los datos a mi algoritmo o modelo? , por\nen los problemas de prediccio'n , existe una serie de datos ejemplo, si los datos son ima'genes, le asigno valores\nrespecto a un evento, comportamiento, entre otros y se desea puestoquelosalgoritmossonmatema'ticos.enestaetapa\nencontrar patrones para realizar una prediccio'n ,generalmente tambie'n se realizan otras labores como la revisio'n de\nse representa con una funcio'n, que podra' generar una esti- datos duplicados y la bu'squeda de datos faltantes, por\nmacio'ndelarelacio'ndelosdatosconsussalidas,unejemplo ejemplo en un set de datos, con algu'n dato faltante,\nes el visto en clase , donde en base al motor del veh'ıculo , se elimino dicho set o calculo la mediana y la introduzco\nobtienen datos respecto al consumo de gasolina y se genera como el dato faltante.\nuna funcio'n la cual sera' capaz de predecir cua'l es el consumo - freature engineering : los freatures son los atributos\nde gasolina de otro veh'ıculo. estos datos son cuantificables o que proporcionan informacio'n u'til de los datos, en esta\nnume'ricos. etapa se pueden generar nuevos freatures a partir de los\nexistentes,seleccionarsololosrelevantesparalasolucio'n\no eliminar freatures que generen algun sesgo.\n- modelselection:eslaetapadeseleccio'ndelmodeloma's"}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_007", "idx": 7, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "adecuadoparacubrirlademandaderecursosyrequisitos\nque genera el problema. entra un concepto importante\npara la seleccio'n del modelo, la explicabilidad, si tengo\nquedar explicacio'ndel porque' de los resultadosgeneradosconstantemente,quiza' lamejorsolucio'nnoesutilizar\nia, si no , podr'ıa escoger un algoritmo que brinde una\nsolucio'n ma's elegante, un ejemplo es para un sistema de\n\nun banco que detecta si un usuario es elegible para un\npre'stamo, entrenar una ia que determine esto puede no\nser la mejor manera , si no , un algoritmo que con datos\n\ndel usuario pueda determinar si es posible o no darle el\npre'stamo.\n- modeltraining:eslafasedeentrenamientodelmodelo,\n\nlos datos son divididos en datos de entrenamiento y\ndatos de validacio'n, en esta parte los algoritmos realizan\noptimizaciones en base a los datos de entrenamiento.\n- model deployment: en esta etapa el modelo se manda"}
{"id_doc": "DOC_004", "segmentacion": "B", "chunk_id": "DOC_004_B_008", "idx": 8, "autor": "Luis Alfredo González Sánchez", "fecha": "2025-08-12", "tema": "Resumen de conceptos clave de IA y enfoques de aprendizaje automático, incluyendo paradigmas de resolución de problemas y componentes del pipeline de machine learning.", "texto": "a produccio'n en donde se debe de supervisar para garantizar un correcto funcionamiento."}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_000", "idx": 0, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "inteligencia artificial\n\napuntesdeclase-14deagostode2025\n1st kendall rodr'ıguez camacho\nescuela de ingenieria en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nkenrodriguez@estudiantec.cr\n\nabstract-en este documento se presentan los apuntes cor- iii. tiposdeaprendizaje\n\nrespondientes a la clase del 14 de agosto de 2025 del curso de\nenmachinelearningexistendistintostiposdeaprendizaje,\ninteligenciaartificial.enprimerlugar,seincluyeunresumende\ncada uno con aplicaciones y caracter'ısticas particulares:\nlasesio'nanterior,enelqueserevisanconceptosdeiaymachine\nlearning.posteriormente,seintroduceela'lgebralinealaplicada - supervisado: se dispone de datos con entradas y salidas\nmediante python, empleando librer'ıas como pytorch, numpy y conocidas x ={x ,y },i=1..n. el modelo aprende a\ni i\npandas, con el objetivo de familiarizarse con las herramientas y predecir la salida y a partir de la entrada x.\nsu implementacio'n pra'ctica.\nindex terms-machine learning, mlops, models, deep - no supervisado: solo se tienen entradas sin etiquetas. el\nlearning, generative ai, vectores, tensores, matrices modelo identifica patrones o estructuras ocultas en los\ndatos."}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_001", "idx": 1, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "i. introduction - semi-supervisado: algunos datos esta'n etiquetados y\nla inteligencia artificial incluye diversas te'cnicas, entre\notrosno.u'tilcuandoetiquetartodoslosdatosescostoso.\nellas el machine learning o aprendizaje automa'tico, que se - auto-supervisado: el modelo genera etiquetas a partir de\nencarga de que los modelos aprendan de los datos. en esta los propios datos, sin necesidad de intervencio'n manual.\nclase se presento' un resumen de los diferentes paradigmas es ampliamente utilizado en procesamiento de lenguaje\nde resolucio'n de problemas en ia, los distintos tipos de natural (nlp) y en redes neuronales concurrentes. ejemaprendizaje y de las etapas que componen el ciclo de vida plo: predecir la siguiente palabra en una oracio'n.\nde un modelo de ml. adema's, se introdujo el a'lgebra lineal - aprendizaje por refuerzo: el agente aprende mediante\nde forma pra'ctica utilizando python y librer'ıas como numpy, recompensas y penalizaciones, ajustando sus acciones\npandas y pytorch, trabajando con conceptos como tensores, para maximizar la recompensa futura. ejemplo: entrenar\nmatrices y vectores. un agente para jugar mario bros, donde el modelo\naprende a avanzar, evitar obsta'culos y recolectar recomii. noticias\npensas.\na. chatgpt 5, ¿un fracaso? - few-shot: aprende a partir de pocos ejemplos (3-4\napocosd'ıasdesulanzamientoporpartedeopenai,chat- muestras). ejemplo: modelos de lenguaje que generan\ngpt 5 comenzo' a recibir cr'ıticas por parte de la comunidad. respuestas correctas con muy pocos ejemplos.\nmuchos usuarios cuestionaron si realmente representaba una - one-shot: aprende con un solo ejemplo. ejemplo: remejorafrenteachatgpt4,sen˜alandoqueenvariasocasiones conocimiento facial usando una sola foto de referencia.\nel modelo tarda demasiado en generar una respuesta y, en - zero-shot:generalizaatareasnuevassinejemplosdirecalgunoscasos,produceerroresorespuestasdecalidadinferior tos, basa'ndose en conocimientos previos.\na lo esperado."}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_002", "idx": 2, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "machinelearningvistodesdelaciencia\nb. alexandr wang y la inversio'n de meta desde la perspectiva cient'ıfica, el fin es generar\nalexandr wang, un empresario de 28 an˜os, es el fundador conocimiento, entender patrones y crear teor'ıas. las me'tricas\ndescaleai,unastartupespecializadaeninteligenciaartificial se utilizan para cuantificar que' tan bien funciona un modelo,\ny en el etiquetado masivo de datos para entrenar modelos. en aportando una base objetiva para comparar resultados y enfo2025, meta realizo' una inversio'n significativa en la compan˜'ıa, ques.\ndestacando el intere's de la empresa en reforzar su estrategia en este contexto, los roles principales incluyen:\nen ia. - data scientist: experimenta con modelos, selecciona alwang se convirtio' en uno de los multimillonarios ma's goritmos, ajusta hiperpara'metros y evalu'a resultados.\njo'venes creados por s'ı mismos, y su trabajo en scale ai - research scientist: investiga nuevos algoritmos, publica\nlo posiciona como una figura relevante en el desarrollo y art'ıculos cient'ıficos y desarrolla teor'ıas para avanzar la\naplicacio'n de tecnolog'ıas de inteligencia artificial avanzada. inteligencia artificial.\nmachinelearningvistodesdelaingenier'ia\ndesde la perspectiva de la ingenier'ıa, el enfoque esta' en\nimplementar, mantener y optimizar modelos en produccio'n."}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_003", "idx": 3, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "entre las principales tareas se incluyen transformar modelos\nparareducirsutaman˜oyaumentarlavelocidad,crearpipelines\nde datos que alimenten los modelos de forma automa'tica y\nmonitorear me'tricas de rendimiento y tiempos de respuesta.\nadema's, las pra'cticas de mlops se aplican para operacionalizarelaprendizajeautoma'tico,demanerasimilaraco'mo\ndevops se utiliza para el desarrollo y despliegue de software.\n\niv. paradigmasderesolucio'ndeproblemas\nentrelosprincipalesparadigmasderesolucio'ndeproblemas\nen inteligencia artificial se incluyen:\nfloat fig.2. ejemplodesolucioneslocalesyglobalesenoptimizacio'n.\nc. prediccio'n y clasificacio'n\na. problemas de bu'squeda\n1) prediccio'n: estimar un valor futuro o desconocido, ense busca encontrar el camino ma's eficiente hacia una\ncontrando patrones segu'n los datos disponibles. por ejemplo,\nsolucio'n.\ncua'nta gasolina quedara' en un carro.\n2) clasificacio'n: asignar elementos a categor'ıas predefinidas, basa'ndose en sus caracter'ısticas. por ejemplo, identificar el modelo de un carro segu'n sus partes.\nfig.1. ejemplodeproblemasdebu'squeda.\nb. problemas de optimizacio'n\nfig.3. ejemplodeprediccio'nyclasificacio'n.\ncuandoexisteungrannu'merodesolucionesposibles,hallar\nla mejor solucio'n absoluta puede ser dif'ıcil.\nd. agrupamiento (clustering)\n1) solucio'n local: mejor solucio'n dentro de un a'rea espec'ıfica.\ndescubrir patrones o grupos naturales en los datos, uti2) solucio'n global: mejor solucio'n en todo el espacio de lizando diferentes aspectos para formar grupos con distintas\nposibles soluciones. formas.\nfig.4. ejemplodeagrupamiento(clustering)."}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_004", "idx": 4, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "v. modelos\n1) determinista: un modelo determinista produce siempre\nel mismo resultado para un mismo conjunto de entradas. fig.6. jerarqu'ıadelaia\nejemplo: ¿hay luz a mediod'ıa? para las mismas condiciones,\nla respuesta siempre sera' la misma: s'ı.\n2) estoca'stico: un modelo estoca'stico puede producir\ndiferentes resultados para el mismo conjunto de entradas,\ndependiendodeprobabilidadesofactoresaleatorios.ejemplo:\n\nvii. pipelinedemachinelearning\n¿cua'l sera' el clima a mediod'ıa? aunque las condiciones\niniciales sean similares, el clima puede variar: puede estar\nsoleado, nublado o lloviendo.\n1) data acquisition: recolectar datos relevantes y de calidad.\n2) data preparation: limpiar y transformar los datos,\nfig. 5. comparacio'n entre modelos determinista y estoca'stico usando el entrega'ndolos en un formato adecuado para su ana'lisis, elimiejemplodelclima. nando duplicados, valores faltantes o incorrectos, y aplicando\nnormalizacio'n o escalamiento de los valores."}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_005", "idx": 5, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "vi. jerarqu'iadelainteligenciaartificial 3) feature engineering: crear y seleccionar las variables\n(features) ma's relevantes para entregar al modelo u'nicamente\nlas necesarias para su entrenamiento y ana'lisis.\n1) inteligencia artificial (ia): algoritmos que imitan la\ninteligencia humana, capaces de tomar decisiones o resolver 4) model selection: elegir el modelo que mejor se adapta\nproblemas. alproblema,considerandonosolosudesempen˜o,sinotambie'n\nsu explicabilidad, es decir, que' tan fa'cil es interpretar y\n2) machine learning (ml): me'todos estad'ısticos que perentender co'mo toma decisiones.\nmiten a los modelos aprender de los datos, como regresio'n o'\na'rboles de decisio'n.\n5) model training: entrenar el modelo y ajustar los hiper3) deep learning (dl): redes neuronales profundas uti- para'metros, utilizando te'cnicas como grid search. durante\nlizadasparaproblemascomplejos,incluyendovisio'nporcom- este proceso, el modelo realiza optimizacio'n basada en los\nputadora y procesamiento de lenguaje natural (nlp). datos para mejorar su desempen˜o y reducir errores.\n4) generative ai (genai): modelos capaces de generar\ncontenido nuevo, como texto, ima'genes o audio, a partir de 6) model deployment: poner el modelo en produccio'n,\npatrones aprendidos. integrarlo con aplicaciones y monitorear su desempen˜o.\nimport torch # para crear y manipular"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_006", "idx": 6, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "tensores\nimport numpy as np # para arreglos y\n\noperaciones numericas\nimport pandas as pd # para manejar datos\n\ntabulares\nc. creacio'n de tensores\nun tensor es un arreglo de nu'meros que puede tener una o\nvarias dimensiones. - si tiene una dimensio'n, se llama vector.\n- con dos dimensiones se llama matriz. - con ma's de dos\ndimensiones, simplemente se denomina tensor de orden k.\npytorch permite crear tensores ya inicializados. por ejemplo, arange(n) genera un vector con valores de 0 hasta\nn-1, almacenado en memoria principal y listo para operaciones en cpu.\ncada valor dentro del tensor se llama elemento. por ejemplo,eltensorxcreadoconarange(12)tiene12elementos.\nse puede inspeccionar el nu'mero total de elementos con\nnumel() y la forma del tensor (taman˜o de cada eje) con\nfig. 7. pipeline t'ıpico de machine learning desde la adquisicio'n de datos\nshape:\nhastaeldesplieguedelmodelo.\nx = torch.arange(12, dtype=torch.float32)"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_007", "idx": 7, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "x.numel() # 12 elementos\n\nviii. manejoymanipulacio'ndedatoscon\n\nx.shape # (12,)\n\npytorchypandasenpython\nenestaseccio'nseabordanlaste'cnicasba'sicasparaprocesar 1) redimensionamiento (reshape): el me'todo reshape\ny manipular datos utilizando pytorch, junto con pandas y permite cambiar la forma de un tensor sin copiar sus datos.\nnumpy. se explicara' co'mo crear y transformar tensores, por ejemplo, un vector de 12 elementos puede transformarse\nrealizar operaciones sobre ellos y cargar datos desde archivos en una matriz de 3 filas y 4 columnas:\ncsv para su ana'lisis y uso en modelos de aprendizaje aux = x.reshape(3, 4)\ntoma'tico. x # matriz de 3x4\na. anaconda y manejo de ambientes\n2) tensores pre-inicializados: se pueden crear tensores\nanaconda es una distribucio'n que incluye lenguajes como ya con valores espec'ıficos, como ceros, unos o nu'meros\npython y r, junto con herramientas para gestionar paquetes aleatorios, u'tiles por ejemplo para para'metros de modelos:\ny entornos de forma aislada. su principal utilidad es permitir\nzeros = torch.zeros((2, 3, 4)) # tensor de\nmanejar dependencias en diferentes ambientes, evitando que"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_008", "idx": 8, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "ceros\nun proyecto interfiera con otro, lo cual es muy u'til en ciencia ones = torch.ones((2, 3, 4)) # tensor de\nde datos y machine learning. unos\npor ejemplo, para instalar pytorch en un ambiente es- randn = torch.randn(3, 4) # valores\n\naleatorios\npec'ıfico se puede usar:\n# activar el ambiente la forma 2×3×4 indica 2 bloques, cada uno con 3 filas\nconda activate ml # o el nombre del ambiente\ny 4 columnas.\n# instalar pytorch dentro del ambiente 3) creacio'n de tensores desde listas de python: pytorch\n\nactivado permite convertir listas de python o arreglos de numpy\npip install torch directamente en tensores:\nesto asegura que pytorch se instale u'nicamente en el a = torch.tensor([[2, 1, 4, 3],\n[1, 2, 3, 4],\nambiente seleccionado, sin afectar otros proyectos o config-\n[4, 3, 2, 1]], dtype=torch.\nuraciones.\nfloat32)\na\nb. configuracio'n\npara trabajar con datos y tensores, se necesita importar estogenerauntensorde3filasy4columnasconlosvalores\nalgunas librer'ıas clave: especificados.\nd. indexacio'n y segmentacio'n (slicing) h. broadcasting\npytorch permite acceder a elementos, filas, columnas o el broadcasting permite realizar operaciones entre tensores\nsubmatrices de manera similar a numpy: de distintas formas, expandiendo automa'ticamente sus dimensiones sin duplicar datos:\nfila_ultima = a[-1] # ultima fila\nsubmatriz = a[1:3] # filas 1 y 2 a = torch.arange(3).reshape((3, 1)) # forma 3\na[1, 2] = 9 # asigna 9 al elemento en la x1\nfila 1, columna 2 b = torch.arange(2).reshape((1, 2)) # forma 1\na[:2, :] = 35 # asigna 35 a todas las x2\ncolumnas de las dos primeras filas broadcast = a + b # tensor resultante de"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_009", "idx": 9, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "forma 3x2\nesto funciona tambie'n para tensores de ma's de dos dimensiones. i. operaciones in-place\n\nlas operaciones in-place modifican directamente el tensor\ne. operaciones elemento a elemento original, ahorrandose memoria. esto es u'til cuando se maneja\nmuchospara'metrosysequiereevitarcrearcopiasinnecesarias.\nenpytorch,lasoperacionesaritme'ticasseaplicanelemento\npor elemento, generando un nuevo tensor: before = id(y) # se guarda la\n\ndireccion de memoria original de y\nx = torch.tensor([1.0, 2, 4, 8]) y = y + x # se crea un nuevo\ny = torch.tensor([2, 2, 2, 2]) tensor con la suma; y ahora apunta a nueva\n\nmemoria\nadd, sub, mul, div, exp = x + y, x - y, x * y, id(y) == before # false, la memoria de\nx / y, x ** y y cambio\nesto permite realizar suma, resta, multiplicacio'n, divisio'n y z = torch.zeros_like(y) # se crea un tensor z\ncon la misma forma que y, lleno de ceros\npotencia de forma directa sobre cada elemento.\nz[:] = x + y # modifica el\ncontenido de z directamente (in-place),\nf. concatenacio'n de tensores sin cambiar su direccion de memoria\nse pueden unir varios tensores en uno solo usando se recomienda usar in-place para eficiencia, pero con\ntorch.cat, especificando el eje sobre el cual concatenar. cuidadosivariasvariablesapuntanalmismotensor,paraevitar\npor ejemplo, concatenando dos matrices a lo largo de las filas inconsistencias.\n(dim=0) se suman las filas, y a lo largo de las columnas\n(dim=1) se suman las columnas: j. conversio'n a numpy"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_010", "idx": 10, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "pytorch permite convertir tensores a arreglos de numpy y\nx = torch.arange(12, dtype=torch.float32).\nreshape((3,4)) viceversa, sin duplicar los datos en memoria:\ny = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3,\na_np = a.numpy() # tensor a arreglo\n4], [4, 3, 2, 1]])\n\nnumpy\ntorch.cat((x, y), dim=0) # concatenar filas\ntype(a_np)\ntorch.cat((x, y), dim=1) # concatenar\n\ncolumnas\na_back = torch.from_numpy(a_np) # arreglo\n\nnumpy a tensor\nadema's, se puede construir tensores binarios mediante type(a_back)\ncomparaciones.porejemplo,x == ygenerauntensordonde\ncada elemento es 1 si coincide y 0 si no:\nk. carga de datos desde csv\nx == y # comparacion elemento a elemento\npara trabajar con datos externos, se puede usar pandas\ny luego convertir a tensores de pytorch. se pueden aplicar\ncodificacio'n *one-hot* y completar valores faltantes:\ng. indexacio'n lo'gica"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_011", "idx": 11, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "import pandas as pd\nse pueden crear ma'scaras booleanas para seleccionar el- import torch\nementos que cumplan cierta condicio'n. por ejemplo, comdf = pd.read_csv('../data/house_tiny.csv')\n\nparando dos tensores se obtiene un tensor de valores true\n# leer csv\no false:\ninputs = df.iloc[:, :2]\n# seleccionar\nmask = x == y # true donde los elementos\n\ncolumnas\ncoinciden, false en caso contrario\ninputs = pd.get_dummies(inputs, dummy_na=true)\n# one-hot encoding\ninputs = inputs.fillna(inputs.mean())\n# completar valores faltantes tensor:\nx_csv = torch.tensor(inputs.to_numpy(dtype= [[[ 1 2 3]\nfloat)) # convertir a tensor [ 4 5 6]]\nx_csv\n[[ 7 8 9]\n[10 11 12]]]\n\nix. conceptosba'sicosdea'lgebralineal\nen esta seccio'n, como continuacio'n de la anterior, se pre- e. hadamard product\nsentan los fundamentos matema'ticos que sustentan la manip- el producto hadamard corresponde a la multiplicacio'n\nulacio'n de datos. se introducen escalares, vectores, matrices elementoaelementodedosmatricesotensoresdeigualforma.\ny tensores, junto con sus operaciones ba'sicas. en python se puede realizar usando el operador *.\na. escalar a = np.array([[1, 2],\n[3, 4]])\nun escalar es un valor nume'rico u'nico que representa una b = np.array([[5, 6],\nsolacantidad.enpytorchsepuederepresentarcomountensor [7, 8]])\ncon un solo elemento:\nc = a * b\nescalar = 6\nc:\nb. vector\n[[ 5 12]\nun vector es un arreglo unidimensional de escalares. cada [21 32]]\nelemento del vector es un escalar.\nimport numpy as np f. propiedades ba'sicas de la aritme'tica de tensores"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_012", "idx": 12, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "sumar o multiplicar un escalar con un tensor produce un\nvector = np.array([1, 2, 3])\ntensordelamismaformaqueeloriginal,dondecadaelemento\nse ve afectado por el escalar:\nvector:\na = 2\n[1 2 3] x = torch.arange(24).reshape(2, 3, 4)\na + x # suma escalar elemento a\n\nelemento\nc. matriz (a * x).shape # multiplicacion escalar,\n\nmantiene la forma\n\nal igual que los escalares son tensores de orden 0 y los\nvectores son tensores de orden 1, una matriz es un tensor de\ntensor([[[ 2, 3, 4, 5],\norden 2. es un arreglo bidimensional de escalares.\n[ 6, 7, 8, 9],\n[10, 11, 12, 13]],\n\nimport numpy as np\n[[14, 15, 16, 17],\n[18, 19, 20, 21],\nmatriz = np.array([[1, 2, 3],\n[22, 23, 24, 25]]])\n[4, 5, 6],\ntorch.size([2, 3, 4])\n[7, 8, 9]])\n1) reduccio'n: sepuedensumarloselementosdeuntensor\nmatriz: usando sum():\n[[1 2 3]\n[4 5 6] x = torch.arange(3, dtype=torch.float32)\n[7 8 9]] x, x.sum() # vector [0,1,2] y su suma\ntensor([0., 1., 2.]), tensor(3.)\nd. tensor\ncuandosetrabajacondatosdema'sdedosdimensiones,se parauntensormultidimensional,sum()pordefectoreduce\nutilizan tensores. son arreglos de orden 3 o superior. todos los ejes:\nimport numpy as np a = torch.arange(6, dtype=torch.float32).\nreshape(2, 3)\ntensor = np.array([[[1, 2, 3], [4, 5, 6]], a.shape, a.sum() # suma de todos los\n[[7, 8, 9], [10, 11, 12]]]) elementos\ntorch.size([2, 3]), tensor(15.)"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_013", "idx": 13, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "se puede especificar un eje para sumar a lo largo de filas o\ncolumnas:\na, a.sum(axis=0), a.sum(axis=1)\ntensor([[0., 1., 2.],\n[3., 4., 5.]]),\ntensor([3., 5., 7.]),\ntensor([3., 12.])\nreducir a mu'ltiples ejes simulta'neamente es equivalente a\nsumar todos los elementos:\na.sum(axis=[0,1]) == a.sum()\n\ntrue\nla media se calcula con mean(), equivalente a la suma\ndividida por el nu'mero de elementos:\na.mean(), a.sum()/a.numel()\na.mean(axis=0), a.sum(axis=0)/a.shape[0]\ntensor(2.5000), tensor(2.5000)\ntensor([1.5, 2.5, 3.5]), tensor([1.5, 2.5,\n3.5])\n2) sumasinreduccio'n: sisequiereconservarlaformadel\ntensor tras sumar:\nsum_a = a.sum(axis=1, keepdims=true)\na_normalized = a / sum_a # broadcasting para\n\nnormalizar filas\na, sum_a, a_normalized\ntensor([[0., 1., 2.],\n[3., 4., 5.]]),\ntensor([[ 3.],\n[12.]]),\ntensor([[0.0000, 0.3333, 0.6667],\n[0.2500, 0.3333, 0.4167]])\n3) sumaacumulada: sepuedecalcularlasumaacumulada\ncon cumsum():\na.cumsum(axis=0) # suma acumulada a lo largo"}
{"id_doc": "DOC_005", "segmentacion": "B", "chunk_id": "DOC_005_B_014", "idx": 14, "autor": "Kendall Rodríguez Camacho", "fecha": "2025-08-14", "tema": "Introducción a álgebra lineal aplicada con Python y fundamentos de machine learning, incluyendo tipos de aprendizaje y uso de librerías como PyTorch, NumPy y Pandas.", "texto": "de las filas\ntensor([[0., 1., 2.],\n[3., 5., 7.]])\n\nreferences\n[1] stevenpachechoportuguez,clasesobrealgebralinealymanipulacio'n\nde datos con pytorch, pandas y numpy, tecnolo'gico de costa rica,\n\n2025."}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_000", "idx": 0, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "inteligencia artificial\napuntesdeclase14/08/2025\njose pablo quesada rodr'ıguez\nescuela ingenier'ıa en computacio'n\ncartago, costa rica\njosepabloqr15@estudiantec.cr\n\nabstract-elpresentedocumentopretendefuncionaramanera oproponermejorasmetodolo'gicasparaquelosresultados\nde resumen detallado de la clase del d'ıa jueves 14/08/2025, se sean cient'ıficamente va'lidos.\nabarca un repaso de los temas vistos en la clase anterior a esta,\n- research scientist: ma's enfocado en la teor'ıa y la\nas'ıtambiencomoconceptosnuevosyexplicacio'ndeherramientas\ninnovacio'n que en la aplicacio'n pra'ctica. su objetivo\nutiles para el curso. se busca ampliar la informacio'n mediante\n\nreferencias bibliogra'ficas y adjuntar el apartado de noticias al principal es desarrollar nuevos modelos, marcos teo'ricos\nfinal del documento. o enfoques de aprendizaje automa'tico que expandan el\ncampo.\n\ni. introduccio'n\nse inicia la clase con una introduccio'n a los tipos de b. ingenier'ıa\naprendizaje, estos siendo un repaso ra'pido de estos ya que\nfueron vistos en la clase anterior - puesta en produccio'n de un modelo: bajo lo seleccionadoporeldatascientistdelmodelocient'ıfico,tomael\nmodelo dado y busca ponerlo en produccio'n para usarios"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_001", "idx": 1, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "tablei\n\ntiposdeaprendizajel masivos\n- transformar el modelo: busca la optimizacio'n de este\ntipo descripcio'n\nsupervisado conjunto de datos que tienen una etiqueta, la cual - onnx:seencargadehacertransformacionesenmodelos\nsupervisaelaprendizajeydeterminaquetanbieno para optimizar los modelos.\nmalseencuentraladescripcion. - mlops: debe pensar como tomar un modelo masivo y\nno-supervisado norequiereetiquetas,utilizaalgoritmosdemachine\n\ndisponibilizarlo para los usuarios\nlearning disen˜ados para descubrir patrones ocultos\no agrupaciones de datos sin la necesidad de intervencio'nhumana\n\niii. pipelineia\n\nsemi-supervisado usa pocos datos etiquetados junto a muchos no\netiquetados.\n\nauto-supervisado genera etiquetas a partir de los propios datos de\nentrada.(sus dataset/samples funcionan como sus\npropiasetiquetas)\nrefuerzo aprende con recompensas o castigos segu'n sus\nacciones.\nfew-shot generalizaapartirdepocosejemplos.\none-shot aprendeconunu'nicoejemplo.\nzero-shot realiza tareas sin ejemplos previos, usando\nconocimientoprevio.\n\nii. enfoquesamachinelearning\na. ciencia\n- generan conocimiento: se centran en la investigacio'n\ny en la produccio'n de nuevo conocimiento y tecnicas"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_002", "idx": 2, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "especificas sobre como utilizar modelos para generar\n\nnuevo conocimiento o pulir el existente\n- me'tricas: se enfocan en criterios cientificos para definir\nla me'tricas como por ejemplo la capacidad explicativa\n(que' tan bien un modelo ayuda a entender un feno'meno)\n- data scientist: orientado a extraer conocimiento de los\ndatos mediante experimentacio'n rigurosa, pero con una el pipeline de machine learning indica como se debe crear,\nvisio'n pra'ctica. a menudo trabajan con datos reales y hacer y mantener una inteligencia artificial segu'n [1] la linea\naplican te'cnicas existentes, pero tambie'n pueden ajustar de este pipeline es:\na. conseguir data b. problemas de optimizacio'n\nenestaseccionseconsiguenladata,seobtienedediferentes en estos problemas se tiene una funcio'n matema'tica con\nformas, esta data va a alimentar el modelo para su entre- un punto minimo, al encontrar ese punto minimo se tiene la\nnamiento, sin embargo esta puede contener informacio'n inutil mejorsolucio'n,existelasolucio'nlocallacualnosdalamejor\npara el entrenamiento del modelo o en formatos diferentes, solucio'n en un a'rea especifica de bu'squeda (sin embargo no\npor lo tanto se necesita limpiar la misma la mejor) y la solucio'n global que da la mejor solucio'n en\ntodo el espacio, (se busca encontrar esta solucio'n)\nb. limpiar la data(data preparation\nc. prediccio'n y clasificacio'n\nse procesa la data, buscando eliminar los datos inutiles o 1) prediccio'n: cuando se quiere determinar el valor real\nrellenar elementos faltantes de los mismos, tambien convertir de una funcio'n o predecir los valores de esta de acuerdo\nformatos, pueden llegar data de diferentes formas, sql o a las caracteristicas que tienen las muestras del data set\nnosql por ejemplo, todo con el objetivo de hacerlos fun- que lo componen, un ejemplo dado por el profesor fue las\ncionales como datos de entrenamiento, estandarizandolos. partesdelosvehiculosenlaprediccio'ndecuantocombustible\nconsume, variables dependientes que dependen de variables\nc. feature engineering independientes\n2) clasificacio'n: enestecasoenvezdebuscarpredecirun"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_003", "idx": 3, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "ayuda a definir las caracteristicas que vayan a definir\nvalor, se busca hallar la categor'ıa a la que pertenece basado\nlas predicciones correctas, consiste en la seleccio'n correcta\nen sus caracter'ısticas, ejemplo la marca del vehiculo con base\n\nde que variables considerar como relevantes para obtener\na sus piezas.\nmejores resultados, el objetivo es proporcionar features ma's\ninformativos y relevantes, elegir que caracteristicas conservar d. agrupamiento\n\ny que deshechar\n\nconjunto de datos no etiquetados y se busca encontrar\npatro'nes de estos mismos datos, aqu'ı aplica el k-means\nd. seleccio'n del modelo\n\nv. modelosdeterministaoestoca'stico\n\nconsiste en saber identificar el problema y que modelo\n1) determinista: este modelo indica que para una entrada\nameritasuuso,unabuenasintesisparadefinirestoesmediante\nde datos retoran una salida siempre consistente, un ejemplo\nla caracter'ıstica de explicabilidad, si se trabaja con problemas\nde esto ser'ıa por ejemplo ¿hay luz al medio d'ıa?\nquevananecesitarunaretroalimentacio'nclaradedondeviene\n2) estoca'stico: este modelo indica que para una entrada\nla informacio'n, un modelo de red neuronal no sera' lo ma's"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_004", "idx": 4, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "puede retornar un resultado a partir de un conjunto de los\nindicado.\nposiblesresultados,muyaleatorio,unejemploser'ıa¿cualsera'\nel clima a medio d'ıa?\ne. entrenar el modelo\n\nvi. conjuntoai\n\nel modelo es entrenado con alguno de los metodos de\naprendizaje vistos, teniendo en consideracio'n parametros e\nhiperpara'metros, siendo estos ultimos los que ayudara'n a\ndirigir al modelo por el camino deseado, un ejemplo de esto\nesk-means,el cualesunalgoritmo pararealizarclusteringen\n\nun modelo de aprendizaje no-supervisado\nf. validar el modelo\n\nen esta parte ya se tiene el modelo entrenado y se busca\nel testeo del mismo y la validacio'n, se prueba con situacio'nes\nsimilaresalasdeentrenamiento,peronoexactamenteiguales,\n\ncon el objetivo de verificar que se comporte de la manera\ndeseada y al final se realiza una validacio'n del modelo, como\nunaespeciedeexamenfinalysecomparaconproduccio'npara\n\nverificar que el mismo"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_005", "idx": 5, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "iv. paradigmaderesolucio'ndeproblemas\na. problemas de bu'squeda\nen estos problemas, los algoritmos tratan de seguir un - inteligencia artificial: algoritmos generativos\ncamino para llegar a una solucio'n optima, la solucio'n optima - machine learning: utiliza metodos estadisticos, como\nse considera siempre la opcio'n ma's \"barata\" regresio'n lineal, log'ıstica y arboles de decisio'n\n- deep learning: redes neuronales (utilizadas para re- x. tensores\nsolver problemas complejos)\n¿que son tensores? segu'n [2] los tensores han existido\n- generative ai: generacio'n de nuevo contenido uti- desde que william hamilton acun˜o' el te'rmino hace 200 an˜os\n\nlizando deep learning\npara describir un objeto matema'tico que representa un conjunto de nu'meros con algunas propiedades de transformacio'n.\n\nvii. iniciodematerialnuevo\na. definicio'n de tensor\n\nviii. jupyternotebook\nuntensoresunobjetomatema'ticoquegeneralizaescalares,\nen el curso se ha explicado que para todas las entregas vectoresymatricesenespaciosdedimensionessuperiores.es\nse van a utilizar jupyter notebooks debido a la facilidad que un arreglo de nu'meros y funciones que abarca magnitudes\ntiene esto para representar tanto celadas de texto como celdas f'ısicas, transformaciones geome'tricas y diversas entidades\nde co'digo. se puede usar una extensio'n de visual studio matema'ticas."}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_006", "idx": 6, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "code para esto existen tensores unidimensionales los cuales son llamados\nvectores, bidimensionales en formas de matriz y cuando k ¿\na. recomendaciones del profesor 2 ejes se deja de usar un nombre especifico y se le conoce\n\ncomo tensor de orden k\n1) utilizar conda: conda es una herramienta que permite\ntener diferentes ambientes de desarrollo por separado, lo cual xi. introduccio'napytorch\n\npermite trabajar con sin errores de compatibilidad al volver a\na. librerias necesarias para la manipulacio'n de tensores,\n\nproyectos antiguos\n\narreglos y estructuras de datos tabulares\n\nix. tutorialdeinstalacio'ndeanaconday\n\njupyternotebook import torch\n\nimport numpy as np\n1) paso uno: ingresar a la pagina oficial de anaconda import pandas as pd\nhttps://anaconda.org/ y descargar la versio'n requerida de\n\nanaconda de acuerdo a su sistema operativo\nb. ¿como crear tensores?\n2) paso dos: instalarlo y crear una nuevo ambiente\nla funcio'n arange (n) funciona para generar tensores prellenados con valores espaciados uniformemente, comenzando\nen 0 (inlcuido) hasta el n (no incluido) los tensores recien"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_007", "idx": 7, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "creados se almacenan en memoria principal\nx= torch.arange(12,dtype=torch.float32)\nx# tensor unidimensional que puede ser operado\n\ncon diversas funciones que pueden ser\n\ninvocadas sobre el\n3) paso tres: seleccionar la versio'n del nuevo ambiente\nc. funciones sobre tensores\n4) paso cuatro: instalar la extensio'n de jupyter en vscode y\ncrear un archivo con extensio'n .ipynb - .numel() indica el numero de elementos que tiene el\n\ntensor\n5) pasocinco:seleccioneenlaopcio'ndekernel,elambiente\nque creo en anaconda - .shape se usa para acceder al taman˜o de cada eje del\n\ntensor\n- .reshape se usa para reorganizar las dimensiones del\n\ntensor sin copiar los datos se puede pasar de un tensor\nde una dimensio'n por ejemplo el ejemplo anterior de 12\nelementos, a un tensor bidimensional de 3 filas por 4"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_008", "idx": 8, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "columnas\n- torch.zeros((z,x,y)) / torch.ones((z,x,y)) /\ntorch.grandn(x,y) se utilizan para crear tensores\nde diferentes dimensiones, relleno con ceros, unos o\na. recomendaciones de profesor\n\nnumeros random\nen caso de que falten dependencias utilizar pip install - operaciones elemento a elemento pytorch permite opcon las dependencias requeridas, es buena idea tener un eracionesaritmeticasentretensoreslascualesseaplicara'n\nrequirements.txt para agilizar el proceso elemento a elemento\n- concatenaciones de tensores mediante\ntorch.cat((x.y),dim=k se pueden concatenar tensores,\ndf = pd.read_csv('../data/house_tiny.csv')\nsiendo k el eje donde sobre el que se apilara'n\ninputs = df.iloc[:, :2]\n- indexacio'n lo'gica mediante x==y siendo x y y ten- inputs = pd.get_dummies(inputs, dummy_na=true)\nsores se pueden realizar mascaras booleanas inputs = inputs.fillna(inputs.mean())\nx_csv = torch.tensor(inputs.to_numpy(dtype=\nxii. creaciondesdelistasdepython float))\nx_csv\na= torch.tensot([[2,1,4,3],\n[1,2,3,4],\n[4,3,2,1]], dtype=torch.float32) xvii. algebralinealintroduccio'n\na\na. escalar\ntensor([[2.,1.,4.,3.],\n[1.,2.,3.,4.], esunvalornume'ricoquerepresentaunasitaucio'nalavez,\n[4.,3.,2.,1.]])\nbasicamente es un nu'mero, un unico elemento\nxiii. indexacionysegmentacio'n(slicing)\nb. vectores\nse puede pensar en un vector como un arreglo de taman˜o\nfila_ultima = a[-1] # ltima fila de x\nsubmatriz = a[1:3] # filas 1 y 2 de x fijo de escalares, un vector no ser'ıa ma's que un conjunto de\nfila_ultima, submatriz escalares\nxiv. broadcasting c. matrices"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_009", "idx": 9, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "el broadcasting en tensores permite operar tensores de\n\nsepuedevercomounarreglodearreglosocomountensor\ndiferentes formas al expandir automaticamente,\n\nde orden 2\na = torch.arange(3).reshape((3, 1)) # forma 3\nx1 d. tensores de orden superior\nb = torch.arange(2).reshape((1, 2)) # forma 1\nx2 ya no tienen nombre espec'ıfico ma's que tensores de orden\nbroadcast = a + b # resultado de forma 3x2 k\ngracias al broadcasting una propiedad util de los escalares, vectores, matrices\n\ny tensores es que las operaciones elemento a elemento\nxv. operacionesin-place generan resultados que tienen la misma forma que sus\n\noperandos\n\nse indica que se tiene que tener cuidado debido a que\n\ncuando se tratan con modelos de machine learning o deep\nlearning, nos encontramos con el hecho de que se ocupa xviii. productohadamard\nmuchisima memoria y hay que procurar ser o'ptimos en este\nsentidoyalhacerejecutaroperacionessepuededejarmemoria consiste en la multiplicacio'n de elemento a elemento de\nasignada o se apunta a nuevas secciones de memoria, lo dos matrices de un mismo taman˜o\ncual puede ser innecesario. por eso se recomienda el uso de"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_010", "idx": 10, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "import numpy as np\noperacio'nes totalmente in-place\n# definicin de matrices\nxvi. conversio'nanumpyycargadedatosdesde\nx = np.array([[1, 2, 3],\ncsv\n[4, 5, 6]])\nse puede interoperar con numpy, convirtiendo tensores a\ny = np.array([[7, 8, 9],\n\narreglos y viceversa sin copiar datos\n[10, 11, 12]])\na_np = a.numpy() # producto de hadamard (elemento a elemento)\nprint(type(a_np)) z = x * y\na_back = torch.from_numpy(a_np)\nprint(type(a_back)) print(z)\n# resultado:\npara cargar datos de un archivo csv usaremos pandas # [[ 7 16 27]\npara convertir sus columnas a tensores. adema's de usar # [40 55 72]]\ncodificacio'n one-hot para completar valores faltantes.\nxix. propiedadesbasicasdelaaritmeticade\na.mean(), a.sum() / a.numel() #se obtiene el\n\ntensores\n\nmismo resultado\nsumando o multiplicando un escalan y un tensor, producira\nun tensor del mismo taman˜o como el tensor original. cada\nelementodeeltensosessumadoomultiplicadoporelescalar.\nxxi. sumasinreduccio'n"}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_011", "idx": 11, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "si se desea conservar el numero de ejes al sumar como\na = 2\nx = torch.arange(24).reshape(2, 3, 4) cuando se desea aprovechar el broadcasting, se usa\na + x, (a * x).shape sum_a = a.sum(axis=1, keepdims=true)\nsum_a, sum_a.shape\nresultado:\n(tensor([[ 3.],\n(tensor([[[ 2, 3, 4, 5],\n[12.]]),\n[ 6, 7, 8, 9],\n[10, 11, 12, 13]],\ntorch.size([2, 1]))\n[[14, 15, 16, 17],\n\nsi se desea calcular la suma acumulada de los elementos\n[18, 19, 20, 21],\nde un tensor, se puede usar cumsum\n[22, 23, 24, 25]]]),\ntorch.size([2, 3, 4]))\na.cumsum(axis=0)\ntensor([[0., 1., 2.],\nxx. reduccio'n\n[3., 5., 7.]])\n\npodemos realizar la suma de tensores se puede invocar\nsum() sin argumentos, esto hara' que se reduzca a un escalar\nxxii. noticiashabladasenclase\na. alexander wang fundador de scale\n# crear una matriz a de forma (2, 3)\ncon valores [0,1,2,3,4,5] se hablo' del caso de alexander wang un joven de solo 28\na=torch.arange(6,dtype=torch.float32).reshape an˜osquefundo' laempresascaleai,empresaporlacualmeta\n(2,3)\ninvirtio' 13 mil millones de euros\n# mostrar la forma de a y la suma\nde todos sus elementos b. guerra de plataformas de llm\na.shape, a.sum()\nsemenciono' quelacompetenciaporserlamejoria,existe\n(torch.size([2, 3]), tensor(15.)) actualmente como si fuera una guerra entre plataformas, de la\nmisma forma que ha ocurrido en otras cosas en el pasado,\ncomo lo reduce a lo largo de sus ejes, se puede especificar\ncomo las plataformas de streaming."}
{"id_doc": "DOC_006", "segmentacion": "B", "chunk_id": "DOC_006_B_012", "idx": 12, "autor": "Jose Pablo Quesada Rodríguez", "fecha": "2025-08-14", "tema": "Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de álgebra lineal y tensores en PyTorch.", "texto": "alguna de sus ejes x o y para sumar a lo largo del respectivo\n\neje usando el parametro axis references\n[1] s.pachecoportuguez,\"clasealgebralinealymanipulacio'ndetensores\na, a.sum(axis=0), a.sum(axis=1) mediantepytorch,\"tecnolo'gicodecostarica,2025.\n[2] i.valchanov,\"¿que'sonlostensores?\"365datascience,2023.[online].\navailable:https://365datascience.com/tutorials/python-tutorials/tensor/\na--->(tensor([[0., 1., 2.],\n[3., 4., 5.]])\na.sum(axis=0) ---> tensor([3., 5., 7.]),\na.sum(axis=1)--->tensor([ 3., 12.]))\n\nsi se reduce a lo largo de todos sus ejes equivale a sumar\n\ntodos los elementos de la matriz\na.sum(axis=[0, 1]) == a.sum()\ntensor(true)\nla media se calcula usando mean, la cual se puede definir\n\ncomo la suma de todos los elementos dividido entre el total\n\nde estos\na.mean(), a.sum() / a.numel()\n#comparacin entre ambas formas de sacar la\n\nmedia"}
{"id_doc": "DOC_007", "segmentacion": "B", "chunk_id": "DOC_007_B_000", "idx": 0, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "inteligencia artificial\n\napuntesdeclase-19deagosto2025\n\njavier rojas rojas\nescuela de ingenier'ıa en computacio'n\ntecnolo'gico de costa rica\ncartago, costa rica\njavrojas@estudiantec.cr\n\nabstract-this paper summarizes the topics covered in class\non august 19, 2025. the session focused on two main areas: a\nreviewoflinearalgebraconcepts,withemphasisonvectors,and u=b-a=(b 1 -a 1 , b 2 -a 2 , ..., b n -a n )\nan introduction to supervised learning. in the first part, vectors\nde esta manera, cada componente del vector se obtiene\nwere examined in terms of their definitions, norms, distances,\nand operations, highlighting their importance as mathematical restandolascoordenadascorrespondientesdelpuntodeorigen\ntools for representing and comparing data. in the second part, a las del punto final.\n\nsupervised learning was introduced through the notions of\nb. norma de un vector\nfeatures, labels, regression, and classification. the connection\nbetween these two areas illustrates how algebraic concepts serve lanormadeunvectorrepresentasumagnitud,esdecir,que'\nas the foundation for machine learning techniques.\ntan grande es o cua'nto mide desde el punto de inicio hasta el"}
{"id_doc": "DOC_007", "segmentacion": "B", "chunk_id": "DOC_007_B_001", "idx": 1, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "i. introduccio'n punto final. se denota como ∥x∥.\n\nexisten diferentes formas de medir la norma de un\nestosapuntesesta'nbasadosenlaclasedel19deagostode\nvector:\n2025 [1]. el a'lgebra lineal es la base matema'tica de muchas\n1) norma l1: distancia manhattan: la distancia mante'cnicas de inteligencia artificial. en particular, los vectores\n\nhattan entre dos puntos se calcula sumando las distancias\npermitenrepresentardatos,medirsimilitudesyrealizaroperaabsolutas de cada componente, desplaza'ndose u'nicamente a\nciones fundamentales en espacios multidimensionales. estos\n\nlo largo de los ejes del espacio desde el punto de inicio hasta\n\nconceptos resultan esenciales para entender el aprendizaje\nelpuntofinal.estame'tricaseutilizaespecialmentecuandono\nsupervisado, paradigma en el que los datos se expresan mese permiten desplazamientos negativos.\ndiante vectores de caracter'ısticas y etiquetas asociadas. dicho\nenfoque permite construir modelos de regresio'n, orientados a (cid:88) n\n|x -y |\npredecir valores continuos, y de clasificacio'n, enfocados en i i\nasignar categor'ıas. este documento resume lo visto en clase, i=1\ndondesetocaronambostemas,destacandolaconexio'nentrela\nteor'ıaalgebraicaysuaplicacio'nenelaprendizajeautoma'tico."}
{"id_doc": "DOC_007", "segmentacion": "B", "chunk_id": "DOC_007_B_002", "idx": 2, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "ii. repasodea' lgebralineal\na. ¿que' es un vector?\n\nun vector es una estructura de datos que permite organizar\ninformacio'n de forma secuencial y que tambie'n puede interpretarse geome'tricamente.\n1) caracter'ısticas:\n- tieneunpuntodeorigen(normalmente(0,0))yunpunto\nfinal.\n- posee una direccio'n espec'ıfica.\n- tiene una magnitud (distancia entre el punto de origen y\npunto final). fig.1. ejemplodedistanciamanhattan\n2) vector como desplazamiento: un vector puede inter- 2) norma l2: distancia euclidiana: la distancia eupretarse como un desplazamiento entre dos puntos en un clidiana representa la distancia directa entre dos puntos y\nespacio n-dimensional. si se considera un punto de origen se calcula usando el teorema de pita'goras. para un vector\na = (a 1 ,a 2 ,...,a n ) y un punto final b = (b 1 ,b 2 ,...,b n ), x=(x 1 ,x 2 ,...,x n ), su norma es:"}
{"id_doc": "DOC_007", "segmentacion": "B", "chunk_id": "DOC_007_B_003", "idx": 3, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "elvectorquerepresentaeldesplazamientodeaabsecalcula\n(cid:113)\ncomo: ∥x∥= x2+x2+...+x2\n1 2 n\nc. propiedades de la norma\n- positividad: ∥x∥≥0 y ∥x∥=0 si y solo si x=0.\n- homogeneidad: ∥αx∥=|α|-∥x∥ para cualquier escalar\nα.\n- desigualdad triangular: ∥x+y∥≤∥x∥+∥y∥.\nd. vector unitario\n- es aquel vector que tiene una longitud o magnitud de 1. fig.3. ejemploproductopunto\n- se puede obtener el vector unitario de un vector u\nf. identidad del coseno\ndividiendo el vector u entre su norma.\n\nla identidad del coseno para el producto punto entre dos\nvectores u y v en funcio'n de sus magnitudes y el a'ngulo θ\nu es:\nuˆ =\n∥u∥\nu-v=∥u∥-∥v∥-cos(θ)\nal multiplicar este vector por un escalar, se puede hacer\ncrecerodecrecer,osisetieneunescalarespec'ıfico,sepodr'ıa\nllegar al vector original.\nfig.4. a'nguloentredosvectoresayb[2]\n¿por que' es esto importante? esta es una de las me'tricas\nma's importantes para determinar la similitud entre vectores,\nya que entre menor sea el a'ngulo, los vectores sera'n ma's\nsimilares.\nen nlp (natural language processing), esto puede ser\nusado para identificar que' tan similares son dos frases, transformando las frases en vectores y midiendo ese a'ngulo.\n1) ejemplo de ca'lculo de a'ngulo entre dos vectores: se\nfig.2. ejemplodeunvectorysuvectorunitario consideran los vectores u=(1,2) y v=(3,4).\npaso 1: calcular el producto punto\nu-v=1-3+2-4=3+8=11\ne. producto punto\npaso 2: calcular las normas\nes el producto interno entre dos vectores, siempre genera (cid:112) √ (cid:112) √\n∥u∥= 12+22 = 5, ∥v∥= 32+42 = 25=5\nun escalar.\npaso 3: aplicar la identidad del coseno"}
{"id_doc": "DOC_007", "segmentacion": "B", "chunk_id": "DOC_007_B_004", "idx": 4, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "u-v 11 11\ncos(θ)= = √ = √ ≈0.9839\n y  ∥u∥-∥v∥ 5-5 5 5\n1\nxty∈rn = (cid:2) x 1 x 2 ... x n (cid:3)     y . . . 2    = (cid:88) n x i y i paso 4: calcul θ ar = el ar a' c n c g o u s l ( o 0.9839)≈10.3◦\ni=1\ny\nn g. vector co-direccional\nson dos vectores que siguen la misma direccio'n, pero con\nes especialmente u'til en inteligencia artificial, ya que se magnitudes distintas:\npuede usar como indicador de importancia de features, utiu=k-v\nlizandounvectordepesos,yalrealizarlaoperacio'nnosdar'ıa\nun indicador de la importancia del feature. estos al ser iguales en direccio'n, su a'ngulo es de 0◦.\n1) ejemplo:vectorysuvectorunitario: seconsiderav= j. ortogonalidad\n(4,3) y su vector unitario u=(0.80,0.60). dos vectores x y y son ortogonales si y so'lo si:\nv=(4,3), u=(0.80,0.60) x-y=0\npaso 1: calcular el producto punto y se denota como x⊥y.\nk. ortonormalidad\nu-v=0.80-4+0.60-3=3.2+1.8=5.0\nsiadema'sdeserortogonales,∥x∥=1y∥y∥=1,esdecir,\npaso 2: calcular las normas ambos vectores son unitarios, entonces se dice que x y y son\nortonormales.\n(cid:112) √ √\n∥u∥= 0.802+0.602 = 0.64+0.36= 1=1"}
{"id_doc": "DOC_007", "segmentacion": "B", "chunk_id": "DOC_007_B_005", "idx": 5, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "iii. supervisedlearning\n(cid:112) √ √ setieneundataset,dondesetieneunconjuntodeejemplos\n∥v∥= 42+32 = 16+9= 25=5 que se toma de algu'n feno'meno, y unas etiquetas que nos\npaso 3: usar la identidad del coseno ayudara'n a realizar nuestro proceso de aprendizaje.\na. feature\n\nu-v 5\ncos(θ)= = =1⇒θ =cos-1(1)=0◦ - es una propiedad o atributo medible de una entidad.\n∥u∥-∥v∥ 1-5\n- normalmente se representa nume'ricamente para ser\nconclusio'n:sedemuestraqueela'nguloentredosvectores procesada\nco-direccionales es cero. - ejemplos:\nh. ¿que' ocurresicalculaelproductopuntoconsigomismo? - altura de una casa\n- peso de un individuo\nsesabequeunvectoresco-direccionalconsigomismo,por\nlo tanto su a'ngulo es de 0◦: - intensidad de un pixel en una imagen\nu-u=∥u∥-∥u∥-cos(0◦) b. vector de caracter'ısticas\nse denota un vector de caracter'ısticas como\nu-u=∥u∥2\nx=(x ,x ,...,x )∈rd\nse puede expresar la distancia l2 en te'rminos de producto 1 2 d\npunto: √ donde cada x i es un feature. este vector agrupa todas las\nu-u=∥u∥ propiedades de un ejemplo en una sola estructura.\nsi se tiene solamente el vector u, se puede calcular su\nmagnitud, o hacer la operacio'n inversa. nu'merodehabitaciones metroscuadrados cantidaddejardines\n2 250 1"}
{"id_doc": "DOC_007", "segmentacion": "B", "chunk_id": "DOC_007_B_006", "idx": 6, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "i. ¿que' sucede con vectores que tienen a'ngulo de 90◦?\ncuando dos vectores forman un a'ngulo de 90◦: tablei\nejemplodevectordecaracter'isticas\nu-v=∥u∥-∥v∥-cos(90◦)=∥u∥-∥v∥-0=0\nc. label\nelproductopuntoenestecasoser'ıacero.sedicequeestos\nvectores son perpendiculares. - valor objetivo que se quiere predecir\n- puede ser\n- continuo: y ∈r (regresio'n)\n- discreto: y ∈ {1,...,k} (clasificacio'n, donde en\nel conjunto finito se representar'ıa la categor'ıa)\nd. dataset\nconjunto de datos que se utilizara'n que tiene la forma\n{(x ,y )}n\ni i i=1\nnu'merodehabitaciones metroscuadrados cantidaddejardines precio\n2 250 1 250000\n3 350 5 650000\n\ntableii\n\nejemplodedataset\nfig.5. ejemplovectorescona'ngulode90◦\ndonde las primeras columnas son features y \"precio\" ser'ıa clasificacio'n? respuesta: regresio'n, ya que se esta'\nel label. prediciendo valores continuos de esperanza de vida.\n2) basado en datos sobre animales, se dispone del peso de\ne. subcategor'ıas principales\ncada ejemplar y de si tiene alas o no. se esta' tratando\n1) regresio'n:\nde determinar cua'les animales son pa'jaros. ¿es este\n- consiste en ajustar una curva o l'ınea que pase lo ma's un problema de regresio'n o clasificacio'n? respuesta:\ncerca posible de un conjunto de puntos de datos. clasificacio'n, ya que se esta' determinando si pertenece\n- se emplea para analizar tendencias, por ejemplo: a la categor'ıa \"pa'jaro\" o no.\n- ¿existe una relacio'n directa entre las iniciativas de 3) basado en datos sobre dispositivos informa'ticos, se\nmarketing (anuncios en l'ınea) y las ventas reales de cuenta con el taman˜o de pantalla, el peso y el sistema\nun producto? operativo de varios dispositivos. se quiere determinar\n- ¿co'mo afecta el tiempo al valor de una crip- cua'les dispositivos son tablets, laptops o tele'fonos. ¿es\ntomoneda? ¿aumentara' exponencialmente su valor este un problema de regresio'n o clasificacio'n? recon el paso del tiempo? spuesta: clasificacio'n, ya que se esta' asignando dispositivos a categor'ıas discretas.\n4) basado en datos meteorolo'gicos, se tiene la cantidad\nde precipitacio'n y un valor de humedad. se quiere\ndeterminar la humedad en diferentes e'pocas del an˜o.\n¿es este un problema de regresio'n o clasificacio'n?\nrespuesta:regresio'n,yaqueseesta' prediciendovalores\ncontinuos de humedad."}
{"id_doc": "DOC_007", "segmentacion": "B", "chunk_id": "DOC_007_B_007", "idx": 7, "autor": "Javier Rojas Rojas", "fecha": "2025-08-19", "tema": "Revisión de álgebra lineal y aprendizaje supervisado, enfatizando el papel de los vectores y su aplicación en regresión y clasificación.", "texto": "references\n[1] s.a.p.portuguez,\"apuntesdelaclasedeinteligenciaartificial,\"cartago,\ncostarica,agosto2025,clasedel19deagostode2025.\n[2] f. explicadas. (2025) a'ngulo entre dos vectores. [online]. available:\nhttps://www.formulasexplicadas.com/angulo-entre-dos-vectores/\nfig.6. ejemploderegresiones\n2) clasificacio'n:\n- tienecomoobjetivopredecirlacategor'ıaoclasealaque\npertenece un ejemplo, segu'n sus caracter'ıstica.\n- ejemplo pra'ctico:\n- ¿sepuededeterminarsiunveh'ıculoesunautomo'vil\no un camio'n basa'ndonos en su nu'mero de ruedas,\npeso y velocidad ma'xima?\nfig.7. ejemplodeclasificacio'n\n\niv. ejercicioscomprobatoriosdesupervised\n\nlearning\n1) basado en datos sobre ratas, se tiene una caracter'ıstica\n\nde esperanza de vida y una de obesidad. se esta' intentando encontrar una correlacio'n entre las dos caracter'ısticas. ¿es este un problema de regresio'n o"}
{"id_doc": "DOC_008", "segmentacion": "B", "chunk_id": "DOC_008_B_000", "idx": 0, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "repaso de álgebra lineal y aprendizaje\n\nsupervisado\ninstituto tecnológico de costa rica\nescuela de ingeniería en computación\n\ninteligencia artificial\nmariana quesada sánchez\n19 de agosto de 2024\n\nabstract-this paper reviews concepts of linear algebra necesario para trasladarse de un punto a otro. por ejemplo,\nrelevant to artificial intelligence, including vectors, norms, el vector (4,3) puede interpretarse como la posición de un\ndistances, dot product, orthogonality, and orthonormality. it\npuntoenelplanocartesiano,perotambiénpuederepresentar\nalsointroducestheprinciplesofsupervisedlearning,describing\neldesplazamientorequeridoparapasardelorigen(0,0)hasta\n\ndatasets as feature-label pairs and distinguishing between\nregressionandclassificationtasksthroughillustrativeexamples. dicho punto.\ny\n\ni. introduction\nel álgebra lineal es la base para representar datos en (4,3)\n\nespacios multidimensionales y para definir operaciones que 3\npermiten medir magnitudes, direcciones y similitudes. estos\n\nfundamentos son indispensables en algoritmos de machine\nlearning, en particular dentro del aprendizaje supervisado,\ndonde los datos se representan como vectores de características asociados a etiquetas.\nx\n4"}
{"id_doc": "DOC_008", "segmentacion": "B", "chunk_id": "DOC_008_B_001", "idx": 1, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "ii. álgebralineal\na. vectores\nun vector se define como una entidad matemática car- fig.1. representacióngráficadelvector(4,3)enelplanocartesiano.\nacterizada por magnitud y dirección. en espacios de dos\no tres dimensiones, puede visualizarse como un segmento\nb. norma o magnitud\norientado que parte del origen y termina en un punto (x,y,z).\nen espacios de dimensión n, se representa como una tupla la norma mide longitud de un vector y se denota como\nordenada (x , x , ..., x ). los vectores constituyen la base ∥x∥.geométricamente,puedeinterpretarsecomoladistancia\n1 2 n\ndelarepresentacióndedatosenespaciosmultidimensionales desde el punto de origen hasta el punto final definido\ny permiten operaciones como la suma, la resta y la multipli- por x. de esta manera, la norma proporciona una medida\ncación por escalares. cuantitativa de la magnitud del vector, independientemente\nel desplazamiento de un vector se define como la difer- de su dirección.\nencia entre un punto final b = (b 1 ,b 2 ,...,b n ) y un para un vector x = (x 1 ,x 2 ,...,x n ), las normas más\npunto inicial a = (a 1 ,a 2 ,...,a n ). formalmente, el vector comunes son:"}
{"id_doc": "DOC_008", "segmentacion": "B", "chunk_id": "DOC_008_B_002", "idx": 2, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "desplazamiento se expresa como\n- norma l1 o manhattan:\na⃗b =b-a=(b -a , b -a , ..., b -a ), la distancia manhattan entre dos puntos a =\n1 1 2 2 n n\n(x ,y ,z ,...,n ) y b = (x ,y ,z ,...,n ) en un\n1 1 1 1 2 2 2 2\nlo cual indica cuánto debe recorrerse en cada componente\nespacio n-dimensional se calcula mediante la fórmula:\nparapasardeaab.porejemplo,sia=(1,2)yb =(4,6),\nentonces a⃗b = (3,4), lo que representa un movimiento de\nn\ntres unidades en el eje x y cuatro en el eje y. (cid:88)\n∥x∥ = |x -y |\n1 i i\nes importante distinguir entre un vector de posición y\ni=1\nun vector de desplazamiento. un vector de posición ubica\nun punto específico en el espacio con respecto al origen, se interpreta como la distancia recorrida siguiendo los\nmientrasqueunvectordedesplazamientodescribeelcambio ejes de la cuadrícula.\n- norma l2 o euclidiana: el producto punto entre dos vectores u y v se define de\n(cid:118) dos formas equivalentes:\n(cid:117) n\n∥x∥ 2 = (cid:117) (cid:116) (cid:88) x2 i - definición algebraica:\nn\ni=1 (cid:88)\nu-v = u v\ni i\ncorrespondealadistanciaenlínearectaentreelorigen\ni=1\nyelpuntofinal,deacuerdoconelteoremadepitágoras.\nej 1. sea\n   \n1 4\nx=2, y =5.\n3 6\ncalculamos el producto punto:\n \n4\nxty = (cid:2) 1 2 3 (cid:3) 5\n6\n=1-4+2-5+3-6=4+10+18= 32\n- definición geométrica:\nfig.2. comparaciónentredistanciamanhattanyeuclidiana[1]. u-v =∥u∥∥v∥cos(θ)\ndonde θ es el ángulo entre ambos vectores.\nuna función es considerada una norma si cumple las\nej 2. sea\nsiguientes propiedades: (cid:20) (cid:21) (cid:20) (cid:21)\n1 3\n1) positividad: ∥x∥ ≥ 0 y ∥x∥ = 0 si y sólo si x es el u= , v = .\n2 4\nvector nulo.\npaso 1: calcular el producto punto\n2) homogeneidad:∥αx∥=|α|∥x∥paracualquierescalar\nα∈r. u-v =1-3+2-4=3+8=11\n3) desigualdad triangular: ∥x+y∥ ≤ ∥x∥+∥y∥ para\npaso 2: calcular las normas\ntodos los vectores x e y. (cid:112) √ (cid:112) √\n∥u∥= 12+22 = 5, ∥v∥= 32+42 = 25=5\nc. vectores unitarios\nfinalmente, usando la definición geométrica del producto\nunvectorunitarioesaquelcuyanormaesigualauno.se\npunto:\nobtiene normalizando un vector v mediante su magnitud:\nu= v √ 11\n∥v∥ 11= 5-5-cos(θ) ⇒ cos(θ)= √ ≈0.9839\n5 5\nde esta manera, u conserva la dirección de v, pero con\nlongitud unitaria. θ =cos-1(0.9839)≈10.3◦\ne. vectores codireccionales"}
{"id_doc": "DOC_008", "segmentacion": "B", "chunk_id": "DOC_008_B_003", "idx": 3, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "dos vectores se consideran codireccionales cuando\nmantienen la misma dirección, aunque difieran en magnitud.\nestarelaciónsecumplesiexisteunescalarktalquev =k-u.\nen este caso, el ángulo entre ambos vectores es nulo y\nel coseno del ángulo es igual a uno. el vector unitario\nes un caso particular, ya que al ser multiplicado por un\n\nescalar recupera la magnitud del vector original sin alterar\nsu dirección.\nsesabequesidosvectoressoncodireccionales,elángulo\nentre ellos es de 0◦. en consecuencia, el producto punto se\nfig.3. vectorunitario.[2]\n\nexpresa como\nd. producto punto u-u=∥u∥-∥u∥-cos(0)=∥u∥2.\nel producto punto entre dos vectores es la suma de las de esta forma, la norma de un vector puede escribirse\nmultiplicaciones de sus componentes, lo que produce un como\n√\nvalor real. esta operación es fundamental en inteligencia ∥u∥= u-u.\nartificial, ya que un vector puede representar características\nasimismo, la distancia euclidiana puede expresarse en"}
{"id_doc": "DOC_008", "segmentacion": "B", "chunk_id": "DOC_008_B_004", "idx": 4, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "de los datos y otro vector puede representar los pesos\ntérminos de producto punto:\nasociados a dichas características. si un peso es cero, la\n√ (cid:112)\ncaracterística correspondiente no contribuye al resultado. u-u= ∥u∥2 =∥u∥.\nf. ortogonalidad y ortonormalidad a. ejercicios de aprendizaje supervisado resueltos\ndosvectoressonortogonalessisuproductopuntoescero: 1) ratas: esperanza de vida vs. obesidad\ndado un conjunto de datos con dos características\nu-v =0 (esperanza de vida y obesidad) se busca modelar la\nrelación entre ambas.\nademás, un conjunto de vectores es ortonormal si además tipo: regresión (la salida es un valor continuo).\nde ser ortogonales, cada vector es unitario. 2) animales: identificar aves\ntomando en cuenta datos sobre animales, el peso y si\n\niii. aprendizajesupervisado tiene alas, se desea determinar cuáles son pájaros.\ntipo: clasificación (se clasifica cada ejemplar como"}
{"id_doc": "DOC_008", "segmentacion": "B", "chunk_id": "DOC_008_B_005", "idx": 5, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "elaprendizajesupervisadoconsisteenentrenarunmodelo\nave o no empleando el peso y la presencia de alas).\n\na partir de un conjunto de datos donde cada ejemplo se\n3) dispositivos: tablet, laptop o teléfono\nencuentra representado por un vector de características x\ni\ncon tamaño de pantalla, peso y sistema operativo, se\ny una etiqueta asociada y . las características describen\ni\ndebe asignar cada dispositivo a una de varias catepropiedadescuantificablesdelfenómenoobservado,mientras\ngorías.\nque la etiqueta corresponde al valor que se desea predecir.\ntipo: clasificación.\nexisten dos tareas principales dentro del aprendizaje su4) meteorología: precipitación → humedad.\n\npervisado\ncon cantidad de precipitación y un valor de humedad,\nla regresión busca predecir valores continuos, como el\nse desea predecir la humedad en distintas épocas del\nprecio de una vivienda en función de atributos como área,\naño.\nnúmero de habitaciones o ubicación. la fig. 4 corresponde\ntipo: regresión (humedad como variable continua).\naunproblemaderegresiónporquebuscaajustarunafunción\nque modele la relación entre una variable independiente references\n(carat) y una variable dependiente continua (precio).\n[1] s.raniandg.sikka,\"recenttechniquesofclusteringoftimeseries\ndata:asurvey,\"artificialintelligencereview,vol.46,no.1,pp.2744,2016.available:https://www.researchgate.net/figure/comparativebetween-euclidean-and-manhattan-distancef ig1332432569\n[2] s. pacheco, \"repaso de matemática: álgebra lineal,\" presentación,\ninstitutotecnológicodecostarica,2025.\nfig.4. ejemploderegresión.[2].\nla clasificación, en cambio, asigna cada instancia a una\ncategoríadiscretaapartirdesuscaracterísticas.porejemplo,\npredecir el tipo de vehículo dependiendo de cuántas llantas\ntiene y cuánto pesa.\nfig.5. ejemplodeclasificación.[2].\nen ambos casos, el objetivo es construir un modelo que\ngeneralice más allá de los datos de entrenamiento y que"}
{"id_doc": "DOC_008", "segmentacion": "B", "chunk_id": "DOC_008_B_006", "idx": 6, "autor": "Mariana Quesada Sánchez", "fecha": "2025-08-19", "tema": "Repaso de álgebra lineal y fundamentos del aprendizaje supervisado, con énfasis en regresión, clasificación y la representación vectorial de datos", "texto": "logre realizar predicciones confiables sobre ejemplos no\n\nobservados."}
{"id_doc": "DOC_009", "segmentacion": "B", "chunk_id": "DOC_009_B_000", "idx": 0, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "inteligencia artificial\napuntes de la clase del dia 21/08/2025\n\njulio varela venegas-2019008041\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\njuliojvv20@estudiantec.cr\n\nresumen-this session of the artificial intelligence course iii. repasodematema'tica:a' lgebralineal\nbegan with a review of fundamental linear algebra concepts,\nemphasizing their relevance for data analysis and manipulation. en esta parte de la clase retomamos conceptos ba'sicos de\nthe class then introduced the principles of supervised learning a'lgebra lineal, que son fundamentales para entender la inteliand its main characteristics. using visual studio code with a\ngenciaartificial.comotextosdeapoyoseusaronintroduccio'n\njupyternotebook,practicalexampleswerepresentedtoillustrate\n\na la inteligencia artificial y dive into machine learning -\ndataset operations. the professor highlighted the advantages\nof applying vectorized programming over traditional iterative algebra, cap'ıtulo 2. a continuacio'n se resumen los puntos\napproaches,showinghowvectorizationoptimizesdataprocessing ma's importantes.\nand facilitates the use of linear algebra operations.\nindex terms-inteligencia artificial, aprendizaje supervisado, a'lgebra lineal, programacio'n vectorial, jupyter notebook, iii-a. vectores y representacio'n\nprocesamiento de datos.\nun vector es un objeto matema'tico con direccio'n y magnitud, definido en un espacio n-dimensional. en 2d o 3d se"}
{"id_doc": "DOC_009", "segmentacion": "B", "chunk_id": "DOC_009_B_001", "idx": 1, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "i. introduccio'n puede representar gra'ficamente, pero en dimensiones mayores\n(4d, 500d, etc.) aunque no se pueda visualizar, s'ı se puede\nen esta sesio'n del curso de inteligencia artificial se reforoperar matema'ticamente.\nzaron conceptos de a'lgebra lineal vistos en la clase anterior,\n\nlos cuales son fundamentales para el manejo eficiente de\ndatos, destacando su aplicacio'n en el contexto de modelos de\naprendizaje automa'tico. luego, se hablo' sobre algunos de los\n\nconceptos clave del aprendizaje supervisado como una de las\nramas centrales de la disciplina, explicando sus caracter'ısticas\ny objetivos. a nivel pra'ctico, la clase incluyo' el uso de visual\n\nstudio code junto con jupyter notebook con codigos de\nejemploenfocadosendarunademostracio'nsobrelostemasde\n\nalgebra vistos en clase y como estos pueden permitir trabajar\nconmayoreficienciasobreundeterminadodataset.poru'ltimo\nse logro' evidenciar co'mo las te'cnicas vectorizadas no solo\nsimplifican la implementacio'n, sino que tambie'n optimizan el\nrendimiento en tareas de procesamiento de datos. figura1. representacio'ngra'ficadeunvector."}
{"id_doc": "DOC_009", "segmentacion": "B", "chunk_id": "DOC_009_B_002", "idx": 2, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "ii. noticiasycontextoreciente un vector se puede ver como un segmento con un punto\ninicial y uno final; la operacio'n fundamental es restar las\nen esta parte de la clase se discutio' brevemente la impor- coordenadas correspondientes. normalmente se asume que\ntanciadeasistiracharlasorganizadasporgruposestudiantiles todoslosvectorespartendelorigen(0,0)ylosejesdependen\ncomoieee,yaqueestasactividadespermitenelcontactocon de la dimensio'n que estemos usando.\nprofesionales del a'rea y fomentan tanto el aprendizaje complementario como la creacio'n de redes de colaboracio'n que\niii-b. vectores en lenguaje natural\npueden abrir oportunidades laborales en el futuro. asimismo,\nse menciono' el surgimiento de nuevas versiones de modelos enia,laspalabrassepuedenrepresentarcomovectoresen\nde lenguaje como gpt-5. aunque se destaco' su relevancia un espacio sema'ntico. por ejemplo, las palabras rey, reina,\nen el a'mbito de la inteligencia artificial, se comento' que la hombre, mujer pueden combinarse de forma vectorial para\nrecepcio'n por parte de los usuarios no fue del todo positiva. ver relaciones: rey - hombre + mujer da un vector cercano\nma's alla' de esta observacio'n, no se abordaron en detalle otras a reina. esto muestra co'mo los vectores son la base para\nnoticias recientes durante la sesio'n. representar conceptos en ia.\niii-c. magnitud y distancias iii-f. identidad del coseno\nlamagnitudindicaladistanciaentreelpuntoinicialyfinal la identidad del coseno mide la similitud entre vectores.\ndel vector, calculada con la norma ∥v∥. es u'til para comparar palabras en un espacio vectorial. la\ndistancia manhattan (l1): suma de los valores abso- fo'rmula es:\nlutos de las diferencias en cada eje: u-v =||u||-||v||-cos(θ)\nn\n(cid:88)\nd(x,y)= |x -y |\ni i\ni=1\ndistancia euclidiana (l2): hipotenusa del tria'ngulo\nformado por los vectores:\n(cid:118)\n(cid:117) n\n(cid:117)(cid:88) figura4. analog'ıassema'nticasusandovectores.\nd(x,y)=(cid:116) (x\ni\n-y\ni\n)2\ni=1 interpretacio'n: - a'ngulo pequen˜o → vectores similares. -\na'ngulo 0◦ → mismo vector. - a'ngulo grande → vectores\nlejanos, sin relacio'n.\naunque se ilustre en 2d, normalmente se calcula en espacios de 500 o 1000 dimensiones.\niii-f1. ejemplo de ca'lculo: para u=(1,2) y v =(3,4):"}
{"id_doc": "DOC_009", "segmentacion": "B", "chunk_id": "DOC_009_B_003", "idx": 3, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "1. calcular u-v. 2. calcular ||u|| y ||v||. 3. sustituir en la\nfo'rmula del coseno.\nresultado:a'ngulo∼10,3◦ →vectorescasicodireccionales.\niii-f2. vectores codireccionales: dos vectores con la\nmisma direccio'n pero diferente magnitud cumplen u = k-v.\na'ngulo 0◦, misma direccio'n.\nfigura2. distanciaeuclidianaentredospuntos.\nproducto punto consigo mismo:\n√\niii-d. propiedades de la norma u-u=||u||2 =⇒ ||u||= u-u\nla norma cumple varias propiedades:\npositividad: siempre es mayor o igual a cero.\nhomogeneidad: escalar el vector escala tambie'n su\nnorma.\ndesigualdad triangular: la norma de la suma es menor\no igual que la suma de las normas.\nun caso especial es el vector unitario, cuya norma es 1.\nnormalizar un vector lo convierte en unitario y simplifica figura5. demostracio'ndeu-u=||u||2.\nca'lculos.\niii-f3. ortogonalidad y ortonormalidad: vectores con\niii-e. producto punto a'ngulo90◦→productopunto=0→ortogonales().siadema's\ntienen norma 1 → ortonormales.\nel producto punto se define como:\nn\n(cid:88) iv. supervisedlearning\nx-y= x y\ni i\ni=1 en aprendizaje supervisado tenemos dos cosas importantes:\nfeatures y labels. - features: propiedades medibles de una\nentidad(altura,peso,intensidaddep'ıxel,etc.).-labels:valor\nobjetivo que queremos predecir.\nun vector de features x = (x ,x ,...) describe todas\n1 2\nlas propiedades de un ejemplo, y la variable dependiente y\ndepende de estas.\nfigura3. fo'rmuladelproductopunto. regresio'n:predecirvalorescontinuos(porejemplo,precio de una casa).\nel resultado siempre es un escalar. en ia, x puede ser un\nclasificacio'n: asignar a categor'ıas discretas (tipo de\nvector de caracter'ısticas y y un vector de pesos, permitiendo\nveh'ıculo: moto, carro, camio'n).\ncuantificar la importancia relativa de cada feature.\nv-e. exploracio'n y visualizacio'n de datos\n- ver primeras/u'ltimas filas. - contar elementos por columna. - acceder a columnas individuales. - calcular estad'ısticas\ndescriptivas (media, desviacio'n, cuartiles, percentiles). - graficar histogramas para ver distribuciones.\nv-f. clasificacio'n de nuevos samples\nfigura6. estructuradeundatasetconfeaturesylabels."}
{"id_doc": "DOC_009", "segmentacion": "B", "chunk_id": "DOC_009_B_004", "idx": 4, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "se entran 100 samples nuevos y se quiere clasificarlos\nusando el dataset previo.\nestrategia: k-nearest neighbors (knn) 1. tomar dataset\n\nv. notebook:programacio'nvectorialy\nde entrenamiento. 2. para cada nuevo sample: - calcular\noperacio'ncondatasets\ndistancia a todos los samples de entrenamiento. - tomar los k\nse exploraron ventajas de operar un dataset con programa- ma's cercanos. - asignar la clase ma's frecuente entre ellos.\ncio'nvectorialya'lgebralinealvsusarciclostradicionales.esto notas: - permite clasificar segu'n proximidad a ejemplos\nmejoralaeficienciaypermitetrabajarcongrandesvolu'menes conocidos.-kesunhiperpara'metroquedefinecua'ntosvecinos\nde datos. considerar. - puede aplicarse a clasificacio'n o, en variantes, a\nregresio'n.\nv-a. introduccio'n a numpy\n\nvi. conclusio'n\nnumpy permite: - arrays multidimensionales (vectores,\nla inteligencia artificial combina matema'ticas, programamatrices, tensores). - operaciones vectorizadas. - ca'lculos\ncio'n y consideraciones pra'cticas. entender los fundamentos,\nra'pidos gracias a c y fortran. - funciones de a'lgebra lineal y"}
{"id_doc": "DOC_009", "segmentacion": "B", "chunk_id": "DOC_009_B_005", "idx": 5, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "cuidar la calidad de los datos y practicar con herramientas\nestad'ısticas.-integracio'nconpandas,matplotlib,scikit-learn.\n\ncomo numpy y pandas es clave para agilizar el manejo de\n- compatibilidad con gpu mediante bibliotecas externas.\ngrandes cantidades de datos y la optimizacio'n de tiempos de\nejecucio'n.\nv-b. creacio'n y manipulacio'n de arrays (pseudoco'digo)\n\nreferencias\n- crear array 1d con nu'meros consecutivos. - crear matriz\nidentidad 3x3. - realizar operaciones ba'sicas (suma, mul- [1] apuntes de la clase de inteligencia artificial, profesor steven andres\npachecoportuguez,institutotecnolo'gicodecostarica,21dejuliode\ntiplicacio'n). - calcular media, desviacio'n esta'ndar y otras\n2025.\nestad'ısticas.\nv-c. ca'lculo de la distancia euclidiana (pseudoco'digo)\nciclos tradicionales: sumar las diferencias al cuadrado y\nsacar ra'ız. programacio'n vectorial: restar vectores, elevar\nal cuadrado, sumar y sacar ra'ız. comparacio'n: con vectores\ngrandes, vectorial es mucho ma's ra'pido.\nv-d. creacio'n de datasets con pandas y numpy (pseudoco'digo)\n- generar 3 clases con distribucio'n normal. - asignar\netiquetas 0, 1, 2. - combinar datos en un solo dataset. - crear\ndataframe con features y labels. - visualizar distribucio'n con\ngra'ficos."}
{"id_doc": "DOC_009", "segmentacion": "B", "chunk_id": "DOC_009_B_006", "idx": 6, "autor": "Julio Varela Venegas", "fecha": "2025-08-21", "tema": "Aplicación del álgebra lineal y la programación vectorial en IA, con enfoque en aprendizaje supervisado, representación de vectores y uso de NumPy y Jupyter Notebook.", "texto": "figura7. distribucio'ndeclaseseneldatasetgenerado."}
{"id_doc": "DOC_010", "segmentacion": "B", "chunk_id": "DOC_010_B_000", "idx": 0, "autor": "Andrés Sánchez Rojas", "fecha": "2025-08-26", "tema": "Implementación del algoritmo KNN y fundamentos de regresión lineal, incluyendo función de pérdida, descenso del gradiente y comparación entre MSE y MAE.", "texto": "1\n\napuntes semana 4\nandrés sánchez rojas\nescuela de ingeniería en computación\ninstituto tecnológico de costa rica\n26/8/2025\n\nabstract-la clase comenzó con un quiz de 4 preguntas\nrelacionadas a la materia vista en clases anteriores, luego el ventajas:\nprofesor nos explicó las respuestas del quiz antes de comenzar\n- es sencillo de implementar\ncon la materia de la clase. durante la clase vimos el algoritmo\nde knn, hicimos un repaso de derivadas y pasamos a ver cómo - sirve para regresión y clasificación\nse construye y optimiza un modelo de regresión lineal. desventajas:\n- es muy costoso\n- features irrelevantes pueden distorcionar las distancias\n\ni. quiz\n- noesmuyconsistenteyaquelaclasificaciónpuedevariar\n1) 1.anoteydescribalas3propiedadesdelanorma.30pts dependiendo del k usado\nr//\na) positividad:∥x∥≥0y∥x∥=0siysolosix=0.\n\niii. regresiónlineal\nb) homogeneidad: ∥αx∥=|α|∥x∥ para todo escalar método estadístico que intenta hallar la relación entre una\nα. variable dependiente y un conjunto de variables independic) desigualdad triangular: ∥x+y∥≤∥x∥+∥y∥. entes.\n2) 2. describa los tipos de aprendizaje supervised, unsupervised y one-shot learning. 30 pts\nr//\na) supervised: se utiliza un conjunto de datos con\ncaracterísticasyetiquetas.lasetiquetassirvenpara\nvalidar y corregir las aproximaciones del sistema.\nb) unsupervised:nohayetiquetasconlasqueevaluar\no corregir, se usa en algoritmos de cluster para\nagrupar valores.\nc) one-shotlearning:seledaunejemploalmodelo"}
{"id_doc": "DOC_010", "segmentacion": "B", "chunk_id": "DOC_010_B_001", "idx": 1, "autor": "Andrés Sánchez Rojas", "fecha": "2025-08-26", "tema": "Implementación del algoritmo KNN y fundamentos de regresión lineal, incluyendo función de pérdida, descenso del gradiente y comparación entre MSE y MAE.", "texto": "y luego debe resolver un ejercicio similar\nfig.1. ejemploderegresiónlineal.\n3) 3.siuyvsondosvectorescolinealesconmagnitudesde\n5 y 6 respectivamente. ¿desarrolle cuál es el resultado\ndel producto punto entre u y v? a. ¿qué queremos hacer?\nr//\n\nbuscamos construir un modelo\na) ∥u∥=5, ∥v∥=6.\nf (x)=wx+b\nb) u-v =∥u∥ ∥v∥ cosθ. w,b\nc) cos0=1 - x es un vector d-dimensional\nd) 5-6-1. - w es un vector d-dimensional\ne) u-v =30 - b es un número real\n\n4. ¿quién propone las redes generativas adversarias - y=f w,b (x)\nr// ian goodfellow lo que queremos es encontrar los valores de w y b óptimos\npara nuestro modelo. es importante recordar que no tiene que\nser perfecto (mínimo absoluto) pero debemos buscar que sea\n\nii. k-nearestneighbors(knn)\nóptimo (mínimo local) para las necesidades que tengamos."}
{"id_doc": "DOC_010", "segmentacion": "B", "chunk_id": "DOC_010_B_002", "idx": 2, "autor": "Andrés Sánchez Rojas", "fecha": "2025-08-26", "tema": "Implementación del algoritmo KNN y fundamentos de regresión lineal, incluyendo función de pérdida, descenso del gradiente y comparación entre MSE y MAE.", "texto": "se tiene un conjunto de datos tiquetados y se le quiere\nasignar una etiqueta a un dato basado en otros datos similares b. loss function\na este. estos datos similares son los k vecinos más cercanos.\nesta función nos permite calcular qué tan bueno es nuestro\nuna vez que se tiene a los vecinos más cercanos se revisa las\nmodelo. con esta función calculamos el error cometido por el\netiquetas de estos en una \"votación\" la etiqueta más común\nmodelo en cada muestra. la función de pérdida penaliza más\nen estos k vecinos se le asigna al dato nuevo. este k es\nlos errores grandes por el error cuadrático.\nun hiperparámetro y normalmente es un número impar para\n(cid:0) (cid:1)2\nevitar empates. f (x )-y\nw,b i i\n2\nc. cost function f. descenso de gradiente\nel profe puso un ejemplo para explicar este concepto.\neselerrorpromediodellossfunctionsobretodoeldataset.\nestamos en la cima de una montaña con los ojos vendados y\nnuestro objetivo es minimizarla ajustando los parámetros w\ndebemosencontrarlarutamáscortaalpuntomásbajoposible.\ny b. si tenemos un l grande quiere decir que el modelo da\nel proceso para esto sería:\nvalores muy distintos a las etiquetas. un l pequeño indica lo\nopuesto. - buscar la dirección de mayor pendiente hacia abajo\n- descender por ese camino hacia abajo\nn\nl= 1 (cid:88)(cid:0) f (x )-y (cid:1)2 - en cada paso repetimos el proceso.\nn w,b i i tenemos la función:\ni=1\nx =x -α∇f(x )"}
{"id_doc": "DOC_010", "segmentacion": "B", "chunk_id": "DOC_010_B_003", "idx": 3, "autor": "Andrés Sánchez Rojas", "fecha": "2025-08-26", "tema": "Implementación del algoritmo KNN y fundamentos de regresión lineal, incluyendo función de pérdida, descenso del gradiente y comparación entre MSE y MAE.", "texto": "nuevo antiguo t\nd. repaso de derivadas α es la taza de aprendizaje que es un hiperparámetro y\n∇f(x ) es el gradiente o la derivada. debemos tener cuidado\npropiedades de las derivadas: t\nal definir el α. si se utiliza un α muy grande el algoritmo\nprobablementevaasaltarseelpuntoóptimomuchasveces.un\nd\n[k]=0, α muy pequeño nos va a forzar a hacer muchas iteraciones.\ndx\ndebemos pensar bien en el learning rate que se utilizará pero\nd\n[x]=1, serecomiendaquesearelativamentepequeñoparanosaltarnos\ndx\nd (cid:2) xn(cid:3) =nxn-1, e\ne\nl\nn\np\nd\nu\ne\nn\nfi\nto\nni\nó\nr\np\nu\nt\nn\nim\nv\no\nal\no\nor\nus\nra\na\nz\nr\no\ne\nn\nl\na\ne\nb\na\ne\nrl\nd\ny\ne\ns\nl\nto\ny\npp\nd\nin\net\ng\nen\nm\ner\net\nl\nh\na\no\nf\nd\nu\n.\nn\ne\nc\ns\nió\nte\nn\nc\nc\no\nu\nn\na\ns\nn\nis\nd\nt\no\ne\ndx\nd (cid:2) f(x)+g(x) (cid:3) =f′(x)+g′(x), se llega a ese valor de l.\ndx\nd (cid:2) f(x)-g(x) (cid:3) =f′(x)-g′(x),\ndx\nd (cid:2) kf(x) (cid:3) =kf′(x),\ndx\nd (cid:2) f(x)g(x) (cid:3) =f′(x)g(x)+f(x)g′(x),\ndx\nd (cid:20) f(x) (cid:21) f′(x)g(x)-f(x)g′(x)\n= ,\ndx g(x) (cid:2) g(x) (cid:3)2\nd (cid:2) f (cid:0) g(x) (cid:1)(cid:3) =f′(cid:0) g(x) (cid:1) - g′(x).\ndx\nejemplo de derivada parcial:\nsea f(x,y)=2x+3y,\nfig.3. ilustracióndedescensodegradiente.\n∂f\nal calcular , tratamos x como constante,\n∂y\n∂f\n=3\n∂y\ne. función convexa vs no convexa\nlafunciónconvexasólotieneunmínimoabsolutomientras fig.4. ejemplodedescensodegradientecondiferenteslearningrates\nque la no convexa puede tener múltiples mínimos locales.\nfig.2. ejemplodeunafunciónconvexayunanoconvexa.\n3\ng. ¿por qué utilizamos mse (mean squared error) y no\nmae(mean absolute error)\n- el mse penaliza más los errores grandes, el mae los"}
{"id_doc": "DOC_010", "segmentacion": "B", "chunk_id": "DOC_010_B_004", "idx": 4, "autor": "Andrés Sánchez Rojas", "fecha": "2025-08-26", "tema": "Implementación del algoritmo KNN y fundamentos de regresión lineal, incluyendo función de pérdida, descenso del gradiente y comparación entre MSE y MAE.", "texto": "penaliza de manera lineal\n- mae no tiene una derivada continua ya que no es\n\nderivable en 0\nfig.5. ilustracióndemaevsmse\n\nreferences\n[1] stevenpachecoportuguez,clasesobreregresiónlineal,tecnológicode\n\ncostarica,2025."}
{"id_doc": "DOC_011", "segmentacion": "B", "chunk_id": "DOC_011_B_000", "idx": 0, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "apuntes de clase\nluis felipe calderón pérez\nescuela de ingeniería en computación\ntecnológico de costa rica\ncartago, costa rica\n2021048663\n26-08-2025\n\nresumen-este documento presenta los apuntes de la cuarta one-shot: basta con mostrarle una única vez como\nsemana del curso de inteligencia artificial. primeramente se dan realizarlatareaparaqueelmódelopuedareproducirla.\nlasrespuestasdelprimerquiz.serepasalatareadeclasificación,\n\n3. si u y v son dos vectores colineales con magnitudes 5\nelalgoritmodelosk-nearestneighbors.seintroduceeltemade\ny 6 respectivamente.¿desarrolle cuál es el resultado del\nlaregresiónlíneal,funcióndepérdida,mínimoslocales,mínimos\nglobales, el descendo del gradiente. además, se repasaron las producto punto entre u y v?\nderivadas y se terminó con la pregunta de porque escoger mse respuesta:\ny no mae.\nindexterms-ia,derivadas,descensodelgradiente,regresión u-v =∥u∥-∥v∥-cos(θ)\n\nlineal\nu-v = 5- 6-cos(0)\n\ni. preguntasyrespuestasdelprimerquiz ∴u-v =30\n\n1. anote y describa las tres propiedades de la norma 4. ¿quién propone las redes generativas adversarias?.\nrespuesta: respuesta: ian goodfellow\npositividad: ∥x∥≥0 y ∥x∥=0 si y solo si x=0."}
{"id_doc": "DOC_011", "segmentacion": "B", "chunk_id": "DOC_011_B_001", "idx": 1, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "ii. clase\nhomogeneidad:∥αx∥=|α|-∥x∥paracualquierescalar\nii-a. clasificación\nα.\ndesigualdad triangular: ∥x+y∥≤∥x∥+∥y∥. ii-a1. k-nearest neighbors: algoritmo en donde un conjunto de datos etiquetados recibe un nuevo dato. a ese nuevo\n\n2. describa los tipos de aprendizaje supervised, unsupervidato se le calcula la distancia con sus datos vecinos, una\nsed y one-shot learning.\nvez se encuentra a los vecinos más cercanos, se realiza una\nrespuesta:\nvotación,paradeterminaraquecategoriaotipopertenece.en\nsupervised: el modelo aprende a partir de datos que\neste algoritmo el hiperparámetro es el k.\nincluyen etiquetas, las cuales sirven como referencia\ndurante el entrenamiento. 1. ventajas\nunsupervised: : el modelo trabaja con datos sin eti- no requiere fase de entrenamiento.\nquetas y se encarga de encontrar patrones en los datos fácil de implementar.\nocultos. flexible: regresión y clasificación."}
{"id_doc": "DOC_011", "segmentacion": "B", "chunk_id": "DOC_011_B_002", "idx": 2, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "2. desventajas donde w y x son vectores, y su producto punto genera\npoco eficiente. un escalar; b es un escalar. los valores de w y b afectan\nlas features irrelevantes distorcionan las distancias directamentelosresultados,porloquedebemosencontrar\nentre los datos. los valores óptimos de ambos para obtener un modelo\npuede ser costoso a nivel computacional. óptimo.\ndependiendo del k usado, cambia la clasificación del nota: se van a trabajar desde modelos simples de regredato ingresado. sión líneal, hasta multiple linear.\nnota: se requiere normalización o estandarización en caso de\nfigura1. regresiónlínealsimple\n\nque se dispare o haya gran diferencia en las distancia entre\nlos datos.\nii-b. regresión líneal\nes un algoritmo usado para encontrar un modelo en el conjunto de los números reales, para predecir valores contiguos."}
{"id_doc": "DOC_011", "segmentacion": "B", "chunk_id": "DOC_011_B_003", "idx": 3, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "1. existen 2 tipos de variables:\nvariables independientes, representan los features que\nintroducimos en el modelo. figura2. multiplelinear\nvariables dependientes, representan las etiquetas o el\nobjetivo que deseamos predecir.\n\ncuadroi\nrelaciónhorasdeestudioconnotas\nhoras de estudio (x) nota (y)\n1 50\n2 55\n3 65\n4 70\n5 75\nenelejemploanteriorlaregresiónlínealquecorresponde\nes:\ny =5x+45\nen donde 5x representa la inclinación de la función y 45\nii-c. función de pérdida\nel intercept o en donde corta el eje y.\nlafuncióndepérdidamideelerrorcometidoporelmodelo\nel modelo debe cumplir con la siguiente función:\nen cada muestra (sample). una de sus caracteristicas es el\nf (x)=w-x+b errorcuadrático,quepenalizademaneramásfuerteloserrores\nw,b\ngrandes.lafuncióndepérdidadeunamuestrasedenotacomo ii-e. descenso del gradiente\nl i =(f w,b (x i )-y i )2. se propone una analogía sobre que se esta en una montaña\npara evaluar el desempeño del modelo en todo el conjunto muy elevada, se tiene los ojos vendados y la meta es bajar\nde datos, se calcula la función de costo (cost function), que con la menor cantidad de esfuerzo y los más rápido posible.\nes el promedio de la función de pérdida sobre todas las n y la solución es desde el punto inicial, calcular el lado que\nmuestras: tiene más pendiente, dar un paso y repetir ese mismo proceso\nhasta llegar a un punto mínimo de altura.\nn\n1 (cid:88)(cid:0) (cid:1)2\nl= f (x )-y , (1) matemáticamente, esto se formaliza mediante la regla de\nn w,b i i\ni=1\nactualización:\ndondef (x )eslaprediccióndelmodeloparalamuestra\nw,b i\ni, y i es el valor real, y n es el número total de muestras. x nuevo =x antiguo -α∇f(x t ),\nsi logramos minimizar l, reducimos la discrepancia entre\ndonde α es la tasa de aprendizaje (learning rate) y ∇f(x )\nt\nlasprediccionesdelmodeloylosvaloresreales,obteniendoasí\nrepresenta el gradiente de la función en el punto x ."}
{"id_doc": "DOC_011", "segmentacion": "B", "chunk_id": "DOC_011_B_004", "idx": 4, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "antiguo\nunmejorajuste.otraformademejorarelmodeloesajustando\nel valor de α es crítico: un valor demasiado grande puede\nlos valores de w y b, evitando el underfitting, o modificando\nprovocar que el algoritmo oscile y no converja, mientras que\nel conjunto de datos (dataset).\nun valor demasiado pequeño ocasiona una convergencia muy\nlenta.paramitigarestosproblemas,sesuelenemplearestrateii-d. función convexa vs no convexa\ngias como la búsqueda de una tasa de aprendizaje óptima o el\nal realizar regresiones lineales, como parte de la fórmula earlystopping,quedetieneelentrenamientocuandolafunción\nestá elevada al cuadrado, sabemos que es posible encontrar de pérdida deja de mejorar significativamente o alcanza un\nuna solución óptima (mínimo local). sería ideal siempre que valor aceptable.\nla función sea convexa, porque a diferencia de la no convexa nota: los términos derivada, pendiente y gradiente son\nes fácil identificar un mínimo global. equivalentes.\nd\nderivadadeunaconstante: [c]=0\ndx\nd\nderivadadeunavariable: [x]=1\nfigura3. funciónconvexavsnoconvexa. dx\nd\nderivadadeconstanteporvariable: [c-x]=c\ndx\nd\nregladelapotencia: [xn]=nxn-1\ndx\nd\nderivadadeunasuma: [f(x)+g(x)]=f′(x)+g′(x)\ndx\nd\nregladelproducto: [f(x)g(x)]=f′(x)g(x)+f(x)g′(x)\ndx\n∂f\nderivadasparciales:\n∂xi\n=derivadadefrespectoaxi\n∂f ∂f\nejemplodeparciales:f(x,y)=x2y+3xy2, =2xy+3y2, =x2+6xy\n∂x ∂y"}
{"id_doc": "DOC_011", "segmentacion": "B", "chunk_id": "DOC_011_B_005", "idx": 5, "autor": "Luis Felipe Calderón Pérez", "fecha": "2025-08-26", "tema": "Repaso del algoritmo KNN y regresión lineal, con análisis de la función de pérdida, convexidad, gradiente y la diferencia entre MSE y MAE.", "texto": "pregunta final\nse concluye la clase con la siguiente pregunta, ¿porque\nescoger mse y no mae?\nrespuesta: mae no es derivable en 0 y nos lleva a errores\nde cálculo\n\nreferencias\n[1] s. a. p. portuguez, \"apuntes de la clase de inteligencia artificial,\"\n\ncartago,costarica,agosto2025,clasedel26deagostode2025."}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_000", "idx": 0, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "inteligencia artificial\n\napuntes del 28 de agosto de 2025 - semana 4\njuan diego jiménez valverde - 2019199111\njuand0908@estudiantec.cr\n\nabstract-estos apuntes resumen la clase del 28 de agosto de b. regresión lineal\n2025, en la que se analizaron noticias recientes sobre modelos\nconcepto básico\nde lenguaje y sus impactos socioeconómicos, así como conceptos\nmatemáticos clave para algoritmos de aprendizaje supervisado. - busca construir un modelo estadístico lineal.\nse cubrieron temas como knn, regresión lineal, funciones de - la relación entre variables debe representarse como una\npérdida,derivadas,descensodelgradienteysusvariantes,epochs recta.\nybatches,yladiferenciaentremseymae.elresumenenfatiza\nlarelaciónentreteoríayprácticaenlaoptimizacióndemodelos - si no es lineal, cae en otra categoría de modelos.\npredictivos. variables\nindex terms-modelos de lenguaje, aprendizaje supervisado,\n- variable dependiente (y): valor que queremos predecir.\nknn, regresión lineal, mse, descenso del gradiente, optimización, epochs, batches. - variable independiente (x): valor usado para explicar/predicir."}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_001", "idx": 1, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "i. introducción - x: vector d-dimensional (características o features).\nla clase del 28 de agosto de 2025 se centró primero en - w: vector d-dimensional (pendientes/pesos).\ndiscutircómolosavancesenmodelosdelenguaje,desdesmall - b: número real (intersección con el eje y).\nlanguage models hasta llms, están afectando el empleo modelo\ny la economía, especialmente en ocupaciones susceptibles f w,b (x)=w-x+b\nde automatización. luego, se abordaron los fundamentos\n- f(x): predicción del modelo.\nmatemáticos que sustentan algoritmos de aprendizaje super-\n- w-x: producto punto → asegura que el resultado sea un\nvisado, incluyendo knn y regresión lineal, así como las\nescalar.\nherramientas necesarias para evaluar y optimizar modelos,\n- interpretación: combinación lineal de características.\ncomo funciones de pérdida, derivadas, descenso del gradiente\nparámetros del modelo\ny sus variantes (batch, stochastic y mini-batch), epochs y\nbatches, y la comparación entre mse y mae. lo que sigue - f: vector de variables independientes.\nson mis apuntes y reflexiones personales sobre estos temas, - w: pendientes.\nexplicando cómo los entendí y cómo se aplican en la práctica - b: intersección con el eje y.\nde la modelación predictiva. - modelo parametrizado por w y b.\n- objetivo: encontrar valores óptimos de w y b que permiii. noticiasdeldía\ntan predicciones más acertadas.\nse mencionaron dos trabajos importantes: primero, un - óptimo ̸= perfecto, siempre existe error.\nartículo que argumenta que small language models (slms) - restricción: solo se pueden modificar w y b, el x es fijo\npueden ser más prácticos que llms en muchos despliegues (sample).\npor costo y adaptabilidad; y segundo, un estudio que muestra función de pérdida\nefectos tempranos de la ia generativa en el empleo, espe-\n- midequétanbienomalestáfuncionandoelmodelo(qué\ncialmente perjudicando a jóvenes en ocupaciones altamente\ntan lejos están las predicciones de los valores reales).\nautomatizables. estas observaciones nos ayudaron a contextuplot residual\nalizarporquélaeficienciacomputacionalylainterpretabilidad\nson temas relevantes hoy. [1], [2] - un residual es la diferencia entre el valor real y la\npredicción."}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_002", "idx": 2, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "iii. repaso:conceptosclave - el plot residual muestra gráficamente esas diferencias\na. knn - k nearest neighbor para analizar la calidad del ajuste.\nknn es un algoritmo lazy (perezoso): no aprende un\nc. función de costo: error cuadrático medio (mse)\nmodelo global, simplemente guarda los datos y en tiempo de\ndefinición\nconsulta busca los k vecinos más cercanos.\nk: es el hiperparámetro a seleccionar. 1 (cid:88) n\nl= (f (x )-y )2\nventajas: sencillo, interpretable, sin entrenamiento costoso. n w,b i i\ndesventajas: costoso en memoria y consulta; sensible a la i=1\nescala de las features. conceptos clave\n- lossfunction:(f w,b (x i )-y i )2 midelapenalidadoerror importancia del α (learning rate)\nde cada ejemplo individual. - el tamaño del paso α debe ser pequeño (ejemplo: 0.1)\n- error cuadrático: penaliza más los errores grandes. para no sobrepasar el mínimo.\n- cost function: promedio de la loss function en todo el - al acercarnos al mínimo, los saltos se reducen porque el\ndataset;esunamedidaglobaldeldesempeñodelmodelo. gradiente disminuye.\n- objetivo: minimizar l ajustando los parámetros w y b. - un α muy grande puede provocar oscilaciones o incluso\ninterpretación alejarse del mínimo.\n- l pequeño → mejor modelo. - un α demasiado pequeño ralentiza la convergencia.\n- l grande → peor modelo. nota\n- reducir l implica mejorar la capacidad predictiva del - el learning rate (α) es un hiperparámetro que debe\nmodelo. seleccionarse cuidadosamente.\nfunciones convexas vs. no convexas\n- convexa: garantiza un único mínimo global.\n- no convexa: pueden aparecer mínimos locales y globales.\nd. repaso de derivadas\nreglas básicas\n- f(x)=k ⇒ f′(x)=0\nejemplo: f(x)=2 ⇒ f′(x)=0\nfig.1. impactodellearningrateengradientdescent\n- f(x)=x ⇒ f′(x)=1\n- f(x)=kx ⇒ f′(x)=k f. ¿por qué usar mse y no mae?\nejemplo: f(x)=2x ⇒ f′(x)=2 el mean squared error (mse) es más utilizado que el\npotencias mean absolute error (mae) en optimización con descenso\n- f(x)=xn ⇒ f′(x)=nxn-1 del gradiente porque:\nejemplo: f(x)=x2 ⇒ f′(x)=2x - la función mse es suave (diferenciable en todos sus\nsuma puntos), lo que permite calcular derivadas de forma\n- f(x)=u(x)+v(x) ⇒ f′(x)=u′(x)+v′(x) sencilla y aplicar métodos basados en gradiente.\nejemplo: u(x) = 2x, v(x) = 3x ⇒ f(x) = - en contraste, la función mae no es diferenciable en\n5x, f′(x)=5 0 (presenta una esquina), lo que complica el uso de\nderivadas directas y hace más difícil la optimización con"}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_003", "idx": 3, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "producto\ngradiente puro.\n- f(x)=u(x)v(x) ⇒ f′(x)=u′(x)v(x)+u(x)v′(x) - gracias a su naturaleza cuadrática, el mse penaliza más\nconstante sumada fuertemente los errores grandes, favoreciendo un ajuste\n- f(x)=u(x)+z ⇒ f′(x)=u′(x) más preciso en esos casos.\nejemplo: f(x)=2x+5 ⇒ f′(x)=2 error cuadrático medio (mse):\n\nderivadas parciales n\n- sea f(x,y)=2x+3y mse = n 1 (cid:88) (f w,b (x i )-y i )2\ni=1\n∂f ∂f\n=2, =3\n∂x ∂y\ne. descenso del gradiente\nconcepto básico\n- la cantidad de pasos se calcula como: pendiente × α\n(learning rate).\n- ejemplo: si x=1, el gradiente es dy =2x=2.\ndx\n- para acercarnos al mínimo, nos movemos en la dirección\ndel gradiente negativo con un paso de tamaño α.\nregla de actualización\nfig.2. mse\nx =x -α-(2x)"}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_004", "idx": 4, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "nuevo antiguo\nerror absoluto medio (mae):\n- donde 2x es el gradiente.\nn\n- el proceso se repite hasta que el gradiente sea 0 (punto mae = 1 (cid:88) |f (x )-y |\nde mínimo). n w,b i i\ni=1\n∂\n(wx +b-y )=0+1-0=1.\n∂b i i\nsustituyendo (forma idéntica a la de tu imagen, antes de\nsimplificar):\n∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) -1.\n∂b i i\nforma final por muestra:\nfig.3. mae ∂ ∂ ℓ b i =2 (cid:0) (wx i +b)-y i (cid:1) .\n\niv. materialdeclase sumando y normalizando para el mse completo:\na. derivadasdelafuncióndepérdida(mse)-notacióncon\nn n\n∂/∂ ∂l = 1 (cid:88)∂ℓ i = 2 (cid:88)(cid:0) (wx +b)-y (cid:1) .\n∂b n ∂b n i i\nrecordemos la función: i=1 i=1\nn\n1 (cid:88)(cid:0) (cid:1)2 nota:\nl= (wx +b)-y ."}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_005", "idx": 5, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "n i i - observaquelasexpresionespormuestracoincidenconlo\ni=1\nque aparece en tu imagen: para w aparece el factor extra\nderivada por muestra (desglose) - respecto a w: para la\nx (porque (wx )′=x ), y para b queda solo 2((wx +\ncontribución de la muestra i: i i i i\nb)-y ) (porque la derivada de b es 1).\ni\nℓ i =\n(cid:0)\n(wx i +b)-y i\n(cid:1)2\n. - estas son las cantidades que se usan en la regla de\nactualización por descenso de gradiente:\naplicando la regla de la cadena con derivadas parciales:\n∂l ∂l\n∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) - ∂ (cid:0) wx +b-y (cid:1) . w ←w-α ∂w , b←b-α ∂b .\n∂w i i ∂w i i\nb. epochs, batches y tipos de descenso por gradiente\ndesglose término a término en la parte interior:\nepoch:\n∂ ∂ ∂ ∂\n∂w (wx i +b-y i )= ∂w (wx i )+ ∂w (b)- ∂w (y i ), - una epoch es una iteración completa sobre todo el\nconjunto de entrenamiento.\n∂ ∂ ∂\n(wx )=x , (b)=0, (y )=0, - es un hiperparámetro (p. ej. epochs = 5).\n∂w i i ∂w ∂w i - ejemplo: si hay 10000 samples y ejecutamos 5 epochs,\n∂ recorremos los 10000 samples 5 veces en total.\n(wx +b-y )=x .\n∂w i i i - podemos aplicar el descenso del gradiente al finalizar un\nsustituyendo: epoch (actualizaciones por epoch) o antes (por batches).\nbatch:\n∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) x .\n∂w i i i - un batch es un subconjunto del conjunto de entrenamiento usado para calcular la gradiente y actualizar\nsumando sobre las muestras y normalizando:\nparámetros.\n∂l = 1 (cid:88) n ∂ℓ i = 2 (cid:88) n (cid:0) (wx +b)-y (cid:1) x . - e ne je c m es p it l a o n : 1 1 0 00 b 0 at 0 ch s e a s m p p a l r e a s c y om b p a l t e c t h ar s 1 ize epo = ch 1 . 000 ⇒ se\n∂w n ∂w n i i i\ni=1 i=1 - no se espera a procesar todo el dataset: cada partición\n(batch) sirve para calcular la gradiente y actualizar los\nderivada por muestra (desglose) - respecto a b: para la\nparámetros.\ncontribución de la muestra i:\n- cada vez que procesamos un batch actualizamos los\n(cid:0) (cid:1)2\nℓ = (wx +b)-y . parámetros (o acumulamos gradientes según la estratei i i\ngia).\nregla de la cadena (forma no simplificada):\ntipos de descenso por gradiente:\n∂ℓ i =2 (cid:0) (wx +b)-y (cid:1) - ∂ (cid:0) wx +b-y (cid:1) . a) batch gradient descent (vanilla):\n∂b i i ∂b i i\n- calcula la gradiente usando todo el dataset: ∇l =\ndesglose término a término en la parte interior: 1 (cid:80)n ....\nn i=1\n∂ ∂ ∂ ∂ - actualización cuando se ha procesado el conjunto com-\n(wx +b-y )= (wx )+ (b)- (y ),\n∂b i i ∂b i ∂b ∂b i pleto.\n∂ ∂ ∂ - ventajas: gradiente estable, pasos consistentes.\n∂b (wx i )=0, ∂b (b)=1, ∂b (y i )=0, - desventajas:\n- requiere tener todo el dataset en memoria. - ayuda a evitar mínimos locales y aporta robustez en\n- en datasets grandes, las actualizaciones son lentas la optimización.\n(cada paso es costoso). - desventajas:\n- gradiente muy estable puede ocultar señales útiles\n- introduce un hiperparámetro adicional: batch size.\ny hacer que el proceso converja a parámetros no\n- hay que elegir el tamaño del batch cuidadosamente\ndeseados según la topología (según el problema).\n(trade-off entre estabilidad y velocidad).\nfig.4. batchgradientdescent\nfig.6. mini-batchgradientdescent\nb) stochastic gradient descent (sgd):"}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_006", "idx": 6, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "v. comentariosprácticosytareas\n- actualizalosparámetrosporcadasampledeltrainingset se mencionó que se asignará una tarea práctica: imple-\n(o mezcla aleatoria de samples).\nmentar (solo con numpy) un pipeline de regresión lineal que\n- ventajas: detecta rápidamente si el algoritmo puede con- incluya:\nverger; útil para datasets muy grandes.\n1) exploración visual del dataset.\n- desventajas:\n2) ingeniería simple de features (transformaciones no lin-\n- señales de gradiente ruidosas (alto ruido en las\neales cuando aplique).\nactualizaciones).\n3) implementación de mse y pasos de descenso (batch /\n- muchas actualizaciones (computacionalmente cosmini-batch).\ntoso si no se optimiza).\nesoayudaaentenderporquéalgunasfuncionesnosonsmooth\n- la trayectoria del parámetro es muy oscilatoria:\ny cómo afecta a las derivadas y la optimización.\n∂l\nw ←w-α\n∂w\n\nvi. conclusión\nen esta clase se consolidó la comprensión de conceptos"}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_007", "idx": 7, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "ejecutado por muestra puede producir movimientos\n\nfundamentales para implementar algoritmos de aprendizaje\nmuy erráticos.\nsupervisado de manera eficiente y correcta. se destacó la\nrelevancia de elegir adecuadamente funciones de pérdida,\nhiperparámetroscomoellearningrateylaestrategiadeactualización de gradientes, así como la importancia de comprender\nla teoría detrás de knn y regresión lineal. asimismo, los\napuntes reflejan la relación entre teoría y práctica, preparando\n\nal estudiante para aplicar estos conceptos en tareas concretas\ny proyectos de programación.\n\nreferences\n[1] belcaketal.,\"smalllanguagemodelsarethefutureofagenticai\",\nnvidiaresearch,2025.https://arxiv.org/abs/2506.02153\nfig.5. stochasticgradientdescent\n[2] e.brynjolfsson,a.chandar,yz.chen,\"canariesinthecoalmine?six\nfacts about the recent employment effects of artificial intelligence\",\nc) mini-batch gradient descent: stanford digital economy lab, 2025. https://digitaleconomy.stanford.\nedu/publications/canaries-in-the-coal-mine/\n- combina ambas estrategias: se calcula la gradiente sobre\nbatches de tamaño intermedio.\n- ventajas:\n- reduce el ruido respecto a sgd (más estable) y es\nmás eficiente que batch gd.\n- mejora la explotación de hardware (vectorización,"}
{"id_doc": "DOC_012", "segmentacion": "B", "chunk_id": "DOC_012_B_008", "idx": 8, "autor": "Juan Diego Jiménez Valverde", "fecha": "2025-08-28", "tema": "Análisis de modelos de lenguaje y fundamentos de aprendizaje supervisado, abarcando KNN, regresión lineal, funciones de pérdida, derivadas, gradiente y optimización con MSE.", "texto": "gpus)."}
{"id_doc": "DOC_013", "segmentacion": "B", "chunk_id": "DOC_013_B_000", "idx": 0, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "apuntes semana 4 clase #2\n28/08/2025\n\nalex steven naranjo masis\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nemail: alnaranjo@estudiantec.cr\n\nresumen-para esta clase se repasaron temas de la clase b. regresio'n lineal\nanteriorcomolosonknn,regresio'nlineal,meansquareerror,\ndescensodelgradienteyunrepasogeneraldederivadas.yluego lo que queremos hacer es encontrar la l'ınea que mejor se\ndel repaso continuamos viendo temas como lo son: derivadas ajuste a los datos, para poder realizar una prediccio'n de un\nparciales con respecto a w y b en la funcio'n de pe'rdida con\nvalor.\nel fin de actualizarlos y ajustar la funcio'n, y por u'ltimo vimos\nepoch y batch. b1. variables:\nindex terms-knn, regresio'n lineal, mean square error,\nvariables independientes: son las caracter'ısticas de la\nmae, descenso del gradiente, epoch y batch\nmuestra.\n\ni. noticasdelasemana variables dependientes: es el valor a predecir y es"}
{"id_doc": "DOC_013", "segmentacion": "B", "chunk_id": "DOC_013_B_001", "idx": 1, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "afectada por las varibales independientes\na. small language models are the future of agentic ai\n\ncon esto lo que queremos hacer es encontrar un modelo\nenelart'ıculosedicequelosmodelosdelenguajepequen˜os\nestad'ıstico lineal: f (x)=wx+b\n(slms) son ma's adecuados que los grandes (llms) para w,b\ndonde:\nciertossistemasinteligentesauto'nomos(agenticai),especialmente en tareas especializadas y repetitivas. [1] x es un vector d-dimensional.\nw es un vector d-dimensional.\nb. canaries in the coal mine? six facts about the recent\nb un nu'mero real.\n\nemployment effects of artificial intelligence\nwx es un producto punto, da'ndonos como resultado un\nel estudio analiza co'mo la adopcio'n de la inteligencia escalar.\nartificialgenerativahaafectadoalmercadolaboralenee.uu.,\nel modelo esta' parametrizado por w y b, por lo que\nutilizando datos administrativos mensuales de no'minas de\ndebemos encontrar los valores o'ptimos de w y b que hara'n\nadp,elmayorprocesadordeno'minasdelpa'ıs,elcualabarca\nquelafuncio'nrealicelasprediccionesma'sprecisas.peroojo,\nmillonesdetrabajadoresendecenasdemilesdeempresas.[2]\noptimo̸=perfecto"}
{"id_doc": "DOC_013", "segmentacion": "B", "chunk_id": "DOC_013_B_002", "idx": 2, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "ii. repasoclaseanterior\na. k nearest neighbor (knn)\nen resumen, cuando obtenemos una nueva instancia, medimos contra todos los elementos del dataset, y tomamos las\ndistancias ma's cercanas, y en base a eso determina'bamos la\nclase de la nueva instancia.\ncontamos con el hiperpara'metro k.\nes un algoritmo de lazy learning, porque realmente no se\naprende de los datos.\na1. ventajas:\nsencillo de implementar.\nes flexible: aplica tanto para regresion como clasificafigura1. tiposderegresio'n\ncio'n.\na2. desventajas:\nc. funcio'n de pe'rdida\nlas caracter'ısticas irrelevantes pueden distorsionar las\ndistancias necesitamos de un me'todo que' nos permita cuantificar que'\nes computacionalmente costoso. tan bien se ajusta nuestro modelo a los datos. funcio'n de\npoco eficiente en grandes volu'menes de datos. pe'rdida = medida del error del modelo\nd. error cuadra'tico medio (mse)\nes el resultado del modelo contra la etiqueta. sumamos\ntodos los errores de los samples y lo promediamos.\nn\nl= 1 (cid:88) (f (x )-y )2\nn w,b i i\ni=1\nd1. conceptos clave):\nfigura2. comparacio'ndedistintosvaloresparaalpha\nloss function: (f (x )-y )2 es la medida de penaw,b i i\nlidad que cuantifica el error de cada ejemplo."}
{"id_doc": "DOC_013", "segmentacion": "B", "chunk_id": "DOC_013_B_003", "idx": 3, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "iii. contenidodelaclase\nerror cuadra'tico: penaliza los errores grandes.\na. funcio'n de pe'rdida y sus derivadas parciales\ncostfunction:eselpromediodelalossfunctionsobre\ntodo el dataset. para optimizar los para'metros w y b de nuestro modelo,\nobjetivo: minimizar l para ajustar los parametros w,b. necesitamos actualizar sus valores de manera que la funcio'n\nde pe'rdida se minimice. para esto, evaluamos co'mo cada\nel motivo por el cual queremos minimizar l, es porque\npara'metro afecta la pe'rdida utilizando derivadas parciales con\nentre menor sea l, significa que tenemos un mejor modelo, y\nrespecto a w y b.\nentre ma's grande significa que tenemos un peor modelo.\nconsiderando la funcio'n de pe'rdida basada en el error\ncuadra'ticomedio(mse)paranuestromodelolinealf (x)=\nw,b\ne. ¿por que' mse y no mae?\nwx+b, tenemos:\nesdebidoaque' escuadra'tica,yestonosaseguraquevamos\nn\na tener un punto m'ınimo. y ta'mbien es porque la funcio'n no l(w,b)= 1 (cid:88) ((wx +b)-y )2\nmae no es smooth, por lo que no nos va a permitir obtener n i i\ni=1\nlas derivadas en todos los puntos, lo que induce a errores de"}
{"id_doc": "DOC_013", "segmentacion": "B", "chunk_id": "DOC_013_B_004", "idx": 4, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "las derivadas parciales de l con respecto a w y b se\nca'lculo\ncalculan como:\nf. derivadas generales n\n∂l 2 (cid:88)\n= ((wx +b)-y )x\n∂w n i i i\nregla funcio'nf(x) derivadaf′(x) i=1\n\nconstante k 0 n\nidentidad x 1 ∂l 2 (cid:88)\n= ((wx +b)-y )\nconstantemultiplicativa kx k ∂b n i i\npotencia xn nxn-1 i=1\nsuma u(x)+v(x) u′(x)+v′(x) estas derivadas nos indican la direccio'n y magnitud del\nproducto u(x)v(x) u′(x)v(x)+u(x)v′(x)\nconstantesumada u(x)+z u′(x) ajuste necesario para cada para'metro, permitiendo aplicar\nderivadasparciales f(x,y)=2x+3y ∂f =2,∂f =3 algoritmos de optimizacio'n como el gradient descent para\n∂x ∂y\nactualizar w y b.\n\ncuadroi\nrepasodederivadasba'sicas\nb. epoch\nunaepochesunaiteracio'ncompletasobretodoelconjunto\ndeentrenamiento.esunhiperpara'metroquedefinecua'ntasveg. descenso del gradiente ces se recorrera' el dataset completo durante el entrenamiento,\npor ejemplo, epochs = 5.\nel descenso del gradiente es un algoritmo iterativo de opti- si tenemos 10 000 muestras y ejecutamos 5 epochs, signimizacio'n para encontrar el m'ınimo de una funcio'n. funciona fica que se procesara'n todas las muestras 5 veces en total. la\nactualizando repetidamente los para'metros en la direccio'n actualizacio'n de los para'metros puede realizarse al finalizar\nopuesta al gradiente de la funcio'n de costo. cada epoch o de manera ma's frecuente utilizando batches.\ng1. regla de actualizacio'n:\nc. batch\nx nuevo =x antiguo -α-(2x) un batch es un subconjunto del conjunto de entrenamiento"}
{"id_doc": "DOC_013", "segmentacion": "B", "chunk_id": "DOC_013_B_005", "idx": 5, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "que se utiliza para calcular la gradiente y actualizar los\ng2. importancia del α: es el learning rate, debe ser para'metros del modelo.\npequen˜o para no pasarnos del punto m'ınimo. este es un por ejemplo, si tenemos 10 000 muestras y un batch\nhiperpara'metro size = 1 000, necesitaremos 10 batches para completar\nuna epoch. cada batch permite calcular la gradiente y actua- c3. mini-batch gradient descent: el mini-batch gralizar los para'metros sin esperar a procesar todo el dataset. dient descent combina las estrategias anteriores: calcula la\ndependiendo de la estrategia, se puede actualizar los para'me- gradiente sobre batches de taman˜o intermedio.\ntros despue's de cada batch o acumular gradientes antes de la ventajas:\nactualizacio'n. reduce el ruido respecto a sgd y es ma's estable.\nc1. batch gradient descent (vanilla): el batch gra- ma's eficiente que batch gd.\ndient descent calcula la gradiente utilizando todo el dataset: mejora la explotacio'n de hardware (vectorizacio'n,\ngpus).\nn\n1 (cid:88) ∂l\n∇l=\nn ∂θ\ni\ni=1\ny actualiza los para'metros solo despue's de procesar el\nconjunto completo.\nventajas:\ngradiente estable y pasos consistentes.\nayuda a evitar m'ınimos locales y aporta robustez en la\noptimizacio'n.\nfigura5. mini-batchgradientdescent\ndesventajas:\nrequiere todo el dataset en memoria. referencias\nlas actualizaciones son lentas para datasets grandes.\n[1] belcak, p., et al, \"small language models are the future of agentic\nla gradiente muy estable puede ocultar sen˜ales u'tiles.\nai\"2025.\n[2] e.brynjolfssonetal.,\"canariesinthecoalmine?sixfactsaboutthe\nrecentemploymenteffectsofartificialintelligence\"2025.\nfigura3. batchgradientdescent\nc2. stochastic gradient descent (sgd): el stochastic\ngradient descent actualiza los para'metros despue's de cada\nmuestra del dataset (o un pequen˜o conjunto aleatorio de\nmuestras).\nventajas:\ndetecta ra'pidamente si el algoritmo puede converger.\nu'til para datasets muy grandes.\ndesventajas:\nlas actualizaciones pueden ser muy ruidosas.\nla trayectoria de los para'metros es oscilatoria.\nmuchasactualizacionespuedensercostosascomputacionalmente.\n∂l\nw ←w-α\n∂w"}
{"id_doc": "DOC_013", "segmentacion": "B", "chunk_id": "DOC_013_B_006", "idx": 6, "autor": "Alex Steven Naranjo Masís", "fecha": "2025-08-28", "tema": "Repaso de KNN, regresión lineal, derivadas parciales y optimización mediante descenso del gradiente, incluyendo conceptos de Epoch y Batch.", "texto": "figura4. stochasticgradientdescent"}
{"id_doc": "DOC_014", "segmentacion": "B", "chunk_id": "DOC_014_B_000", "idx": 0, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "repaso de derivadas, regresio'n lineal y\n\nsesgo-varianza en aprendizaje supervisado\n\nian murillo campos\ninstituto tecnolo'gico de costa rica\nescuela de ingenier'ıa en computacio'n\n\ninteligencia artificial gr 2\n\nabstract-this paper reviews key elements of supervised posterior a esto se aplica el algoritmo del decenso\nlearning.itintroducestheuseofpartialderivativesandgradient del gradiente, el cual permite optimizar la posicio'n redescent to optimize the mean squared error function. it also\nspecto a w y b representado por la siguiente fo'rmula:\n\nexamines issues in linear regression such as nonlinearity and\noutliers,describingstatisticalmethodstoaddressthem.finally,it\noutlinesdatasetpartitioningintotraining,validation,andtesting\nsets,andexplainsthebias-variancetradeoffasatooltoevaluate\nmodel generalization.\n\ni. introduction\n\nel aprendizaje supervisado entrena modelos predictivos a\npartir de ejemplos con etiquetas. las derivadas parciales\npermiten calcular la influencia de cada para'metro sobre la b. vocabulario\nfuncio'n de pe'rdida y se aplican en el descenso de gradiente.\n- epoch:todaslasiteracionesquehacemossobretodoslos\nen la regresio'n lineal, los problemas comunes incluyen la\nsamples.esunhiperparametroquemidetodoelrecorrido\nno linealidad de la relacio'n entre variables y la presencia de"}
{"id_doc": "DOC_014", "segmentacion": "B", "chunk_id": "DOC_014_B_001", "idx": 1, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "de inicio a fin de todo mi set de entrenamiento\noutliers, que pueden corregirse con te'cnicas estad'ısticas.\n- batch: tomar ciertos subconjuntos del epoch. funciona\nla divisio'n de datos en entrenamiento, validacio'n y prueba\npara la optimizacio'n de las pruebas.\npermitemedirlacapacidaddegeneralizacio'ndelmodelo.este\nana'lisisserelacionaconelsesgoylavarianza,cuyoequilibrio iii. potencialesproblemasalrealizarregresio'n\nevita tanto el sobreajuste como el subajuste. lineal\n\nentrelospotencialesproblemasquenospodemosencontrar\n\nii. repasodederivadas\nesta'n la no linealidad de la relacio'n respuesta predictor, los\na. funcio'n de pe'rdida (mse)\ndatos sobresalientes y la colinealidad.\ndeesteu'ltimonosevaahablarenlaclase,quedacomotema\nde investigacio'n personal.\na. no linealidad de la relacio'n respuesta predictor\nuno de los principales supuestos de la regresio'n lineal es\nse busca optimizar esta funcio'n con valores que aumentan\nque existe una relacio'n lineal entre las variables predictoras y\nl, siendo l una parabola. dicho de otro modo, se busca\nla variable respuesta.\nencontrar la pendiente de algu'n punto de la parabola en el\n1) ¿que' ocurre si la verdadera relacio'n no es lineal?:\nque nos ubiquemos y buscamos descender sobre la funcio'n\npara llegar a su punto m'ınimo. - el modelo lineal no podra' captar adecuadamente la\nrelacio'n.\nse busca calcular cuanto influye el valor w sobre el valor l,\ncalculando sus derivadas parciales. - se obtendra'n errores sistema'ticos en los residuos,\ndefinidos como:\nesto deja como resultado lo siguiente:\n∂l 1 (cid:88) n (cid:0) (cid:1) e i =y i -yˆ i\n= 2 (wx +b)-y -x\n∂w n i i i"}
{"id_doc": "DOC_014", "segmentacion": "B", "chunk_id": "DOC_014_B_002", "idx": 2, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "donde\ni=1\ny\n\ntambien se tiene que realizar el procedimiento derivando con i\nbase en el bias, dando el siguiente resultado: es el valor real y\nyˆ\nn i\n∂l 1 (cid:88) (cid:0) (cid:1)\n= 2 (wx +b)-y\n∂b n i i es la prediccio'n del modelo.\ni=1\nesta'n ma's alejados, provocando que el modelo busque hacer\nun \"trade off\" entre los datos. esto no esta' bien ya que el\nmodelo quedar'ıa sesgado por los outliers y no se utilizar'ıan\ncorrectamente los datos que si quiero buscar predecir.\n\nestos datos en su mayoria son ocasionados por errores de\ncaptura, y la forma de corregirlos puede ser eliminarlos\ndirectamente del dataset.\notras te'cnicas pueden ser:\n- standardized residuals: escalar el residuo crufo por una\ndesviacio'n esta'ndar global de los errores.\ndonde:\nfig.1. ejemplodegra'ficaderegresio'nlineal.\n- e =y -yˆ es el residuo crudo"}
{"id_doc": "DOC_014", "segmentacion": "B", "chunk_id": "DOC_014_B_003", "idx": 3, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "i i i\n- n=nu'mero de observaciones\n\nenlafigura1sevealaizquierdaunplotresidualquemuestra\n- p=nu'mero de para'metros estimados en el modelo (incluye el intercepto)\n\nla fiderencia entre los puntos que tenia que predecir y cuanto\nla razo'n de utilizar una desviacio'n estandar es que\nse alejan entre si, el modelo de plot residual ma's correcto es\nteniendotodoestandarizado,sepuedesaberqueaciertas\nel que este' ma's cercano al cero.\ndesviaciones estandar de la media se encuentra un poren la figura de la derecha es el arreglo a los datos, utilizando\ncentajedelosdatos.conestosepuededefinirunumbral\nte'cnicas para que una funcio'n cuadratica como la de la\ndonde los datos son sobresalientes.\nizquierda se comporte ma's como una funcio'n lineal.\n- regla del rango intercuart'ılico: utilizar directamente los\nla forma de solucinarlo es extender el modelo lineal incorpodatos en lugar del modelo, tomamos el rango intercuanrando transformaciones po'linomicas del predictor, con eso:\ntilico que existe entre todos los datos, por definicio'n se\n- aunque la relacio'n es no lineal en los datos, el modelo ve de la siguiente forma:\nsigue siendo lineal en los para'metros.\n- se puede resolver con regresio'n lineal esta'ndar. iqr=q 3 -q 1\nb. outliers la regla para detectar los outliers es la siguiente:\nson datos que se salen de la distribucio'n que se trata de (cid:2) (cid:3)\nq -1.5-iqr, q +1.5-iqr\npredecir. 1 3\nal tratar de entrenar el modelo de ia, este se va a centrar\n- valores <q -1.5-iqr ⇒ outliers inferiores.\n1\n- valores >q +1.5-iqr ⇒ outliers superiores.\n3\nen la figura 3 se puede ver de forma gra'fica la regla del\nrango intercuart'ılico.\nnota: el 1.5 * iqr es aproximadamente equivalente a\nfig.3. ejemplodeoutlier.\n2-2.7 desviaciones esta'ndar de la media (depende de la\nforma de la distribucio'n).\n- winsorizacio'n: te'cnica que reemplaza los valores extremos por percentiles l'ımite, en lugar de eliminarlos.\nfig.2. ejemplodeoutlier.\nel procedimiento es el siguiente:\ntanto en los modelos cercanos a la linea como a los que - elegir percentiles de corte (ej. 5% y 95%).\n- valores menores al percentil 5 se reemplazan por el midiendo el rendimiento del modelo real'ısticamente simuvalor del percentil 5. lando datos que nunca ha vistohaciendo posible la com-\n- valores mayores al percentil 95 se reemplazan por paracio'n.\nel valor del percentil 95.\n1) caso overfitting: una solucio'n es dividir ptra parte de\ndentro de sus ventajas se encuentra que: los datos en datos de validacio'n, que se ejecuten como tests\n- conserva el taman˜o de la muestra. cada cierto tiempo durante la etapa de entrenamiento y que\n- reduce la influencia de valores extremos. esos datos aseguren que no se da un overfitting."}
{"id_doc": "DOC_014", "segmentacion": "B", "chunk_id": "DOC_014_B_004", "idx": 4, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "iv. sesgoyvarianza\na. dataset\nlosdatossedividenentredatosdeentrenamientoypruebas,\ncomo se ve en la figura 4, los datos de entrenamiento son con\nlos que se optimiza el modelo de ia, mientras que los de\npruebas son para verificacio'n, una divisio'n de 80% y 20% es\nlo ma's comu'n.\nd. validation set\nson un conjunto de datos que sirven para valorar la capacidad de generalizacio'nde mi modelo a datos nunca vistos,\n\neste set de datos brinda resultados con los que puedo tomar\nfig.4. ejemplodedataset. decisiones sobre el proceso de entrenamiento y es un set\nesencial para el ajuste de hiperpara'metros.\nb. training set\nse utiliza para ajustar el modelo ajustando los para'metros\ne. te'cnicas para subdividir el dataset\nde acuerdo a las muestras disponibles.\nel modelo identifica patrones basado en estos datos, ya que\n1) random sampling: se divide aleatoriamente el dataset,\nestos deber'ıan representar la diversidad de escenarios que se\nes u'til para datos con clases balanceados ya que no se agrega\nesperaencontrar.deestaformapermitira' almodeloentrenado\nningu'n sesgo al momento de hacer la divisio'n."}
{"id_doc": "DOC_014", "segmentacion": "B", "chunk_id": "DOC_014_B_005", "idx": 5, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "predecirdatosnuncavistosantesyencontrarpatronesentrelas\n\nlos datos imbalanceados pueden producir validation o testing\nentradasysalidas.porlotanto,sirveparaestablecerrelaciones\nsets con menos datos o ninguno, de las clases menos repreentre las variables y los pesos o para'metros del modelo.\nsentadas.\neste debe ser duficientemente grande para que sea significa2) stratifiedsampling: seutilizaparadatosimbalanceados\ntivo, pero sin causar overfitting. el overfitting ocurre cuando\nya que asegura una representacio'n de todas las clases en cada\n\nlos datos son muy especializados y adaptados al conjunto de\nsplit.\n\nentrenamiento por lo que el modelo se vuelve incapaz de\nmantiene la misma distribucio'n de datos para cada clase en\ngeneralizar adecuadamente.\ncada subconjunto, lo que da un modelo ma's robusto.\nc. testing set\n3) k-fold cross-validation: se divide el subconjunto en k\n\nse utiliza para evaluar el modelo con ejemplos que no se partes y el modelo se entrena con k-1 partes ya que una se\nutilizaron en el entrenamiento. reserva para validacio'n.\ndebe ser independiente del set de entrenamiento. secontinuaesteprocesorotandolossubconjuntosusadospara\nsimula la aplicacio'n de un examen a nuestro modelo y con el el entrenamiento y validacio'n. permite tomar el promedio del\nse calculan me'tricas como : accuracy, loss, etc... rendimiento del modelo y es u'til cuando tenemos pocos datos\nel objetivo de este set es crear un modelo que generalice y deseamos validar nuestro modelo. se puede ver de forma\nadecuadamente todos los escenarios. ma's gra'fica en la siguiente imagen:\nf. posibles escenarios\ndentro de los posibles escenarios de estos me'todos se\nencueentran:\n- bajo error en training, bajo error en testing. otro escenario es el siguiente:\n- escenario ideal. - alto error en training, alto error en testing.\n- modelo evita el ruido existente en los datos. - underfitting.\n- puede generalizar correctamente. - el modelo no esta' aprendiendo nada de los datos.\n- modelo muy simple.\nvisualmente se puede ver de la siguiente forma:\n- alto sesgo\nvisualmente se ve de la siguiente forma:\notro escenario es cuando se tiene lo siguiente:\n- bajo error en training, alto error en testing. para solucionar este u'ltimo caso se utiliza un bias-variance\n- overfitting. tradeoff.\n- no es capaz de generalizar."}
{"id_doc": "DOC_014", "segmentacion": "B", "chunk_id": "DOC_014_B_006", "idx": 6, "autor": "Ian Murillo Campos", "fecha": "2025-09-02", "tema": "Análisis del aprendizaje supervisado, descenso del gradiente y manejo de problemas de regresión lineal, outliers y el tradeoff sesgo-varianza.", "texto": "v. bias-variancetradeoff\n- alta varianza.\n\nse busca un modelo que tenga baja varianza y\nvisualmente se ve de la siguiente forma: bajo sesgo, para eso se editan valores en las pruebas,\nvisualmente se ve un arreglo de la siguiente forma:\n\nreferences\n[1] s. pacheco, \"repaso de matema'tica: a'lgebra lineal,\" presentacio'n,\ninstitutotecnolo'gicodecostarica,2025.\n[2] s.pacheco,\"sesgoyvarianza,\"presentacio'n,institutotecnolo'gicode\n\ncostarica,2025."}
{"id_doc": "DOC_015", "segmentacion": "B", "chunk_id": "DOC_015_B_000", "idx": 0, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "apuntes semana 5 clase #1s\n\neder vega suazo\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\n\nic-6200 - inteligencia artificial gr2\n\nresumen-este documento es un resumen de la clase de ii. desaf'iosenmodeladopredictivo\ninteligencia artificial correspondiente a la semana 5, enfocando\nen los fundamentos del aprendizaje supervisado. se abordan ii-a. relaciones no lineales entre variables\ntemas clave como la optimizacio'n de modelos mediante ca'lculo\nla regresio'n lineal presume una relacio'n lineal entre prediferencial y el algoritmo de descenso de gradiente aplicado a la\ndictoresyvariablerespuesta.cuandoestasuposicio'nseviola,\nfuncio'ndeerrorcuadra'ticomedio.adema's,seexaminandesaf'ıos\ncomunes en el modelado predictivo, incluyendo el manejo de el modelo resulta inadecuado y muestra patrones sistema'ticos\nrelaciones no lineales entre variables y la deteccio'n de valores en los residuos:\nat'ıpicos.tambiendiscutenestrategiasparalaevaluacio'ndemoe =y -yˆ\ndelos mediante particio'n de datasets y se analiza el compromiso i i i\nentre sesgo y varianza, crucial para desarrollar modelos con\nla solucio'n implica transformar las variables predictoras\ncapacidad de generalizacio'n efectiva.\nmediante expansio'n polinomial o otras transformaciones que"}
{"id_doc": "DOC_015", "segmentacion": "B", "chunk_id": "DOC_015_B_001", "idx": 1, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "i. optimizacio'nmedianteca'lculodiferencial\npermitancapturarrelacionesnolinealesmanteniendolalineai-a. funcio'n de error cuadra'tico medio lidad en los para'metros.\nen problemas de regresio'n, la funcio'n de costo ma's comu'n\nesta' dada por:\nn\nl= 1 (cid:88) (f (x )-y )2, i=1,...,n\nn w,b i i\ni=1\ndonde h (x ) representa la prediccio'n del modelo para la\nθ i\ninstancia i-e'sima.\nel proceso de optimizacio'n busca minimizar esta funcio'n\nmediante el ca'lculo de gradientes:\nn\n∂l 1 (cid:88)\n= 2((wx +b)-y )-x figura 1: ejemplo de relacio'n no lineal y su ajuste mediante\n∂w n i i i transformacio'n polinomial.\ni=1\nn\n∂l 1 (cid:88)\n= 2((wx +b)-y )\n∂b n i i ii-b. manejo de valores at'ıpicos\ni=1\ni-b. algoritmo de descenso de gradiente las observaciones extremas pueden distorsionar significativamentelosmodelosderegresio'n.existenmu'ltiplesenfoques\nla actualizacio'n de para'metros se realiza de forma iterativa\npara su identificacio'n y tratamiento:\nmediante:\nw(t+1) =w(t)-α ∂l ii-b1. identificacio'n de valores at'ıpicos:\nb(t+1) =b(t)-α\n∂\n∂\nw\nl\n(t) r\nde\ne\ns\ns\nv\nid\nia\nu\nc\no\nio'\ns\nn\ne\ne\ns\ns\nt\nt\na\na'\nn\nnd\nd\na\na\nr\nri\nd\nz\ne\nad\nlo\no\ns\ns:\nre\nz"}
{"id_doc": "DOC_015", "segmentacion": "B", "chunk_id": "DOC_015_B_002", "idx": 2, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "s i idu\n=\nos. σ\nei\ne\ndonde σ\ne\n\nes la\n∂b(t) rango intercuart'ılico: valores fuera de [q - 1,5 -\n1\ndonde α representa la tasa de aprendizaje que controla la iqr,q +1,5-iqr] se consideran at'ıpicos.\n3\nmagnitud de cada actualizacio'n. ii-b2. te'cnicas de tratamiento:\ni-c. terminolog'ıa fundamental eliminacio'n:removerobservacionesidentificadascomo\ne'poca(epoch):ciclocompletodepresentacio'ndetodos at'ıpicas.\nlos ejemplos de entrenamiento al modelo. winsorizacio'n: reemplazar valores extremos por perlote (batch): subconjunto de ejemplos utilizados para centiles espec'ıficos (ej. percentil 5 y 95).\ncalcular una actualizacio'n de para'metros. transformaciones: aplicar funciones como logaritmo\ntasa de aprendizaje: hyperpara'metro que determina la o ra'ız cuadrada para reducir la influencia de valores\nvelocidad de convergencia del algoritmo. extremos.\ncuadro ii: caracter'ısticas de modelos con sesgo o varianza"}
{"id_doc": "DOC_015", "segmentacion": "B", "chunk_id": "DOC_015_B_003", "idx": 3, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "elevados\nme'trica altosesgo altavarianza\n\nerrorentrenamiento alto bajo\nerrorvalidacio'n alto alto\n\ncomportamiento subajuste sobreajuste\nsoluciones modelos ma's comple- regularizacio'n, ma's\n\njos datos\nfigura2:efectodevaloresat'ıpicosenunmodeloderegresio'n\n\niv. sesgoyvarianza\nlineal.\niv-a. diagno'stico de problemas comunes\niv-b. estrategias de mejora\n\niii. evaluacio'nyvalidacio'ndemodelos para alto sesgo: aumentar la complejidad del modelo,\nagregar caracter'ısticas adicionales o reducir regularizaiii-a. particio'n de datasets cio'n.\npara alta varianza: aumentar datos de entrenamiento,\nla divisio'n adecuada de los datos es crucial para evaluar la aplicar te'cnicas de regularizacio'n o reducir la complejicapacidad de generalizacio'n: dad del modelo.\ncompromiso o'ptimo: seleccionar la complejidad del\ncuadro i: propo'sitos de los diferentes subconjuntos de datos modelo que minimice el error de generalizacio'n.\nsubconjunto propo'sito\nentrenamiento ajuste de para'metros del modelo mediante optimizacio'n\nvalidacio'n seleccio'n de hyperpara'metros y monitorizacio'n del"}
{"id_doc": "DOC_015", "segmentacion": "B", "chunk_id": "DOC_015_B_004", "idx": 4, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "sobreajuste\nprueba evaluacio'n final del rendimiento con datos nunca\n\nvistos\niii-b. te'cnicas de muestreo\n\n1. muestreoaleatorio:divisio'nrandomizadaquepreserva\nla distribucio'n original de los datos.\n\n2. muestreo estratificado: mantiene la proporcio'n de clasesencadaparticio'n,crucialparadatosdesbalanceados.\n\n3. validacio'n cruzada: divide los datos en k particiones\ny realiza k iteraciones de entrenamiento/validacio'n.\nfigura 4: relacio'n entre complejidad del modelo y error de\ngeneralizacio'n.\n\nv. conclusiones\n\nla efectividad de los modelos de aprendizaje supervisado\ndependecr'ıticamentedelaadecuadaoptimizacio'ndepara'metros, el manejo de relaciones complejas entre variables, la\nidentificacio'n y tratamiento de valores at'ıpicos, y la evaluacio'n rigurosa mediante te'cnicas de validacio'n apropiadas. el\n\nentendimiento del compromiso entre sesgo y varianza permite\n\ndesarrollar modelos que generalizan efectivamente a nuevos\ndatos, balanceando complejidad y capacidad predictiva.\nfigura 3: esquema de validacio'n cruzada con k =5 particioreferencias\nnes. [1] apuntes de la clase de inteligencia artificial, profesor s. pacheco,"}
{"id_doc": "DOC_015", "segmentacion": "B", "chunk_id": "DOC_015_B_005", "idx": 5, "autor": "Eder Vega Suazo", "fecha": "2025-09-02", "tema": "Optimización de modelos de aprendizaje supervisado mediante cálculo diferencial y descenso del gradiente, con tratamiento de valores atípicos y análisis sesgo-varianza.", "texto": "institutotecnolo'gicodecostarica,2025."}
{"id_doc": "DOC_016", "segmentacion": "B", "chunk_id": "DOC_016_B_000", "idx": 0, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "inteligencia artificial\napuntes semana 5, clase #2\n\nluis fernando benavides villegas\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nlubenavides@estudiantec.cr\n\nabstract-este documento recopila los apuntes de la clase del patrones sistema'ticos (por ejemplo, en forma de para'bola) en\njueves 04 de septiembre de 2025 para el curso de inteligencia lugar de distribuirse de manera aleatoria. esto indica que el\nartificial. se repasan conceptos clave de regresio'n lineal y sus\nmodelo lineal no es adecuado. para resolverlo, una opcio'n es\nlimitaciones, as'ı como los problemas de overfitting y underfitting.\naplicar feature engineering, agregando te'rminos polino'micos\ntambie'n se describen te'cnicas de subdivisio'n de datasets y\nestrategias para mejorar la capacidad de generalizacio'n de los que transformen las variables originales y permitan que la\nmodelos.finalmente,seintroducelaregresio'nlog'ısticacomoun relacio'nseaproximemejoraunaformalineal.deestamanera,\nmodelo de clasificacio'n binaria, explicando la funcio'n sigmoide, aunque la relacio'n real sea curva, el modelo puede ajustarse\nsuderivadayelprocesodeoptimizacio'ndepara'metrosmediante\ncon menor error.\ndescenso del gradiente.\n2) datos sobresalientes: surgen por ruido, errores de\nindexterms-inteligenciaartificial,regresio'nlineal,regresio'n\nlog'ıstica,funcio'nsigmoide,overfitting,underfitting,optimizacio'n medicio'n o datos at'ıpicos y pueden afectar el ajuste del\nmodelo. una forma de tratarlos es estandarizar los residuos\ndividiendoentreladesviacio'nesta'ndar.unaveznormalizados,"}
{"id_doc": "DOC_016", "segmentacion": "B", "chunk_id": "DOC_016_B_001", "idx": 1, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "i. noticiasdelasemana se mide cua'ntas desviaciones esta'ndar se aleja cada dato. si\nun dato esta' muy lejos (ma's de 2 o 3 desviaciones esta'ndar),\na. ingenieeer'ıa costa rica\nse considera sobresaliente. otras te'cnicas que vimos fueron el\nun evento de ingenier'ıa que organiza ieee costa rica. rangointercuart'ılico,queeselqueseusaengra'ficosdecaja\nhabra' charlas de profesores distinguidos en diversas a'reas y ybigotes,ylawinsorizacio'n,dondeenvezdeeliminardatos\nparticipacio'n de empresas. [1] at'ıpicos se reemplazan por valores en percentiles l'ımite.\n3) colinealidad: sedacuandodosoma'spredictoresesta'n\nb. referencias falsas en ia\naltamente correlacionados entre s'ı. esto hace dif'ıcil separar\nlas\"alucinaciones\"eninteligenciaartificialsoncuandolos el efecto de cada variable en la prediccio'n, afectando la\nmodelos generan referencias aparentemente va'lidas pero que estabilidad de los coeficientes del modelo. en consecuencia,\nen realidad no existen. esto fue debatido en el grupo parma los para'metros estimados se vuelven poco confiables y muy\ndel tec y se resalto' la importancia de siempre verificar las sensibles a cambios en los datos. para detectarla, se pueden\nfuentes. la responsabilidad recae en el usuario de confirmar usar medidas como el vif (variance inflation factor). una\nla veracidad de la informacio'n antes de tomarla como cierta. solucio'n comu'n es eliminar variables redundantes o aplicar\nte'cnicas de regularizacio'n.\nc. google nano banana\nb. dataset\nel nuevo modelo de google enfocado en la edicio'n de\nima'genes que se llama nano banana. a diferencia de otros es el conjunto completo de datos disponibles para entrenar\ngeneradores que recrean la imagen completa desde cero, este y evaluar un modelo. normalmente se subdivide en diferentes\nmodelo conserva mejor los detalles originales y el contexto. partesparapodermedirlacapacidaddegeneralizacio'nyevitar\nas'ı,aleditarunafotomantienelacoherenciaentreiteraciones. problemas como el overfitting.\nse hablo' tambie'n de posibles sesgos en los modelos, al notar\nque repeticiones en ima'genes de personas modificaban rasgos c. training set\nhacia un perfil ma's latino."}
{"id_doc": "DOC_016", "segmentacion": "B", "chunk_id": "DOC_016_B_002", "idx": 2, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "subconjunto usado para entrenar el modelo y ajustar sus\npara'metros. es donde el algoritmo aprende los patrones preii. repasodelaclaseanterior\nsentes en los datos.\na. potenciales problemas al aplicar una regresio'n lineal\nd. validation set\n1) no linealidad: un supuesto de la regresio'n lineal es\nque la relacio'n entre las variables predictoras y la variable re- subconjunto usado durante el entrenamiento para evaluar\nspuestaeslineal.cuandonosecumple,losresiduosmuestran el rendimiento intermedio del modelo. sirve para medir si\n\nlo aprendido se generaliza a datos no vistos y para ajustar\nhiperpara'metros.permitedetectarproblemasdesobreajustede\nmanera temprana sin necesidad de esperar a la prueba final.\ne. te'cnicas para subdividir el dataset\n1) random sampling: consiste en dividir aleatoriamente\nlos datos entre entrenamiento y prueba. es adecuado cuando\nlasclasesesta'nbalanceadas,yaquegarantizarepresentatividad\nsin introducir sesgos. el problema surge si las clases esta'n\ndesbalanceadas, porque puede que un subconjunto quede con\nmuy pocos o incluso sin ejemplos de alguna clase.\nfig.4. ej.deregresio'ndeunderfitfig.3. ej.deunderfitting\n2) stratified sampling: se usa cuando las clases esta'n ting\ndesbalanceadas. mantiene la misma proporcio'n de clases en\nlos conjuntos de entrenamiento y prueba. de esta forma, si en 3) caso ideal: el error en training es bajo y tambie'n lo\nel dataset original una clase representa el 90% y otra el 10%, es en validation. el modelo logra ajustarse a los datos sin\nesa relacio'n se conserva en las divisiones. sobreajustarsealruidoypuedegeneralizarbienaejemplosno\n3) k-fold cross-validation: el conjunto de entrenamiento vistos. representa un buen equilibrio entre sesgo y varianza.\nse divide en k partes (folds). en cada iteracio'n se usa\nk -1 folds para entrenar y el fold restante para validar. el\nproceso se repite k veces, rotando el fold de validacio'n. esto"}
{"id_doc": "DOC_016", "segmentacion": "B", "chunk_id": "DOC_016_B_003", "idx": 3, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "permite aprovechar mejor los datos disponibles y obtener una\nevaluacio'n ma's robusta del modelo.\nf. posibles escenarios de comportamiento de training y validation\n1) overfitting: el error en training es bajo pero el error\nen validation comienza a aumentar despue's de cierto punto.\n\nel modelo memoriza los datos de entrenamiento en lugar de\naprenderpatronesgenerales.secapturatambie'nelruidodelos\nfig.5. ej.delcasoideal fig.6. ej.deregresio'ndelcasoideal\ndatos,loqueprovocaquenopuedageneralizar.secaracteriza\npor tener alta varianza.\n4) bias-variance tradeoff: es un caso su'per raro porque\nel error en training es alto pero el error en validation es bajo.\nsi sucede, puede deberse a errores de ca'lculo o valores mal\ntomados, no a un aprendizaje real del modelo.\n\niii. altobias\n\nse presenta cuando el modelo es demasiado simple y\nno logra capturar el patro'n real de los datos, provocando\nunderfitting. tanto el error en training como en validation\nson altos, ya que el modelo asume demasiado sobre la forma\nde los datos.\nfig.1. ej.deoverfitting fig.2. ej.deregresio'ndeoverfitting a. causas\n- modelo demasiado simple (ej. lineal para datos con\nuna te'cnica para evitarlo es el early stopping, que consiste relaciones cuadra'ticas).\nen detener el entrenamiento en la e'poca donde el error de - no se utilizan todas las variables relevantes.\nvalidacio'n empieza a empeorar. - los features disponibles no son buenos predictores de la\n2) underfitting: tanto el error en training como en val- variable objetivo.\nidation son altos. el modelo no logra aprender patrones de\nb. posibles soluciones"}
{"id_doc": "DOC_016", "segmentacion": "B", "chunk_id": "DOC_016_B_004", "idx": 4, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "los datos porque es demasiado simple o incorrecto para el\nproblema. se caracteriza por alto sesgo, es decir, asume una - incrementar la complejidad del modelo (por ejemplo,\nforma equivocada de los datos (por ejemplo, usar un modelo pasar de lineal a cuadra'tico o a un modelo ma's flexible).\nlineal para datos con comportamiento cuadra'tico). - incorporar ma's features o transformar los existentes.\n- sustituirorecolectarmejoresfeaturesquerepresentende\nmanera adecuada el problema.\n\niv. altavarianza\n\nse presenta cuando el modelo se ajusta demasiado a los\ndatos de entrenamiento pero falla al generalizar en el conjuntodevalidacio'n.estoprovocaoverfitting,dondepequen˜as\n\nvariaciones en los datos de entrada pueden generar malas\npredicciones.\nfig.7. regresio'nlinealvslog'ıstica\na. causas\nb. distribucio'n de bernoulli\n- el modelo es demasiado complejo y aprende patrones\nirrelevantes o ruido. cada etiqueta y es una variable aleatoria que sigue una\ni\n- exceso de dimensionalidad: agregar muchas variables distribucio'n de bernoulli. la probabilidad de que ocurra el\naumenta el riesgo de overfitting. evento (y =1) o no ocurra (y =0) se define como:\n- muy pocos ejemplos en el conjunto de entrenamiento,\nespecialmente en problemas con clases desbalanceadas.\np(y =k)=pk(1-p)1-k, k ∈{0,1}\nb. posibles soluciones donde:\n- reducirlacomplejidaddelmodelo(ej.usarmenoscapas - p es la probabilidad de e'xito (y =1).\no un modelo ma's simple). - k es la etiqueta observada (0 o 1).\n- disminuir la dimensionalidad eliminando variables irrelas'ı, si k = 1, la probabilidad es p; y si k = 0, la\nevantes.\nprobabilidad es 1-p.\n- obtener ma's ejemplos de entrenamiento para mejorar la\nrepresentacio'n de todas las clases."}
{"id_doc": "DOC_016", "segmentacion": "B", "chunk_id": "DOC_016_B_005", "idx": 5, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "vi. funcio'nsigmoide\n- aplicar te'cnicas de regularizacio'n que penalizan la complejidad del modelo, como: lafuncio'nsigmoideesunaherramientaclaveporqueintroducenolinealidadalmodeloytieneunrangodesalidaentre\n- l1 y l2 (penalizacio'n sobre los para'metros).\n0 y 1, lo cual la hace ideal para trabajar con probabilidades.\n- dropout (apagar ciertas neuronas durante el entrenamiento). se define como:\n1\nσ(x)=\n\nv. regresio'nlog'istica 1+e-x\nal tomar valores de entrada muy negativos, la salida se\naunque su nombre incluya \"regresio'n\", la regresio'n\nacerca a 0; mientras que con valores grandes y positivos, se\nlog'ıstica es un modelo de clasificacio'n, no de regresio'n.\nacerca a 1.\nse utiliza principalmente para problemas binarios, donde las\netiquetas y toman los valores 0 o 1.\na. diferencia con la regresio'n lineal\n- en regresio'n lineal se predicen valores continuos en los\nreales (r).\n- en regresio'n log'ıstica se predice la probabilidad de\npertenecer a una clase u otra. el resultado final es una\nclasificacio'n: 0 o 1.\npor ejemplo, con una variable como el taman˜o de una\ncalabaza:\nfig.8. gra'ficadelafuncio'nsigmoide\n- regresio'nlineal:prediceelprecioaproximadoenvalores\nreales. adema's,elargumentoxpuedesercualquiervaloroincluso\n- regresio'n log'ıstica: predice si la calabaza es naranja (1) otrafuncio'n(composicio'ndefunciones),loquedaflexibilidad\no no lo es (0). para modelar relaciones ma's complejas.\nla idea es tomar la salida de un modelo lineal y conver- en regresio'n lineal usamos el error cuadra'tico medio\ntirla en una probabilidad. si partimos de una funcio'n lineal (mse), pero en clasificacio'n esto deja de ser u'til, porque ya\nf (x)=wx+b,alaplicarlelafuncio'nsigmoideobtenemos: no predecimos valores continuos, sino probabilidades.\nw,b\nel procedimiento general sigue siendo el mismo:\n1\nyˆ=σ(f (x))=\nw,b 1+e-(wx+b) - definimos una funcio'n de pe'rdida l apropiada para\nprobabilidades.\nde esta forma: - calculamos sus derivadas respecto a w y b.\n- si yˆ<0.5, se clasifica como 0. - usamos esas derivadas en el algoritmo de descenso del\n- si yˆ≥0.5, se clasifica como 1. gradiente,iterandosobrelosdatosdeentrenamientopara\nir actualizando los para'metros y minimizar la pe'rdida.\nesto convierte la regresio'n log'ıstica en un modelo de\nclasificacio'n binaria. queremos hacerlo as'ı porque calcular\nc. derivada de la funcio'n sigmoide\nuna funcio'n lineal es simple computacionalmente, es un buen\nme'todo para mantener la relacio'n entre variables y pesos y\n1\npermite modelar problemas con mayor complejidad. σ(x)=\n1+e-x\na. diagrama computacional de la regresio'n log'ıstica\n1′-(1+e-x)-1-(1+e-x)′\nσ′(x)=\n(1+e-x)2\ne-x\nσ′(x)=\n(1+e-x)2\ne-x+1-1\nσ′(x)=\n(1+e-x)2\ne-x+1 1\nσ′(x)= -\n(1+e-x)2 (1+e-x)2\nfig.9. diagrama\n1 1\nσ′(x)= -\n1) los inputs (features) x ingresan junto con un vector de 1+e-x (1+e-x)2\npesos w y un bias b.\n2) secalculaelproductopuntoentreelvectorxyelvector 1 (cid:18) 1 (cid:19)\nσ′(x)= - 1w. 1+e-x 1+e-x\n3) se le aplica la funcio'n no lineal σ(z), obteniendo como\nsalida una probabilidad.\nσ′(x)=σ(x)(1-σ(x))\n4) finalmente,estaprobabilidadsecomparaconunumbral\npara asignar una etiqueta de clase (0 o 1).\nd. hallar la funcio'n de pe'rdida\nen algunos textos, al valor lineal z = wx + b se le\nllama pre-activacio'n, y a la aplicacio'n de la sigmoide se le ¿mse? esto y ma's en la siguiente clase.\nllama activacio'n. la salida de la activacio'n corresponde a la\nprobabilidad estimada yˆ. vii. conclusio'n"}
{"id_doc": "DOC_016", "segmentacion": "B", "chunk_id": "DOC_016_B_006", "idx": 6, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "en esta clase se reforzaron conceptos esenciales para\nb. optimizacio'n\ncomprender co'mo los modelos de aprendizaje supervisado\nnuestroobjetivoesoptimizarlospara'metroswybparaque aprenden a partir de datos. se revisaron las limitaciones de\nel modelo aprenda correctamente. tenemos: la regresio'n lineal y los problemas comunes asociados al\nsesgo y la varianza, as'ı como te'cnicas para evaluar y mejorar\n1\nyˆ=σ(f w,b (x))= 1+e-(wx+b) la generalizacio'n de los modelos. adema's, se introdujo la\nregresio'n log'ıstica como un modelo de clasificacio'n, destapara ajustar los para'metros, necesitamos calcular las cando el papel de la funcio'n sigmoide y su derivada en el\nderivadas parciales de la funcio'n de pe'rdida respecto a w y proceso de optimizacio'n. estos fundamentos sientan la base\nb. sin embargo, antes de derivar, debemos definir una funcio'n para profundizar en funciones de pe'rdida espec'ıficas y en el\nde pe'rdida adecuada. entrenamiento de modelos ma's complejos en futuras sesiones."}
{"id_doc": "DOC_016", "segmentacion": "B", "chunk_id": "DOC_016_B_007", "idx": 7, "autor": "Luis Fernando Benavides Villegas", "fecha": "2025-09-04", "tema": "Revisión de regresión lineal, overfitting, underfitting y regresión logística, con enfoque en la función sigmoide y optimización de parámetros.", "texto": "referencias\n[1] ieee costa rica. \"ingenieeer'ıa costa rica.\" [en l'ınea]. disponible:\nhttps://r9.ieee.org/costarica/ingenieeeria\n[2] a. shervine. \"hoja de referencia de aprendizaje automa'tico.\" stanford university. [en l'ınea]. disponible:\nhttps://stanford.edu/∼shervine/l/es/teaching/cs-229/\n\nhoja-referencia-aprendizaje-automatico-consejos-trucos"}
{"id_doc": "DOC_017", "segmentacion": "B", "chunk_id": "DOC_017_B_000", "idx": 0, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "apuntes de semana 5, clase #2\n\nmauricio campos cerdas\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nmaucampos@estudiantec.cr\n\nabstract-this document presents class notes on handling\noutliers, the concepts of bias and variance, and an introduction\ntologisticregressionasaclassificationalgorithm.techniquesfor\nidentifying and addressing outlying values are discussed, along\nwith methods for splitting datasets and common scenarios encounteredduringtrainingandvalidation.aswellasthesigmoid\n\nfunction and parameter optimization in logistic regression are\nintroduced, including the derivation of the sigmoid function.\nindexterms-outliers,bias,variance,logisticregression,classification,sigmoidfunction,parameteroptimization,trainingand\nvalidation, overfitting, underfitting\n\ni. noticiasdelasemana\nfig.1. residualplots\na. evento ieee\nieee esta' organizando un evento donde se tocara'n temas\nmuyinteresantes,incluyendolainteligenciaartificial.vendra'n - datos sobresalientes: siempre existira'n outliers, ya sea\npor ruido o error humano. lo que pasa es que nos afecta\npersonas de gran renombre a dar charlas, habra' comida y\na nuestro modelo, siempre habra' cierta sensibilidad hay\ndema's. se pide registrarse para calcular la alimentacio'n para"}
{"id_doc": "DOC_017", "segmentacion": "B", "chunk_id": "DOC_017_B_001", "idx": 1, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "que tratarlos para evitar que nos afecte en gran medida\nel d'ıa del evento.\nnuestro modelo.\nb. problema con las referencias y la ia\na. me'todos para tratar outliers\nse esta' produciendo un feno'meno en el que cada vez ma's\nart'ıculos, notas y sitios web son generados con inteligencia - standardized residuals: tenemos el ca'lculo de\nartificial y se referencian entre s'ı. esto puede llevar a que los residuos y calculamos la desviacio'n esta'ndar,\nla propia ia se cite a s'ı misma, provocando un aumento de para asegurarnos de que nuestros datos siguen una\n\nreferencias generadas artificialmente. distribucio'n normal. a partir de que los tenemos\nestandarizados, calculamos a cua'ntas desviaciones\nc. modelo nano banana\nesta'ndar se encuentra ese dato. lo que nos dira'\ngoogle lanzo' un nuevo modelo llamado nano banana. su es el l'ımite de hasta do'nde se consideran datos\natractivo se encuentra que a diferencia de otros modelos, este sobresalientes.\nagarra la imagen que esta' como input y la modifica sin tener ∗ |z|>2: posible outlier.\nque generarla otra vez. se dio' un ejemplo de un experimento ∗ |z| > 3: outlier muy probable, se recomienda\ndonde una ia ten'ıa que modificar una foto varias veces y se excluir.\nllego' a evidenciar que hubo un sesgo de generar la imagen de\n- regla del rango intercuart'ılico (iqr): definido\nla persona cada vez con rasgos ma's latinos.\ncomoiqr=q3-q1.losdatosqueseencuentran"}
{"id_doc": "DOC_017", "segmentacion": "B", "chunk_id": "DOC_017_B_002", "idx": 2, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "ii. potencialesproblemasdelaregresio'nlineal fuera del intervalo [q1-1.5-iqr,q3+1.5-iqr]\nse consideran outliers.\n- no linealidad: en regresio'n lineal se asume que existe una relacio'n lineal entre las variables predictoras - winsorizacio'n: te'cnica que consiste en reemplazar\ny la variable respuesta. sin embargo, esto no siempre los valores extremos por los percentiles l'ımite (por\nse cumple, lo que provoca que el modelo no capture ejemplo, 5% y 95%).\nadecuadamente la relacio'n y que los residuos presenten\npatronessistema'ticos(porejemplo,conformaparabo'lica)\n\niii. sesgoyvarianza\nen lugar de distribuirse aleatoriamente (ver fig. 1). el dataset suele dividirse en train y test (80/20)\n\nuna estrategia para enfrentar este problema es aplicar\na. training set\nfeature engineering. un ejemplo es incorporar te'rminos\npolino'micos adicionales a las variables, lo que permite se utiliza para ajustar el modelo. nos puede pasar que\naproximar mejor relaciones no lineales. entrenemos el modelo mucho tiempo, lleguemos al final y\nfig.2. underfitting,ideal,overfittingplots\nnos damos cuenta de que fallamos el examen. si dedicamos\nmucho al entrenamiento pero nada a generalizar, se llama\nfig.3. linearvslogisticregression\noverfitting. por eso queremos hacer tests pequen˜os durante el\nentrenamiento, con el validation set.\ne. alto bias\nb. validation set"}
{"id_doc": "DOC_017", "segmentacion": "B", "chunk_id": "DOC_017_B_003", "idx": 3, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "cuando el modelo comete muchos errores en el training\nnos dice si los hiperpara'metros son adecuados o no, para set, se produce underfitting. esto ocurre porque el modelo\nno continuar si no lo son y as'ı no desperdiciar recursos. asume demasiado del training set, no utiliza todas los features\ndisponibles y es demasiado simple para capturar la complejic. te'cnicas de subdividir el dataset dad de los datos. para evitar un alto sesgo, se puede utilizar\nun modelo ma's complejo. adema's, es importante revisar que\n- randomsampling:seusasiemprequetengamosclases los features del training set sean adecuadas para la naturaleza\nbalanceadas. si los datos no esta'n balanceados, pueden\ndel problema, ya que si no tienen la capacidad de capturar la\nquedar mal distribuidos, con ma's datos de una clase que\ninformacio'n relevante, el modelo no podra' hacer predicciones\nde la otra.\ncorrectas.\n- stratified sampling: usado para datos imbalanceados,\nasegura una representacio'n de todas las clases por sepa- f. alta varianza\nrado."}
{"id_doc": "DOC_017", "segmentacion": "B", "chunk_id": "DOC_017_B_004", "idx": 4, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "ocurre cuando el modelo se ajusta demasiado a los datos\n- k-fold cross-validation: divisio'n en k partes, en cada de entrenamiento y no es capaz de generalizar correctamente.\niteracio'n se usan k - 1 para entrenamiento y 1 para esto suele suceder cuando los datos son de alta dimensionvalidacio'n. alidad y hay pocos ejemplos disponibles. para evitar la alta\nvarianza, se pueden usar modelos ma's simples, reducir la\nd. escenarios posibles dimensionalidad de los datos, obtener ma's ejemplos y aplicar\nte'cnicas de regularizacio'n.\n- escenario ideal: el modelo presenta bajo error tanto en\ntraining como en testing. puede evitar el ruido de los\n\niv. regresio'nlog'istica\ndatos y generalizar correctamente. por cada e'poca de\nentrenamientoelerrordeber'ıairdisminuyendo,tendiendo aunque su nombre contenga la palabra regresio'n, en resiempre a la baja. alidad la regresio'n log'ıstica es un algoritmo de clasificacio'n\nbinaria. distingue entre dos clases (0 y 1), estimando proba-\n- overfitting: ocurre cuando el error en el validation set\nbilidades. fig. 3).\nempieza a crecer o se estanca. esto indica que el modelo\nerabuenohastaciertae'pocadeentrenamiento,peroluego\na. distribucio'n de bernoulli\nempieza a sobreajustarse a los datos de entrenamiento,\nproduciendo overfitting. a esta te'cnica de detener el utilizamos una distribucio'n de bernoulli para la ocurrencia\nentrenamiento antes de que esto suceda se le llama early de un evento binario.\nstopping.\np(y =k)=pk(1-p)1-k, k ∈{0,1}\n- underfitting: se da cuando el error es alto tanto en\ntraining como en testing. esto se conoce como under- b. funcio'n sigmoide\nfitting, que ocurre cuando el modelo no logra ajustarse\nes una funcio'n que no se comporta linealmente. tiene un\ncorrectamente a los datos. es lo opuesto al overfitting y\ncodominio de [0, 1]\nse caracteriza por un alto sesgo. para ver gra'ficamente\nestos escenarios, ver fig. 2). 1\nσ(x)=\n1+e-x\n- bias-variance tradeoff: validacio'n con buen resultado,\npero entrenamiento con alto error. es raro que suceda y nota:xpuedesercualquiernu'mero,hastaelresultadodeotra\ntal vez hay errores de ca'lculo. funcio'n. ver fig. 4).\ne-x+1-1\nσ′(x)=\n(1+e-x)2\ne-x+1 1\nσ′(x)= -\n(1+e-x)2 (1+e-x)2\nde la fraccio'n izquierda, puedo cancelar\n1 1\nσ′(x)= -\n(1+e-x) (1+e-x)2\naplicamos factor comu'n\n1 1\nσ′(x)= -(1- )\n(1+e-x) (1+e-x)\ncomo 1 =σ(x), decimos que:\n(1+e-x)\nfig.4. sigmoidplot σ′(x)=σ(x) (cid:0) 1-σ(x) (cid:1)"}
{"id_doc": "DOC_017", "segmentacion": "B", "chunk_id": "DOC_017_B_005", "idx": 5, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "references\nc. clasificador\n[1] amazon web services, \"model fit: underfitting vs. overfit-\n- si y <0.5, se clasifica como 0. ting,\". available: https://docs.aws.amazon.com/machine-learning/latest/\ndg/model-fit-underfitting-vs-overfitting.html.\n- si y ≥0.5, se clasifica como 1. [2] university of virginia library, \"understanding diagnostic plots for\nel umbral puede ajustarse segu'n el problema. linearregressionanalysis,\".available:https://library.virginia.edu/data/\narticles/diagnostic-plots.\n[3] ml4a, \"neural networks,\". available: https://ml4a.github.io/ml4a/es/\nd. modelo combinado\nneural networks/.\nalaplicarlasigmoideaunafuncio'nlinealf (x)=wx+\nw,b\nb, obtenemos:\n1\nf (x)=\nw,b 1+e-(wx+b)\nla relacio'n de los features y pesos se da por regresio'n lineal.\nlo que nos da es la probabilidad de que un evento suceda.\ne. optimizacio'n\nen la regresio'n log'ıstica necesitamos optimizar los pesos\nw y el sesgo b. para actualizar estos pesos, es necesario\ncontar con una funcio'n de pe'rdida l que sea adecuada para\nprobabilidades, ya que el mse ya no es lo apropiado en este\ncaso.\nf. derivada de la sigmoide\n1\nσ(x)=\n1+e-x\nusando la regla del cociente:\n1′-(1+e-x)-(1-(1+e-x)′)\nσ′(x)=\n(1+e-x)2\n0-1-(1′+(e-x)′)\nσ′(x)=\n(1+e-x)2\n-(0-(e-x))\nσ′(x)=\n(1+e-x)2\ne-x\nσ′(x)="}
{"id_doc": "DOC_017", "segmentacion": "B", "chunk_id": "DOC_017_B_006", "idx": 6, "autor": "Mauricio Campos Cerdas", "fecha": "2025-09-04", "tema": "Tratamiento de outliers, sesgo-varianza y fundamentos de regresión logística: función sigmoide, Bernoulli y optimización de parámetros.", "texto": "(1+e-x)2"}
{"id_doc": "DOC_018", "segmentacion": "B", "chunk_id": "DOC_018_B_000", "idx": 0, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "apuntes semana 6\n\napuntesdel09deseptiembre\njuan pablo rodr'ıguez cano\n\nic-6200 inteligencia artificial\ntecnolo'gico de costa rica\njp99@estudiantec.cr\n\nabstract-en este documento se detallan las indicaciones de - elme'tododescribe()resumelosdatosanal'ıticosqueson\nla tarea 1 de inteligencia artifical y se introduce el tema importantes para saber co'mo se comportan los features\nde regresio'n log'ıstica como un modelo de clasificacio'n cuyas\n- no debe haber co'digo en el informe, solo resultados,\npropiedades de funcio'n son aptas para modelar problemas\nana'lisis etc.\ncomplejos y la optimizacio'n de recursos.\nindex terms- - el notebook sera' evidencia del trabajo\n- elobjetivoesversilarelacio'nconlaprediccio'neslineal,\n\ni. preguntasdelquiz y si no aplicar un feature engineering\n1) describa que' es \"overfitting\" y \"underfitting\". - figurasenieeesiemprevanenlapartesuperioroinferior\nde las columnas.\nr/ \"overfitting\" es cuando el modelo tiene una mejor\nme'trica con el conjunto de entrenmaiento que con el - el formato es de ieee para conferencias\nconjunto de testing, lo cual indica una pobre generaliii. actividaddeieee\nizacio'n con datos nuevos. \"underfitting\" es cuando el\nes un evento anual que se dara' esta vez en noviembre\nmodel no logra captar la relacio'n entre los features de\nen la sabana. es una oportunidad para conocer sobre temas\nmanera que los puntajes de me'trica son bajos para el\ninnovadores en inteligencia artificial y biolog'ıa molecular. es\nconjunto de entrenamiento y testeo."}
{"id_doc": "DOC_018", "segmentacion": "B", "chunk_id": "DOC_018_B_001", "idx": 1, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "una oportunidad para crear contactos dentro de la industria\n2) describa k-fold cross-validation\nya que los presentadores suelen ser receptivos al pu'blico y\nsesubdivideelconjuntodeentrenamientoenk-1partes.\ndisponen de tiempo para hablar.\nen cada e'poca se entrenan k-1 partes y se utiliza el otro\nsubconjunto para la validacio'n, el iv. contenidodeclase\n3) ¿que' es un m'ınimo global y m'ınimo local en una\na. regresio'n log'ısitca\nfuncio'n?\nun m'ınimo local es el valor m'ınimo de una funcio'n en a diferencia de la regresio'n lineal que es un modelo que\nuna vecindad reducida, mientras que el m'ınimo global predice un nu'mero real a partir de los features, la regresio'n\nse refiere al m'ınimo global a trave's de todo el dominio log'ıstica es un modelo de clasificacio'n binaria. el resultado\nde la funcio'n. de dicho modelo es la probabilidad de que suceda un evento\ny esta' basado en la distribucio'n de bernoulli: p(x = k) =\n4) desarrolleladerivadaparcialdelconrespectoawde:\npk(1-p)1-k\n1 (cid:88)\nl= ((wx +b)-y )\nn i i b. funcio'n sigmoide\n∂l 2 (cid:88)\n= ((wx +b)-y )x )\n∂w n i i i"}
{"id_doc": "DOC_018", "segmentacion": "B", "chunk_id": "DOC_018_B_002", "idx": 2, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "ii. indicacionesdelatarea\n- la tarea se deber realizar en grupos de 3 personas.\n- la fecha de entrega es el 16 de septiembre.\n- solo hace falta que una persona del grupo suba la tarea.\n\nen el nombre del archivo zip debe venir el nombre de\ntodos.\n- nosepuedeutilizarningunabibliotecaquenoseanumpy\n\no pandas\n- kagg;e es una plataforma con datasets para machine\nlearning para el pu'blico y tambie'n presentan oportunidades para participar en concursos de ml. esta funcio'n es conveniente porque puede modelar com-\n- la funcio'n de pe'rdida y la gra'ficacio'n debe se manual portamientos no lineales, el cual es un comportamiento muy\ncomu'n en la mayor'ıa de problemas. trae consigo una mayor porloquehayqueconvertirunproblemademaximizacio'nen\ncomplejidad pero a su vez logra resolver problemas ma's minimizacio'n.paraesto,simplementesedavueltaalafuncio'n\ncomplejos. de ln multiplicando por -1."}
{"id_doc": "DOC_018", "segmentacion": "B", "chunk_id": "DOC_018_B_003", "idx": 3, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "sucodominioesde0a1yestoesmuyconvenienteyaque\nlos valores probabil'ısticos comparten ese mismo espacio.\nla funcio'n sigmoide se expresa de la siguiente manera\n1 1\nσ(x)= ⇒σ(f (x))=\n1+e-x w,b 1+e-fw,bx\nla manera en que esta funcio'n se convierte en un clasificador es al escoger un umbral. este umbral se utiliza para\ndefinir un punto a partir de cua'l se calsifica un evento con\nunaetiquetaolaotra.porlogeneralsesueleescogerunvalor\numbral de 0.5.\nc. derivada de la funcio'n sigmoide\ncomo la regresio'n log'ıstica es un clasificador, se debe\nencontrar una funcio'n de pe'ridica adecuada para el problema.\npara esto se debe analizar la derivada de la funcio'n sigmoide,\nya que es necesario para cualquier problema de optimizacio'n.\n1′(1+e-x)-(1(1+e-x)′)\nσ′(x)=\n(1+e-x)2\n⇒σ′(x)=σ(x)(1-σ(x))\ncomo se puede notar, la derivada se puede expresar en\nte'rminosdelafuncio'nmisma,locuallohacemuyconveniente"}
{"id_doc": "DOC_018", "segmentacion": "B", "chunk_id": "DOC_018_B_004", "idx": 4, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "ya que no se requieren operaciones muy complejas y con esto\nse obtiene una mayor eficiencia.\nd. funcio'n de pe'rdida: verosimilitud\nen vez de utilizar mse o mae, se utiliza la verosimilitud.\nesta esta' dada por la siguiente ecuacio'n\n(cid:89)\nl= f (x )yi(1-f (x ))1-yi\nw,b i w,b i\nel resultado que se obtiene para un punto en esta ecuacio'n\n\nes la probabilidad de que su etiqueta sea y con los pesos\ni\nw actuales. como se quiere optimizar los pesos para los\ncuales se obtiene una mejor me'trica, se debe derivar esta\nfuncio'n. sin embargo, existe un problema con esta expresio'n\ndonde una multiplicacio'n incluye polinomios muy grandes,\n\ny calcular la dervida respectiva se vuelve muy complejo\ny computacionalmente costoso. adema's, como se trata de\nvalores probabil'ısticos, o sea, de 0 a 1, su multiplicacio'n\nse vuelve extremadamente pequen˜a y as'ı la derivada de la\nfuncio'n se vuelve virtualmente cero, y esto no cambia los\npesosenelpasodeentrenamiento.aestoseleconocecomoel\nfeno'meno de \"vanishing gradients\". por esta razo'n se aplican\nlos teoremas de logaritmo y se obtiene la siguiente expresio'n.\n(cid:88)\nln(l)= ln(f (x )yi +ln((1-fw,b(x ))1-yi)\nw,b i i\n(cid:88)\n⇒ln(l)= y ln(f (x )+1-y ln((1-fw,b(x )))\ni w,b i i i\nestoseconvierteenunatareama'sfa'cildeoptimizacio'n.sin"}
{"id_doc": "DOC_018", "segmentacion": "B", "chunk_id": "DOC_018_B_005", "idx": 5, "autor": "Juan Pablo Rodríguez Cano", "fecha": "2025-09-09", "tema": "Introducción a regresión logística, función sigmoide, verosimilitud y regla de la cadena aplicadas al descenso de gradiente.", "texto": "embargo, la funcio'n de logaritmo es estrictamente creciente,"}
{"id_doc": "DOC_019", "segmentacion": "B", "chunk_id": "DOC_019_B_000", "idx": 0, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": "apuntes de inteligencia artificial - semana 6\n\nashley vasquez\n\napuntes del 09 de septiembre\n\nabstract-estedocumentoreu'neyreformulalosapuntesdela iii. actividadieee\nsemana 6 del curso de inteligencia artificial. incluye preguntas\ndel quiz, instrucciones de la tarea i, una breve nota sobre una en noviembre se llevara' a cabo un evento ieee en la\nactividad de ieee y los contenidos principales de clase sobre sabana.estecongresoreu'nepresentacionessobreinteligencia\nregresio'n log'ıstica. asimismo, se profundizo' en la funcio'n de artificial y biolog'ıa molecular, y es una oportunidad para\nverosimilitud,elusodelogaritmosparasimplificarderivadas,la\nestablecer conexiones con investigadores y profesionales.\nregla de la cadena y la actualizacio'n de para'metros. se an˜aden\nejemplos pra'cticos (como el caso de la calabaza naranja / no\nnaranja) para reforzar la comprensio'n del modelo. iv. regresio'nlog'istica\na. definicio'n\n\ni. preguntasdelquiz\nla regresio'n log'ıstica es un modelo de clasificacio'n binaria\n1) overfittingyunderfitting:eloverfittingocurrecuando que estima la probabilidad de que un dato pertenezca a una\nelmodeloaprendedemasiadobienelconjuntodeentre- clase.adiferenciadelaregresio'nlineal,queentregaunvalor\nnamiento, pero no logra generalizar en datos nuevos. el continuo,estemodelotransformalasalidaenunaprobabilidad\nunderfitting, en cambio, refleja que el modelo no logra entre 0 y 1, y se basa en la distribucio'n de bernoulli.\ncaptar la relacio'n entre las variables, obteniendo bajo\nrendimiento en ambos conjuntos.\nb. funcio'n sigmoide\n2) k-fold cross-validation: se divide el conjunto de\nentrenamiento en k subconjuntos. en cada iteracio'n se 1\nσ(x)=\nentrenan k-1 y el restante se utiliza para validar. al 1+e-x\nfinalizar, se promedian los resultados.\nla funcio'n sigmoide transforma cualquier nu'mero real en un\n3) m'ınimos locales y globales: un m'ınimo local es el\nvalor en [0,1]. se define un umbral (generalmente 0.5) para\nvalor ma's bajo dentro de una regio'n reducida de la\ndecidir la clase asignada. como se observa en la figura 1, es\nfuncio'n. el m'ınimo global es el valor ma's bajo en todo\nla base para convertir salidas lineales en probabilidades.\nel dominio.\n4) derivada parcial de l con respecto a w:\n1 (cid:88)(cid:0) (cid:1)2 ∂l 2 (cid:88)(cid:0) (cid:1)\nl= (wx +b)-y , = (wx +b)-y x .\nn i i ∂w n i i i"}
{"id_doc": "DOC_019", "segmentacion": "B", "chunk_id": "DOC_019_B_001", "idx": 1, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": "ii. indicacionesdelatareai\n- realizar la tarea en equipos de tres personas. fecha de\nentrega: 16 de septiembre.\n- solo un integrante debe subir el archivo comprimido con\nlos nombres de todos los miembros.\n- se permite u'nicamente el uso de numpy y pandas.\n- elinformenodebecontenerco'digo,u'nicamenteana'lisis,\nresultados y conclusiones.\nfig. 1: funcio'n sigmoide\n- el notebook sera' evidencia del trabajo realizado.\n- la funcio'n de pe'rdida y las gra'ficas deben hacerse de\nforma manual.\nc. derivada de la sigmoide\n- el formato debe ser ieee.\n- se debe comprobar si la relacio'n entre las variables es\nlineal; si no, aplicar feature engineering. σ′(x)=σ(x)(1-σ(x)).\n- el me'todo describe() ayuda a resumir los datos de\nforma estad'ıstica. el hecho de que la derivada se exprese en funcio'n de la\n- figuras deben colocarse en parte superior o inferior de propia sigmoide la hace eficiente y pra'ctica en optimizacio'n.\ncolumnas. la figura 2 ilustra este comportamiento.\nfig. 2: derivada de la funcio'n sigmoide fig. 4: ejemplo: calabaza no es naranja\nd. verosimilitud vs. error cuadra'tico\nmientrasqueelerrorcuadra'ticomedio(mse)esidealpara\npredecir valores continuos, la verosimilitud se utiliza cuando\nel resultado es una probabilidad. en regresio'n log'ıstica, se"}
{"id_doc": "DOC_019", "segmentacion": "B", "chunk_id": "DOC_019_B_002", "idx": 2, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": "busca maximizar la probabilidad de que el modelo asigne la\nclase correcta a cada ejemplo. la figura 3 compara ambos\nenfoques.\nfig. 5: ejemplo: calabaza s'ı es naranja\ng. composicio'n de funciones y regla de la cadena\nel modelo puede expresarse como:\nz(x)=wx+b, a(z)=σ(z), f (x)=a(z).\nw,b\nentonces la funcio'n de costo es:\nfig. 3: comparacio'n entre mse y verosimilitud\nl=y ln(a(z))+(1-y )ln(1-a(z)).\ni i\ne. interpretacio'n de la verosimilitud aplicando la regla de la cadena:\n\nlaverosimilitudseentiendecomolaprobabilidaddeobser-\n∂l ∂l ∂a ∂z\nvar los datos dados los para'metros actuales. analicemos los = - - .\n∂w ∂a ∂z ∂w\ncasos:\n- caso y i = 1: la probabilidad es σ(wx+b). ejemplo: de manera ana'loga para b.\nwx+b = 1.458 =⇒ σ(1.458) = 0.81. esto significa\nh. derivadas parciales paso a paso\nque hay un 81% de probabilidad de que la calabaza no\nsea naranja, como se muestra en la figura 4. - con respecto a a(z):\n- casoy i =0:laprobabilidades1-σ(wx+b).ejemplo: ∂l y 1-y\nwx + b = -1.32 =⇒ σ(-1.32) = 0.21. entonces =- i + i .\n∂a a(x) 1-a(x)\n1-0.21 = 0.79, lo que se interpreta como un 79% de\nprobabilidaddequelacalabazas'ıseanaranja(figura5). - con respecto a z:\nf. uso de logaritmos ∂a\n=σ(z)(1-σ(z)).\nmultiplicar probabilidades pequen˜as genera valores cer- ∂z\ncanos a cero, causando inestabilidad. aplicando logaritmos se\n- con respecto a w y b:\ntransforma en sumas:\n(cid:88)(cid:2) (cid:3) ∂z ∂z\nln(l)= y ln(f (x ))+(1-y )ln(1-f (x )) . =x, =1.\ni w,b i i w,b i ∂w ∂b"}
{"id_doc": "DOC_019", "segmentacion": "B", "chunk_id": "DOC_019_B_003", "idx": 3, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": "i. actualizacio'n de para'metros\nfinalmente, los para'metros se actualizan con descenso de\ngradiente:\n∂l ∂l\nw =w-α , b=b-α .\n∂w ∂b\nel valor de α (tasa de aprendizaje) es crucial. el flujo de\nca'lculo se muestra en la figura 6.\nfig. 6: flujo de ca'lculo y actualizacio'n de para'metros\nj. aspectos pra'cticos\n- epochs: nu'mero de veces que el modelo recorre todo\nel dataset. ma's epochs permiten aprender mejor, pero\ntambie'n aumenta el riesgo de overfitting.\n- batch size: cantidad de ejemplos procesados antes de\nactualizar para'metros. un batch pequen˜o hace el entrenamiento ma's ruidoso pero puede mejorar la generalizacio'n.\n- gradiente descendente estoca'stico (sgd): actualiza\npara'metros con un ejemplo a la vez, lo que lo hace ma's\nra'pido pero inestable."}
{"id_doc": "DOC_019", "segmentacion": "B", "chunk_id": "DOC_019_B_004", "idx": 4, "autor": "Ashley Vásquez", "fecha": "2025-09-09", "tema": "Fundamentos de regresión logística, función de verosimilitud, uso de logaritmos y actualización de parámetros con gradiente descendente.", "texto": "v. conclusiones\n\ndurante esta semana se consolidaron los fundamentos de\nla regresio'n log'ıstica. se estudiaron sus bases matema'ticas,\nla funcio'n sigmoide y su derivada, la diferencia entre mse\ny verosimilitud, el uso de logaritmos para simplificar expresiones y la actualizacio'n de para'metros mediante gradiente\ndescendente. los ejemplos pra'cticos de la calabaza facilitaron\nlainterpretacio'ndeprobabilidades,yladescomposicio'npasoa\npasodederivadasmostro' co'moseaplicalaregladelacadena\nen la pra'ctica. con esto se sientan las bases para enfrentar\n\nalgoritmos ma's avanzados en aprendizaje supervisado."}
{"id_doc": "DOC_020", "segmentacion": "B", "chunk_id": "DOC_020_B_000", "idx": 0, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "apuntes semana 6\n\napuntes del 11 de setiembre de 2025\nandrey ureña bermúdez - 2022017442\n\ninteligencia artificial\nandurena@estudiantec.cr\n\nresumen-en este documento, se resume la clase del 11 de caso y i =0:\nsetiembrede2025,enlacuálserealizóprimeramenteunrepaso\ndelovistoenlaclaseanterior.demanerageneral,estedocumento f w,b (x i )yi(1-f w,b (x i ))(1-yi) =f w,b (x i )0(1-f w,b (x i ))1\nrecopilainformaciónsobreverosimilitudenlaregresiónlogística, (1)\nlafuncióndecostoeinformaciónsobreunnotebookderegresión\n=(1-f (x ))1 (2)\nlogística compartido por el profesor. w,b i\nindexterms-verosimilitud,regresiónlogística,gradientedescon la misma fórmula puedo estudiar de que ocurra o no\ncendiente, función sigmoide, derivada.\nun evento. ejemplo: calabaza es naranja:\n\ni. notasobretareai wx+b=-1,32\nsehacerecordatoriosobredarleimportanciaynodescuidar f (x )=σ(wx +b)=σ(-1,32)=0,21\nw,b i i\neltrabajoescritodelatarea,asícomosudocumentación,pues =f (x )0(1-f (x ))1\nw,b i w,b i\nde este se dará el feedback para los escritos que haya que\n=(1-f (x ))1 =(1-σ(-1,32))\nrealizar en tareas próximas y etapas del proyecto. w,b i\n=(1-0,21)=0,79"}
{"id_doc": "DOC_020", "segmentacion": "B", "chunk_id": "DOC_020_B_001", "idx": 1, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "ii. repasosobreclasedelmartes\nla probabilidad de que x sea naranja es 0,79.\ni\nii-a. verosimilitud\n\nal final lo que obtenemos es la probabilidad de que la\n\nes la probabilidad de observar cada uno de los datos\nmuestra x tenga la etiqueta y .\ncambiando ciertos parámetros. lo que se busca es maximizar i i\npara llegar al punto de máxima probabilidad. ii-b. derivada de la función de costo\n\nladiferenciaentremseymaximumlikelihoodradicaensu\nprimero, se debe calcular la probabilidad de que x tome la\naplicación: para la predicción de valores continuos, se utiliza i\netiqueta de y , así con cada muestra.\nmse, mientras que para modelar probabilidades, se utiliza i\ndado que esto implica la multiplicación de probabilidades,\nmaximum likelihood.\nel cálculo de la derivada se vuelve complejo. para simasí, nuestra función de costo es:\nplificarlo, se busca una expresión equivalente que evite la\nn multiplicación, lo cual se logra aplicando logaritmos.\n(cid:89)\nl= f (x )yi -(1-f (x ))(1-yi) (1)\nw,b i w,b i ii-c. logaritmos\ni=1\nln(an)=n-ln(a)\nse vió el desarrollo de cada uno de los casos de y en\ni\nf\nw,b\n(x\ni\n)yi(1-f\nw,b\n(x\ni\n))(1-yi): ln(a-b)=ln(a)+ln(b)\nln(an-bn)=n-ln(a)+n-ln(b)\ncaso y =1:\ni\nf (x )yi(1-f (x ))(1-yi) =f (x )1(1-f (x ))0 ii-d. aplicación de logaritmo a la verosimilitud\nw,b i w,b i w,b i w,b i\n(1) l= (cid:81) f\nw,b\n(x\ni\n)yi -(1-f\nw,b\n(x\ni\n))(1-yi)\n=f w,b (x i )1 (2) ln(l)= (cid:80) ln(f w,b (x i )yi)+ln((1-f w,b (x i ))(1-yi))\n(cid:80)\nacá el modelo nos da el valor directo. ejemplo: calabaza ln(l)= y -ln(f (x ))+(1-y )-ln(1-f (x ))\ni w,b i i w,b i\nno es naranja:\nesto lo vamos a llamar log-likelihood. es mucho más fácil\nwx+b=1,458\ndecomputaryderivar,ademásdequequitaerroresalmomento\nf w,b (x i )=σ(wx i +b)=σ(1,458)=0,81 decomputarlasmultiplicacionesdeprobabilidades.ahoraesta\n=f (x )1(1-f (x ))0 es la función de costo que se va a usar.\nw,b i w,b i\n=f (x )1 =σ(1,458) paraminimizarmaximizandoloquesepuedehaceresdarle\nw,b i\nvuelta a la función, para eso se multiplica por -1:\n=0,81\n1 (cid:88)\nla probabilidad de que x i no sea naranja es 0,81. l= n y i -ln(f w,b (x i ))+(1-y i )-ln(1-f w,b (x i ))\n1 (cid:104)(cid:88) (cid:105)\nl=- y -ln(f (x ))+(1-y )-ln(1-f (x )) 1) cálculo de derivadas parciales\nn i w,b i i w,b i\nimportante recordar que el l que se está usando es:\nahora puedo minimizar la función, lo que permite aplicar\nel descenso del gradiente que se ha estado trabajando. l=-[y -ln(a(z(x)))+(1-y )-ln(1-a(z(x)))]\ni i"}
{"id_doc": "DOC_020", "segmentacion": "B", "chunk_id": "DOC_020_B_002", "idx": 2, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "primero se inicia calculando la derivada parcial de l con\nrespecto a la función sigmoide:\n(cid:20)(cid:18) (cid:19) (cid:18) (cid:19)(cid:21)\n∂l 1 1\n=- y - -a(x)′ + (1-y )- -(1-a(x))′\n∂a i a(x) i 1-a(x)\n(cid:20)(cid:18) (cid:19) (cid:18) (cid:19)(cid:21)\ny (1-y )\n=- i -1 + i --1\na(x) 1-a(x)\n(cid:20)(cid:18) (cid:19) (cid:18) (cid:19)(cid:21)\ny (1-y )\n=- i - i\na(x) 1-a(x)\n-y (1-y )\n= i + i\na(x) 1-a(x)\nfigura1. gráficaminimizandol\nahora,secalculaladerivadaparcialdelafunciónsigmoide\naquí lo ideal es intentar que el loss llegue a cero; si se respecto a z. importante recordar que la derivada de sigmoide\nobtiene un loss negativo, significa que algo se está haciendo es σ(x)-(1-σ(x)), por lo que la derivada parcial sería:\nmal.\n∂a\n=σ(z(x))-(1-σ(z(x)))\nii-e. actualización de parámetros ∂z\npor último, se debe calcular de manera individual la deries necesario actualizar los parámetros w y b, ya que son\nvada parcial de cada uno de los parámetros con respecto a la"}
{"id_doc": "DOC_020", "segmentacion": "B", "chunk_id": "DOC_020_B_003", "idx": 3, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "losquepermitenmodificarlosresultadosdelasprobabilidades\nregresión lineal:\nobtenidas.\nz(x)=wx+b\npara actualizar el parámetro w se necesita: ∂l\n∂w\n∂z\npara actualizar el parámetro b se necesita: ∂l =x\n∂b ∂w\nii-f. composición de funciones ∂z\n=1\nse va a utilizar el concepto de composición de funciones ∂b\npara que el cálculo de derivadas sea más sencillo. ya que se hizo el cálculo de cada derivada de manera\nderivada función de costo para un sample: individual, se prosigue a realizar las multiplicaciones:\nl=y -ln(f (x ))+(1-y )-ln(1-f (x ))\ni w,b i i w,b i\nmodelo: f (x)=a(z(x))\nw,b\na(x)=σ(x)= 1\n1+e-x\nz(x)=wx+b\nel resultado de combinar ambas es:\nl=y -ln(a(z(x)))+(1-y )-ln(1-a(z(x)))\ni i\ncuandosehabladelatécnicadecomposicióndefunciones\nseaplicalaregladelacadena.sedebencalcularlasderivadas figura2. derivadaparcialdelrespectoaz\nparciales:\nl=y -ln(a(z(x)))+(1-y )-ln(1-a(z(x)))\ni i\n∂l ∂l ∂a ∂z\n= - -\n∂w ∂a ∂z ∂w\n∂l ∂l ∂a ∂z\n= - - figura3. derivadaparcialdelrespectoawyb\n∂b ∂a ∂z ∂b\nse procede a actualizar parámetros: se actualizan los valores de w y b, y se calcula el error en\nz(x)=wx+b cada iteración.\nw =w-α∂l la función predict se encarga de predecir la clase de una\n∂w nueva muestra en base al umbral que se define. una vez\nb=b-α∂l\n∂b obtenida la probabilidad, se asigna la clase correspondiente.\ndonde α es un hiperparámetro (learning rate)."}
{"id_doc": "DOC_020", "segmentacion": "B", "chunk_id": "DOC_020_B_004", "idx": 4, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "iii. código\n\nse muestra un notebook con el fin de comprender mejor\ncómo hacer una regresión logística. enlace a notebook.\nfigura4. códigoclasificación\ncomo se muestra en la figura 4, se hace la importación de\nlibreríasnecesarias,muchasdelascualespertenecenasklearn.\nluego, se hace la clasificación con make_classification, el\ncual es un método para crear un dataset de clasificación. en\neste caso, se indica que sea de 1000 samples, con 2 features\ninformativas, sin features redundantes y con un solo clúster\npor clase.\nposteriormente, se visualiza el conjunto de datos utilizando\nplt.scatter, donde los puntos se colorean según su clase (y).\nluego, se crea un dataframe con pd.dataframe que contiene\nlas dos características (feature_1 y feature_2) y la variable\nfigura5. códigoregresiónlogística\nobjetivo (target).\nfinalmente, se divide el dataset en entrenamiento y prueba\nfinalmente, el modelo se implementa instanciando la clase\ncontrain_test_split,reservandoel80%delosdatosparaentrey entrenándola con x train y y train. luego, se evalúa con\nnamiento y el 20% para prueba, asegurando reproducibilidad\nx test y y test, calculando la accuracy y generando un clas\ncon random_state=225.\nsification report para medir su desempeño, el cuál muestra\nla figura 5 muestra la implementación manual de la remétricas como el nivel de accuracy, precision, recall, f1-score\ngresiónlogística.laclaserecibecomoparámetroslacantidad\ny support.\nde epochs a ejecutar, el learning rate que se aplicará y los\nparámetros de la regresión w y b, que serán ajustados durante\nel entrenamiento.\nprimero, se define la función sigmoide, utilizada para\nconvertir la predicción lineal en una probabilidad. luego, se\nimplementa la función de costo binary_cross_entropy_loss,\nquecalculalapérdidanegativaconelobjetivodeminimizarla\ndurante el entrenamiento.\nenlafunciónfit,serecibentodoslosfeaturesylasetiquetas\ncorrespondientes. antes de iniciar el ajuste de los parámetros,\nse inicializan aleatoriamente los valores de w, cuyo tamaño figura6. entercaption\ncorresponde al número de features, ya que cada uno necesita\nun peso asociado. luego, se ejecuta el ciclo de entrenamiento de igual forma, en lugar de implementar la regresión\npor la cantidad de epochs definida, donde primero se calcula logísticamanualmente,sepuedeutilizarelmétodoquefacilita\nla predicción lineal, que luego pasa por la función sigmoide sklearn,talycomosemuestraenlafigura6.enestecaso,\npara obtener una probabilidad. a partir de esta probabilidad, se instancia el modelo de regresión logística con"}
{"id_doc": "DOC_020", "segmentacion": "B", "chunk_id": "DOC_020_B_005", "idx": 5, "autor": "Andrey Ureña Bermúdez", "fecha": "2025-09-11", "tema": "Profundización en verosimilitud, log-likelihood y actualización de parámetros en regresión logística mediante gradiente descendente.", "texto": "iv. conclusión\na lo largo de este documento se profundizó en los fundamentos de la regresión logística, en particular en el uso de\nla verosimilitud como función de costo y en la aplicación del\nlogaritmoparasimplificarsuderivación.serevisaronejemplos\nprácticos que ilustran cómo interpretar probabilidades según\nlosvaloresdeentrada,yseabordóelprocesodeactualización\nde parámetros mediante gradiente descendente. además, el\nrepaso permitió conectar la teoría con la implementación\npráctica en python, reforzando la comprensión del modelo y\nsu utilidad en la clasificación de datos. con esto, se sientan\n\nlasbasesparacontinuarcontécnicasmásavanzadasdeaprendizaje supervisado."}
{"id_doc": "DOC_021", "segmentacion": "B", "chunk_id": "DOC_021_B_000", "idx": 0, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "apuntes de la clase\n\napuntes semana 6\n\napuntes del 11 de setiembre de 2025\nsahid rojas chacón - 2018319311\ncurso: inteligencia artificial\nreds@estudiantec.cr\n\nresumen-en este documento, se resume la clase del 11 de caso \"no es naranja\" (clase 0).: si wx+b = 1,458,\nsetiembrede2025,enlacuálserealizóprimeramenteunrepaso entonces σ(1,458)≈0,81; como aquí me interesa que no sea\ndelovistoenlaclaseanterior.demanerageneral,estedocumento\nnaranja (y =0), la probabilidad es 1-σ(1,458)≈0,19. para\ni\nrecopilainformaciónsobreverosimilitudenlaregresiónlogística,\nel evento complementario (no naranja como etiqueta positiva\nlafuncióndecostoeinformaciónsobreunnotebookderegresión\nlogística compartido por el profesor. enesaformulación),sereportó0,81;elpuntoesqueelmismo\nindexterms-verosimilitud,regresiónlogística,gradientedes- f sirve para ambos casos cambiando y i .\ncendiente, función sigmoide, derivada. caso \"sí es naranja\" (clase 1).: si wx+b = -1,32,\nentonces σ(-1,32) ≈ 0,21; la probabilidad de sí ser naranja\n\ni. notasobretareai (clase 1) se obtiene con 1-σ(-1,32)≈0,79. estos números\nilustran cómo interpretar f(x) en los dos escenarios."}
{"id_doc": "DOC_021", "segmentacion": "B", "chunk_id": "DOC_021_B_001", "idx": 1, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "serecuerdaotorgarladebidaimportanciaalinformeescrito\ny a su documentación, por cuanto constituirán la base de la\n\niii. porquémetemoslogaritmos\nretroalimentación para entregas posteriores y para las etapas\nmultiplicar muchas probabilidades puede complicar la desubsiguientes del proyecto. el informe deberá presentar con\nrivada y además es numéricamente inestable. usamos identiclaridadlosobjetivos,lametodologíaylosresultados,asegurar\ndades de logaritmos:\nla reproducibilidad (datos, código, semillas y versiones), e\nincluir instrucciones de ejecución suficientes y verificables,\nln(an)=nlna, ln(ab)=lna+lnb,\nmanteniendo coherencia entre texto, figuras y conclusiones.\npara convertir el producto en suma. aplicando ln a (1):\n\nii. verosimilitud:ideayfunciónparaeldataset\nn\n(cid:88)(cid:2) (cid:3)\nii-a. qué es verosimilitud lnl(w,b)= y lnf (x )+(1-y )ln(1-f (x )) .\ni w,b i i w,b i\nverosimilitud es: dado un conjunto de datos y un modelo i=1\n(4)\nconparámetros,¿quétanprobableesobservaresosdatosbajo\nesosparámetros?enbinario,nuestromodelof (x)devuelve esto se llama log-likelihood y es mucho más amigable para\nw,b\nuna probabilidad en (0,1) y la verosimilitud del dataset se derivar.\nconstruye multiplicando las probabilidades individuales."}
{"id_doc": "DOC_021", "segmentacion": "B", "chunk_id": "DOC_021_B_002", "idx": 2, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "iv. demaximizaraminimizar:negandoel\nii-b. modelo y notación log-likelihood\nusaré f (x)=σ(w⊤x+b), donde σ es la sigmoide: en entrenamiento solemos minimizar. como maximizar (4)\nw,b\nes equivalente a minimizar su opuesto, definimos la pérdida\n1\nσ(t)= . logística promedio:\n1+e-t\nn\nii-c. verosimilitud del conjunto l(w,b)=- 1 (cid:88)(cid:104) y lnf (x )+(1-y )ln (cid:0) 1-f (x ) (cid:1)(cid:105) .\nn i w,b i i w,b i\npara datos {(x ,y )}n con y ∈{0,1}: i=1\ni i i=1 i (5)\nnota : esta pérdida es ≥ 0 en práctica; si te da negativa,\nn\nl(w,b)= (cid:89) f (x )yi (cid:0) 1-f (x ) (cid:1)1-yi. (1) probablemente hay un bug de signos o promedios. el objetivo\nw,b i w,b i\nes empujarla hacia cero.\ni=1\nii-d. ejemplo narrativo: calabaza/naranja v. quéparámetrossípodemosactualizar\nla misma fórmula (1) explica los dos casos: los parámetros libres del modelo son w (vector de pesos)\ny =1⇒f (x\n)1(cid:0)\n1-f (x )\n(cid:1)0\n=f (x ), (2)\ny b (sesgo). todo lo demás depende de ellos. por eso,\ni w,b i w,b i w,b i necesitamos∂l/∂w y∂l/∂bparapoderaplicardescensopor\ny =0⇒f (x\n)0(cid:0)\n1-f (x )\n(cid:1)1\n=1-f (x ). (3) gradiente.\ni w,b i w,b i w,b i"}
{"id_doc": "DOC_021", "segmentacion": "B", "chunk_id": "DOC_021_B_003", "idx": 3, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "vi. composicióndefuncionesyregladela vii. actualizacióndeparámetros(descensopor\ncadena gradiente)\nprimero reescribo el modelo de forma explícita como com- con learning rate α>0:\nposición:\nn\n1 (cid:88)(cid:0) (cid:1)\nz(x)=w⊤x+b, a(z)=σ(z), f w,b (x)=a(z(x)). w ←w-α n a i -y i x i , (14)\ni=1\nla pérdida por muestra (sin el promedio y con el signo n\n1 (cid:88)(cid:0) (cid:1)\nnegativo puesto) queda: b←b-α a -y . (15)\n\nn i i\n(cid:104) (cid:0) (cid:1)(cid:105) i=1\nl=- y lna(z(x))+(1-y) ln 1-a(z(x)) .\n(nota : si la curva de pérdida sube o se vuelve errática, probá\nreducir α.)\nusamos regla de la cadena:\n∂l\n=\n∂l\n-\n∂a\n-\n∂z\n, (6)\n\nviii. código\n∂w ∂a ∂z ∂w\n∂l ∂l ∂a ∂z en esta sección explico únicamente el código fuente y\n= - - . (7) cómo implementa la teoría vista en clase: verosimilitud →\n∂b ∂a ∂z ∂b\nlog-likelihood → log-loss, derivadas por regla de la cadena\nvi-a. paso 1: derivada de l respecto a a\ny actualización de los parámetros w y b. las figuras 1-3\nderivando los logaritmos (regla de la cadena incluida): corresponden a tres capturas del archivo main.py.\n∂l (cid:104) 1 1 (cid:105)\n=- y- -(a)′ + (1-y)- -(1-a)′ viii-a. definiciones del modelo: clase, sigmoide y pérdida\n∂a a 1-a (cap01)\n(cid:104) 1 1 (cid:105)\n=- y- -1 + (1-y)- -(-1) la figura 1 muestra la clase logisticregressionai."}
{"id_doc": "DOC_021", "segmentacion": "B", "chunk_id": "DOC_021_B_004", "idx": 4, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "a 1-a\ny 1-y en el constructor (__init__) se fijan los hiperparámetros\n=- + . (8) lr (tasa de aprendizaje) y epochs (épocas de entrenamiena 1-a\nto), y se inicializan los parámetros entrenables w ∈ rd y\nvi-b. paso 2: derivada de a respecto a z b∈r, que son los únicos valores que el algoritmo ajusta.\nla derivada de la sigmoide es la famosa forma cerrada: el método sigmoid implementa la activación logística\n∂a =σ(z) (cid:0) 1-σ(z) (cid:1) =a(1-a). (9) σ(z)= 1 , z =w⊤x+b,\n∂z 1+e-z\nvi-c. paso 3: derivadas de z respecto a w y b que mapea la combinación lineal z a una probabilidad a =\nde z(x)=w⊤x+b: σ(z)∈(0,1).\nlafunciónbinary_cross_entropy_losscodificala\n∂z ∂z\nlog-loss (negativa del log-likelihood promedio):\n=x, =1. (10)\n∂w ∂b\nn\nvi-d. juntando todo: primero ∂l/∂z l(w,b)=- 1 (cid:88)(cid:2) y lna +(1-y )ln(1-a ) (cid:3) ,\nn i i i i (16)\nes útil agrupar ∂l = ∂l - ∂a y luego propagar a w y b: i=1\n∂z ∂a ∂z a =σ (cid:0) w⊤x +b (cid:1) .\ni i\n∂l (cid:16) y 1-y(cid:17)\n= - + -a(1-a)\n∂z a 1-a antesdeaplicarloslogaritmos,elcódigorealizaclippingcon\n=-y(1-a)+(1-y)a ε=10-15 para evitar log(0) y mejorar la estabilidad numérica.minimizar(16)esequivalenteamaximizarlaverosimilitud\n=a-y. (11) (cid:81)\ni\nay\ni\ni(1-a\ni\n)1-yi.\nvi-e. gradientes finales de w y b\nviii-b. entrenamiento vectorizado y predicción (cap02)\nusando (11) y (10):\nlafigura2concentraelcorazóndelaprendizaje:elmétodo\n∂l\n=(a-y)x, (12) fit (bucle de entrenamiento) y predict (inferencia).\n∂w forward (vectorizado).: con x ∈ rn×d y w ∈ rd, se\n∂l\n=(a-y). (13) calcula\n∂b\nz =xw+b1, a=σ(z),\npromediando sobre el dataset (dividiendo entre n y sumando\nen i) recuperamos las expresiones habituales de la pérdida usando multiplicación matricial de numpy (np.dot). este\npromedio (5). paso implementa la composición x→z→a vista en clase.\nfigura 3. cap03: flujo completo: imports, generación del dataset con\nmake_classification, división entrenamiento/prueba, entrenamiento\ndelmodelopropioybloqueanálogoconsklearn.\nviii-c. bloque principal: imports, dataset y flujo del entrenamiento (cap03)\nfigura1. cap01:claselogisticregressionai.hiperparámetros(lr,\nepochs),parámetrosw,b,sigmoideypérdida(16).\ncomo se muestra en la figura 3, se realiza la importación de librerías necesarias (numpy y módulos de\nsklearn). luego, se crea un dataset de clasificación con\nmake_classification, que genera datos sintéticos controlados para problemas binarios. en este caso, se indica:\nn_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=225."}
{"id_doc": "DOC_021", "segmentacion": "B", "chunk_id": "DOC_021_B_005", "idx": 5, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "esto produce dos features informativas sin redundancia y un\nsolo clúster por clase, coherente con los ejemplos del curso.\na continuación, se realiza la división entrenamiento/prueba con train_test_split, manteniendo la\nproporción de clases mediante stratify=y y fijando\nrandom_state para reproducibilidad. seguidamente, se\ninstancia y entrena el modelo implementado desde cero:\nmodelo_manual = logisticregressionai(lr=0.001, epochs=6000),\nejecutando el bucle explicado en la subsección anterior (forward, gradientes y actualización). finalmente, el bloque incluye logisticregression de sklearn para replicar\nfigura 2. cap02: fit (forward z→a, gradientes dw/db, actualización) y el mismo enfoque con la librería estándar, lo que sirve como\npredict(umbral0,5).\nreferencia y valida que la implementación manual respeta la\nteoría.\ngradientes(regladelacadena).: deladerivaciónteórica\nviii-d. resumen código ↔ teoría\n\nse obtiene\nverosimilitud → log-loss. el código minimiza (16), que es\n∂l ∂l 1 ∂l 1\n=a-y, = x⊤(a-y), = 1⊤(a-y). -logl; así, \"minimizar la pérdida\" equivale a \"maximizar la\n∂z ∂w n ∂b n verosimilitud\".\nregla de la cadena. la ruta x→z→a→l da ∂l =a-y,\nen el código aparecen como dw = (1/n_samples) ∂z\nbase de los gradientes vectorizados dw y db.\n* x.t @ (a - y) y db = (1/n_samples) *\nparámetros actualizables. solo w y b cambian; el resto (signp.sum(a - y).\nmoide, datos) son transformaciones/entradas fijas conforme a\nactualización (descenso por gradiente).: con tasa α =\nla formulación del modelo.\nlr:\n∂l ∂l\nw ←w-α , b←b-α , ix. conclusiones\n∂w ∂b\nlaclaveparanoperderseesmirarlacomposiciónx→z →\nque en el código se implementa como w = w - lr * dw a→l y empujar las derivadas con la regla de la cadena. el\ny b = b - lr * db. al disminuir (16) se incrementa la uso de logaritmos cambia productos por sumas y, al negar\nverosimilitud del modelo. el log-likelihood, pasamos a minimizar una función estable\npredicción.: el método predict repite z =xw+b y y derivable. con los gradientes compactos (a-y)x y (aa=σ(z)yumbralizacon0,5paradevolveretiquetasbinarias y), actualizar w y b se vuelve mecánico con descenso por\nyˆ∈{0,1}, consistente con decidir por la clase más probable. gradiente.\napéndice:fórmulasrelevantes\nsigmoide y derivada:\nσ(t)= 1 , σ′(t)=σ(t) (cid:0) 1-σ(t) (cid:1) .\n1+e-t\nverosimilitud y log-likelihood:\nl= (cid:89) f(x )yi (cid:0) 1-f(x ) (cid:1)1-yi, lnl= (cid:88)(cid:2) y lnf(x )+(1-y )ln(1-f(x )) (cid:3) ."}
{"id_doc": "DOC_021", "segmentacion": "B", "chunk_id": "DOC_021_B_006", "idx": 6, "autor": "Andrés Mora Ugalde", "fecha": "2025-09-11", "tema": "Evaluación de modelos mediante métricas clásicas y avanzadas (Accuracy, Recall, F1, ROC, AUC) y su relación con la calidad de los datos y el preprocesamiento.", "texto": "i i i i i i\ni i\npérdida logística promedio (lo que minimizo):\n1 (cid:88)(cid:104) (cid:0) (cid:1)(cid:105)\nl(w,b)=- y lnf(x )+(1-y )ln 1-f(x ) .\n\nn i i i i\ni\ngradientes (por muestra):\n∂l y 1-y ∂a ∂z ∂z\n=- + , =a(1-a), =x, =1,\n∂a a 1-a ∂z ∂w ∂b\n∂l ∂l ∂l\n=a-y, =(a-y)x, =(a-y).\n\n∂z ∂w ∂b"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_000", "idx": 0, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "apuntes de la clase del 16 de setiembre de 2025\n\ncursodeinteligenciaartificial\n\nnelson rojas obando\nestudiante ingeniería en computación\nnelson.rojas@estudiantec.cr\n\nresumen-this paper summarizes the main topics discussed la regresión lineal y la regresión logística son técnicas\nduring the artificial intelligence course on september 16, 2025. fundamentales en el aprendizaje supervisado, pero se\n\nit covers the quiz about concepts such as linear and logistic\naplican a diferentes tipos de problemas:\nregression, concepts as techniques for handling outliers, and\nstrategies to reduce high bias and high variance in machine regresión lineal: se utiliza cuando la variable delearning models. furthermore, it presents evaluation metrics pendiente es continua. el modelo estima una relación\nincluding accuracy, precision, recall, f1-score, confusion matrix, lineal entre las variables independientes y la variable\nroc curve, and auc, illustrated with practical case studies.\ndependiente, siguiendo la forma:\nfinally,thepaperhighlightstheimportanceofdatapreprocessing\ntasks-such as cleaning, integration, reduction, transformation,\ny =β +β x +---+β x +ε"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_001", "idx": 1, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "and discretization-as essential steps to improve the quality of 0 1 1 n n\ndatasets and ensure better performance of predictive models.\ndonde y puede tomar cualquier valor real.\nindex terms-inteligencia artificial, machine learning, métricas de evaluación, matriz de confusión, roc, auc, data regresión logística: se emplea cuando la variable\npreprocessing dependiente es categórica, típicamente binaria (0 o 1).\n\nel modelo estima la probabilidad de pertenecer a una\n\ni. introducción clase utilizando la función sigmoide:\nla inteligencia artificial (ia) y el aprendizaje automático\n1\nrequieren no solo del diseño de modelos predictivos, sino p(y =1|x)=\n1+e-(β0+β1x1+---+βnxn)\ntambién de procesos rigurosos para evaluar su desempeño\ny garantizar su aplicabilidad en escenarios reales. en este de esta forma, la salida está acotada en el intervalo\ndocumento se abordan conceptos fundamentales que permiten [0,1] y se interpreta como probabilidad.\ncomprender la relación entre los modelos, las métricas de 2. describa 3 técnicas para el tratamiento de datos sobresaevaluación y la calidad de los datos utilizados en su entre- lientes.\nnamiento. respueta:\nenprimerlugar,seestudianmétricasclásicascomolaexac- los datos sobresalientes, también conocidos como\ntitud, precisión, exhaustividad y f1-score, así como métricas outliers, son observaciones que se desvían significativamás avanzadas como la curva roc y el área bajo la curva mente del resto de los datos. su presencia puede afectar\n(auc), las cuales proporcionan una visión más completa del de manera negativa el rendimiento de los modelos de\nrendimiento de un clasificador. aprendizaje automático. entre las técnicas más comunes\nademás, se presenta la matriz de confusión como herra- para su tratamiento se encuentran:\nmienta central para interpretar los aciertos y errores de los a) eliminacióndeoutliers:consisteendescartaraquellas\nmodelos, junto con un caso práctico aplicado a la detección observaciones que superan un umbral definido, por\nde cáncer en pacientes. asimismo, se destacan las principales ejemplo, valores que se encuentran a más de tres\ntareas del preprocesamiento de datos, entre ellas la limpie- desviaciones estándar de la media. esta técnica es útil\nza, integración, reducción, transformación y discretización, cuando los outliers provienen de errores de medición\nesenciales para enfrentar problemas como datos incompletos, o registro.\ninconsistentes o ruidosos. b) transformacionesdelosdatos:aplicartransformacionesmatemáticas,comolatransformaciónlogarítmicao"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_002", "idx": 2, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "ii. aspectosadministrativos\nla raíz cuadrada, puede reducir la influencia de valores\nver dos lecturas asociadas con lectura procesamiento de extremos, estabilizando la varianza y mejorando la\ndatos y de redes neuronales además de dos capítulos de un distribución de los datos.\nlibro. c) imputación o sustitución de valores: reemplazar\nse realizó el quiz #3, donde al finalizar se vieron las los outliers por valores más representativos, como la\nrespuestas correspondientes. el quiz consistió en: media, la mediana o un valor obtenido mediante inter1. mencione la diferencia de regresión lineal y logística. polación.estatécnicaconservaeltamañodelconjunto\nrespuesta: dedatosyesútilcuandolaeliminaciónnoesdeseable.\n\n3. mencione2técnicasparaevitarunaltosesgoy2técnicas\npara evitar alta varianza.\nrespuesta:\nen el contexto del aprendizaje automático, el alto sesgo\n(underfitting) y la alta varianza (overfitting) son problemas comunes. algunas técnicas para mitigarlos son las\nsiguientes:\npara evitar un alto sesgo:\na) aumentar la complejidad del modelo, por ejemplo, utilizando modelos polinómicos en lugar de\nregresión lineal simple, o redes neuronales más\nprofundas. figura1. entercaption\nb) reducir la regularización excesiva, ajustando los\nhiperparámetrosdetécnicascomol1/l2odropout,"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_003", "idx": 3, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "que en exceso limitan la capacidad de aprendizaje\ndel modelo.\npara evitar una alta varianza:\na) aumentar la cantidad de datos de entrenamiento,\nmediante recolección adicional o técnicas de data\naugmentation.\nb) aplicar regularización, como l1/l2, dropout o\nearly stopping, con el fin de penalizar la complejifigura2. entercaption\ndad excesiva y mejorar la generalización.\n\n4. anote la derivada de la función sigmoide\n1 elcasodelamujer,siexistelaposibilidadporloquesepodría\nσ(x)=\ndar un resultado equivocado, y se conoce como error de tipo\n1+e-x\n2.\nrespuesta:\nejemplos de métricas clásicas:\nσ′(x)=σ(x) (cid:0) 1-σ(x) (cid:1) accuracy (exactitud): mide la proporción de prediccionescorrectassobreeltotaldeprediccionesrealizadas.es\n\niii. métricas\nampliamente usada en problemas de clasificación, como\nson medidas que se utilizan para indicar el rendimiento en la regresión logística. se define como:\nde un modelo predictvo. constituyen la forma más objetiva\nde evaluar y comparar modelos de aprendizaje automático, tp +tn\naccuracy=\npermitiendo determinar qué tan bien se ajustan a los datos de tp +tn +fp +fn\nentrenamiento y, sobre todo, qué tan bien generalizan a datos\ndonde tp (verdaderos positivos), tn (verdaderos negano vistos.\ntivos), fp (falsos positivos) y fn (falsos negativos).\nasimismo, se emplean benchmarks, que son conjuntos de"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_004", "idx": 4, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "este tipo de metrica otorga importancia igual a todas las\n\ndatos o pruebas estandarizadas utilizadas en la comunidad\nclases. es importante tomar esto en cuenta si las clases\ncientífica para comparar de manera justa el desempeño de\nno están balanceadas. puede no ser suficiente y da como\ndistintos modelos.\nresultado un valor porcentual (de 0 a 1). para un modelo\n\niv. matrizdeconfusión bien hecho se esperaría que se acerque bastante a 1.\nen la matriz de confusión se colocan las clases y se realiza precisión (precision): mide la proporción de predicuna clasificación según su posibilidad y veracidad, como se ciones positivas correctas entre todas las predicciones\nmuestra en la figura 1. de esta forma se obtienen true positivas realizadas, como los errores de tipo 1:\npositive (verdadero positivo), false positive (falso positivo),\ntp\ntrue negative (verdadero negativo) y false negative (falso precision=\ntp +fp\nnegativo). esta tabla puede ser n x n clases y al hacer un\nploteo de esta tabla se espera que la diagonal esté dando exhaustividad (recall): indica la proporción de verdavalores verdaderos. deros positivos identificados correctamente sobre el total\nun ejemplo práctico de esto es el caso de la figura 2. en el de elementos positivos, y se usa para medir los errores\nqueseevaluánelresultadodeembarazoenhombresymujeres. de tipo 2:"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_005", "idx": 5, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "claramente un hombre no puede embarazarse por lo que de\ntp\nobtener un resultado positivo este sería un error de tipo 1. en recall=\ntp +fn\nf1-score:eslamediaarmónicaentreprecisiónyexhaus- 1\ntividad, útil cuando existe un desbalance de clases:\n\nprecision-recall\nf1=20,8\nprecision+recall\ncaso de estudio: dado un conjunto de 1000 pacientes se\nhan realizado estudios para determinar la presencia de cancer.\n0,6\ndel total de pacientes, 30 son pacientes con cáncer (clase\npositiva) y 970 pacientes sin cáncer (clase negativa).\nmatriz de confusión:\n0,4\npredicción/objetivo cáncer no cáncer\ncáncer 25(tp) 20(fp)\nno cáncer 5(fn) 950(tn) 0,2\nmétricas de evaluación:\naccuracy:\n0\ntp +tn 25+950 0 0,2 0,4 0,6 0,8 1\naccuracy= = =0,975(97,5%)\n\ntotal 1000\ntasa de falsos positivos (fpr)\nrecall (sensibilidad):\n\ntp 25\nrecall= = =0,833(83,3%)\ntp +fn 25+5\nindica la capacidad del modelo para identificar correctamente a los pacientes con cáncer. a pesar del alto\naccuracy, el recall muestra espacio de mejora en la\ndetección de la clase positiva (cáncer).\nprecisión:"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_006", "idx": 6, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "tp 25\nprecisión= = =0,55(55%)\ntp +fp 25+20\nel modelo presenta una baja precisión, lo cual implica\nque muchas predicciones positivas son en realidad falsos positivos. este valor debe considerarse con cautela,\ndependiendo del contexto clínico.\nf1-score:\n2-precisión-recall 2-0,55-0,833\nf1= = ≈0,662(66,2%)\nprecisión+recall 0,55+0,833\nel f1-score refleja un balance bajo entre precisión y\nsensibilidad, indicando que la capacidad del modelo para\nclasificar correctamente la clase minoritaria (cáncer) aún\nno es adecuada.\notras métricas no tan básicas:\nreceiver operating characteristic (roc)\nlacurvarocesunarepresentacióngráficaquemuestra\n\nel rendimiento de un clasificador binario a diferentes\numbralesdedecisión.enelejexserepresentalatasade\nfalsos positivos (fpr) y en el eje y la tasa de verdaderos\npositivos (tpr o recall). una curva más cercana a la\nesquina superior izquierda indica un mejor desempeño\ndel modelo.\narea under the curve (auc)\nel auc corresponde al área bajo la curva roc. su\nvalor varía entre 0 y 1, donde un valor de 0,5 indica un\nmodelosincapacidaddediscriminación(equivalenteaun\nclasificador aleatorio), mientras que un valor cercano a 1\nrepresenta un modelo con alto poder de discriminación.\n)rpt("}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_007", "idx": 7, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "sovitisop\n\nsoredadrev\ned\n\nasat\n\nclasificador aleatorio\n\nmodelo\nfigura3. ejemplodecurvaroc.unáreabajolacurva(auc)máscercano\na1indicamejorrendimiento.\nde darse un caso en el que la curva, como por ejemplo en\nla figura3, se acercaramucho a larecta perderíavalor porque\nestaríadandolosresultadosincorrectos,deunaformacasique\naleatoria. el área bajo la curva debe de ser de al menos 0,5.\n\nprocesamiento de datos\n\nproblemas encontrados\nincompletitud: valores faltantes en atributos importantes,\nej. si el producto estaba en oferta.\ninexactitud o ruido: errores y valores atípicos en las\ntransacciones.\ninconsistencia: discrepancias en los códigos de departamentos o categorías.\nsediocomocomparaciónelcasodeladiabetesylapresión\nsanguínea. si se registran mediciones, se espera que tenga un\nvalor sino tiene presión sanguínea entonces no tendría sentido\no el sujeto estaría muerto, sería un valor que no aporta pero\nesimportantenoelimiarelregistroyaquepodríanhaberotras\nfeatures que si aporten valor.\nporestarazón,losdatasetsrequirenpreprocesamientoantes\nde aplicar técnicas de minería o aprendizaje. es un problema\ndel mundo real.\nporque pueden ser inexactos?\ninstrumentos de recolección de datos defectuosos"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_008", "idx": 8, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "errores humanos o computacionales en la entrada de\n\ndatos\nusuarios que ingresan valores falsos para campos obligatorios (ej. fecha por defecto '1 de enero' para ocultar\ncumpleaños)\n- conocido como datos faltantes disfrazados\ninconsistencias en convenciones de nombres, códigos o\nformatos (ej. fechas con distintos formatos)\ntuplas duplicadas que requieren procesos de data clea- ◦ el borde más cercano del bin.\nning - ejemplo: valores de salarios.\npor qué los datos pueden estar incompletos? suavizado de ruido:\natributos de interés no siempre están disponibles - ajustar una función matemática a los datos (puede ser\nnoseincluyenporquenoseconsideraronimportantesen lineal o no lineal).\n\nel momento de la entrada\n◦ ejemplo: ventas mensuales con fluctuaciones; se\n\ndatos relevantes no se registran por malentendidos o\najusta una regresión lineal para capturar la tendenfallos de equipo"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_009", "idx": 9, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "cia general y se consideran ruido los valores que\ndatos inconsistentes con otros registros pueden ser elise desvían demasiado.\n\nminados\n- aplicar técnicas de filtrado, como la media móvil.\n\nhistorial o modificaciones de datos pasados pueden no\nhaberse registrado ◦ ejemplo: datos diarios de temperatura con ruido;\nvalores faltantes en atributos clave pueden necesitar ser se calcula la media móvil de 7 días.\ninferidos. data integration: manejo de redundancia:\npor que los datos pueden ser inconsistentes? lamismainformaciónpuedeestarregistradavariasveces\ndiferencias en convenciones de nombres o códigos usa- o con valores distintos.\ndos para clasificar elementos - ejemplo: un cliente registrado como \"juan pérez\" y\nformatos de entrada distintos para un también como \"j. a. pérez\".\nprincipales tareas en el preprocesamiento de datos: - direccionesalmacenadascomo\"sanjosé,costarica\"\ndatacleaning(limpiezadedatos):eliminaciónderuido, en una base de datos y como \"sj-cr\" en otra.\ncorrección de inconsistencias y tratamiento de valores uso de pruebas estadísticas para detectar redundancia o\nfaltantes. asociación entre variables:\ndata integration (integración de datos): combinación - prueba de correlación χ2 para datos nominales:\nde información proveniente de múltiples fuentes hetero- mide la asociación entre variables categóricas.\ngéneas en un repositorio coherente y unificado. - hipótesis de independencia:\ndata reduction (reducción de datos): disminución del\nh :p(a ∩b )=p(a )p(b )\nvolumen mediante selección de atributos relevantes, re- 0 i j i j\nducción de dimensionalidad o discretización. - las variables se consideran independientes si:\ndatatransformation(transformacióndedatos):incluyenormalización,estandarización,agregaciónyconstrucχ2"}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_010", "idx": 10, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "calculado\n≤χ2\nα,df\nción de nuevas variables.\ndata discretization (discretización de datos): transformación de atributos continuos en atributos categóricos\npara facilitar el análisis y la modelización.\ndata cleaning: tratamiento de valores faltantes y ruido:\ntratamiento de valores faltantes:\n- ignorar tuplas con valores faltantes (puede llevar a la\npérdida de datos).\n- completar manualmente los valores faltantes (costoso\ny poco práctico en grandes datasets).\n- usar un valor global constante (por ejemplo: \"desconocido\", ∞).\n- rellenar con la media, mediana o moda. también\npuede hacerse por clase.\n◦ ejemplo: en clasificación de clientes por riesgo\ncrediticio, reemplazar con el ingreso promedio de\nclientes en la misma categoría de riesgo.\n- inferir valores mediante modelos estadísticos o de\naprendizaje automático (regresión, k-nn, árboles de\ndecisión).\nbinning (agrupación en intervalos):\n- reemplazar cada valor por:\n◦ la media del bin."}
{"id_doc": "DOC_022", "segmentacion": "B", "chunk_id": "DOC_022_B_011", "idx": 11, "autor": "Nelson Rojas Obando", "fecha": "2025-09-16", "tema": "Comparación entre regresión lineal y logística, métricas de evaluación (Accuracy, Precision, Recall, AUC) y preprocesamiento de datos para mejorar modelos predictivos.", "texto": "◦ la mediana del bin."}
{"id_doc": "DOC_023", "segmentacion": "B", "chunk_id": "DOC_023_B_000", "idx": 0, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "apuntes de la clase semana 7 2025\n\ncurso de inteligencia artificial\n\nrafael vargas solis\n\napuntes del 16 de setiembre de 2025\n\nresumen-este documento presenta un resumen de los temas deestamanera,sedisminuyesuinfluenciaenlavarianza\nclave abordados en la clase de inteligencia artificial del 16 del modelo y se mejora la distribucio'n de los datos.\nde setiembre de 2025. comienza describiendo las diferencias\n- winsorizacio'n (recorte): sustituir los valores at'ıpicos\nfundamentalesentrelaregresio'nlinealylaregresio'nlog'ıstica,as'ı\npor valores ma's cercanos dentro de un rango definido,\ncomolaste'cnicascomunesparaelmanejodevaloresat'ıpicosylas\nestrategiasparaenfrentarelsesgoylavarianzaenlosmodelosde usualmente basado en percentiles (por ejemplo, 1% y\naprendizajeautoma'tico.posteriormente,sediscutenlasme'tricas 99%). esta te'cnica conserva la estructura general de los\nde evaluacio'n tanto para clasificacio'n como para regresio'n, in- datos y evita que los valores extremos distorsionen los\ncluyendolamatrizdeconfusio'n,precisio'n,exhaustividad(recall),\nresultados.\nf1-score, curva roc y el a'rea bajo la curva (auc), apoyadas\nen ejemplos pra'cticos. finalmente, el documento resalta los"}
{"id_doc": "DOC_023", "segmentacion": "B", "chunk_id": "DOC_023_B_001", "idx": 1, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "3. mencione dos te'cnicas para evitar un alto sesgo y dos\nproblemas ma's frecuentes en la calidad de los datos -como\nla incompletitud, la inexactitud y la inconsistencia- y enfatiza para evitar alta varianza.\nla importancia de las tareas de preprocesamiento, tales como en el aprendizaje automa'tico, es fundamental lograr un\nlimpieza,integracio'n,reduccio'n,transformacio'nydiscretizacio'n, equilibrio entre sesgo y varianza para obtener modelos con\n\npara garantizar el desarrollo de modelos predictivos robustos y\nbuena capacidad de generalizacio'n. a continuacio'n, se deconfiables.\nscriben algunas te'cnicas para abordar ambos problemas:\nindex terms-inteligencia artificial, aprendizaje automa'tico,\nregresio'n, valores at'ıpicos, sesgo, varianza, me'tricas de eval- para reducir sesgo (underfitting):\nuacio'n, matriz de confusio'n, roc, auc, preprocesamiento de - incrementar la complejidad del modelo: utilizar moddatos\nelos ma's sofisticados, como polinomiales en lugar de\nlineales,redesneuronalesma'sprofundasoalgoritmosno"}
{"id_doc": "DOC_023", "segmentacion": "B", "chunk_id": "DOC_023_B_002", "idx": 2, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "i. preguntasdelquiz\nlineales, permite capturar relaciones ma's complejas entre\n\n1. ¿cua'l es la diferencia entre regresio'n lineal y re- las variables.\ngresio'n log'ıstica? - incorporar nuevas variables o caracter'ısticas: mela regresio'n lineal se utiliza para predecir variables con- diante te'cnicas de feature engineering, se pueden intinuas, ajustando una recta que minimiza el error cuadra'tico cluir atributos relevantes que enriquezcan la informacio'n\nmedio. por ejemplo, estimar el precio de una vivienda segu'n disponible, mejorando as'ı la capacidad predictiva del\nsu taman˜o. modelo.\nen cambio, la regresio'n log'ıstica se aplica a problemas para reducir varianza (overfitting):\nde clasificacio'n, donde la variable dependiente es catego'rica - aplicar regularizacio'n: me'todos como l1 (lasso) y\n(binaria en la mayor'ıa de casos). utiliza la funcio'n sigmoide\nl2 (ridge) an˜aden penalizaciones a los coeficientes del"}
{"id_doc": "DOC_023", "segmentacion": "B", "chunk_id": "DOC_023_B_003", "idx": 3, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "paramapearlosvaloresdeentradaenprobabilidadesentre0y\nmodelo,limitandosumagnitudyevitandoqueelmodelo\n\n1. ejemplo: predecir si un estudiante aprobara' o no un curso.\nse ajuste excesivamente a los datos de entrenamiento.\n- aumentar los datos o usar te'cnicas de ensamble:\n\n2. describa tres te'cnicas para el tratamiento de datos\nincrementar el taman˜o del conjunto de entrenamiento o\nsobresalientes (outliers).\naplicarme'todoscomobaggingyrandomforestmejorala\nlos outliers o valores at'ıpicos son observaciones que se\n\nestabilidad del modelo y reduce la sensibilidad al ruido\nalejan significativamente del patro'n general de los datos y\nde los datos.\npuedenafectarlaprecisio'ndelosmodelospredictivos.existen\ndiversas estrategias para tratarlos, entre las cuales destacan:\n\n4. ¿cua'l es la derivada de la funcio'n sigmoide σ(x) =\n- eliminacio'n: consiste en remover los outliers identifi- 1 ?\n1+e-x\ncados cuando se determina que son producto de errores la funcio'n sigmoide se define como:\nde medicio'n, registros defectuosos o inconsistencias en\n1\nla recoleccio'n de datos. esta te'cnica debe aplicarse con σ(x)= (1)\ncautela para no eliminar informacio'n valiosa. 1+e-x\n- transformacio'n de variables: aplicar funciones su derivada es:\nmatema'ticas como logaritmos, ra'ıces cuadradas o escalados que reduzcan la magnitud de los valores extremos. σ′(x)=σ(x)(1-σ(x)) (2)"}
{"id_doc": "DOC_023", "segmentacion": "B", "chunk_id": "DOC_023_B_004", "idx": 4, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "ii. me'tricas a. accuracy\n\nson medidas que se utilizan para indicar el rendimiento\ntp +tn\nde un modelo predictivo. constituyen la forma ma's objetiva accuracy = (3)\ntp +tn +fp +fn\nde evaluar y comparar modelos de aprendizaje automa'tico,\npermitiendo determinar que' tan bien se ajustan a los datos de mide la proporcio'n de predicciones correctas. es u'til en datos\nentrenamiento y, sobre todo, que' tan bien generalizan a datos balanceados, pero engan˜osa en clases desbalanceadas.\nno vistos.\nb. precisio'n\nasimismo, se emplean benchmarks, que son conjuntos de\n\ndatos o pruebas estandarizadas utilizadas en la comunidad\ntp\ncient'ıfica para comparar de manera justa el desempen˜o de\nprecision= (4)\ndistintos modelos. el uso de benchmarks permite establecer tp +fp\nun esta'ndar de referencia que facilita la reproducibilidad y la indicaque' proporcio'ndeprediccionespositivasfueroncorreccomparacio'n entre diferentes enfoques. tas. relevante cuando los falsos positivos son costosos.\nen general, las me'tricas pueden dividirse en:\nc. recall (sensibilidad)\n- me'tricasdeclasificacio'n:accuracy,precision,recall,f1score, roc y auc.\ntp\n- me'tricas de regresio'n: error cuadra'tico medio (mse), recall= (5)\nerror absoluto medio (mae) o coeficiente de determi- tp +fn\nnacio'n (r2). mide la capacidad del modelo para identificar correctamente\nlos positivos. importante en contextos donde los falsos negaiii. matrizdeconfusio'n tivos son cr'ıticos.\nlamatrizdeconfusio'norganizalosresultadosdeunmodelo\nd. f1-score\ndeclasificacio'nenfuncio'ndelasprediccionesrealizadasylas\nclases reales. se definen cuatro componentes:\n2-precision-recall\n- true positive (tp):positivoscorrectamenteclasificados. f1= precision+recall (6)\n- false positive (fp): negativos clasificados incorrectaes la media armo'nica entre precisio'n y recall, usada en casos\nmente como positivos (error tipo i).\nde desbalance de clases.\n- true negative (tn): negativos correctamente clasificados."}
{"id_doc": "DOC_023", "segmentacion": "B", "chunk_id": "DOC_023_B_005", "idx": 5, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "v. casodeestudio\n- false negative (fn): positivos clasificados incorrectamente como negativos (error tipo ii). se evaluo' un modelo de deteccio'n de ca'ncer con 1000\npacientes.\nen problemas multiclase, la matriz puede extenderse a\nn ×n. un clasificador ideal concentra todos los valores en - clase positiva: 30 pacientes con ca'ncer.\nla diagonal principal. - clase negativa: 970 pacientes sin ca'ncer.\nmatriz de confusio'n:\nca'ncer no ca'ncer\nca'ncer 25 (tp) 20 (fp)\nno ca'ncer 5 (fn) 950 (tn)\nresultados:\n- accuracy: 25+950 =97.5%\n1000\n- recall: 25 =83.3%\n25+5\n- precisio'n: 25 =55%\n25+20\n- f1-score: 2-0.55-0.833 ≈66.2%\n0.55+0.833\na pesar del alto valor de accuracy, las me'tricas muestran\nlimitaciones en la deteccio'n de la clase positiva.\n\nvi. me'tricasavanzadas\nfig.1. ejemplodematrizdeconfusio'nenclasificacio'nbinaria.\na. curva roc\nla curva roc (receiver operating characteristic) muesiv. me'tricascla'sicas\ntra el desempen˜o de un clasificador binario para distintos\na partir de la matriz de confusio'n se derivan las me'tricas umbrales. representa la tasa de verdaderos positivos (tpr)\nma's utilizadas: frente a la tasa de falsos positivos (fpr).\n- inconsistencias en convenciones de nombres, co'digos o\nformatos.\n- registros duplicados que requieren procesos de data\ncleaning.\nc. causas de incompletitud\n- atributos de intere's no siempre disponibles o considerados irrelevantes en el momento de captura.\n- fallos te'cnicos o malentendidos durante la recoleccio'n.\n- eliminacio'n de registros por inconsistencias.\n- ausencia de historial o modificaciones no registradas.\n- valores faltantes en atributos clave que deben ser inferidos.\nd. causas de inconsistencias\n- diferencias en convenciones de nombres o co'digos.\n- formatos de entrada distintos para un mismo atributo.\n- conflictos entre bases de datos heteroge'neas.\nfig.2. ejemplodecurvarocyca'lculodeauc.\n- errores en la integracio'n de fuentes mu'ltiples.\n- actualizaciones parciales que dejan registros contradicb. a'rea bajo la curva (auc) torios.\nel auc mide el a'rea bajo la curva roc:\ne. principales tareas en el preprocesamiento\n- auc = 0.5: clasificador aleatorio.\n- auc cercano a 1: modelo con gran poder de discrimi- - data cleaning: eliminacio'n de ruido, correccio'n de inconsistencias y tratamiento de valores faltantes.\nnacio'n.\n- data integration: combinacio'n de datos provenientes de"}
{"id_doc": "DOC_023", "segmentacion": "B", "chunk_id": "DOC_023_B_006", "idx": 6, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "vii. procesamientodedatos mu'ltiples fuentes heteroge'neas en un repositorio unificado.\na. problemas encontrados en la calidad de datos\n- data reduction: reduccio'n de volumen mediante seen escenarios reales, los datos suelen presentar problemas leccio'n de atributos, reduccio'n de dimensionalidad o\nque afectan directamente la efectividad de los algoritmos de discretizacio'n.\nminer'ıa y aprendizaje automa'tico. los principales son: - data transformation: normalizacio'n, estandarizacio'n,\n- incompletitud:valoresfaltantesenatributosimportantes. agregacio'n y construccio'n de nuevas variables.\nejemplo: si un producto estaba en oferta y no se registro' - data discretization: transformacio'n de atributos continla variable. uos en categor'ıas o intervalos.\n- inexactitud o ruido: errores de medicio'n, valores\nat'ıpicos o entradas ano'malas en transacciones. f. data cleaning: tratamiento de valores faltantes y ruido\n- inconsistencia: discrepancias en nombres, co'digos 1) valores faltantes:\no formatos. ejemplo: fechas almacenadas como\n- ignorar tuplas con valores faltantes (riesgoso si se pierde\ndd/mm/aaaa en una base de datos y como\nmucha informacio'n).\nmm-dd-yyyy en otra.\n- completar manualmente (costoso en grandes datasets).\nuncasoilustrativoeslarecoleccio'ndedatosme'dicos:enla\n- usar un valor global constante (ej. \"desconocido\").\nmedicio'n de presio'n sangu'ınea, un valor faltante no implica - rellenar con la media, mediana o moda, tambie'n por\nque el registro deba eliminarse, ya que otras caracter'ısticas clase.\n(edad, peso, historial cl'ınico) s'ı aportan informacio'n valiosa. - inferir valores mediante modelos estad'ısticos o de ml\nesto demuestra que los datasets requieren preprocesamiento (regresio'n, k-nn, a'rboles de decisio'n).\nantes de aplicar te'cnicas de miner'ıa o aprendizaje.\n2) binning: binning agrupa valores en intervalos (bins) y\nb. causas de datos defectuosos reemplaza cada valor por:\n- instrumentos de recoleccio'n defectuosos. - la media del bin.\n- errores humanos o computacionales en la entrada de - la mediana del bin.\ndatos. - el borde ma's cercano del bin.\n- valoresfalsosencamposobligatorios(ejemplo:fechapor ejemplo: salarios ruidosos [2950, 3000, 3020, 8000]. el bin\ndefecto\"1deenero\"paraocultarcumplean˜os),conocidos (2900-3100) se reemplaza por la media (2990), mientras que\ncomo datos faltantes disfrazados. 8000 queda como posible outlier.\n3) suavizado de ruido:\n- ajustar una funcio'n matema'tica (lineal o no lineal) para\nsuavizar fluctuaciones. ejemplo: regresio'n lineal en ventas mensuales.\n- aplicar te'cnicas de filtrado como la media mo'vil:\n6\n1(cid:88)\nma (t)= x\n7 7 t-i\ni=0\ndonde x es el valor en el d'ıa t. esto genera una curva\nt\nsuavizada que refleja la tendencia real.\ng. data integration: manejo de redundancia\nla misma informacio'n puede estar registrada varias veces\no con diferencias. ejemplo: un cliente como \"juan pe'rez\"\nen una base de datos y \"j. a. pe'rez\" en otra. se aplican\npruebas estad'ısticas como la chi-cuadrado (χ2) para detectar\nredundancia o asociaciones entre variables catego'ricas:\nh :p(a ∩b )=p(a )p(b )\n0 i j i j\nsiχ2 ≤χ2 ,seaceptalahipo'tesisdeindependencia.\ncalculado α,df"}
{"id_doc": "DOC_023", "segmentacion": "B", "chunk_id": "DOC_023_B_007", "idx": 7, "autor": "Rafael Vargas Solís", "fecha": "2025-09-16", "tema": "Análisis de métricas de clasificación y regresión (Precision, Recall, F1, ROC, AUC) y problemas de calidad de datos abordados mediante técnicas de preprocesamiento.", "texto": "references\n[1] a.burkov,thehundred-pagemachinelearningbook.andriyburkov,\n\n2019.[online].available:https://themlbook.com/"}
{"id_doc": "DOC_024", "segmentacion": "B", "chunk_id": "DOC_024_B_000", "idx": 0, "autor": "Darío Espinoza Aguilar", "fecha": "2025-09-18", "tema": "Repaso de métricas de rendimiento, matriz de confusión, precisión, recall, F1-score y tareas esenciales de preprocesamiento de datos (cleaning, integration, reduction).", "texto": "apuntes semana 7 - 18/09/2025\n1st dar'ıo espinoza aguilar\n2020109109\n\ncomputer engenieering\ndarioespinoza477@estudiantec.cr\n\nabstract-este documento corresponde a los apuntes de la - tp: true positive\nclasedel18deseptiembrede2025,dondeserepasanlosconceptos - tn: true negative\ndeme'tricasyfo'rmulasparaevaluarlosmodelos.adema's,sehace\n- fp: false positive\nun repaso del proceso de preparacio'n y procesamiento de datos\nantes de poder entrenar el modelo. - fn: false negative\nindexterms-me'tricas,datacleaning,procesamientodedatos, a partir de estos se pueden hacer varias combinaciones\ndatasets para calcular ciertas me'tricas\n\ni. introduccio'n\n1) accuracy: clasificacio'ncorrectaentretodoslosintentos.\nse menciono' que se iba a dejar la tarea 2 la otra semana la fo'rmula es la siguiente:\ny el proyecto 1 la semana que le sigue. adema's, nos dio la\ntp +tn\ninvitacio'n al evento de ingenier'ıa para lo que quiera asistir. accuracy =\ntp +tn +fp +fn\nen la parte de noticias, se menciono' que xbox gaming esu'tilcuandoloserroresporclasesonigualdeimportantes.\nesta desarrollado un ia para desarrollar juegos viejos, ma's otorga importancia igual a todas las claases.\nque un desarrollador es un porteador de juego viejos para que 2) precision: mide los errores tipo 1 (fp). tasa de predicpuedan ser jugables. ciones positivas correctas entre todas las predicciones positivas.\nel profe menciona que se lanzo' un protocolo nuevo para\ntp\npago a trave's de agentes. el protocolo es ap2, fue lanzado precision=\ntp +fp"}
{"id_doc": "DOC_024", "segmentacion": "B", "chunk_id": "DOC_024_B_001", "idx": 1, "autor": "Darío Espinoza Aguilar", "fecha": "2025-09-18", "tema": "Repaso de métricas de rendimiento, matriz de confusión, precisión, recall, F1-score y tareas esenciales de preprocesamiento de datos (cleaning, integration, reduction).", "texto": "por google y lo que se busca es que los pagos por medios\nelectro'nicos se puedan realizar sin necesidad de intervencio'n 3) recall: mide los errores de tipo 2 (fn). tasa de\nhumana. menciono la posibilidad de hacer pasant'ıa en la predicciones correctas entre todos los ejemplos positivos del\nempresa donde e'l trabaja. conjunto de datos.\ntp\nsehablodelaspro'ximasevaluacionesquevamosatener,el recall=\ntp +fn\nprofesor menciono que la mayor'ıa de las tareas programadas\n4) f1-score: esta es un me'trica que contempla ambos\n\nyaseanproyectosotareasvanaserdemodelosclasificatorios\nerrores. comu'nmente utilizada en problemas de clasificacio'n,\n\nya que tiene afinidad por ellos y que se van a tener tareas de\nespecialemnte cuando tenemos desequelibrio de clases.\ninvestigacio'n.\n2-precision-recall\n\nii. repasodeme'tricas f1=\nprecision+recall\nson las me'tricas asociadas a un modelo que nos indica el\nrendimiento de un modelo predictivo. la forma ma's objetiva a. roc (receiver operating characteristic)\nde evaluar y comparar un modelo. en todos los modelos que ela'reabajolacurvasiempretienequeser>0.5.sies=5\nse sacan siempre hay me'tricas o benchmarks que nos indican loquevamosateneresunrandomclasifier,esunclasificador\nque tan bueno es el modelo. aleatorio. si es cada vez mayor a 0.5 el modelo va siendo\nmejor.\nse repaso lo que es la matriz de confusio'n. que de los"}
{"id_doc": "DOC_024", "segmentacion": "B", "chunk_id": "DOC_024_B_002", "idx": 2, "autor": "Darío Espinoza Aguilar", "fecha": "2025-09-18", "tema": "Repaso de métricas de rendimiento, matriz de confusión, precisión, recall, F1-score y tareas esenciales de preprocesamiento de datos (cleaning, integration, reduction).", "texto": "iii. respasodeprocesamientodedatos\nalgoritmos de clasificacio'n tenemos 2 etiquetas se pueden ver\ncomo positivo y negativo, y de esas 2 etiquetas se pueden enlapra'cticapodemostenerciertosproblemasconnuestros\ntener 4 4 posibles valores que la matriz de confusio'n nos datos:\nayuda a visualizar esos valores. en esta matriz se tienen los - incompletitud:valoresfaltantesenatributosimportantes\ntarget class que es la etiqueta tenemos en el dataset y el - inexactitud o ruido: errores y valores at'ıpico en las\npredicted class que es la prediccio'n de nuestro modelo. los transacciones\ncuatros combinaciones - inconsistencia:discrepanciaenloco'digodedepartamentos o categor'ıa\na. ¿por que' los datos pueden ser inexactos? - inferir valores mediante modelos estadistico o de ml\n(regresio'n, k-nn, a'rboles de decisio'n)\n- instrumentos de recoleccio'n de datos defectuosos.\n- errores humanos o computacionales en la entrada de 2) data cleaning (noisy data):\ndatos. - agrupar valores en intervalos(bins).\n- usuarios que ingresan valores falsos para campos obli- - sepuedeutilizarcondatosmuyruidosos,sereemplazael\ngatorios. valor por: la media del bin, la mediana del bin, el borde\n- conocido como datos faltantes disfrazados. ma's cercano del bin.\n- inconsistencia en convenciones de nombres, co'digos o 3) data integration (redundancy handling):\nformatos. - seajustaunafuncio'nmatema'tica(linealonolineal)para\n- tuplas duplicadas que requieren procesos de data clean- suavizar el ruido de los datos.\ning. - aplicar te'cnicas de filtrado para suavizar fluctuaciones,\nse puede utilizar la media mo'vil (utilizar los u'ltimos 7\nb. ¿por que' los datos pueden estar incompletos?\nelementos para ir calculando la media).\n- atributos de intere's no siempre esta'n disponibles\n- noseincluyenporquenoseconsideraronimportantesen"}
{"id_doc": "DOC_024", "segmentacion": "B", "chunk_id": "DOC_024_B_003", "idx": 3, "autor": "Darío Espinoza Aguilar", "fecha": "2025-09-18", "tema": "Repaso de métricas de rendimiento, matriz de confusión, precisión, recall, F1-score y tareas esenciales de preprocesamiento de datos (cleaning, integration, reduction).", "texto": "el momento de la entrada\n- datos relevantes no se registran por malentendidos o\n\nfallos del equipo\n- datos inconsistentes con otros registros pueden ser eliminados\n- historial o modificaciones de datos pasados pueden no\n\nhaberse registrado\n- valores faltantes en atributos claves pueden necesitar ser\n\ninferidos\nc. ¿por que' los datos pueden ser inconsistentes?\n- diferencias en convenciones de nombres o co'digos usados para clasificar elementos\n- formatos de entrada distintos para un mismo atributo\n- conflictos entre bases de datos o sistemas que manejan\nel\n- errores al integrar datos de mu'ltiples fuentes heteroge'neas\n- actualizaciones parciales o incorrectas que dejan registros contradictorios\nd. principales tareas en el preprocesamiento de datos\n- data cleaning eliminacio'n de ruido, correccio'n de inconsistencias, tratamiento de valores faltantes\n- data integration combinacio'n de datos de mu'ltiples\nfuentes heteroge'neas en un repositorio coherente\n- data reduction reduccio'n de volumen mediante seleccio'n de atributos, reduccio'n de dimensionalidad o\ndiscretizacio'n\n- data transformation normalizacio'n, estandarizacio'n,\nagregacio'n, construccio'n de nuevas variables\n- data discretization transformacio'n de atributos continuos en atributos catego'ricos\n1) data cleaning (missing values):\n- ignorar tuplas con valores faltantes (riesgo si la perdida\nde datos es significtiva)\n- completar manualmente los valores (costoso y poco\npra'ctico en grandes datasets)\n- usar un valor global constante."}
{"id_doc": "DOC_024", "segmentacion": "B", "chunk_id": "DOC_024_B_004", "idx": 4, "autor": "Darío Espinoza Aguilar", "fecha": "2025-09-18", "tema": "Repaso de métricas de rendimiento, matriz de confusión, precisión, recall, F1-score y tareas esenciales de preprocesamiento de datos (cleaning, integration, reduction).", "texto": "- rellenar con la media (normal), mediana o moda"}
{"id_doc": "DOC_025", "segmentacion": "B", "chunk_id": "DOC_025_B_000", "idx": 0, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "apuntes de clase: me'tricas y preprocesamiento de\n\ndatos en ia\n\nisaac david brenes torres\n\ninteligencia artificial\ntecnolo'gico de costa rica\nibreto@estudiantec.cr\n18 de septiembre del 2025\n\nresumen-este documento presenta una formalizacio'n de seguridad y autenticacio'n: utiliza criptograf'ıa avanzaapuntesdeclasecorrespondientesalcursodeinteligenciaartifi- da para verificar la identidad de los agentes y asegurar\ncial.seabordanconceptosfundamentalesdeme'tricasdeevalualas transacciones.\ncio'ndemodelosdeclasificacio'n-comoprecisio'n,exhaustividad\ndescentralizacio'n: puede operar en entornos distribui-\n(recall), f1-score y curvas roc-, as'ı como te'cnicas esenciales\nde preprocesamiento de datos, incluyendo limpieza, integracio'n, dos, reduciendo puntos u'nicos de fallo.\nreduccio'n y transformacio'n. adicionalmente, se incluye una escalabilidad: esta' disen˜ado para manejar un alto voluinvestigacio'n sobre el protocolo de pagos para agentes de ia men de micro-transacciones simulta'neas.\n(agent payments protocol, ap2).\ninteroperabilidad: permite la integracio'n con mu'ltiples\nsistemas de pago y plataformas blockchain."}
{"id_doc": "DOC_025", "segmentacion": "B", "chunk_id": "DOC_025_B_001", "idx": 1, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "i. introduccio'n\nap2 representa un avance significativo hacia la econom'ıa\nlos sistemas de inteligencia artificial (ia) requieren no auto'noma, donde los agentes de ia pueden participar en\nsolamentedealgoritmosrobustos,sinotambie'ndeunaevalua- mercados digitales sin intervencio'n humana constante.\ncio'n rigurosa y un preprocesamiento adecuado de los datos.\n\niii. me'tricasdeevaluacio'nparaclasificacio'n\nlasme'tricasdeevaluacio'npermitencuantificarelrendimiento\nde un modelo, mientras que las te'cnicas de preprocesamiento iii-a. matriz de confusio'n\naseguran la calidad y idoneidad de los datos de entrada. estos lamatrizdeconfusio'nesunaherramientafundamentalpara\naspectos son cr'ıticos para el desarrollo de aplicaciones de ia evaluar el rendimiento de un modelo de clasificacio'n binaria.\nconfiables y efectivas. este documento organiza y expande organiza las predicciones del modelo en cuatro categor'ıas\nlos apuntes de clase sobre estos to'picos, integrando adema's (tabla i).\nunainvestigacio'nsobreelemergenteagentpaymentsprotocol\ncuadro i: matriz de confusio'n para clasificacio'n binaria.\n(ap2)."}
{"id_doc": "DOC_025", "segmentacion": "B", "chunk_id": "DOC_025_B_002", "idx": 2, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "valorreal\n\nii. agentesdeiayagentpaymentsprotocol prediccio'n negativo(n) positivo(p)\n(ap2) positivo(p) fp vp\nnegativo(n) vn fn\nii-a. agentes de ia\nverdadero positivo (vp): el modelo predice la clase\nlosagentesdeiasonentidadesauto'nomasquepercibensu\npositiva correctamente.\nentorno mediante sensores y actu'an sobre dicho entorno mefalso positivo (fp): error tipo i. el modelo predice\ndiante actuadores. estos agentes pueden variar desde simples\npositivo cuando la clase real es negativa.\n\nprogramasreflexivoshastasistemascomplejosqueaprendeny\nfalso negativo (fn): error tipo ii. el modelo predice\nse adaptan. un componente clave en los agentes modernos es\nnegativo cuando la clase real es positiva.\nsu capacidad para realizar transacciones de manera auto'noma,\nverdadero negativo (vn): el modelo predice la clase\nlo cual requiere protocolos seguros y eficientes.\nnegativa correctamente.\nii-b. agent payments protocol (ap2) de google cloud\niii-b. exactitud (accuracy)\nel agent payments protocol (ap2) es un framework la exactitud mide la proporcio'n de predicciones correctas\ndesarrollado por google cloud disen˜ado espec'ıficamente para sobre el total de instancias."}
{"id_doc": "DOC_025", "segmentacion": "B", "chunk_id": "DOC_025_B_003", "idx": 3, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "permitirquelosagentesdeiarealicenpagosdeformasegura\ny automatizada [1]. ap2 facilita las transacciones econo'micas vp +vn\nexactitud=\nentre agentes, servicios y usuarios, actuando como una capa vp +vn +fp +fn\nde confianza y estandarizacio'n. es u'til cuando las clases esta'n balanceadas, pero puede ser\ncaracter'ısticas principales de ap2: engan˜osa en conjuntos de datos desequilibrados.\niii-c. precisio'n (precision) iv-a. limpieza de datos (data cleaning)\nlaprecisio'nevalu'alacapacidaddelmodeloparanoetique- valores faltantes: se pueden ignorar tuplas, completar\ntar como positivo un ejemplo negativo. mide la proporcio'n de manualmente, usar un valor global constante, rellenar\nverdaderos positivos entre todas las predicciones positivas. con la media/mediana/moda, o inferir valores mediante\nmodelos.\nvp datos ruidosos: te'cnicas como binning (agrupacio'n en\nprecisio'n=\nvp +fp intervalos), regresio'n o filtrado (media mo'vil) ayudan a\nsuavizar el ruido.\niii-d. exhaustividad (recall o sensibilidad)\niv-b. integracio'n y reduccio'n de datos\nla exhaustividad mide la capacidad del modelo para encontrar todos los ejemplos positivos. calcula la proporcio'n de integracio'n: combina datos de mu'ltiples fuentes, aborverdaderos positivos identificados correctamente. dando problemas de duplicados e inconsistencias (ej.\nestandarizacio'n de formatos).\nvp reduccio'n:reduceelvolumendedatosmanteniendola\nrecall=\nvp +fn informacio'n esencial. incluye reduccio'n de dimensionalidad (pca) y seleccio'n de caracter'ısticas.\niii-e. puntuacio'n f1 (f1-score)\niv-c. transformacio'n y discretizacio'n\nel f1-score es la media armo'nica entre la precisio'n y\nla exhaustividad. es especialmente u'til cuando existe un transformacio'n: normalizacio'n (min-max) y estandadesbalance entre clases. rizacio'n (z-score) para escalar caracter'ısticas.\ndiscretizacio'n: convierte variables continuas en ca2×precisio'n×recall tego'ricas mediante binning (igual-ancho o igualf1=\nprecisio'n+recall frecuencia).\niii-f. ejemplo de caso: deteccio'n de ca'ncer iv-d. prueba de correlacio'n χ2 para datos nominales\nconsidere un conjunto de 1000 pacientes, donde 30 tienen esta prueba estad'ıstica determina si existe una asociacio'n\nca'ncer (clase positiva) y 970 no (clase negativa). la matriz de significativa entre dos variables catego'ricas. la hipo'tesis nula\nconfusio'n obtenida es: (h ) supone independencia. se calcula el estad'ıstico χ2 com0\nparandolasfrecuenciasobservadas(o )yesperadas(e )en"}
{"id_doc": "DOC_025", "segmentacion": "B", "chunk_id": "DOC_025_B_004", "idx": 4, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "ij ij\ncuadro ii: matriz de confusio'n para el caso de deteccio'n de\nuna tabla de contingencia:\nca'ncer.\nprediccio'n/realidad ca'ncer noca'ncer χ2 = (cid:88) r (cid:88) c (o ij -e ij )2\nca'ncer 25(vp) 20(fp) e\nij\nnoca'ncer 5(fn) 950(vn) i=1j=1\nsi χ2 >χ2 , se rechaza h , indicando correlacio'n.\ncalculado α,gl 0\nprecisio'n = 25 2 + 5 20 =0,555 (55.5%) v. conclusio'n\nexactitud = 25+950 =0,975 (97.5%)\n1000 este documento basado en los apuntes de clase sobre\nexhaustividad = 25 =0,833 (83.3%)\n25+5 me'tricasdeevaluacio'nypreprocesamientodedatosenia.se\nf1-score = 2×0,555×0,833 ≈0,662 (66.2%)\n0,555+0,833 cubrieron las principales me'tricas de clasificacio'n, ilustradas\naunque la exactitud es alta (97.5%), la precisio'n y el conunejemplopra'ctico,ysedetallaronlas etapascr'ıticasdel\nf1-score revelan problemas significativos en la identificacio'n preprocesamiento de datos. adicionalmente, se investigo' el\ncorrecta de la clase minoritaria (ca'ncer). agent payments protocol (ap2) como un avance tecnolo'gico\nrelevante para la autonom'ıa de los agentes de ia. la clariiii-g. curva roc y a'rea bajo la curva (auc) ficacio'n y expansio'n de estos conceptos busca servir como\nla curva caracter'ıstica de operacio'n del receptor (roc) base para futuras investigaciones y aplicaciones pra'cticas en\ngrafica la tasa de verdaderos positivos (exhaustividad) vs. el campo de la inteligencia artificial.\nla tasa de falsos positivos (1 - especificidad) para distintos"}
{"id_doc": "DOC_025", "segmentacion": "B", "chunk_id": "DOC_025_B_005", "idx": 5, "autor": "Isaac David Brenes Torres", "fecha": "2025-09-18", "tema": "Métricas de evaluación y preprocesamiento de datos en IA, incluyendo precisión, recall, F1, ROC-AUC y el Agent Payments Protocol (AP2) aplicado a agentes autónomos.", "texto": "referencias\numbrales de clasificacio'n. el a'rea bajo la curva (auc)\n[1] google cloud, \"announcing the agent payments protocol (ap2)\ncuantificalacapacidaddelmodeloparadistinguirentreclases.\nfor ai commerce,\" google cloud blog, 2024. [en l'ınea].\nunauccercanoa1indicaunexcelenteclasificador,mientras disponible: https://cloud.google.com/blog/products/ai-machine-learning/\nque un auc de 0.5 corresponde a un clasificador aleatorio. announcing-agents-to-payments-ap2-protocol.[accedido:18-sep-2025].\n\niv. preprocesamientodedatos\nla calidad de los datos es crucial para el e'xito de cualquier\n\nproyecto de ia. las te'cnicas de preprocesamiento incluyen:"}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_000", "idx": 0, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "apuntes de clase: redes neuronales\nbrandon emmanuel sa'nchez araya\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\nbrandon01sanchez@estudiantec.cr\n23 setiembre 2025\n\nabstract-este documento presenta una formalizacio'n de\napuntes de clase correspondientes al curso de inteligencia artificial. se abordan los conceptos fundamentales de la regresio'n\nlog'ıstica (binaria y multiclase), el uso del dataset mnist y\nla representacio'n de ima'genes mediante flatten. asimismo, se\nintroduce la codificacio'n one-hot, la formulacio'n matricial con\npesosysesgos,ylarelacio'ndeestosmodelosconlaconstruccio'n\nde redes neuronales. finalmente, se destacan las propiedades\nesencialesdelasredes,comolanolinealidad,laorganizacio'nen\ncapasysucapacidadpararesolverproblemascomplejosatrave's\nde la optimizacio'n por gradiente.\n\ni. eldatasetmnist\neldatasetmnist(modifiednationalinstituteofstandards\nandtechnology)esunodelosconjuntosdedatosma'sfamosos\nen el a'rea de aprendizaje automa'tico. fue creado a partir de fig.1. ejemploderepresentacio'ndeund'ıgitoenmnistysusp'ıxelesen\nla recopilacio'n de miles de d'ıgitos manuscritos provenientes escaladegrises.\n\nde estudiantes de secundaria y empleados de la oficina del\ncensodelosestadosunidos.laideaoriginaleradisponerde\nb. por que' esto es un problema complejo"}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_001", "idx": 1, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "unconjuntoestandarizadoquesirvieraparaprobarycomparar\nalgoritmos de reconocimiento de escritura. aunque un d'ıgito \"5\" tiene una forma reconocible, cada\n- conjunto de ima'genes:d'ıgitosescritosamano(del0al persona lo escribe distinto. la variacio'n en trazo, grosor,\n9). inclinacio'n y ubicacio'n hace que sea dif'ıcil usar reglas fijas;\n- taman˜o original: 128×128 p'ıxeles. necesitamos un modelo que aprenda a partir de ejemplos.\n- taman˜o transformado: 28×28 pixeles.\n- flatten: cada imagen se convierte en un vector de 784 c. clasificacio'n binaria: \"¿es un 5 o no?\"\ncaracter'ısticas.\n- 1 channel: un solo canal, es decir en blanco y negro. laregresio'nlog'ısticaeslabasedelasredesneuronales.se\n- cantidad de ejemplos: 60,000 para entrenamiento y usa para clasificar entre dos clases (ej: ¿es un 5 o no lo es?).\n10,000 para prueba.\n1\nen la figura 1 se muestra un ejemplo de co'mo un d'ıgito f (x)=\nw,b 1+e-(wx+b)\nmanuscritoserepresentaenmnistcomounamatrizde28×\n28 p'ıxeles, que luego puede convertirse en un vector de 784\ncaracter'ısticas (flatten).\nh(x)=g(f(x))\n."}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_002", "idx": 2, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "ii. ¿co'modisen˜arunprogramaquereconozca\ntodoslosnu'merosquelaspersonaspuedenhacer?\ng(x)=\n1\n1+e-x\na. p'ıxeles activos e inactivos & formacio'n de la figura\nen una imagen de mnist, cada p'ıxel tiene una intensidad\n(0 = \"apagado\", valores altos = \"encendido\"). la figura del f w,b (x)=wx+b\nd'ıgito se forma por el patro'n de p'ıxeles activos/inactivos. el\naprendizaje consiste en ajustar pesos para que ciertas config- enlafigura2seobservaco'molaregresio'nlog'ısticapuede\nuraciones de p'ıxeles (patrones) produzcan la clase correcta. interpretarse como una red neuronal muy simple.\ninput layer (x∈r784) capa de salida (r10)\ncapa de entrada (r5)\nf output yˆ∈(0,1)\nfig. 2. modelo de regresio'n log'ıstica como red neuronal: entradas →\ncombinacio'nlineal→funcio'nsigmoide.\nd. ¿co'mo alimentar una regresio'n log'ıstica con una matriz?\nsea x ∈ r28×28 la imagen (matriz de p'ıxeles). se aplana\n(flatten) en un vector columna:\nx=vec(x)∈r784.\nfig.3. regresio'nlog'ısticamultinomial:5entradasconectadasdirectamente\ncon10salidas.\ntaman˜o de entrada (input layer) y conteo de para'metros\ncompactacio'n: de 10 vectores a una sola matriz\n- input layer: 784 features (un p'ıxel por entrada).\nen vez de calcular 10 regresiones por separado, apilamos\n- pesos en binario: 784 pesos en w + 1 bias = 785\nsus pesos en una matriz:\npara'metros en total.\n \n-\nw 0 ⊤ "}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_003", "idx": 3, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "iii. regresio'nlog'isticamultinomial   -  \n(experimentoenclase)   -    b 0 \n\nw 1\n⊤\n b 1\nejercicio del profe: 10 regresiones que responden \"s'ı/no\" ∈r (cid:124) 1 w (cid:123) 0 (cid:122) × (cid:125) 784 =    - . .     , (cid:124) ∈ (cid:123) r b (cid:122) 1 (cid:125) 0 =   . . .    , z =w (cid:124) ∈ x (cid:123) r (cid:122) 1 + 0 (cid:125) b.\nse eligio' a 10 estudiantes, cada uno \"especialista\" en un   .   b 9\n - \nd'ıgito (0-9). cada imagen se le pregunta a los especialista  \nw⊤\nuno por uno y ellos respondieron \"s'ı es mi nu'mero\" o \"no 9\n-\nes\". si la respuesta no coincide con la etiqueta verdadera, se\nhace refuerzo (entrenamiento). como resultado se obtiene: i'ndices: w es el peso que conecta el feature i (p'ıxel i) con\nj,i\nla neurona/clase j.\n- mi w es una matriz w ∈r10×784.\none-hot vector - mi b es un vector b∈r10 (un bias por neurona/clase).\nlaetiquetacorrectasecodificacomounvectorconunu'nico ¿que' sucede con el para'metro b?\n1 en la posicio'n del d'ıgito correcto:\ncada neurona/clase tiene su propio sesgo: b =\n0 1 2 3 4 5 6 7 8 9 (b 0 ,...,b 9 )⊤. cantidad de neuronas = taman˜o de b.\ny (one-hot) 0 0 1 0 0 0 0 0 0 0"}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_004", "idx": 4, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "iv. ejercicio:devectoramatriz\nese 1 marca cua'l estudiante (regresio'n) deber'ıa decir \"s'ı\". 1) una sola regresio'n binaria (vector x)\nla figura 3 representa la extensio'n al caso multinomial.\nsea\nen este modelo, las entradas se conectan directamente con  3   3 \nmu'ltiples salidas, de manera que cada una corresponde a 4 2\nuna clase distinta. de esta forma se pueden reconocer si- x= 5   , w = 4   , b=2.\nmulta'neamente los diez d'ıgitos de mnist. 6 5\nentonces neurona (para decidir entre dos clases, por ejemplo \"s'ı\"\n  o \"no\") o varias neuronas (para elegir entre mu'ltiples\n3\ncategor'ıas, como los 10 d'ıgitos en mnist).\nz =w⊤x+b=[3 2 4 5]   4 +2=67+2=69, yˆ=σ(z).\n5 profundidad y complejidad\n6\nentre ma's capas profundas tenga la red, ma's puede \"des2) varias regresiones a la vez menuzar\" el problema en representaciones intermedias, lo"}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_005", "idx": 5, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "que le permite identificar patrones complejos que una simple\nahora dos regresiones (piensa \"dos neuronas de salida\").\napilamos sus pesos en una matriz w y sus sesgos en un regresio'n log'ıstica no podr'ıa capturar.\nvector b: en la figura 4 se muestra un ejemplo de red neuronal con\n(cid:20) (cid:21) (cid:20) (cid:21) tres entradas, una capa oculta de cinco neuronas y cuatro\n3 2 4 5 2\nw = ∈r2×4, b= ∈r2. salidas.esteesquemailustraco'molaintroduccio'ndecapasin4 3 2 1 3\n\ntermedias permite transformar las representaciones y capturar\ncon el mismo x de arriba: relaciones no lineales ma's complejas en los datos.\n \n3\n(cid:20) 3 2 4 5 (cid:21) 4 (cid:20) 2 (cid:21) (cid:20) 69 (cid:21) capa oculta\nz =wx+b=  + = .\n4 3 2 1 5 3 43 capa de salida\n6 capa de entrada"}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_006", "idx": 6, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "v. redneuronal\nuna red neuronal es un modelo matema'tico inspirado en\nel funcionamiento del cerebro humano. esta' compuesta por\nunidades llamadas neuronas, organizadas en capas. las capas\nesta'nconectadasentres'ı,demaneraquelasalidadeunacapa\nsirve como entrada de la siguiente.\n\npropiedades clave\n- nolinealidad:permiteresolverproblemascomplejosque\nun modelo lineal no podr'ıa.\nfig.4. redneuronalpequen˜a:3entradas,1capaocultade5neuronasy4\n- capas: la profundidad de la red es un hiperpara'metro salidas.\nque define su capacidad, y en cada una de las capas hay\nneuronas.\n\nvi. conclusiones\n- diferenciabilidad: cada capa debe ser diferenciable para\nque podamos optimizar mediante gradiente descendente. el estudio de la regresio'n log'ıstica permite comprender\n- optimizacio'n: si puedo derivar, puedo optimizar. los cimientos de las redes neuronales modernas. a partir de\nproblemas de clasificacio'n binaria simples se llega de manera\ncuando aplicamos una red neuronal despue's de un clasinatural a la extensio'n multiclase, donde se introducen la forficador multinomial, la lo'gica cambia respecto a una clasifimulacio'n matricial y la codificacio'n one-hot. estos elementos\ncacio'nbinariatradicional.enunaclasificacio'nbinariasimple,\nmuestran co'mo mu'ltiples regresiones pueden integrarse en un\nlarelacio'neslinealentrelasentradas(features)ylasalida.en\nsolo modelo ma's general.\nuna red neuronal, la salida ya no depende directamente de la"}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_007", "idx": 7, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "el concepto de red neuronal surge al conectar varias de\nimagen original, sino de las activaciones de la capa anterior.\nestas operaciones en capas sucesivas, incorporando funciones\nestructura t'ıpica de activacio'n no lineales que ampl'ıan la capacidad de representacio'n. la diferenciabilidad de cada capa asegura la\n- capadeentrada:eslaquerecibedirectamentelosdatos\nposibilidad de entrenar el modelo mediante optimizacio'n,\ndel problema. cada neurona de esta capa representa una\n\nmientras que la profundidad incrementa su habilidad para\ncaracter'ıstica (feature) de la entrada. por ejemplo, en el\ncapturar patrones complejos. en s'ıntesis, las redes neuronales\ncaso de mnist cada p'ıxel de la imagen se convierte en\nsonunaevolucio'ndirectadelaregresio'nlog'ıstica,potenciadas\nuna neurona de la capa de entrada.\nporlaorganizacio'nencapasylaintroduccio'ndenolinealidad.\n- capas intermedias (ocultas): son las que procesan la\ninformacio'n recibida. aqu'ı la red va combinando y\ntransformando los datos para encontrar patrones ma's\nabstractos.sellaman\"ocultas\"porquenointeractu'ancon\nel mundo exterior: solo comunican informacio'n entre la\nentrada y la salida.\n- capa de salida: es la que entrega el resultado final del"}
{"id_doc": "DOC_026", "segmentacion": "B", "chunk_id": "DOC_026_B_008", "idx": 8, "autor": "Brandon Emmanuel Sánchez Araya", "fecha": "2025-09-23", "tema": "Fundamentos de redes neuronales y regresión logística aplicadas al dataset MNIST, con codificación one-hot, formulación matricial y principios de optimización por gradiente.", "texto": "modelo. dependiendo del problema, puede ser una sola"}
{"id_doc": "DOC_027", "segmentacion": "B", "chunk_id": "DOC_027_B_000", "idx": 0, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "apuntes de clase: redes neuronales con el dataset mnist\nfabián díaz barboza\nestudiante ing. computación\ntecnológico de costa rica\ncartago, costa rica\nfdiaz@estudiantec.cr\n23/09/2025\n1 el dataset mnist y la representación de 1.3 píxeles activos e inactivos: la semántica del\ncaracterísticas input\n1.1 descripción del dataset mnist un píxel con intensidad 0 se considera \"apagado\" y\nvalores altos indican un píxel \"encendido\".\nimágenes en blanco y negro (1 canal).\nincluso la regresión logística binaria más simple exige\n10 clases (dígitos 0-9).\n784 pesos (w )+1 sesgo (b)=785 parámetros,\ni\ntamaño estándar: 28×28 píxeles (entrada comúnmente utilizada).\nlo que muestra la complejidad del espacio de entrada.\nconjunto: 60000 ejemplos de entrenamiento y 10000\nde prueba.\n2 la regresión logística binaria: la neurona"}
{"id_doc": "DOC_027", "segmentacion": "B", "chunk_id": "DOC_027_B_001", "idx": 1, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "fundamental\n1.2 proceso de aplanamiento (flattening)\nuna imagen de entrada x ∈r28×28 se convierte en un 2.1 clasificación binaria como problema inicial\nvector columna mediante flatten:\nla regresión logística estima la probabilidad de que\nx∈r784, 28×28=784. una entrada pertenezca a la clase positiva; la salida está\nen (0,1).\ncada uno de los 784 elementos es una característica\n(feature) que alimenta el modelo.\n2.2 ecuaciones fundamentales de la neurona\npotencial de activación:\nz =w⊤x+b.\nfunción sigmoide:\n1\ng(z)= .\n1+e-z\nsalida del modelo:\nyˆ=h(x)=g(w⊤x+b).\nfigura: diagrama esquemático de la neurona\n(entradas → combinación lineal → activación →\nsalida)\nfigura 1: ejemplo de la representación de un dígito en\nmnist como matriz 28×28 y su aplanamiento a un figura 2: diagrama esquemático que interpreta la regrevector de 784 características. sión logística como la neurona más simple.\n1\n3 extensión a la clasificación multinomial y la 4.2 ejemplo numérico de clase: de vector a matriz\ncodificación one-hot"}
{"id_doc": "DOC_027", "segmentacion": "B", "chunk_id": "DOC_027_B_002", "idx": 2, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "v.b.1. cálculo de una sola regresión (vector de\n4 features):\n3.1 ejemplo de clase: 10 regresiones logísticas, una\npor alumno    \n3 3\npara manejar las 10 clases se puede entrenar una regre- w =   2 , b=2, x=   4 .\nsión logística por estudiante (una por clase); la capa de\n4 5\n5 6\nsalida tendría 10 neuronas (una por clase).\nz =w⊤x+b=(3-3)+(2-4)+(4-5)+(5-6)+2=69.\n3.2 codificación one-hot de las etiquetas (y)\n\nla etiqueta escalar se codifica como un vector one-hot\nen r10. yˆ=σ(z).\n\nv.b.2. cálculo de varias regresiones a la vez (2\nclase (dígito) vector one-hot (y ∈r10) esperada neuronas):\n0 [1,0,0,0,0,0,0,0,0,0] neurona 0\n2 [0,0,1,0,0,0,0,0,0,0] neurona 2  3 \n9 [0,0,0,0,0,0,0,0,0,1] neurona 9 (cid:20) 3 2 4 5 (cid:21) (cid:20) 2 (cid:21) 4\nw = , b= , x= .\n4 3 2 1 3 5\ncuadro 1: codificación one-hot de etiquetas (ejemplos). 6\n(cid:20) (cid:21)\n69\nz =wx+b= .\n43\n4 compactación por álgebra lineal\n5 arquitectura de las redes neuronales profun4.1 formulación matricial de pesos y sesgos\ndas"}
{"id_doc": "DOC_027", "segmentacion": "B", "chunk_id": "DOC_027_B_003", "idx": 3, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "stackeando los vectores de pesos obtenemos la matriz\n5.1 definición y estructura típica\nde pesos y el vector de sesgos:\nunaredneuronalartificialesunmodelodecómputo\nw ∈r10×784, b∈r10. inspirado en el cerebro humano, compuesto por unidades\nllamadas neuronas artificiales. cada neurona recibe\nla combinación lineal de la capa de salida se escribe un conjunto de entradas x, aplica una combinación lineal\ncomo: con sus pesos w y un sesgo b, y luego pasa el resultado\npor una función de activación g:\nz =wx+b, z ∈r10.\nh(x)=g(w⊤x+b).\ncapa de entrada: recibe los 784 píxeles (flatten).\nelemento símbolo dimensión\ncapas ocultas: transforman la información en reentrada x 784×1\npresentaciones abstractas.\nmatriz de pesos w 10×784\nsesgos b 10×1 capa de salida: entrega la predicción (10 neuronas\npotencial de activación z 10×1 para mnist).\ncuadro 2: dimensiones en la formulación matricial para\nmnist.\nfigura 3: matriz de pesos w en la capa fully connected:\ncada fila corresponde a una neurona de salida y cada figura 4: ejemplo esquemático de una red neuronal con\ncolumna a un píxel de entrada. capa de entrada, capa(s) oculta(s) y capa de salida.\n2\n5.2 el rol del sesgo b 5.4 propiedades esenciales de la red\nretomando, el parámetro b (bias o sesgo) podriamos 1. nolinealidad:lasfuncionesdeactivación(sigmoide,\nverlo como un desplazamiento en la función de acti- relu, etc.) permiten que la red modelice relaciones\nvación. sin b, todas las funciones aprendidas por la red no lineales entre entradas y salidas.\ntenderíanapasarporelorigen,loquelimitalaflexibilidad"}
{"id_doc": "DOC_027", "segmentacion": "B", "chunk_id": "DOC_027_B_004", "idx": 4, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "2. capasyprofundidad:amayorprofundidad,mayor\ndel modelo.\ncapacidad para representar abstracciones jerárquicas.\nen el caso de mnist:\n\n3. diferenciabilidad: la diferenciabilidad de las funtenemos 10 regresiones logísticas (una por cada clacionesinternasesrequisitoparaaplicarretropropaga-\nse).\nción y optimizar los parámetros mediante gradiente\ncada regresión tiene un vector de pesos w i ∈r784 y descendente.\nun sesgo b .\ni\nenconjunto,lospesosformanlamatrizw ∈r10×784 6 conclusiones\ny los sesgos forman un vector b∈r10.\nenconclusióndelaclase,lasredesneuronalessoncomo\nes importante corregir una confusión que se habló en una evolución natural de la regresión logística: partienclase: no existe un único b de dimensión 784 por ejemplo. do de la clasificación binaria, pasando por la extensión\nen cambio, hay un sesgo por neurona de salida. cada multinomial y compactando parámetros mediante álgecomponente b actúa como umbral independiente para la bra lineal, se llega a arquitecturas fully connected que\ni\nneuronai,permitiendodesplazarsufuncióndeactivación permiten mayor expresividad y paralelización. la ecuay ajustar su probabilidad de disparo de forma individual. ción z =wx+b nos sintetiza el paso fundamental hacia\nla representación matricial; pero el verdadero salto en\n5.3 fully connected (completamente conectadas) capacidad proviene de combinar esa formulación con funciones de activación no lineales y con múltiples capas\nlas capas fully connected (fc) son aquellas en las\ndiferenciables, lo que habilita la retropropagación y el"}
{"id_doc": "DOC_027", "segmentacion": "B", "chunk_id": "DOC_027_B_005", "idx": 5, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "que cada neurona de una capa se conecta con todas las\n\nentrenamiento eficiente de modelos capaces de abstraer\nneuronas de la capa anterior.\ncaracterísticas complejas de datos como mnist.\nen nuestro ejemplo de mnist:\ncada neurona de salida (de las 10) recibe conexión\nde los 784 píxeles de entrada.\ncada conexión tiene su propio peso, y además cada\nneurona tiene su sesgo b .\ni\nesta estructura convierte el modelo en un clasificador mucho más potente que una sola regresión logística\nbinaria, porque permite:\n\n1. aprender múltiples fronteras de decisión en paralelo.\n\n2. combinar la información de todos los píxeles de\nforma diferenciada para cada clase.\n\n3. ajustar umbrales específicos gracias a los b .\ni\nen otras palabras, una red fully connected extiende el\npoder de una regresión logística binaria: al apilar capas\ncon activaciones no lineales, las salidas de una capa se\nconvierten en features no lineales que alimentan la siguiente, permitiendo construir clasificadores mucho más\nexpresivos.\n5.3.1 de la multiclase al clasificador binario\nunaarquitecturaútilconsisteenusarprimerolas10regresioneslogísticas(capamulticlase)yluegoaplicarsobre\nsu salida un clasificador binario adicional. por ejemplo,\npara determinar si la imagen corresponde al dígito \"5\" o\nno, la decisión puede tomarse a partir de las 10 salidas\n(o de una combinación entrenada de ellas), en lugar de\nhacerlo directamente sobre los píxeles. de este modo, las\ncapas previas actúan como extraedores de características\nno lineales que potencian una decisión binaria final más\nrobusta."}
{"id_doc": "DOC_027", "segmentacion": "B", "chunk_id": "DOC_027_B_006", "idx": 6, "autor": "Fabián Díaz Barboza", "fecha": "2025-09-23", "tema": "Representación de imágenes y codificación one-hot en redes neuronales con MNIST, formulación matricial de pesos y sesgos, y arquitectura fully connected.", "texto": "3"}
{"id_doc": "DOC_028", "segmentacion": "B", "chunk_id": "DOC_028_B_000", "idx": 0, "autor": "Gerardo Alberto Gómez Brenes", "fecha": "2025-09-25", "tema": "Conceptos clave de redes neuronales: regresión logística, softmax, activaciones (sigmoide, ReLU, tanh) y retropropagación, con enfoque en estructura, dimensionalidad y costo computacional.", "texto": "apuntes semana 8, clase\ngerardo alberto gómez brenes - 2022089271\n\nresumen-resumen compacto y organizado de la clase. in- v. one-hot,softmaxydecisión\ncluye: motivación breve, pautas para la entrega, conceptos clave\nde redes neuronales y las fórmulas que el profesor mencionó o lasetiquetasmulti-claseserepresentancomovectoresoneutilizó como referencia. hot.paraobtenerunadistribucióndeprobabilidadsobreclases\nse usa softmax:\n\ni. motivaciónynotasgenerales\nse compartió un ejemplo de robótica donde un modelo softmax(z) = exp(z i ) .\nadapta el comportamiento del robot ante la pérdida o modi- i (cid:80) exp(z )\nj j\nficación de una pata. esto ilustra la capacidad de adaptación\n(aprendizaje por refuerzo y transferencia) y su potencial en la predicción final corresponde al índice con mayor probabiaplicacionescomoprótesis.mensajepráctico:hayáreasdeml lidad.\nquenosonsolomodelosdelenguaje;robóticaymanipulación\nson opciones reales. vi. arquitectura:capasyconexiones\nse enfatizó también la forma correcta de entregar tareas:\ndocumentos cortos y autocontenidos, con figuras y tablas una red densa (fully connected / dense) conecta todas las\ndentro del texto, y máximo unas pocas páginas para esta salidas de una capa con todas las entradas de la siguiente.\nactividad (usar el grupo para dudas). añadir capas y activaciones no lineales permite resolver relacionesnolinealesqueunperceptrónsimplenopuede(ejemplo"}
{"id_doc": "DOC_028", "segmentacion": "B", "chunk_id": "DOC_028_B_001", "idx": 1, "autor": "Gerardo Alberto Gómez Brenes", "fecha": "2025-09-25", "tema": "Conceptos clave de redes neuronales: regresión logística, softmax, activaciones (sigmoide, ReLU, tanh) y retropropagación, con enfoque en estructura, dimensionalidad y costo computacional.", "texto": "ii. pautasparalaentrega clásico: xor).\nel trabajo debe ser claro y compacto. incluir dentro del\ndocumento: vii. funcionesdeactivaciónygradientes\nresultadosrelevantes(figuras,tablas)ysuinterpretación\nfunciones mencionadas en clase:\nbreve.\n\nreferencias a notebooks o repositorios solo como com- sigmoide: σ(z) = 1/(1 + e-z). derivada: σ′(z) =\nplemento, no como sustituto. σ(z)(1-σ(z)). tiene problemas de vanishing gradient\nselección crítica de gráficos: mostrar los que aporten a en extremos.\nla conclusión. relu: relu(z) = m'ax(0,z). es eficiente, pero puede\ngenerar neuronas \"muertas\" cuando la derivada es cero.\n\niii. entradayprimermodelo:regresión\nleaky relu: variante con pequeña pendiente negativa\nlogística para evitar neuronas muertas.\nimágenes 28 × 28 se representan como vectores de 784 tanh(z): acotada en (-1,1), útil en algunos contextos.\npíxeles. la regresión logística usa la transformación lineal\nseguida de la sigmoide: viii. forward,pérdidayretropropagación\n1\nz =wtx+b, σ(z)= . el forward calcula salidas capa a capa. con una función de\n1+e-z\npérdida l se aplica retropropagación para obtener derivadas\npara clasificación binaria (ej.: \"¿es 5 o no?\") σ(z) da una parciales ∂l/∂w y actualizar parámetros. regla de actualizaprobabilidad entre 0 y 1. ción (descenso de gradiente):"}
{"id_doc": "DOC_028", "segmentacion": "B", "chunk_id": "DOC_028_B_002", "idx": 2, "autor": "Gerardo Alberto Gómez Brenes", "fecha": "2025-09-25", "tema": "Conceptos clave de redes neuronales: regresión logística, softmax, activaciones (sigmoide, ReLU, tanh) y retropropagación, con enfoque en estructura, dimensionalidad y costo computacional.", "texto": "iv. debinarioamulticlaseynotaciónmatricial ∂l\nw ←w-η ,\npara 10 clases se puede entrenar una regresión por clase o ∂w\nusar una salida vectorial. notación común:\ndonde η es la tasa de aprendizaje. para el perceptrón se\nz =xwt +b, mencionó el hinge loss:\ndonde, por ejemplo, w puede tener forma 10 × 784 (10 l =m'ax(0, 1-y(wtx+b)).\n\nhinge\nneuronas de salida y 784 entradas). con un batch de tamaño\nb la entrada x es b×784 y el resultado z es b×10. laretropropagaciónusalaregladelacadenaparapropagar\nejemplo numérico: con 10 salidas y 784 entradas hay 10× sensibilidades hacia atrás; por eso es necesario que las capas\n784=7840 parámetros sólo en esa capa. sean diferenciables.\n\nix. costoscomputacionalesydimensionalidad\naumentar neuronas y capas incrementa parámetros y costo\ndeoptimización.lamaldicióndeladimensionalidadcomplica\nlabúsquedadesolucionesóptimas.ejemplo:siunacapatiene\n256 neuronas y la siguiente tiene 10, los pesos entre ellas son\n10×256=2560.\nrecomendación práctica: reducir dimensiones innecesarias\n(filtrado de features, pca) cuando sea posible."}
{"id_doc": "DOC_028", "segmentacion": "B", "chunk_id": "DOC_028_B_003", "idx": 3, "autor": "Gerardo Alberto Gómez Brenes", "fecha": "2025-09-25", "tema": "Conceptos clave de redes neuronales: regresión logística, softmax, activaciones (sigmoide, ReLU, tanh) y retropropagación, con enfoque en estructura, dimensionalidad y costo computacional.", "texto": "x. notassobrerepresentacionesy\ncnn/embeddings\npara imágenes, las cnn aplican kernels que extraen patrones locales (bordes, texturas, formas). en lenguaje, embeddings condensan palabras o frases en vectores de dimensión\nfija;lasimilitudsemánticasemidepordistanciaeneseespacio\n\nvectorial."}
{"id_doc": "DOC_029", "segmentacion": "B", "chunk_id": "DOC_029_B_000", "idx": 0, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "1\n\napuntes de la clase del 25 de setiembre de 2025\n\nkevin carranza jimenez\nescuela de ingenier'ıa en computacio'n\ntecnolo'gico de costa rica\nkcarranza@estudiantec.cr\n\nabstract-this document summarizes the lecture held on iii. redesneuronales\nseptember 25, 2025, which included the presentation of the\nuna red neuronal artificial (rna) es un modelo comcompanyskild.ia,focusedonapplyingartificialintelligencealgoputacional inspirado en la estructura y funcionamiento del\nrithmsforrobotcontrol.italsoprovidesareviewoftheprevious\nlecture,coveringneuralnetworksfromlogisticregressiontotheir cerebro humano, compuesto por nodos (neuronas artificiales)\napplication in binary classifiers using multinomial expressions. organizados en capas y conectados entre s'ı mediante pesos.\nthe session then introduces the perceptron model, defined as a estas redes aprenden patrones complejos a partir de datos\nlinearregressionwithahingelossfunction.itisemphasizedthat\nde entrada a trave's de un proceso iterativo de ajuste de\na single perceptron cannot solve non-linear functions, although\npesos, permitiendo resolver tareas de clasificacio'n, prediccio'n\nmultiple perceptrons can be combined to achieve this. finally,\nthemultilayerperceptronisintroducedasaformofdeepneural y reconocimiento en diversos dominios [2].\nnetwork with biological inspiration.\nindex terms-skild, regresio'n lineal, multinomial red neu- a. clasificador de mnist\nronal, perceptro'n.\nmnistesundatasetcon60kmuestrasdenu'merosdel0al\n9enunsolocanal.enelresumendelaclasesedaaentender\nque se esta' intentando desarrollar un clasificador utilizando"}
{"id_doc": "DOC_029", "segmentacion": "B", "chunk_id": "DOC_029_B_001", "idx": 1, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "i. introduction\neste dataset. que cada una de estas imagenes esta' compuesta\nen el desarrollo del curso, las clases recientes han por un grupo de p'ıxeles.\n\nabordado los fundamentos de las redes neuronales y\nsu evolucio'n hacia modelos ma's complejos. la sesio'n del\nb. regresio'n log'ıstica\n25 de septiembre de 2025 incluyo' como tema de intere's un\n\nen el resumen de la clase anterior se menciona que para el\nvideo de la empresa skild.ia, que tiene como objetivo utilizar\n\nclasificador de mnist se comenzaba tratando de hacer una\n\nalgoritmos de inteligencia artificial para controlar robots y\nclasificacio'n binaria respecto a la imagen. hasta el momento\nestos puedan emplear cualquier tarea, trayendo la inteligencia\ndeestaclaseelu'nicoalgoritmoconocidoparadesarrollaresta\nartificial al mu'ndo f'ısico. tambie'n se incluyo' tanto la\nclasificacio'n es la regresio'n log'ıstica. para esto se pasan la\nrevisio'n de conceptos previamente estudiados, entre ellos la\ninformacio'n de cada uno de los pixeles de la imagen como\nregresio'n log'ıstica para desarrollar expresiones multinomiales\nentrada para le regresio'n log'ıstica. la situacio'n es que el\ny con multiples capas de estas, desarrollar redes neuronales.\nproblema no puede ser resuelto con una regresio'n log'ıstica\ntambie'n la introduccio'n del perceptro'n, considerado el punto\nu'nicamente, si no con una regresio'n log'ıstica multinomial, ya\nde partida para las redes neuronales profundas.\nque requerimos 10 clases y la regresio'n log'ıstica solo permite\n1."}
{"id_doc": "DOC_029", "segmentacion": "B", "chunk_id": "DOC_029_B_002", "idx": 2, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "ii. skild.ia\nc. multinomial\nskildaiesunastartupemergentededicadaaldesarrollode para esto, al problema requerir 10 clases, se desarrollan\nuna inteligencia artificial de propo'sito general para el control 10 regresiones log'ısticas, una por cada clase y a cada una\nde robots de mu'ltiples tipos (humanoides, brazos robo'ticos, se le pasa como entrada la informacio'n de los pixeles de\nplataformas de locomocio'n, etc.). la imagen, por lo que una de las regresiones logisticas dara'\nlapropuestacentraldeskildaiescrearun\"cerebrorobo'tico mayor probabilidad que las dema's. y en este punto tenemos\nomni-corporal\" -denominado skild brain- que permita en la figura 1 una arquitectura que ya podr'ıa llamarse red\nque un mismo modelo de ia controle diferentes cuerpos neuronal, aunque todav'ıa faltar'ıa agregar una siguiente capa\nrobo'ticos sin necesidad de reentrenamientos espec'ıficos para para poder resolver problemas no lineales.\ncada hardware. tambie'n se menciona en el resumen de la clase anterior\nun aspecto clave de su disen˜o es la capacidad de adaptacio'n que en lugar de calcular cada regresio'n lineal de forma\na fallos o cambios dra'sticos en la morfolog'ıa del robot: vectorial, cambiamos los vectores por matrices para hacer 1\ncuando un robot pierde una extremidad o sufre un dan˜o, el sola operacio'n y no n. donde n es el taman˜o de la capa\nmodelo puede reorganizar su control para seguir operando, siguiente utilizando conceptos de a'lgebra lineal. en cada una\naprovechando la experiencia aprendida previamente [1]. de las filas sera' representado las neuronas para la siguiente\ncapa y las entradas las columnas.\n2\nfig. 2: inspiracio'n biolo'gica de la red neuronal.\nsen˜alaron que este modelo no pod'ıa resolver funciones no\nlinealmente separables, siendo el ejemplo cla'sico la funcio'n\nlo'gica xor. adema's, advirtieron sobre su limitada expresividad computacional y su escasa capacidad para generalizar en\nproblemas ma's complejos, lo que contradec'ıa las expectativas"}
{"id_doc": "DOC_029", "segmentacion": "B", "chunk_id": "DOC_029_B_003", "idx": 3, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "iniciales de que los perceptrones pudieran resolver tareas\nfig. 1: primer red neuronal.\nde visio'n y reconocimiento de patrones. estas observaciones\ndemostraron que, aunque los perceptrones eran u'tiles para\nciertos problemas lineales, su aplicacio'n pra'ctica era muy\nel resumen de la clase anterior concluye definiendo algunas limitada. el impacto de estas cr'ıticas fue significativo, concaracteristicas de las redes neuronales, las cuales son que al tribuyendoalprimerinviernodelainteligenciaartificial,hasta\nno ser lineales nos permite atacar problemas complejos, esta' que el desarrollo del perceptro'n multicapa y el algoritmo de\ncompuesta por capas, estas capas son el hiper para'metro de retropropagacio'n permitieron superar estas restricciones [5].\nla red neuronal y es importante que sean diferenciables. si la\n\nred neuronal se puede derivar se puede optimizar y que en\nb. inspiracio'n biolo'gica\ncada capa hay neuronas.\nlas redes neuronales artificiales se inspiran en el funiv. elperceptro'n cionamiento de las neuronas del cerebro humano, donde cada\nel perceptro'n es uno de los modelos ma's simples de red neurona recibe sen˜ales de mu'ltiples conexiones sina'pticas, las\nneuronal artificial, propuesto por frank rosenblatt en 1958. procesa y genera una respuesta que se transmite a otras neuconsiste en una unidad de procesamiento que recibe un con- ronas.demaneraana'loga,enlasredesneuronalesartificiales,\njuntodeentradasponderadas,lascombinalinealmenteyaplica cadanodoo\"neurona\"recibeentradasponderadas,aplicauna\nuna funcio'n de activacio'n para producir una salida binaria. su funcio'n de activacio'n y transmite su salida a las siguientes\nobjetivoprincipalesclasificarpatroneslinealmenteseparables. capas, reproduciendo de forma simplificada el procesamiento\naunquelimitadoparaproblemasnolineales,constituyelabase distribuido y paralelo del sistema nervioso biolo'gico. esta\nconceptualdearquitecturasma'scomplejascomoelperceptro'n inspiracio'n biolo'gica se ilustra en la figura 2, donde se\nmulticapa y las redes neuronales profundas [3]. muestra la correspondencia entre una neurona biolo'gica y su\nmodelo artificial.\na. invierno de la ai\nel invierno de la inteligencia artificial hace referencia a c. funcio'n de activacio'n\nper'ıodos histo'ricos en los que las expectativas generadas\nen regresio'n log'ıstica se llama funcio'n no-lineal (sigmoid).\nalrededor de la investigacio'n en ia no se cumplieron, provoesta depende de si la sen˜al activa o no la neurona. dependicando una disminucio'n dra'stica en la financiacio'n, el intere's\nendo de la intensidad de la sen˜al que se haya recibido, esta\nacade'mico y el desarrollo industrial en este campo. durante\ndejara' pasar la informacio'n, la bloqueara' o la transformara' y\nestos periodos, los avances en ia se ralentizaron debido\nexisten varias funciones de activacio'n.\na limitaciones tecnolo'gicas, falta de resultados pra'cticos y\ncr'ıticas hacia la viabilidad de los enfoques predominantes. se 1) funcio'n sigmoide: la funcio'n sigmoide transforma un\nreconocen principalmente dos inviernos de la ia: el primero valor de entrada en un rango entre 0 y 1, lo que permite\na mediados de los an˜os 1970, y el segundo a finales de los interpretarlacomounaprobabilidad.sudesventajaprincipales\nan˜os 1980 hasta principios de los 1990 [4]. lasaturacio'ndegradientesenvaloresextremos,loquedificulta\nen 1969, marvin minsky y seymour papert publicaron el el entrenamiento en redes profundas [6].\nlibro perceptrons, en el que sen˜alaron limitaciones fundamen1\ntales del perceptro'n simple. entre los problemas destacados, σ(x)=\n1+e-x\n3\n2) funcio'n tangente hiperbo'lica (tanh): la tangente 1) pca: el ana'lisis de componentes principales (pca,\nhiperbo'lica es similar a la sigmoide, pero su rango va de por sus siglas en ingle's: principal component analysis) es un\n-1 a 1, lo que permite que las salidas este'n centradas en me'todo estad'ıstico ampliamente utilizado para la reduccio'n\ncero. esto ayuda a mitigar algunos problemas de gradientes de dimensionalidad, que transforma un conjunto de varien comparacio'n con la sigmoide, aunque au'n puede sufrir de ables posiblemente correlacionadas en un nuevo conjunto\nsaturacio'n [6]. de variables no correlacionadas denominadas componentes\nex-e-x principales. el procedimiento consiste en centrar los datos,\ntanh(x)=\nex+e-x calcular la matriz de covarianza, obtener sus autovalores\ny autovectores, y seleccionar los vectores asociados a los\n3) funcio'n relu (rectified linear unit): la funcio'n"}
{"id_doc": "DOC_029", "segmentacion": "B", "chunk_id": "DOC_029_B_004", "idx": 4, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "mayoresautovaloresparaproyectarlosdatosenunsubespacio\nrelu es una de las ma's utilizadas en redes neuronales\nde menor dimensio'n que conserva la mayor varianza posible\nmodernas. define la salida como 0 para valores negativos\nde la informacio'n original [10], [11], [12].\ny como la propia entrada para valores positivos. es computacionalmente eficiente y mitiga en gran parte el problema\ndel desvanecimiento del gradiente, aunque puede presentar el b. comportamiento jera'rquico\nproblema de \"neurona muerta\" [7].\n\nloshumanosaprendencosassimplesparatransformarloen\nalgo ma's complejo, tal es el caso del mlp conformado por\nf(x)=max(0,x)\nmu'ltiples regresiones lineales, de lo cual se optienen ganan4) funcio'n leaky relu: la funcio'n leaky relu es una cias exponenciales en algunas funciones, como polinomios,\nvariante de la relu que permite pequen˜os valores negativos la composicio'n de funciones que permite reusar funciones\nen la salida (usualmente multiplicados por una constante simples otras de orden superior y que mediante una reprepequen˜a, como 0.01). esto evita el problema de neuronas sentacio'ncompacta,enlaquepocospesossepuedenmodelar\nmuertas al asegurar un gradiente no nulo para entradas nega- funcionescomplejas,comoporejemplo,unaredneuronalque\ntivas [8]. se aproxime a otra.\n(cid:40)\nx si x≥0\nf(x)=\nαx si x<0"}
{"id_doc": "DOC_029", "segmentacion": "B", "chunk_id": "DOC_029_B_005", "idx": 5, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "vi. conclusion\nlaclasepermitio' lacomprensio'ndelosfundamentosdelas\n5) funcio'n softmax: la funcio'n softmax convierte un\nredes neuronales, resaltando su estructura jera'rquica al final\nvector de valores reales en una distribucio'n de probabilidad,\ny las motivaciones biolo'gicas que inspiran su arquitectura.\ndondecadavalorquedaentre0y1ylasumatotalesiguala1.\na partir del ana'lisis del perceptro'n y de sus limitaciones,\n\nseutilizaprincipalmenteenlacapadesalidadeclasificadores\nse introdujo la necesidad de arquitecturas ma's complejas,\nmulticlase [6].\ncomo el mlp, que posibilitan la resolucio'n de problemas\nezi no lineales. esta sesio'n trato' tanto el potencial como los\nσ(z) = para i=1,...,k\ni (cid:80)k ezj desaf'ıos de las redes neuronales, entre ellos la maldicio'n de\nj=1 la dimensionalidad y la importancia de un disen˜o acorde al\nproblemaencuestio'nente'rminosdecapasyneuronas.as'ı,la\n\nv. perceptro'nmulticapa clase proporciono' las bases para comprender las arquitecturas\nmodernas de aprendizaje profundo.\nel perceptro'n multicapa (mlp, por sus siglas en ingle's) es"}
{"id_doc": "DOC_029", "segmentacion": "B", "chunk_id": "DOC_029_B_006", "idx": 6, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "una arquitectura fundamental dentro de las redes neuronales\nartificiales. esta' compuesto por una capa de entrada, una o references\nma's capas ocultas y una capa de salida. a diferencia del\n[1] k. wiggers, \"skild ai emerges from stealth with\nperceptro'n simple, que solo puede resolver problemas lineal- $300m to build a general-purpose ai brain for robots,\"\nmente separables, el mlp utiliza funciones de activacio'n no techcrunch, sep. 2025, accessed: 2025-10-02. [online]. available: https://techcrunch.com/2025/09/16/skild-ai-emerges-from-stealthlineales en sus neuronas ocultas, lo que le permite aproximar\nwith-300m-to-build-a-general-purpose-ai-brain-for-robots/\nfunciones complejas y resolver problemas no lineales. su [2] s.haykin,neuralnetworksandlearningmachines,3rded. prentice\nentrenamiento se realiza comu'nmente mediante el algoritmo hall,2009.\n[3] f. rosenblatt, \"the perceptron: a probabilistic model for information\nderetropropagacio'n(backpropagation),elcualajustalospesos\nstorage and organization in the brain,\" psychological review, vol. 65,\ndelasconexionesminimizandoelerrorentrelasalidapredicha no.6,pp.386-408,1958.\ny la deseada. esta arquitectura constituye la base de los [4] s.j.russellandp.norvig,artificialintelligence:amodernapproach,\n3rded. prenticehall,2010.\nmodelos modernos de aprendizaje profundo [6], [9].\n[5] m. minsky and s. a. papert, perceptrons: an introduction to computationalgeometry. mitpress,1969.\n[6] i.goodfellow,y.bengio,anda.courville,deeplearning. mitpress,\na. maldicio'n de dimensionalidad 2016.\n[7] v.nairandg.e.hinton,\"rectifiedlinearunitsimproverestrictedboltzamayorcantidaddedimensiones,aumentalacomplejidad, mann machines,\" in proceedings of the 27th international conference\na su vez, aumentando la computabilidad y se vuelve ma's onmachinelearning(icml),2010.\n[8] a. l. maas, a. y. hannun, and a. y. ng, \"rectifier nonlinearities\ncomplicado encontrar patrones. para esto existen algoritmos\nimprove neural network acoustic models,\" in proceedings of the 30th\nde deduccio'n de dimensiones como el pca. internationalconferenceonmachinelearning(icml),2013.\n4\n[9] d. e. rumelhart, g. e. hinton, and r. j. williams, \"learning representationsbyback-propagatingerrors,\"nature,vol.323,no.6088,pp.\n533-536,1986.\n[10] i.t.jolliffeandj.cadima,\"principalcomponentanalysis:areviewand\nrecent developments,\" philosophical transactions of the royal society\na:mathematical,physicalandengineeringsciences,vol.374,no.2065,\np.20150202,2016.\n[11] c. m. bishop, pattern recognition and machine learning. springer,\n2006.\n[12] j.shlens,\"atutorialonprincipalcomponentanalysis,\"arxivpreprint"}
{"id_doc": "DOC_029", "segmentacion": "B", "chunk_id": "DOC_029_B_007", "idx": 7, "autor": "José Manuel Rodríguez Gómez", "fecha": "2025-09-25", "tema": "Conceptos avanzados de redes neuronales: funciones de activación (ReLU, tanh, sigmoide, softmax) y backpropagation, con repaso de su implementación matemática y papel en el aprendizaje profundo.", "texto": "arxiv:1404.1100,2014."}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_000", "idx": 0, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "apuntes de clase\n\ninteligencia artificial - semana 9 - 02 de octubre\npriscilla jime'nez salgado\nescuela de ingenier'ıa en computacio'n, tecnolo'gico de costa rica\ncartago, costa rica - 2021022576@estudiantec.cr\n\nabstract-este documento hace un repaso general a diferencia del primer sora, que ten'ıa resultados\ny claro sobre las funciones de activacio'n ma's poco realistas, esta nueva versio'n produce videos\nutilizadas en las redes neuronales, adema's de ma's naturales y coherentes, adema's de incluir audio\nexplicar conceptos importantes sobre co'mo esta'n gracias a su capacidad multimodal. el profesor\ndisen˜adas y co'mo han evolucionado las redes mostro' un ejemplo hecho con la herramienta y\nneuronales artificiales. se presentan funciones explico' que incluso podr'ıa usarse para presentacomorelu,sigmoideysoftmax,entreotras,con ciones acade'micas. tambie'n se menciono' la nueva\nsu base matema'tica. tambie'n se repasa el con- aplicacio'n \"sora by openai\", una plataforma donde\ncepto de perceptro'n y las redes multicapa, y se laspersonaspuedencrearycompartirvideosconincomentan algunos retos cla'sicos en el a'rea, como teligencia artificial a partir de simples descripciones\nel problema del xor y la llamada \"maldicio'n de o prompts.\nla dimensionalidad\"."}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_001", "idx": 1, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "i. reviewdelalectura\nen claseel profesorcomento' deforma muyba'sica\nla lectura from language to action: a review\n\nof large language models as autonomous agents\nand tool users. sen˜alo' que lo importante para\nfig. 1: sora by openai\nel pro'ximo quiz del martes es entender lo esencial: los modelos de lenguaje (llms) ya no solo\n\nii. aspectosadministrativos\ngeneran texto, sino que tambie'n funcionan como\nagentes auto'nomos capaces de razonar, planificar, el profesor compartio' las notas de los trabajos\nusar memoria e interactuar con herramientas ex- pendientes y brindo' retroalimentacio'n individual a\nternas. la lectura diferencia entre sistemas de un cada grupo de trabajo. sin embargo, au'n queda por\nsolo agente y sistemas multi-agente, donde varios entregar la calificacio'n del quiz 4 realizado y de\nmodelos colaboran para resolver problemas ma's la tarea presentada el pasado mie'rcoles, que esta'n\ncomplejos. adema's, se destacan sus aplicaciones pendientes de revisio'n. adema's, se indico' que la\nen investigacio'n, programacio'n, salud, robo'tica y pro'xima semana se asignara' el proyecto del curso.\nsimulaciones, as'ı como los principales retos, entre\na. repaso\nellos la memoria limitada, la seguridad, la e'tica y\nla necesidad de mejores evaluaciones. - el perceptro'n: puede entenderse de forma similar a una regresio'n log'ıstica, aunque se diferencia en\na. noticias de la semana la funcio'n de pe'rdida que utiliza. durante la historia\nen clase se hablo' del lanzamiento de sora 2, el de la inteligencia artificial se produjo el llamado\nnuevo modelo de generacio'n de video creado por \"invierno de la ia\", en parte debido al problema\nopenai como respuesta al nano banana de google. del xor, ya que este no pod'ıa ser representado\nadecuadamenteporunmodeloderegresio'nlog'ıstica tienenlacapacidaddeabordarproblemasnolineales.\nni por un perceptro'n simple. gracias a ello, se ampl'ıa significativamente el rango\ndeproblemasquepuedenresolverseconesteme'todo.\n- prediccio'n de compuertas lo'gicas:\n- inspiracio'n biolo'gica:\nfig. 2. compuertas lo'gicas\nen la figura se ilustran las compuertas lo'gicas or\ny and mediante gra'ficos bidimensionales.\n-\nor(𝑋\n1\n,𝑋 2):lostria'ngulosindicanlasalida1y\nfig. 4. inspiracio'n biolo'gica\nlosc'ırculoslasalida0.estacompuertadevuelve\n1 siempre que al menos una de las entradas sea lasredesneuronalesseinspiranenco'mofuncionan\niguala1. las neuronas en nuestro cerebro. cada neurona esta'\n- and(𝑋 1 ,𝑋 2): corresponde a una compuerta conectadaconotrasatrave'sdesusdendritas,yenel\nand donde la primera entrada esta' negada. la nu'cleoesdondeseprocesalainformacio'n.\nsalida es 1 (tria'ngulo) u'nicamente cuando la si lo comparamos con una regresio'n log'ıstica, las\nprimeraentradaes0ylasegundaes1. dendritas ser'ıan como las entradas de datos (inputs),\n- and(𝑋 1 ,𝑋 2): representa la compuerta and yelnu'cleorepresentar'ıalafuncio'nlinealqueprocesa\ncon la segunda entrada negada. el resultado es esa informacio'n. al final, la neurona decide si deja\n1(tria'ngulo)solocuandolaprimeraentradaes1 pasaronoesasen˜al.\nylasegundaes0.\nen cada gra'fico, la l'ınea punteada marca el l'ımite - funciones de activacio'n: en la regresio'n\nde decisio'n que distingue entre las dos clases de log'ısticaesatransformacio'nseconocecomofuncio'n\nsalida (0 y 1). esta representacio'n facilita la com- nolineal,espec'ıficamentelasigmoide.segu'nlasen˜al\nprensio'n de co'mo las compuertas lo'gicas realizan la recibida, la neurona se activa o no, permitiendo que\nclasificacio'n de sus entradas en un espacio bidimen- lainformacio'ncontinu'e,latransformeolabloquee.\nsional.\n- problema del xor:\nfig. 5. funciones de activacio'n\n- funcio'n relu: la funcio'n 𝑔(𝑥) = max(0,𝑥)\nesta' limitada por debajo de cero y es estrictafig. 3. problema del xor mente creciente. es muy eficiente en modelos\nel principal inconveniente es que el problema no de deep learning, pero presenta el problema\nes linealmente separable, por lo que el algoritmo de las llamadas neuronas muertas, ya que no es\ndel perceptro'n simple no pod'ıa ofrecer una solucio'n derivableentodoslospuntosy,enalgunoscasos,\nadecuada. es en este punto donde surgen las redes el gradiente puede llegar a ser cero, impidiendo\nneuronales o perceptrones multicapa, ya que estos s'ı laactualizacio'ndelospesos.\n- funciones selu y elu: son de la misma\nfamilia.aunquerequierenmayorcostocomputacional,ofrecenunrendimientomuyeficiente.\n- funcio'n sigmoide: convierte la entrada en un\nvalorentre0y1.esmuyusadacuandosenecesita\ninterpretarlassalidascomoprobabilidades.\nfig. 6. ejemplo relu\n- leaky relu: esta funcio'n asigna una pequen˜a\n- perceptro'n multicapa (mlp):\nconstante al valor m'ınimo permitido, lo que"}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_002", "idx": 2, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "ayuda a evitar el problema de las neuronas\nmuertas.aunquerepresentaunamejorarespecto\na la relu original, no se considera la solucio'n\ndefinitiva.\n(cid:40)\n0.01𝑥, 𝑥 < 0\n𝑔(𝑥) =\n𝑥, 𝑥 ≥ 0\nfig. 8. perceptro'n\n(cid:40)\n𝜕𝑔(𝑥) 0.01, 𝑥 < 0 elperceptro'nmulticapa(mlp)esunaevolucio'n\n=\n𝜕𝑥 1, 𝑥 ≥ 0 delperceptro'nsimplequepermiteresolverproblemas\nma's complejos, especialmente aquellos que no son\nlinealmenteseparables.\nel profesor lo explico' de manera sencilla con la\nimagen: en la capa de entrada (input layer) se\nencuentran los datos originales, representados como\nfig. 7. ejemplo leaky relu\n𝑋 , que no cambian porque son las entradas del\n- parametric relu (prelu): esta funcio'n per- 𝑖\nmite aprender un para'metro que controla si sistema. luego aparecen las capas ocultas (hidden\nlayers), que son las responsables de realizar los\nla sen˜al continu'a en la parte negativa. dicho\nca'lculos, transformaciones y operaciones internas,\npara'metroseentrenajuntoconelrestodelared,\nda'ndole a la red la capacidad de aprender relaciones\nloquebrindamayorflexibilidadalmodelo.\nma's complejas. finalmente, esta' la capa de salida\n(cid:40) (output layer), que entrega el resultado final y\n𝑤𝑥, 𝑥 < 0\n𝑔(𝑥) = cuyo taman˜o depende del problema que se este'\n𝑥, 𝑥 ≥ 0\nresolviendo.\n(cid:40) la gran ventaja del mlp es que, gracias a sus\n𝜕𝑔(𝑥) 𝑤, 𝑥 < 0\n= mu'ltiples capas y funciones de activacio'n, introduce\n𝜕𝑥 1, 𝑥 ≥ 0\nno linealidad, lo que le permite resolver problemas\n- funcio'n tanh: tiene una forma parecida a la que el perceptro'n simple no pod'ıa. adema's, se ensigmoide,perosusalidaesta' acotadaenelrango trenautilizandolapropagacio'n del error(backprop-\n(-1,1),loquepermitemanejarvalorespositivos agation),queconsisteencalcularcua'ntoseequivoco'\nynegativos. la red y ajustar los pesos mediante descenso de\n- binary step function:devuelve1silaentrada gradiente,mejorandoas'ıelrendimientodelmodelo.\nesmayorqueceroy0siesmenoroigualacero.\n- funcio'n lineal:ba'sicamentedejapasarlasalida ahora nos preguntamos, ¿co'mo se calcula una\nsinaplicarningunatransformacio'nadicional. pasadaenlared?\nel proceso comienza con la expresio'n ℎ(0) = tarea:softmaxsisetratadeunaclasificacio'nmu'ltiple,\n𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑋𝑊0 + 𝑏0), donde ℎ(0) corresponde a la olinealsiesunproblemaderegresio'n.loimportante\nprimera capa oculta. lo que se hace es calcular esqueseaunafuncio'nnolineal,yaqueesoesloque\nprimero la regresio'n lineal 𝑋𝑊0+ 𝑏0, luego aplicar ledaalaredlacapacidadderesolver\nlafuncio'nsigmoidealresultado,yconesoseobtiene\nelvalordelprimerhidden layer.\ndespue's, para la siguiente capa oculta, el procedimiento es pra'cticamente el mismo: ℎ(1) =\n𝑠𝑖𝑔𝑚𝑜𝑖𝑑(ℎ(0)𝑊1 + 𝑏1). en este caso, el valor de fig. 11. capa de salida\nℎ(0) pasaaserlaentradadelasiguientecapa.\nestemismoprocesoserepitehastallegaralau'ltima - funcio'n costo: esunafuncio'n matema'ticaque\ncapa, que se expresa como ℎ(𝑛) = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(ℎ(𝑛 - calcula el nivel de error del modelo, y cuyo objetivo\n1)𝑊𝑛+𝑏𝑛). principalesminimizardichoerrorduranteelproceso\nen otras palabras, cada capa oculta toma como deentrenamiento.\nentrada el resultado de la capa anterior, y mediante\nuna combinacio'n lineal ma's la activacio'n, se van"}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_003", "idx": 3, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "construyendo paso a paso los valores hasta la salida\nfinaldelared.\n-salidaindependienteydistribucio'n: cadasalida\npuede asociarse a una variable distinta. segu'n el fig. 12. funcio'n de costo\ncaso, la distribucio'n puede ser de tipo catego'rica\n- maldicio'n de dimensionalidad: pasacuandotra-\n(como en el uso de softmax) o continua (como en\nbajamos con datos que tienen much'ısimas variables\nunaregresio'n).\no dimensiones. al ir aumentando esas dimensiones,\n\nlos datos empiezan a dispersarse y quedan muy\nseparados entre s'ı, lo que hace ma's dif'ıcil encontrar\npatrones claros. en otras palabras, el modelo tiene\nque calcular en un espacio cada vez ma's grande y\nconmenosdensidaddeinformacio'n,loquecomplica\nfig. 9. salida independiente\nelaprendizaje.\nfig. 10. distribucio'n fig. 13. maldicio'n de la dimensionalidad\n- capa de salida: es la parte final de la red y se - comportamiento jera'rquico: se utiliza este\ncalcula con la fo'rmula ℎ(𝑛) = 𝑔(ℎ(𝑛 -1)𝑊𝑛 + 𝑏𝑛). enfoque porque imita la forma en que los humanos\nba'sicamente, lo que hace es tomar la salida de aprenden: comienzancon conceptos simplesy luego\nla u'ltima capa oculta, multiplicarla por los pesos, los combinan para formar ideas ma's complejas.\nsumarle un sesgo y luego pasarla por una funcio'n esto permite generar mejoras exponenciales en las\nde activacio'n. esa funcio'n 𝑔(𝑥) no siempre es la funcionesyaprovecharmejorelaprendizaje.\nsigmoide, puede ser otra dependiendo del tipo de permiteconstruirfuncionespolino'micas.\n-\n- utilizalacomposicio'ndefunciones,reutilizando iii. continuacio'ndefuncionesdeactivacio'n\nfuncionessimplesparacrearotrasdemayornivel.\nlas funciones de activacio'n son un elemento\nofrece una representacio'n compacta, donde con\n- fundamentalenlasredesneuronales,yaquepermiten"}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_004", "idx": 4, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "pocos pesos se pueden modelar funciones comintroducirlanolinealidadnecesariapararepresentar\nplejas.\nrelaciones complejas en los datos. a continuacio'n,\n-\nejemplo:unaredneuronalpuedeaproximarotra\nsepresentanlasfuncionesma'simportantesjuntocon\nfuncio'n.\nsusprincipalescaracter'ısticasmatema'ticas.\nfig. 14. comportamiento jera'rquico\nfig. 17. ejemplos de funciones de activacio'n: a la izquierda la\nfuncio'n lineal y a la derecha la funcio'n tangente hiperbo'lica (tanh),\nusada en redes neuronales para introducir no linealidad.\n- mapas de caracter'ısticas en cnn: en una red\nneuronalconvolucional(cnn),lascapasnotrabajan\na. funcio'n lineal\nsolo con los p'ıxeles, sino que van aprendiendo repla funcio'n lineal se define como 𝑓(𝑥) = 𝑥. la\nresentaciones cada vez ma's complejas de la imagen.\nderivadaesconstante,porloqueelmodelonopuede\nal inicio, en las primeras capas, se detectan cosas\n\nusar el descenso del gradiente ni aprender de los\nmuy ba'sicas como bordes o l'ıneas. luego, en las\ndatos.\ncapas intermedias, ya aparecen formas un poco ma's\nclarascomopartesdeojosobocas.finalmente,enlas\nu'ltimas capas, la red es capaz de reconocer objetos\ncompletos,porejemplounrostro.\nfig. 18. funcio'n lineal\nfig. 15. extraccio'n progresiva de caracter'ısticas en una cnn\n- representaciones vectoriales: en procesamiento de lenguaje natural, las palabras se representancomovectoresdealtadimensio'n,estopermite"}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_005", "idx": 5, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "que palabras con funciones similares se agrupen en\nelespaciovectorial.\nfig. 19. ejemplo\nb. sigmoide\ntiene una activacio'n que var'ıa entre 0 y 1, siempre positiva, acotada y estrictamente creciente. sin\nembargo,presentaelproblemadequesuderivadase\nfig. 16. visualizacio'n\naproximaaceroenlosextremosdelafuncio'n,loque\nprovoca gradientes muy pequen˜os. esto hace que el\nentrenamientosevuelvalentoosedetenga,feno'meno\nconocidocomovanishing gradient.\nfig. 22. ejemplo softmax\n- ¿por que' usar 𝑒𝑥 ? porque es una funcio'n\n\nestrictamente creciente y evita valores negativos\nenlasalida.\n- cross-entropy loss: tambie'n llamada logfig. 20. ejemplo loss o logistic loss, se utiliza como funcio'n de\npe'rdida en softmax. representa probabilidades\nc. tangente hiperbo'lica\nenunespaciologar'ıtmicodentrodelrango [0,1]\nyesnume'ricamenteestable.\nla funcio'n tanh tiene un rango de valores entre\nlape'rdidasedefinecomo:\n-1y1.sucomportamientoessimilaraldelafuncio'n\nsigmoide, con la diferencia de que esta' centrada en\nel origen, lo que permite que los valores negativos 𝐿 = log(𝑃(𝑌 = 𝑦 𝑖 | 𝑋 = 𝑥 𝑖 ))\ntambie'n sean considerados. sin embargo, al igual\nyenelcasodeclasificacio'nmulticlase:\nque la sigmoide, presenta el problema del gradiente\ndesvanecido en los extremos, lo que puede dificultar (cid:32) (cid:33)\n𝑒𝑠\n𝑘\nelentrenamientoderedesprofundas. 𝐿 = -log\n(cid:205)𝐶 𝑒𝑠\n𝑗\n𝑗=1\nfig. 21. ejemplo\nfig. 23. ejemplo\nd. funcio'n softmax e. ¿cua'l funcio'n de activacio'n utilizar?\nla funcio'n softmax convierte la capa de salida la eleccio'n de la funcio'n de activacio'n depende\n(output layer) en una distribucio'n de probabilidad, del tipo de problema que se este' resolviendo. las\nyaquenormalizalosvaloresmedianteunasumatoria. funciones sigmoid y tanh suelen presentar el insudefinicio'neslasiguiente: conveniente del vanishing gradient, lo que dificulta\nel entrenamiento en redes profundas. por ello, se\n𝑒𝑥 𝑗 recomienda iniciar con la funcio'n relu, ya que es\n𝜎(𝑥) 𝑗 = (cid:205)𝐾 𝑒𝑥 ra'pida de calcular y ampliamente utilizada en deep\n𝑘\n𝑘=1\nlearning. en caso de que no funcione adecuadaes comu'nmente utilizada en problemas de clasifimente, se pueden emplear variantes como leaky\ncacio'n, donde el vector de entrada se conoce como\nreluoparametric relu,quebuscansuperarestas\nlogits. adema's, se emplea junto con la funcio'n de\nlimitaciones.\npe'rdidacross-entropy loss."}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_006", "idx": 6, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "iv. backpropagation 2) salida: 𝑎𝑙 = 𝑔(𝑧𝑙) donde 𝑔 es nuestra\nfuncio'ndeactivacio'n.\npermite calcular cua'nto contribuye cada peso al\nerror final de la red, actualizando los para'metros en\nvamosaactualizarlospara'metrosde𝑧𝑙,queson𝑤𝑙\ndireccio'n opuesta a la propagacio'n hacia adelante.\ny 𝑏𝑙. para esto emplearemos la regla de la cadena,\n\neste proceso es esencial para que la red aprenda y\nusando la salida de la activacio'n de la capa anterior.\nmejoresudesempen˜oduranteelentrenamiento.\nprofundizando a nivel de neurona, se muestra la\nsiguientefigura.\na. procesos del entrenamiento\n- forward propagation: consiste en calcular la\n\nsalida de la red enviando los datos desde la\ncapa de entrada hacia las capas siguientes, hasta\nobtenerelresultadofinal.\n- backpropagation: implica propagar el error\ndesdelacapadesalidahacialascapasanteriores,\n\ncalculando las derivadas parciales con respecto\na los pesos y sesgos para ajustar los para'metros\ndelmodelo.\nfig. 26. grafo de la capa al y li a detalle\nfig. 24. forward y back propagation c. vector gradiente\nb. optimizacio'n del grafo"}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_007", "idx": 7, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "en este ejemplo se considera una red neuronal en\n\nel vector gradiente se define como el conjunto\nla que cada capa contiene u'nicamente una neurona,\nde derivadas parciales de los para'metros (pesos y\nsuponiendo que la funcio'n de activacio'n utilizada es\nsesgos) de la red neuronal. al calcularlo, es comu'n\nlasigmoide,comosemuestraenlafigura??.\nencontrar operaciones repetidas, lo que se aprovecha\n\nen el algoritmo de backpropagation para optimizar\nlosca'lculos.\nfig. 25. grafo de la red neuronal\ndenominamos a las capas antes de 𝐿 , 𝑎𝑙 hasta\n- 𝑖\n𝑎𝑙-𝑛.\ndefinimoselmsecomo:\n-\n𝐿 𝑖 = (𝑎𝑙 - 𝑦 𝑖 ) 2\ndividimoslaneuronaen2capas:\n-\n1) entrada: 𝑧𝑙 = 𝑤𝑙𝑎𝑙-1 + 𝑏𝑙 donde 𝑎𝑙-1\ncorrespondealosinputs𝑥. fig. 27. vector gradiente\nd. mu'ltiples neuronas e. ca'lculo de funcio'n de perdida\nparaestaseccio'n,ellossglobalseobtienesumando"}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_008", "idx": 8, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "las diferencias entre la salida de cada neurona en\nla capa de activacio'n 𝑗 y su valor esperado 𝑦 ,\n𝑗\nrecorriendotodaslasneuronasdelacapa 𝑙.\n𝑛𝑙\n𝐿 𝑖 = ∑︁ (𝑎( 𝑗 𝑙) - 𝑦 𝑗 ) 2\n𝑗=1\n\nen la siguiente figura se muestra un ejemplo\ndonde se evalu'a la salida de una capa de activacio'n\nfig. 28. grafo con mayor dimensionaldad utilizandoestafuncio'ndepe'rdida.\n- super'ındice: sen˜ala la capa a la que pertenece\nunavariable.ejemplo:𝑎(𝑙) correspondealacapa\n𝑙.\n- sub'ındice: identifica el nu'mero de neurona\ndentro de una capa espec'ıfica. ejemplo: 𝑎(𝑙) se\n𝑗\nrefiereala 𝑗-e'simaneuronaenlacapa 𝑙. fig. 30. ejemplo\n- pesos: se representan con dos sub'ındices: el\n\ncambios a la regla de la cadena\n\nprimero indica la neurona destino y el segundo\nlaneuronadeorigen.ejemplo:𝑤(𝑙) representael como las funciones 𝐿 𝑖 , 𝑧( 𝑗 𝑙) y 𝑎( 𝑗 𝑙) han sido mod𝑗,𝑘\nificadas, es necesario plantear nuevas derivadas que\npesoqueconectalaneurona𝑎(𝑙-1) conlaneurona\n𝑘 permitan actualizar los para'metros de cada neurona\n𝑎(𝑙).\n𝑗 𝑗.\na continuacio'n, en la siguiente figura se ilustra\nen la ecuacio'n se observa que, para ajustar un\nco'mounaneuronadelacapa 𝑙 recibeentradasdesde pesoespec'ıfico𝑤(𝑙),debemoscalcularsusderivadas\n𝑗,𝑘\nvarias neuronas de la capa anterior (𝑙 - 1). este parciales.sinembargo,graciasalconceptodecache',\nprocesosepuededividirendospasos:\nla u'nica derivada que cambia al actualizar un peso\n- preactivacio'n: diferente es 𝛿𝑧( 𝑗 𝑙) , mientras que el resto permanece\n𝑛𝑙-1\n𝛿𝑤(\n𝑗\n𝑙\n,\n)\n𝑘\n𝑧(𝑙)\n=\n𝑏(𝑙)\n+\n∑︁ 𝑤(𝑙)𝑎(𝑙-1) constanteparatodalacapa.\n𝑗 𝑗 𝑗,𝑘 𝑘 lasderivadasseexpresandelasiguienteforma:\n𝑘=1\ndonde 𝑏(𝑙) es el sesgo de la neurona y 𝑤(𝑙) los\n𝑗 𝑗,𝑘 𝛿𝐿\npesosdeconexio'n. 𝛿𝑎𝑙 𝑖 = ((𝑎 1 𝑙 - 𝑦 1 ) 2 + (𝑎 2 𝑙 - 𝑦 2 ) 2 +---+ (𝑎𝑙 𝑛 - 𝑦 𝑛 ) 2 )\n- activacio'n: 𝑗\n𝑎( 𝑗 𝑙) = 𝑔(𝑧( 𝑗 𝑙) ) 𝛿 𝛿 𝑎 𝐿 𝑙 𝑖 = 2(𝑎𝑙 𝑗 - 𝑦 𝑗 )\ndonde 𝑔 representa la funcio'n de activacio'n 𝑗\naplicada. 𝛿𝑎(𝑙)\n𝑗\n=\n𝑔(𝑧(𝑙) )(1-𝑔(𝑧(𝑙)\n))\n𝛿𝑧(𝑙) 𝑗 𝑗\n𝑗\n𝛿𝑧(𝑙)\n𝑗\n=\n𝑎(𝑙-1)\n𝛿𝑤(𝑙) 𝑘\nfig. 29. ejemplo 𝑗,𝑘\ncon esto se logra actualizar los pesos de la capa 𝑙,\naunquelasderivadasnocambian,s'ıdebenmanejarse\nma's'ındicesamedidaquelaredcreceencomplejidad.\ncapa 𝑙-1\ncuando el ca'lculo debe extenderse hacia una capa\nanterior,comolacapa𝑙-1,elprocedimientosevuelve\nma's complejo. esto ocurre porque, segu'n el taman˜o\nde la siguiente capa, el algoritmo requiere combinar\nma's conexiones y para'metros, lo que incrementa la\ndificultaddelca'lculo.\n𝑛𝑙 𝛿𝑧(𝑙) 𝛿𝑎(𝑙)\n𝛿𝐿 𝛿𝐿\n𝑖 ∑︁ 𝑗 𝑗 𝑖\n=\n𝛿𝑎(𝑙-1) 𝛿𝑎(𝑙-1) 𝛿𝑧(𝑙) 𝛿𝑎(𝑙)"}
{"id_doc": "DOC_030", "segmentacion": "B", "chunk_id": "DOC_030_B_009", "idx": 9, "autor": "Javier Alonso Rojas Rojas", "fecha": "2025-10-02", "tema": "Fundamentos de agentes basados en modelos de lenguaje (LLM), comparación entre sistemas de agente único y multiagente, análisis de Sora 2 de OpenAI y repaso de redes neuronales, activaciones y backpropagation.", "texto": "𝑘 𝑗=1 𝑘 𝑗 𝑗"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_000", "idx": 0, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "apuntes inteligencia artificial, clase 02 de octubre\n\njavier alonso rojas rojas\nescuela de ingenier'ıa en computacio'n\ninstituto tecnolo'gico de costa rica\ncartago, costa rica\njavrojas@estudiantec.cr\n\nabstract-estos apuntes reflejan lo conversado en la clase\ndel 02 de octubre donde se mencionaron temas como. el funcionamiento de los agentes basados en modelos de lenguaje de\ngranescala(llm)ysupapelenlainteligenciaartificialmoderna.\n\nsemencionaronlasprincipalesherramientasyframeworkspara\nlacreacio'ndeagentes,juntoconlasdiferenciasentresistemasde\nunsoloagenteymultiagente.adema's,seexaminaelcasodesora\nde openai como ejemplo de modelo multimodal de generacio'n\ndevideoyaudio,considerandotambie'nlosretose'ticosasociados.\nfinalmente, se incluye un repaso de los fundamentos de las\nredes neuronales, sus funciones de activacio'n y el proceso de\nentrenamientomediantebackpropagation,comobaseconceptual\nde los llm actuales.\nfig.1. sora2\n\ni. introduction\nla inteligencia artificial (ia) ha avanzado ra'pidamente\ngracias a los modelos de lenguaje de gran escala (llms) y al iii. sora2byopenai\n\ndesarrollo de agentes inteligentes capaces de actuar y razonar\nen distintos contextos. estos sistemas han transformado la sora 2 es la nueva versio'n del modelo multimodal de\ngeneracio'ndetextoenunprocesodeplanificacio'nyejecucio'n openai, capaz de generar video y audio sincronizados a\nma's complejo, permitiendo la creacio'n de agentes auto'nomos partir de texto. presenta notables mejoras en realismo f'ısico,\nque integran herramientas y colaboran entre s'ı. coherencia visual y control creativo. su funcio'n de cameos\nlos apuntes abordan los fundamentos y la arquitectura de permite insertar la imagen y voz del usuario, bajo consenlos agentes basados en llm, distinguiendo entre sistemas timiento, ampliando las posibilidades narrativas y expresivas.\nindividualesymultiagente,ydestacandoelpapeldelchainof adema's, han desarrollado un tipo de red social donde se\nthought (cot) como mecanismo clave para el razonamiento pueden compartir los videos creados con el modelo.\nestructurado. adema's, se incluye el caso de estudio sora de el modelo ofrece mayor precisio'n en iluminacio'n,\nopenai y un repaso de las bases neuronales del aprendizaje movimientoysonido,adema'sdeopcionesdecontrolestil'ıstico\nprofundo,comolasfuncionesdeactivacio'nyelentrenamiento mediante steerability. openai ha incorporado medidas e'ticas\npor backpropagation. para evitar la reproduccio'n de personas reales o la generacio'n\nde contenido sensible, lanza'ndolo de forma gradual mediante"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_001", "idx": 1, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "ii. mencio'ndelecturadeagentes\nla sora app y futuras apis. sora 2 representa un avance sigse hizo un repaso general de la lectura \"from language to nificativoenlageneracio'naudiovisualconia,aunqueplantea\naction:areviewoflargelanguagemodelsasautonomous retos importantes en materia de privacidad y autenticidad\nagents and tool users\". explico' que lo fundamental para digital.\nel pro'ximo quiz del martes es comprender lo esencial: los\nmodelos de lenguaje (llms) han pasado de ser simples\n\niv. repasoderedesneuronales\ngeneradores de texto a actuar como agentes auto'nomos con\ncapacidad para razonar, planificar, usar memoria e interactuar\na. el perceptro'n y su evolucio'n\ncon herramientas externas. la lectura tambie'n distingue entre\nsistemas de un solo agente y sistemas multiagente, en los el perceptro'n puede entenderse de forma similar a una\nque varios modelos cooperan para resolver tareas ma's com- regresio'n log'ıstica, aunque se diferencia en la funcio'n de\nplejas. adema's, se analizan sus aplicaciones en a'reas como pe'rdida que utiliza. durante la historia de la inteligencia\nla investigacio'n, la programacio'n, la salud, la robo'tica y las artificialsurgio' elllamado\"inviernodelaia\",enpartedebido\nsimulaciones,juntoconlosprincipalesdesaf'ıosqueenfrentan: al problema del xor, ya que este no pod'ıa ser representado\nla memoria limitada, la seguridad, la e'tica y la necesidad de adecuadamente por un modelo lineal ni por un perceptro'n\nmejores me'todos de evaluacio'n. simple.\nfig.3. comportamientojera'rquico\nfig.2. funcionesdeactivacio'n\nb. el problema del xor\nel principal inconveniente del perceptro'n simple es que el\nproblema xor no es linealmente separable, por lo que este\nmodelo no puede ofrecer una solucio'n adecuada. esto dio\nfig.4. funcionamientodelascnn\norigen a las redes neuronales multicapa (mlp), capaces de"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_002", "idx": 2, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "resolver problemas no lineales y ampliar significativamente el\nrango de aplicaciones posibles. cada capa se calcula de la siguiente forma:\nh(0) =σ(xw +b ) (1)\n0 0\nc. inspiracio'n biolo'gica\nh(1) =σ(h(0)w +b ) (2)\n1 1\nlas redes neuronales artificiales se inspiran en el funh(n) =g(h(n-1)w +b ) (3)\ncionamientodelcerebrohumano.cadaneuronarecibesen˜ales n n\na trave's de sus dendritas (entradas), las procesa en el nu'cleo f. capas de salida y distribucio'n\nmediante una combinacio'n lineal, y decide si transmite o no las salidas pueden ser catego'ricas o continuas:\nla sen˜al segu'n una funcio'n de activacio'n.\n- en clasificacio'n, se usa softmax como funcio'n de salida.\n- en regresio'n, se emplea una funcio'n lineal.\nd. funciones de activacio'n\nen todos los casos, la activacio'n final g(x) debe ser no lineal\npara permitir un aprendizaje ma's expresivo.\nlas funciones de activacio'n introducen no linealidad en el\nmodelo, permitiendo que la red aprenda relaciones complejas: g. maldicio'n de la dimensionalidad\n- relu: g(x) = max(0,x); eficiente, pero puede generar cuandosetrabajacondatosdemuchasvariables,lospuntos\n\"neuronas muertas\" cuando el gradiente es cero. se dispersan en un espacio de alta dimensio'n, reduciendo su\n- leaky relu: introduce una pequen˜a pendiente en la densidad y dificultando el hallazgo de patrones significativos.\nparte negativa para evitar neuronas inactivas.\nh. comportamiento jera'rquico\n- tanh: produce salidas en el rango (-1,1), u'til para\nmanejar valores positivos y negativos. como vemos en la figura 3, las redes neuronales apren-\n- sigmoide: transforma la entrada en valores entre 0 y 1, den de forma jera'rquica, combinando funciones simples para\ncomu'n en tareas de clasificacio'n binaria. formarotrasma'scomplejas.estopermiteconstruirrepresentaciones compactas y eficientes, en las que un nu'mero reducido\nde pesos puede modelar funciones avanzadas.\ne. perceptro'n multicapa (mlp)"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_003", "idx": 3, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "i. cnn\nelmultilayerperceptron(mlp)extiendeelperceptro'nsimple an˜adiendo capas ocultas que permiten resolver problemas en las redes convolucionales (cnn), las primeras capas\nno lineales. su estructura general incluye: detectan bordes o patrones ba'sicos, las intermedias aprenden\nestructurasma'sdefinidasylasu'ltimascapasreconocenobjetos\n- capa de entrada: recibe los datos originales x i .\ncompletos, como rostros o figuras, esto representado en la\n- capas ocultas: realizan transformaciones y ca'lculos infigura 4.\nternos.\n- capa de salida: entrega el resultado final, cuyo taman˜o j. representaciones vectoriales\ndepende del tipo de problema.\nenelprocesamientodelenguajenatural(nlp),laspalabras\nel entrenamiento se realiza mediante backpropagation, que serepresentancomovectoresenunespaciodealtadimensio'n,\ncalcula el error del modelo y ajusta los pesos utilizando de modo que las palabras con significados o funciones simidescenso de gradiente. lares se ubican pro'ximas entre s'ı en dicho espacio.\nd. parametric relu (prelu)\nla funcio'n parametric relu (prelu) es una variante\nde la funcio'n relu tradicional, como se muestra en la\nfigura ??. a diferencia de la relu esta'ndar, esta introduce\nun para'metro α que se aprende durante el entrenamiento y\ncontrola la pendiente en la regio'n negativa. de esta manera,\nel modelo puede ajustar automa'ticamente el grado de \"fuga\"\nen los valores menores que cero, evitando el problema de las\nneuronas muertas.\nfig.5. tangentehiperbo'lica\nsu definicio'n matema'tica es la siguiente:\n(cid:40)\nαx, si x<0"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_004", "idx": 4, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "v. funcionesdeactivacio'n g(x)=\nx, si x≥0\nlas funciones de activacio'n son un componente esencial la derivada correspondiente es:\nen las redes neuronales, ya que permiten introducir la no (cid:40)\ndg(x) α, si x<0\nlinealidad necesaria para modelar relaciones complejas entre =\ndx 1, si x≥0\nlos datos. a continuacio'n, se describen las funciones ma's relevantes junto con sus principales caracter'ısticas matema'ticas. elpara'metroαseentrenajuntoconelrestodelospesosde\nlared,loqueotorgaalmodelomayorflexibilidadycapacidad\nde adaptacio'n frente a distintas distribuciones de datos. por\na. lineal\nesta razo'n, la prelu suele ofrecer un mejor desempen˜o en\nla funcio'n lineal se define como: arquitecturasprofundasdondelareluesta'ndarpodr'ıaperder\ngradiente.\nf(x)=ax\ne. softmax\nsu derivada es constante (f′(x) = a), por lo que el modelo la funcio'n softmax transforma las salidas de la capa final\nno puede aprovechar el descenso del gradiente para aprender en una distribucio'n de probabilidad, como vemos en la figura\npatrones complejos. debido a su cara'cter estrictamente lineal, 6. su expresio'n se define como:\nno introduce capacidad de generalizacio'n ni no linealidad en exj\nσ(x) =\nla red. j (cid:80)k exk\nk=1\ndondecadavalorx sedenominalogit.estafuncio'nseutiliza\nj\nb. sigmoide principalmente en problemas de clasificacio'n multiclase, ya\nque garantiza que todas las salidas sean positivas y sumen 1.\nlafuncio'nsigmoideproducesalidasentre0y1,essiempre\npositiva, acotada y estrictamente creciente: - el uso de ex asegura una funcio'n estrictamente creciente\ny evita valores negativos.\n1 - se emplea junto con la funcio'n de pe'rdida crossσ(x)=\n1+e-x entropy loss, tambie'n llamada log-loss.\nla pe'rdida se define como:\na pesar de su utilidad inicial, presenta el problema del\nl=-logp(y =y |x =x )\nvanishing gradient: la derivada tiende a cero en los extremos i i\nde la funcio'n, lo que hace que el aprendizaje sea lento o y, en el caso multiclase:\nincluso se detenga en redes profundas. esk\nl=-log\n(cid:80)c esj\nj=1\nc. tangente hiperbo'lica (tanh)\nf. seleccio'n de la funcio'n de activacio'n\nla funcio'n tangente hiperbo'lica tiene un rango de salida\nla eleccio'n de la funcio'n de activacio'n depende del tipo de\nentre(-1,1)comovemosenlafigura5ysuformaessimilar\nproblemaylaarquitecturadelared.lasfuncionessigmoidy\na la sigmoide, pero centrada en el origen:\ntanh tienden a sufrir el problema del gradiente desvanecido,\nex-e-x por lo que no son recomendadas para redes profundas. en\ntanh(x)= la pra'ctica, se suele comenzar con la funcio'n relu por\nex+e-x"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_005", "idx": 5, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "su eficiencia computacional y buen rendimiento en modelos\nesto permite representar tanto valores positivos como nega- de deep learning. si esta presenta problemas (por ejemplo,\ntivos,loquefacilitalaconvergenciadelmodelo.sinembargo, neuronas muertas), se pueden utilizar variantes como leaky\nal igual que la sigmoide, tambie'n sufre del problema del relu o parametric relu, que permiten mantener un flujo\ngradiente desvanecido en los extremos. de gradiente estable incluso en valores negativos.\nfig.6. usodesoftmax\nfig.8. redneuronalsimple\nfig.9. redneuronalmascompleja\ndonde g representa la funcio'n de activacio'n.\nfig.7. forwardpropagationybackpropagation\nlospara'metrosw(l) yb(l) seactualizanutilizandolaregla\ndelacadena,derivandoelerrorconrespectoacadapara'metro\n\nvi. backpropagation y aprovechando la salida de la capa anterior.\nel algoritmo de backpropagation permite calcular cua'nto\nd. vector gradiente\ncontribuye cada peso al error final de la red, actualizando los\nel vector gradiente esta' formado por todas las derivadas\npara'metros en la direccio'n opuesta a la propagacio'n hacia\nparcialesdelospara'metros(pesosysesgos)delared.durante\nadelante. este proceso es esencial para que la red neuronal\nel ca'lculo del gradiente se identifican operaciones repetidas,\naprendaymejoresudesempen˜oduranteelentrenamiento.esto\nloquepermiteoptimizarlosca'lculosenelalgoritmodebackrepresentado en la figura 7.\npropagation mediante reutilizacio'n de resultados intermedios\na. forward propagation (cache').\nconsiste en calcular la salida de la red, enviando los datos e. redes con mu'ltiples neuronas"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_006", "idx": 6, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "desde la capa de entrada hacia las capas ocultas hasta obtener\n\nen redes con mayor dimensionalidad como la de la figura\nla salida final.\n9, se introducen notaciones adicionales:\nb. backpropagation - super'ındice: indica la capa. ejemplo: a(l) representa la\nactivacio'n en la capa l.\n\nimplica propagar el error desde la capa de salida hacia las\n- sub'ındice: indica la neurona dentro de una capa. ejemcapas anteriores, calculando las derivadas parciales respecto a\nplo: a(l) es la j-e'sima neurona en la capa l.\nlos pesos y sesgos para ajustar los para'metros del modelo. j\n- pesos: se representan como w(l), que conecta la neurona\nj,k\nc. optimizacio'n del grafo computacional a(l-1) con a(l), aca j seria el destino y k el origen.\nk j\nconsideremos una red neuronal como la de la figura 8, cada neurona de la capa l recibe entradas desde todas las\ndonde cada capa contiene una u'nica neurona y la funcio'n de neuronas de la capa anterior (l-1), siguiendo los pasos:\nactivacio'nutilizadaeslasigmoide.elca'lculosepuededividir\n- preactivacio'n:\nen las siguientes partes:\n- funcio'n de pe'rdida (mse): z(l) =b(l)+\nn (cid:88)l-1\nw(l)a(l-1)\nj j j,k k\nl =(a(l)-y )2 k=1\ni i\n- activacio'n:\ndonde a(l) es la salida de la red y y\ni\nel valor esperado. a(l) =g(z(l))\nj j\n- entrada:\nz(l) =w(l)a(l-1)+b(l) para obtener la activacio'n de una neurona destino, se"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_007", "idx": 7, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "calculan las contribuciones de todas las neuronas de la capa\n- salida: anterior,multiplicandolospesosdeconexio'ncorrespondientes\na(l) =g(z(l)) por la activacio'n de cada neurona origen. posteriormente, se\nsumanestosproductosjuntoconelsesgoasociado,repitiendo\nel proceso para cada neurona de la capa.\nf. funcio'n de pe'rdida global\nla funcio'n de pe'rdida global se obtiene sumando las diferenciasentrelasalidadecadaneuronaenlacapadeactivacio'n\nl y su valor esperado y :\nj\nl = (cid:88)\nnl\n(a(l)-y )2\n\ni j j\nj=1\ng. aplicacio'n de la regla de la cadena\ndadoquelasfuncionesl ,z(l) ya(l) esta'nencadenadas,es\n\ni j j\n\nnecesario aplicar la regla de la cadena para derivar cada peso\nw(l) ysesgob(l).sololaderivada ∂z j (l) cambiaconcadapeso\nj,k j ∂w(l)\nj,k\nactualizado, mientras que las dema's se mantienen constantes\ndentro de la capa.\nlas derivadas parciales relevantes son:\n∂l\ni =2(a(l)-y )\n∂a(l) j j\nj\n∂a(l)\nj =g(z(l))(1-g(z(l)))\n∂z(l) j j\nj\n∂z(l)\nj =a(l-1)\n∂w(l) k\nj,k\napartirdeestasderivadas,lospesosysesgosseactualizan\nsiguiendo el descenso del gradiente:\n∂l ∂l\nw(l) ←w(l) -η i , b(l) ←b(l)-η i\nj,k j,k ∂w(l) j j ∂b(l)\nj,k j\ndonde η representa la tasa de aprendizaje.\nen redes ma's profundas, al extender el ca'lculo hacia capas\nanteriores (l-1), el nu'mero de para'metros y combinaciones"}
{"id_doc": "DOC_031", "segmentacion": "B", "chunk_id": "DOC_031_B_008", "idx": 8, "autor": "Rodolfo David Acuña López", "fecha": "2025-10-02", "tema": "Introducción a redes neuronales convolucionales (CNN) y algoritmo de backpropagation, con aplicación al proyecto de reconocimiento de voz mediante espectrogramas y análisis de técnicas de data augmentation.", "texto": "a derivar aumenta considerablemente, incrementando la complejidad computacional del algoritmo."}
