apuntes inteligencia artificial, clase 02 de octubre
javier alonso rojas rojas
escuela de ingenier'ıa en computacio'n
instituto tecnolo'gico de costa rica
cartago, costa rica
javrojas@estudiantec.cr
abstract-estos apuntes reflejan lo conversado en la clase
del 02 de octubre donde se mencionaron temas como. el funcionamiento de los agentes basados en modelos de lenguaje de
granescala(llm)ysupapelenlainteligenciaartificialmoderna.
semencionaronlasprincipalesherramientasyframeworkspara
lacreacio'ndeagentes,juntoconlasdiferenciasentresistemasde
unsoloagenteymultiagente.adema's,seexaminaelcasodesora
de openai como ejemplo de modelo multimodal de generacio'n
devideoyaudio,considerandotambie'nlosretose'ticosasociados.
finalmente, se incluye un repaso de los fundamentos de las
redes neuronales, sus funciones de activacio'n y el proceso de
entrenamientomediantebackpropagation,comobaseconceptual
de los llm actuales.
fig.1. sora2
i. introduction
la inteligencia artificial (ia) ha avanzado ra'pidamente
gracias a los modelos de lenguaje de gran escala (llms) y al iii. sora2byopenai
desarrollo de agentes inteligentes capaces de actuar y razonar
en distintos contextos. estos sistemas han transformado la sora 2 es la nueva versio'n del modelo multimodal de
generacio'ndetextoenunprocesodeplanificacio'nyejecucio'n openai, capaz de generar video y audio sincronizados a
ma's complejo, permitiendo la creacio'n de agentes auto'nomos partir de texto. presenta notables mejoras en realismo f'ısico,
que integran herramientas y colaboran entre s'ı. coherencia visual y control creativo. su funcio'n de cameos
los apuntes abordan los fundamentos y la arquitectura de permite insertar la imagen y voz del usuario, bajo consenlos agentes basados en llm, distinguiendo entre sistemas timiento, ampliando las posibilidades narrativas y expresivas.
individualesymultiagente,ydestacandoelpapeldelchainof adema's, han desarrollado un tipo de red social donde se
thought (cot) como mecanismo clave para el razonamiento pueden compartir los videos creados con el modelo.
estructurado. adema's, se incluye el caso de estudio sora de el modelo ofrece mayor precisio'n en iluminacio'n,
openai y un repaso de las bases neuronales del aprendizaje movimientoysonido,adema'sdeopcionesdecontrolestil'ıstico
profundo,comolasfuncionesdeactivacio'nyelentrenamiento mediante steerability. openai ha incorporado medidas e'ticas
por backpropagation. para evitar la reproduccio'n de personas reales o la generacio'n
de contenido sensible, lanza'ndolo de forma gradual mediante
ii. mencio'ndelecturadeagentes
la sora app y futuras apis. sora 2 representa un avance sigse hizo un repaso general de la lectura "from language to nificativoenlageneracio'naudiovisualconia,aunqueplantea
action:areviewoflargelanguagemodelsasautonomous retos importantes en materia de privacidad y autenticidad
agents and tool users". explico' que lo fundamental para digital.
el pro'ximo quiz del martes es comprender lo esencial: los
modelos de lenguaje (llms) han pasado de ser simples
iv. repasoderedesneuronales
generadores de texto a actuar como agentes auto'nomos con
capacidad para razonar, planificar, usar memoria e interactuar
a. el perceptro'n y su evolucio'n
con herramientas externas. la lectura tambie'n distingue entre
sistemas de un solo agente y sistemas multiagente, en los el perceptro'n puede entenderse de forma similar a una
que varios modelos cooperan para resolver tareas ma's com- regresio'n log'ıstica, aunque se diferencia en la funcio'n de
plejas. adema's, se analizan sus aplicaciones en a'reas como pe'rdida que utiliza. durante la historia de la inteligencia
la investigacio'n, la programacio'n, la salud, la robo'tica y las artificialsurgio' elllamado"inviernodelaia",enpartedebido
simulaciones,juntoconlosprincipalesdesaf'ıosqueenfrentan: al problema del xor, ya que este no pod'ıa ser representado
la memoria limitada, la seguridad, la e'tica y la necesidad de adecuadamente por un modelo lineal ni por un perceptro'n
mejores me'todos de evaluacio'n. simple.
fig.3. comportamientojera'rquico
fig.2. funcionesdeactivacio'n
b. el problema del xor
el principal inconveniente del perceptro'n simple es que el
problema xor no es linealmente separable, por lo que este
modelo no puede ofrecer una solucio'n adecuada. esto dio
fig.4. funcionamientodelascnn
origen a las redes neuronales multicapa (mlp), capaces de
resolver problemas no lineales y ampliar significativamente el
rango de aplicaciones posibles. cada capa se calcula de la siguiente forma:
h(0) =σ(xw +b ) (1)
0 0
c. inspiracio'n biolo'gica
h(1) =σ(h(0)w +b ) (2)
1 1
las redes neuronales artificiales se inspiran en el funh(n) =g(h(n-1)w +b ) (3)
cionamientodelcerebrohumano.cadaneuronarecibesen˜ales n n
a trave's de sus dendritas (entradas), las procesa en el nu'cleo f. capas de salida y distribucio'n
mediante una combinacio'n lineal, y decide si transmite o no las salidas pueden ser catego'ricas o continuas:
la sen˜al segu'n una funcio'n de activacio'n.
- en clasificacio'n, se usa softmax como funcio'n de salida.
- en regresio'n, se emplea una funcio'n lineal.
d. funciones de activacio'n
en todos los casos, la activacio'n final g(x) debe ser no lineal
para permitir un aprendizaje ma's expresivo.
las funciones de activacio'n introducen no linealidad en el
modelo, permitiendo que la red aprenda relaciones complejas: g. maldicio'n de la dimensionalidad
- relu: g(x) = max(0,x); eficiente, pero puede generar cuandosetrabajacondatosdemuchasvariables,lospuntos
"neuronas muertas" cuando el gradiente es cero. se dispersan en un espacio de alta dimensio'n, reduciendo su
- leaky relu: introduce una pequen˜a pendiente en la densidad y dificultando el hallazgo de patrones significativos.
parte negativa para evitar neuronas inactivas.
h. comportamiento jera'rquico
- tanh: produce salidas en el rango (-1,1), u'til para
manejar valores positivos y negativos. como vemos en la figura 3, las redes neuronales apren-
- sigmoide: transforma la entrada en valores entre 0 y 1, den de forma jera'rquica, combinando funciones simples para
comu'n en tareas de clasificacio'n binaria. formarotrasma'scomplejas.estopermiteconstruirrepresentaciones compactas y eficientes, en las que un nu'mero reducido
de pesos puede modelar funciones avanzadas.
e. perceptro'n multicapa (mlp)
i. cnn
elmultilayerperceptron(mlp)extiendeelperceptro'nsimple an˜adiendo capas ocultas que permiten resolver problemas en las redes convolucionales (cnn), las primeras capas
no lineales. su estructura general incluye: detectan bordes o patrones ba'sicos, las intermedias aprenden
estructurasma'sdefinidasylasu'ltimascapasreconocenobjetos
- capa de entrada: recibe los datos originales x i .
completos, como rostros o figuras, esto representado en la
- capas ocultas: realizan transformaciones y ca'lculos infigura 4.
ternos.
- capa de salida: entrega el resultado final, cuyo taman˜o j. representaciones vectoriales
depende del tipo de problema.
enelprocesamientodelenguajenatural(nlp),laspalabras
el entrenamiento se realiza mediante backpropagation, que serepresentancomovectoresenunespaciodealtadimensio'n,
calcula el error del modelo y ajusta los pesos utilizando de modo que las palabras con significados o funciones simidescenso de gradiente. lares se ubican pro'ximas entre s'ı en dicho espacio.
d. parametric relu (prelu)
la funcio'n parametric relu (prelu) es una variante
de la funcio'n relu tradicional, como se muestra en la
figura ??. a diferencia de la relu esta'ndar, esta introduce
un para'metro α que se aprende durante el entrenamiento y
controla la pendiente en la regio'n negativa. de esta manera,
el modelo puede ajustar automa'ticamente el grado de "fuga"
en los valores menores que cero, evitando el problema de las
neuronas muertas.
fig.5. tangentehiperbo'lica
su definicio'n matema'tica es la siguiente:
(cid:40)
αx, si x<0
v. funcionesdeactivacio'n g(x)=
x, si x≥0
las funciones de activacio'n son un componente esencial la derivada correspondiente es:
en las redes neuronales, ya que permiten introducir la no (cid:40)
dg(x) α, si x<0
linealidad necesaria para modelar relaciones complejas entre =
dx 1, si x≥0
los datos. a continuacio'n, se describen las funciones ma's relevantes junto con sus principales caracter'ısticas matema'ticas. elpara'metroαseentrenajuntoconelrestodelospesosde
lared,loqueotorgaalmodelomayorflexibilidadycapacidad
de adaptacio'n frente a distintas distribuciones de datos. por
a. lineal
esta razo'n, la prelu suele ofrecer un mejor desempen˜o en
la funcio'n lineal se define como: arquitecturasprofundasdondelareluesta'ndarpodr'ıaperder
gradiente.
f(x)=ax
e. softmax
su derivada es constante (f′(x) = a), por lo que el modelo la funcio'n softmax transforma las salidas de la capa final
no puede aprovechar el descenso del gradiente para aprender en una distribucio'n de probabilidad, como vemos en la figura
patrones complejos. debido a su cara'cter estrictamente lineal, 6. su expresio'n se define como:
no introduce capacidad de generalizacio'n ni no linealidad en exj
σ(x) =
la red. j (cid:80)k exk
k=1
dondecadavalorx sedenominalogit.estafuncio'nseutiliza
j
b. sigmoide principalmente en problemas de clasificacio'n multiclase, ya
que garantiza que todas las salidas sean positivas y sumen 1.
lafuncio'nsigmoideproducesalidasentre0y1,essiempre
positiva, acotada y estrictamente creciente: - el uso de ex asegura una funcio'n estrictamente creciente
y evita valores negativos.
1 - se emplea junto con la funcio'n de pe'rdida crossσ(x)=
1+e-x entropy loss, tambie'n llamada log-loss.
la pe'rdida se define como:
a pesar de su utilidad inicial, presenta el problema del
l=-logp(y =y |x =x )
vanishing gradient: la derivada tiende a cero en los extremos i i
de la funcio'n, lo que hace que el aprendizaje sea lento o y, en el caso multiclase:
incluso se detenga en redes profundas. esk
l=-log
(cid:80)c esj
j=1
c. tangente hiperbo'lica (tanh)
f. seleccio'n de la funcio'n de activacio'n
la funcio'n tangente hiperbo'lica tiene un rango de salida
la eleccio'n de la funcio'n de activacio'n depende del tipo de
entre(-1,1)comovemosenlafigura5ysuformaessimilar
problemaylaarquitecturadelared.lasfuncionessigmoidy
a la sigmoide, pero centrada en el origen:
tanh tienden a sufrir el problema del gradiente desvanecido,
ex-e-x por lo que no son recomendadas para redes profundas. en
tanh(x)= la pra'ctica, se suele comenzar con la funcio'n relu por
ex+e-x
su eficiencia computacional y buen rendimiento en modelos
esto permite representar tanto valores positivos como nega- de deep learning. si esta presenta problemas (por ejemplo,
tivos,loquefacilitalaconvergenciadelmodelo.sinembargo, neuronas muertas), se pueden utilizar variantes como leaky
al igual que la sigmoide, tambie'n sufre del problema del relu o parametric relu, que permiten mantener un flujo
gradiente desvanecido en los extremos. de gradiente estable incluso en valores negativos.
fig.6. usodesoftmax
fig.8. redneuronalsimple
fig.9. redneuronalmascompleja
donde g representa la funcio'n de activacio'n.
fig.7. forwardpropagationybackpropagation
lospara'metrosw(l) yb(l) seactualizanutilizandolaregla
delacadena,derivandoelerrorconrespectoacadapara'metro
vi. backpropagation y aprovechando la salida de la capa anterior.
el algoritmo de backpropagation permite calcular cua'nto
d. vector gradiente
contribuye cada peso al error final de la red, actualizando los
el vector gradiente esta' formado por todas las derivadas
para'metros en la direccio'n opuesta a la propagacio'n hacia
parcialesdelospara'metros(pesosysesgos)delared.durante
adelante. este proceso es esencial para que la red neuronal
el ca'lculo del gradiente se identifican operaciones repetidas,
aprendaymejoresudesempen˜oduranteelentrenamiento.esto
loquepermiteoptimizarlosca'lculosenelalgoritmodebackrepresentado en la figura 7.
propagation mediante reutilizacio'n de resultados intermedios
a. forward propagation (cache').
consiste en calcular la salida de la red, enviando los datos e. redes con mu'ltiples neuronas
desde la capa de entrada hacia las capas ocultas hasta obtener
en redes con mayor dimensionalidad como la de la figura
la salida final.
9, se introducen notaciones adicionales:
b. backpropagation - super'ındice: indica la capa. ejemplo: a(l) representa la
activacio'n en la capa l.
implica propagar el error desde la capa de salida hacia las
- sub'ındice: indica la neurona dentro de una capa. ejemcapas anteriores, calculando las derivadas parciales respecto a
plo: a(l) es la j-e'sima neurona en la capa l.
los pesos y sesgos para ajustar los para'metros del modelo. j
- pesos: se representan como w(l), que conecta la neurona
j,k
c. optimizacio'n del grafo computacional a(l-1) con a(l), aca j seria el destino y k el origen.
k j
consideremos una red neuronal como la de la figura 8, cada neurona de la capa l recibe entradas desde todas las
donde cada capa contiene una u'nica neurona y la funcio'n de neuronas de la capa anterior (l-1), siguiendo los pasos:
activacio'nutilizadaeslasigmoide.elca'lculosepuededividir
- preactivacio'n:
en las siguientes partes:
- funcio'n de pe'rdida (mse): z(l) =b(l)+
n (cid:88)l-1
w(l)a(l-1)
j j j,k k
l =(a(l)-y )2 k=1
i i
- activacio'n:
donde a(l) es la salida de la red y y
i
el valor esperado. a(l) =g(z(l))
j j
- entrada:
z(l) =w(l)a(l-1)+b(l) para obtener la activacio'n de una neurona destino, se
calculan las contribuciones de todas las neuronas de la capa
- salida: anterior,multiplicandolospesosdeconexio'ncorrespondientes
a(l) =g(z(l)) por la activacio'n de cada neurona origen. posteriormente, se
sumanestosproductosjuntoconelsesgoasociado,repitiendo
el proceso para cada neurona de la capa.
f. funcio'n de pe'rdida global
la funcio'n de pe'rdida global se obtiene sumando las diferenciasentrelasalidadecadaneuronaenlacapadeactivacio'n
l y su valor esperado y :
j
l = (cid:88)
nl
(a(l)-y )2
i j j
j=1
g. aplicacio'n de la regla de la cadena
dadoquelasfuncionesl ,z(l) ya(l) esta'nencadenadas,es
i j j
necesario aplicar la regla de la cadena para derivar cada peso
w(l) ysesgob(l).sololaderivada ∂z j (l) cambiaconcadapeso
j,k j ∂w(l)
j,k
actualizado, mientras que las dema's se mantienen constantes
dentro de la capa.
las derivadas parciales relevantes son:
∂l
i =2(a(l)-y )
∂a(l) j j
j
∂a(l)
j =g(z(l))(1-g(z(l)))
∂z(l) j j
j
∂z(l)
j =a(l-1)
∂w(l) k
j,k
apartirdeestasderivadas,lospesosysesgosseactualizan
siguiendo el descenso del gradiente:
∂l ∂l
w(l) ←w(l) -η i , b(l) ←b(l)-η i
j,k j,k ∂w(l) j j ∂b(l)
j,k j
donde η representa la tasa de aprendizaje.
en redes ma's profundas, al extender el ca'lculo hacia capas
anteriores (l-1), el nu'mero de para'metros y combinaciones
a derivar aumenta considerablemente, incrementando la complejidad computacional del algoritmo.