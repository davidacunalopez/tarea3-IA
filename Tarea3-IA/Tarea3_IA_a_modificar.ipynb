{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oped9o_Fomkf",
        "outputId": "9deffb80-139b-4846-d17f-ccc737b0f388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Iniciando limpieza del entorno de Colab...\n",
            "\n",
            " - Borrando cach√©: /root/.cache/huggingface\n",
            "\n",
            "üîç Versi√≥n de Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "üì¶ Paquetes relevantes actualmente instalados:\n",
            "\n",
            "\n",
            "‚úÖ Limpieza completada. Ahora ejecuta la Celda 0 (diagn√≥stico).\n"
          ]
        }
      ],
      "source": [
        "# === Celda -Limpieza: Limpieza segura del entorno antes del setup ===\n",
        "import os, shutil, glob, sys, subprocess\n",
        "\n",
        "print(\"üßπ Iniciando limpieza del entorno de Colab...\\n\")\n",
        "\n",
        "# 1Ô∏è‚É£ Eliminar cach√©s comunes (Hugging Face, Torch, etc.)\n",
        "for cache_dir in [\n",
        "    \"/root/.cache/huggingface\",\n",
        "    \"/root/.cache/torch/sentence_transformers\",\n",
        "    \"/root/.cache/torch/transformers\",\n",
        "    \"/content/hf_cache\"\n",
        "]:\n",
        "    if os.path.exists(cache_dir):\n",
        "        print(\" - Borrando cach√©:\", cache_dir)\n",
        "        shutil.rmtree(cache_dir, ignore_errors=True)\n",
        "\n",
        "# 2Ô∏è‚É£ Desinstalar posibles restos conflictivos\n",
        "subprocess.run([\n",
        "    sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\",\n",
        "    \"transformers\", \"tokenizers\", \"huggingface-hub\", \"sentence-transformers\"\n",
        "], check=False)\n",
        "\n",
        "# 3Ô∏è‚É£ Borrar distribuciones da√±adas (casos como \"~cipy\")\n",
        "for p in glob.glob(\"/usr/local/lib/python3.12/dist-packages/~cipy*\"):\n",
        "    print(\" - Borrando resto inv√°lido:\", p)\n",
        "    shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "# 4Ô∏è‚É£ Mostrar versiones base del entorno\n",
        "print(\"\\nüîç Versi√≥n de Python:\", sys.version)\n",
        "print(\"üì¶ Paquetes relevantes actualmente instalados:\\n\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"list\"], check=False)\n",
        "\n",
        "print(\"\\n‚úÖ Limpieza completada. Ahora ejecuta la Celda 0 (diagn√≥stico).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785,
          "referenced_widgets": [
            "bec1b02ba4054950ab2d6a5b0d7c784d",
            "7a77711102d342f99afb5e9249a754fd",
            "06dd0ed90ec5487d9eedd7b1d7ad03fa",
            "fe81263a3a19462494df2d76da2d1af5",
            "7cc441931d6d469c86f63ddd556f376b",
            "ef0545f26b214dd3a56295d423d2c4a3",
            "1c2db6efc1bb4977bf29890d2b61cb15",
            "f42949df0bdd42dd8b8dcc5c70043aac",
            "70bc013675c14e57be0a023c33a1b555",
            "a1333f4a234b44c999128226cfb61b86",
            "a80e3b57b4484cb19be43ebcf3cf98d1",
            "40688bb881974f058d1d99d29b138a35",
            "9a86b1ae42084e439959ac6b6fa46163",
            "233b2f7e5eaa480492f0473623c70134",
            "44bd266a2f2248858cffcbbbdb802120",
            "152558aa9fe74f93835d54ff8a8f10c2",
            "e627fcd229aa43a5986092860fc993f5",
            "ae06b35563e24235be4c8abf75a7b4b3",
            "3fa876ad54dd42399e54684f5dfca8e0",
            "d189e8ca580d4137bd87289646c9e26c",
            "1a84fa9fd87b43669eb6748e9dbb97d3",
            "22774cc56b9443df868b10ab4c169c18",
            "ff41fe863a394246ad3f9b15febef8a9",
            "b8a916060e1d48acb9d759dea8ea38d9",
            "e0c8b570190e471a8190170f49402f51",
            "e91931f8d815462d814b72c2862b380a",
            "f7e7c1402dac4b8fa2dfc925d56b67dc",
            "e7146e50e280480d85aa541f0a39e93c",
            "0f587fb5340646c09be2da571b6bcb25",
            "54bc6f6575b64de7af146a097c3e47fb",
            "980fa204feaa4382bfef0b05cd6ddd29",
            "767716710f274910a87179e2040059a9",
            "6f7490d4724c46fd8e06cbb3848f887c",
            "646c5b0f5a9344ddb8204ceae976037a",
            "2816d71c2b914b8f82a170b7c52cbcb5",
            "2a2d268e5e564b5d92898682df4b8624",
            "3eceadeb84e244a3882de6a07566b2cb",
            "31c6166df49149bea00d1a5b9769bba3",
            "8ee2ccd835534163851a7aba0aa2cbef",
            "d4c5eb2e034445eb891eb9a97fb7958e",
            "6609691e167a428ab1011f72510f3b57",
            "5bbfc023c2e44320862c131bb2dce916",
            "bf8cfd738d244596b3f00b9e1fd548e7",
            "b2e1f2cbffdb46369143fc659885ec50",
            "e956d5f41a474bb29ff1c68cb1d6eeef",
            "981549a57226444c97eb4a50cdc3ff98",
            "874e51415ca349469bb564e684f5d339",
            "23291c78b4424f229278e05cac25c8e1",
            "cccb761dd3724110854fad7435c75d87",
            "84e725bc6cd54c7c9983b7d57804842a",
            "c4321fe496014372ab9fe341fc795c67",
            "2a53c4b274b04e7eb8672c5ac616185c",
            "76bf6cc8c7fe4166b679f34de0bee2ae",
            "9896a774ff0047b48b0ad7a5276384f9",
            "f0b25f15ecbd46a5a9b9eb57346d488c",
            "d1410f247e294852a0455bc8f6d65e6c",
            "a3f63ff1845644e9be2f87fcedc722b2",
            "55018d28d5c64215bcbc820f9a68914d",
            "8a2935b7b7b94791b8cfeaaead422724",
            "310b273527d34127abf31a1a285e68c4",
            "165d48948e184f909a4dde05f0647b98",
            "fae1806f351f42d997bc9a614e1f68a9",
            "c629f04e8e2444b68052e373de38a725",
            "9de40b988e5c478a9be83ab2a2b49f52",
            "e5f5924a3f0b4956bb896e82722856dd",
            "dc68c7da6f514ddfa4489bad6949406b",
            "c158bb21101d433a9a569018c5acd637",
            "54b6e834f6ca4076b92d167396df2eb8",
            "bfa8a5811cfc43d9acfb30a6ac1eac28",
            "bb80de04d5c940bdaac1ffc40c109d25",
            "8fb3a9985a9f413b97a708795806207e",
            "8f415be8be994eb3a1c1aab955484ebf",
            "4b10b489a3d34c469996cf9a326c0097",
            "29e4d75a2b3d4032aa3dbc4f2c9975d4",
            "03335e7e3afc415cb0aa7d6f11db5dfb",
            "2542d4f04c7544278e5993633bb5b9f6",
            "1973786fee7e41daba689e40a6183d73",
            "d3299261ba0c43ee82633ba43ff73a3c",
            "4f12f92037df4b4faa89c2502f9002a1",
            "cc6ea8c18cda44d197c2c2475485252c",
            "a68bcc3ce0cb43149ef067b917eb9fa6",
            "550d10d3e0d04fa0bd2baba0a2a69b58",
            "9523a16e02214039924e5613276a1b45",
            "dc211367670446008bf1f27636ffc9b1",
            "1c0f7d99607f4710b9beb6987b32ead5",
            "5625cd5c38ec41cd9b626a89f7715d04",
            "e2e31ab141554b25b02b20a8d1a80110",
            "71c39c8de1ff405b8ce43b3f457db8cd",
            "8bbb01eb45864b1f95532c2eee3407b4",
            "b98d460e08de465ebcb31c684f871583",
            "1ed86fc53e2e418da663b48b5c3fd351",
            "bf25cdf5389b49a39f276642599e82b3",
            "73a8c317825143afbf48114d8b32d04b",
            "b10c889d6280498ca29d31d17f78e55f",
            "4db1b7c585024fc1a745a4e731c70474",
            "105b408281fd414daec81817fa60b1bf",
            "f08921db6e934d4eb31dfad55c5f51ef",
            "19ec7be049df4b7b93507d8112bba69b",
            "35e5cbdcad04429f87d34797225b372d",
            "a8ad90def4eb4adeb20c8fb50d937beb",
            "97933dd8beb1467fb8c7899a7bb38318",
            "fd5304d2a2dd453cb90cf830f25dbbe2",
            "eb01ab1e60af487d83ba8aeddcf98bdc",
            "3bb8fdec708940eda4b54188e06439d5",
            "e70c6b9dbd8142a7819452e1a3f57d8c",
            "2926b3da7a1a4b5b978976cc5d510ac0",
            "1bf187771aa845a28e82ee311ac07a9d",
            "654eb87658c04a3387ec3972ecd2974f",
            "a97f3324b844472da11fd5fc012559fd",
            "d98d8fa4bf504991ac5ec3d9aa2c39d3",
            "ccef152d9aba444ea986551f4326cfce",
            "90f60ecb21164af592148bffc8f50750",
            "5b06c4a31fee472fa5895fffc9077dd6",
            "9d99f1eae99446ff914b802fe878d264",
            "ee0b1717bc9642d6a935e3d259d6d947",
            "02446af37b9a43a6a18648ce8a88a595",
            "7ec59a972e7141408fc60928b4141d33",
            "d555664d92aa4427a91f8091a82cf097",
            "8faea8294be0492d8e1dafae8325d759",
            "748ef51e7ac14f3184373c35baa887ae",
            "055542ecb0e04ef49714c32001c19330"
          ]
        },
        "id": "JSe9ewE6l7KG",
        "outputId": "8ad55081-c306-4df4-a785-56685eaa569f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.12\n",
            "\n",
            "üì¶ Versiones detectadas:\n",
            "numpy                        2.0.2\n",
            "scipy                        1.16.3\n",
            "sklearn                      1.6.1\n",
            "torch                        2.8.0+cu126\n",
            "transformers                 4.57.1\n",
            "tokenizers                   0.22.1\n",
            "huggingface_hub              0.36.0\n",
            "sentence_transformers        5.1.2\n",
            "langchain                    0.3.27\n",
            "langchain_community          0.4.1\n",
            "langchain_huggingface        (sin __version__)\n",
            "\n",
            "üîç Verificando instalaci√≥n completa de transformers...\n",
            "‚úÖ M√≥dulo BertConfig importado correctamente\n",
            "\n",
            "üîç Comprobando integraci√≥n de LangChain + HuggingFace...\n",
            "Origen del wrapper: langchain_huggingface (moderno ‚úÖ)\n",
            "Dispositivo: cpu\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bec1b02ba4054950ab2d6a5b0d7c784d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40688bb881974f058d1d99d29b138a35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff41fe863a394246ad3f9b15febef8a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "646c5b0f5a9344ddb8204ceae976037a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e956d5f41a474bb29ff1c68cb1d6eeef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1410f247e294852a0455bc8f6d65e6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c158bb21101d433a9a569018c5acd637",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3299261ba0c43ee82633ba43ff73a3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bbb01eb45864b1f95532c2eee3407b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8ad90def4eb4adeb20c8fb50d937beb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccef152d9aba444ea986551f4326cfce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Embeddings funcionando correctamente\n",
            "Dimensi√≥n del embedding: 384\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Celda 0: Diagn√≥stico completo del entorno (LangChain + HF)\n",
        "# ============================================================\n",
        "\n",
        "import importlib, pkgutil, sys\n",
        "\n",
        "def ver(mod):\n",
        "    \"\"\"Imprime versi√≥n del m√≥dulo si est√° instalado.\"\"\"\n",
        "    try:\n",
        "        m = importlib.import_module(mod)\n",
        "        print(f\"{mod:28s}\", getattr(m, \"__version__\", \"(sin __version__)\"))\n",
        "    except Exception as e:\n",
        "        print(f\"{mod:28s}\", \"‚Äî no instalado ‚Äî\", \"|\", e)\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"\\nüì¶ Versiones detectadas:\")\n",
        "for mod in [\n",
        "    \"numpy\", \"scipy\", \"sklearn\", \"torch\",\n",
        "    \"transformers\", \"tokenizers\",\n",
        "    \"huggingface_hub\", \"sentence_transformers\",\n",
        "    \"langchain\", \"langchain_community\", \"langchain_huggingface\"\n",
        "]:\n",
        "    ver(mod)\n",
        "\n",
        "# ============================================================\n",
        "# Verificaci√≥n de que transformers est√° completamente instalado\n",
        "# ============================================================\n",
        "print(\"\\nüîç Verificando instalaci√≥n completa de transformers...\")\n",
        "try:\n",
        "    from transformers.models.bert.configuration_bert import BertConfig\n",
        "    print(\"‚úÖ M√≥dulo BertConfig importado correctamente\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è Error al importar BertConfig: {e}\")\n",
        "    print(\"   Esto indica que transformers no est√° completamente instalado.\")\n",
        "    print(\"   Ejecuta la Celda 1 (instalaci√≥n) nuevamente o reinicia el runtime.\")\n",
        "\n",
        "# ============================================================\n",
        "# Comprobaci√≥n autom√°tica de HuggingFaceEmbeddings disponible\n",
        "# ============================================================\n",
        "print(\"\\nüîç Comprobando integraci√≥n de LangChain + HuggingFace...\")\n",
        "\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    origen = \"langchain_huggingface (moderno ‚úÖ)\"\n",
        "    HuggingFaceEmbeddings_available = True\n",
        "except ModuleNotFoundError:\n",
        "    try:\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "        origen = \"langchain_community.embeddings (cl√°sico ‚öôÔ∏è)\"\n",
        "        HuggingFaceEmbeddings_available = True\n",
        "    except ModuleNotFoundError:\n",
        "        HuggingFaceEmbeddings = None\n",
        "        HuggingFaceEmbeddings_available = False\n",
        "        origen = \"‚ùå Ning√∫n m√≥dulo de HuggingFaceEmbeddings disponible\"\n",
        "\n",
        "print(\"Origen del wrapper:\", origen)\n",
        "\n",
        "# ============================================================\n",
        "# Prueba funcional (si existe HuggingFaceEmbeddings)\n",
        "# ============================================================\n",
        "if HuggingFaceEmbeddings_available and HuggingFaceEmbeddings is not None:\n",
        "    try:\n",
        "        import torch\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(\"Dispositivo:\", device)\n",
        "\n",
        "        embeddings_model = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            model_kwargs={\"device\": device},\n",
        "            encode_kwargs={\"normalize_embeddings\": True},\n",
        "        )\n",
        "\n",
        "        test_text = \"La inteligencia artificial aprende patrones del lenguaje humano.\"\n",
        "        vec = embeddings_model.embed_query(test_text)\n",
        "        print(\"\\n‚úÖ Embeddings funcionando correctamente\")\n",
        "        print(\"Dimensi√≥n del embedding:\", len(vec))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error al probar embeddings: {e}\")\n",
        "        print(\"   Posible causa: transformers no est√° completamente instalado.\")\n",
        "        print(\"   Soluci√≥n: Reinicia el runtime y ejecuta las celdas en orden.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No se pudo inicializar HuggingFaceEmbeddings.\")\n",
        "    print(\"Ejecuta la Celda 1 (instalaci√≥n) para instalar los paquetes necesarios.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b86Ow0ynmAtU",
        "outputId": "4d85b2b7-128b-4f97-9ecb-dccb7c36c032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Configurando versiones compatibles de dependencias base...\n",
            "üîÑ Reinstalando transformers para corregir m√≥dulos faltantes...\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m173.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m209.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m243.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m148.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m216.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m219.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m162.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m232.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m222.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m254.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m172.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m180.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m194.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m190.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m214.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m182.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m158.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m179.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m============================================================\n",
            "üì¶ VERIFICACI√ìN DE INSTALACI√ìN\n",
            "============================================================\n",
            "‚úÖ transformers: 4.57.1\n",
            "‚úÖ tokenizers: 0.22.1\n",
            "‚úÖ sentence-transformers: 5.1.2\n",
            "‚úÖ huggingface-hub: 0.36.0\n",
            "‚úÖ langchain: 0.3.27\n",
            "‚úÖ langchain-huggingface: disponible\n",
            "‚úÖ langchain-openai: disponible\n",
            "‚úÖ BERT module: disponible\n",
            "============================================================\n",
            "‚úÖ Instalaci√≥n completada\n",
            "\n",
            "üìù NOTA SOBRE WARNINGS:\n",
            "   Los warnings sobre dependencias (numpy, fsspec, etc.) son NORMALES en Colab.\n",
            "   Son informativos y generalmente NO afectan el funcionamiento.\n",
            "   Solo preoc√∫pate si ves ERRORES reales al ejecutar el c√≥digo.\n",
            "\n",
            "   Si hay errores, reinicia el runtime: Runtime > Restart runtime\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# CELDA 1: Instalaci√≥n de dependencias\n",
        "# ============================================================\n",
        "# IMPORTANTE: Si ves warnings sobre dependencias, son normales en Colab.\n",
        "# Si hay errores cr√≠ticos, reinicia el runtime (Runtime > Restart runtime)\n",
        "# ============================================================\n",
        "\n",
        "# 1) Fijar versiones de dependencias base para minimizar conflictos con Colab\n",
        "# Estas versiones son compatibles con los paquetes preinstalados en Colab\n",
        "print(\"üì¶ Configurando versiones compatibles de dependencias base...\")\n",
        "%pip install --quiet \\\n",
        "  \"requests==2.32.4\" \\\n",
        "  \"fsspec==2025.3.0\"\n",
        "\n",
        "# 2) Reinstalar transformers completamente para evitar problemas de m√≥dulos faltantes\n",
        "# El error \"No module named 'transformers.models.bert.configuration_bert'\" indica\n",
        "# que transformers no est√° completamente instalado\n",
        "print(\"üîÑ Reinstalando transformers para corregir m√≥dulos faltantes...\")\n",
        "%pip install --quiet --force-reinstall --no-cache-dir \\\n",
        "  \"transformers>=4.45.0,<5.0.0\" \\\n",
        "  \"tokenizers>=0.20.0\"\n",
        "\n",
        "# 3) Instalar sentence-transformers y huggingface-hub\n",
        "%pip install --quiet --upgrade \\\n",
        "  \"sentence-transformers>=2.7.0\" \\\n",
        "  \"huggingface-hub>=0.36.0\"\n",
        "\n",
        "# 4) Instalar LangChain y conectores (versi√≥n 1.x para compatibilidad)\n",
        "%pip install --quiet --upgrade \\\n",
        "  \"langchain>=1.0.0\" \\\n",
        "  \"langchain-huggingface>=0.0.3\" \\\n",
        "  \"langchain-community>=0.2.15\" \\\n",
        "  \"langchain-openai>=0.1.0\"\n",
        "\n",
        "# 5) Re-fijar requests y fsspec al final para asegurar que no se actualicen\n",
        "# (algunos paquetes pueden intentar actualizarlos durante la instalaci√≥n)\n",
        "%pip install --quiet --no-deps \"requests==2.32.4\" \"fsspec==2025.3.0\"\n",
        "\n",
        "# 3) Verificaci√≥n r√°pida\n",
        "import importlib.util\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üì¶ VERIFICACI√ìN DE INSTALACI√ìN\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"‚úÖ transformers: {transformers.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå transformers: {e}\")\n",
        "\n",
        "try:\n",
        "    import tokenizers\n",
        "    print(f\"‚úÖ tokenizers: {tokenizers.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå tokenizers: {e}\")\n",
        "\n",
        "try:\n",
        "    import sentence_transformers\n",
        "    print(f\"‚úÖ sentence-transformers: {sentence_transformers.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå sentence-transformers: {e}\")\n",
        "\n",
        "try:\n",
        "    import huggingface_hub\n",
        "    print(f\"‚úÖ huggingface-hub: {huggingface_hub.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå huggingface-hub: {e}\")\n",
        "\n",
        "try:\n",
        "    import langchain\n",
        "    print(f\"‚úÖ langchain: {langchain.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå langchain: {e}\")\n",
        "\n",
        "try:\n",
        "    import langchain_huggingface\n",
        "    print(f\"‚úÖ langchain-huggingface: disponible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå langchain-huggingface: {e}\")\n",
        "\n",
        "try:\n",
        "    import langchain_openai\n",
        "    print(f\"‚úÖ langchain-openai: disponible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå langchain-openai: {e}\")\n",
        "\n",
        "# Verificar m√≥dulo BERT\n",
        "bert_available = importlib.util.find_spec(\"transformers.models.bert.modeling_bert\") is not None\n",
        "print(f\"‚úÖ BERT module: {'disponible' if bert_available else 'no disponible'}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ Instalaci√≥n completada\")\n",
        "print(\"\\nüìù NOTA SOBRE WARNINGS:\")\n",
        "print(\"   Los warnings sobre dependencias (numpy, fsspec, etc.) son NORMALES en Colab.\")\n",
        "print(\"   Son informativos y generalmente NO afectan el funcionamiento.\")\n",
        "print(\"   Solo preoc√∫pate si ves ERRORES reales al ejecutar el c√≥digo.\")\n",
        "print(\"\\n   Si hay errores, reinicia el runtime: Runtime > Restart runtime\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_AGoVpHmCNd",
        "outputId": "28b7693c-3e38-4127-c20d-96f725a21f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers           4.57.1\n",
            "tokenizers             0.22.1\n",
            "huggingface_hub        0.36.0\n",
            "sentence_transformers  5.1.2\n",
            "BERT presente -> True\n"
          ]
        }
      ],
      "source": [
        "# Celda 2: confirmar que todo qued√≥ consistente\n",
        "import importlib, importlib.util\n",
        "\n",
        "def ver(mod):\n",
        "    m = importlib.import_module(mod)\n",
        "    print(f\"{mod:22s}\", getattr(m, \"__version__\", \"(sin __version__)\"))\n",
        "\n",
        "for mod in [\"transformers\",\"tokenizers\",\"huggingface_hub\",\"sentence_transformers\"]:\n",
        "    ver(mod)\n",
        "\n",
        "# Chequeo de BERT presente\n",
        "import transformers, importlib.util\n",
        "print(\"BERT presente ->\", importlib.util.find_spec(\"transformers.models.bert\") is not None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCDWMcXdmEqF",
        "outputId": "6e5d06bf-22f3-4c6f-f5fa-99521cb51760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK; dimensi√≥n: 384\n"
          ]
        }
      ],
      "source": [
        "# Celda 3: prueba de humo; si falla, limpia cach√© y reintenta una vez\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os, shutil\n",
        "\n",
        "def try_load(clean_cache=False):\n",
        "    cache_dir = \"/content/hf_cache\" if clean_cache else None\n",
        "    if clean_cache:\n",
        "        for p in [\"/content/hf_cache\", os.path.expanduser(\"~/.cache/huggingface\")]:\n",
        "            if os.path.exists(p):\n",
        "                print(\"Limpiando cach√©:\", p); shutil.rmtree(p, ignore_errors=True)\n",
        "    m = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                            cache_folder=cache_dir, trust_remote_code=False)\n",
        "    v = m.encode([\"hola\",\"mundo\"], normalize_embeddings=True)\n",
        "    print(\"OK; dimensi√≥n:\", len(v[0]))\n",
        "\n",
        "try:\n",
        "    try_load(clean_cache=False)\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Falla inicial:\", e)\n",
        "    print(\"‚Üí Reintentando con cach√© limpia‚Ä¶\")\n",
        "    try_load(clean_cache=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O1b3oG0mGZE",
        "outputId": "aaf07714-d8ea-4c45-e4ea-dc6518ff7d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cpu\n",
            "‚úÖ LangChain conectado con √©xito.\n",
            "Dimensi√≥n del embedding: 384\n"
          ]
        }
      ],
      "source": [
        "###Prueba de que las librerias quedaron bien instaladas y funcionan respecto a langchain\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Versi√≥n sin cache_dir (usa la ruta por defecto)\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": device},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Prueba simple\n",
        "test_text = \"La inteligencia artificial aprende patrones del lenguaje humano.\"\n",
        "vec = embeddings_model.embed_query(test_text)\n",
        "print(\"‚úÖ LangChain conectado con √©xito.\")\n",
        "print(\"Dimensi√≥n del embedding:\", len(vec))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9504BR1-w8t",
        "outputId": "1e7386d7-c474-42e3-a2f7-b11f464c747b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Drive montado.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 1) Montar Google Drive\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive montado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwb41D3L_FuD",
        "outputId": "4356f4c2-6cdf-4f07-dd35-7e257edb29f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Proyecto: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA\n",
            "üìÅ Metadata: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "üìÅ PDFs: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/RepositorioApuntesPdf\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 2) Definir las rutas base en Drive\n",
        "#    Ajustadas a tu estructura:\n",
        "#    Mi unidad / Colab Notebooks / Tarea3-IA / ...\n",
        "# ============================================\n",
        "import os\n",
        "\n",
        "# Ruta base a \"Mi unidad\"\n",
        "BASE_DRIVE = \"/content/drive/MyDrive\"\n",
        "\n",
        "# Carpeta ra√≠z del proyecto de la tarea\n",
        "PROYECTO_DIR = os.path.join(BASE_DRIVE, \"Colab Notebooks\", \"Tarea3-IA\")\n",
        "\n",
        "# Carpetas espec√≠ficas\n",
        "METADATA_FILE = os.path.join(PROYECTO_DIR, \"MetadataRAW.csv\")\n",
        "PDFS_DIR = os.path.join(PROYECTO_DIR, \"RepositorioApuntesPdf\")\n",
        "\n",
        "print(\"üìÅ Proyecto:\", PROYECTO_DIR)\n",
        "print(\"üìÅ Metadata:\", METADATA_FILE)\n",
        "print(\"üìÅ PDFs:\", PDFS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH70dK4RDelg",
        "outputId": "370c3e44-f1c3-41da-b0cc-12a730118721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Carpeta del proyecto encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA\n",
            "‚úÖ Archivo de metadata encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "‚úÖ Carpeta de PDFs encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/RepositorioApuntesPdf\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 3) Verificar que las carpetas existen\n",
        "#    (esto ayuda a detectar typos en el nombre)\n",
        "# ============================================\n",
        "\n",
        "def check_dir(path, name):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"‚úÖ {name} encontrada: {path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {name} NO encontrada. Revisa el nombre en Drive: {path}\")\n",
        "\n",
        "check_dir(PROYECTO_DIR, \"Carpeta del proyecto\")\n",
        "check_dir(METADATA_FILE, \"Archivo de metadata\")\n",
        "check_dir(PDFS_DIR, \"Carpeta de PDFs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBcFaeTIkRSM",
        "outputId": "557dac4c-54ce-4e89-f182-d7bfde194598"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö PDFs encontrados: 46\n",
            "  - 5_Semana_AI_20250904_2.pdf\n",
            "  - 6_Semana_AI_20250911_2.pdf\n",
            "  - 11_Semana_AI_20251016_4.pdf\n",
            "  - 11_Semana_AI_20251014_3.pdf\n",
            "  - 10_SEMANA_AI_20251007_1-222887296.pdf\n",
            "  - 7_Semana_AI_20250916_2.pdf\n",
            "  - 12_SEMANA_AI_20251021_3.pdf\n",
            "  - 2_Semana_AI_20250812_3.pdf\n",
            "  - 11_Semana_AI_20251014_2.pdf\n",
            "  - 8_Semana_AI_20250925_2.pdf\n",
            "  - 11_Semana_AI_20251014_1.pdf\n",
            "  - 5_Semana_AI_20250904_1.pdf\n",
            "  - 10_SEMANA_AI_20251009_1.pdf\n",
            "  - 12_SEMANA_AI_20251021_4.pdf\n",
            "  - 8_Semana_AI_20250923_1.pdf\n",
            "  - 1_Semana_AI_20250807_2.pdf\n",
            "  - 2_Semana_AI_20250814_1.pdf\n",
            "  - 2_Semana_AI_20250814_2.pdf\n",
            "  - 5_Semana_AI_20250902_2.pdf\n",
            "  - 6_Semana_AI_20250911_1.pdf\n",
            "  - 5_Semana_AI_20250902_1.pdf\n",
            "  - 11_SEMANA_AI_20251016_2.pdf\n",
            "  - 4_SEMANA_AI_20250826_1.pdf\n",
            "  - 6_Semana_AI_20250909_2-220676337.pdf\n",
            "  - 8_Semana_AI_20250925_1.pdf\n",
            "  - 7_Semana_AI_20250918_1.pdf\n",
            "  - 12_SEMANA_AI_20251021_1.pdf\n",
            "  - 3_Semana_AI_20250819_2.pdf\n",
            "  - 4_Semana_AI_20250828_1.pdf\n",
            "  - 7_Semana_AI_20250918_2.pdf\n",
            "  - 4_SEMANA_AI_20250826_2.pdf\n",
            "  - 3_Semana_AI_20250821_1.pdf\n",
            "  - 2_SEMANA_AI_20250812_1.pdf\n",
            "  - 12_Semana_AI_20251021_2.pdf\n",
            "  - 12_SEMANA_AL_20251023_2.pdf\n",
            "  - 7_Semana_AI_20250916_1.pdf\n",
            "  - 1_SEMANA_AI_20250807_1.pdf\n",
            "  - 12_SEMANA_AI_20251023_1.pdf\n",
            "  - 10_SEMANA_AI_20251007_1.pdf\n",
            "  - 12_Semana_AI_20251023_3.pdf\n",
            "  - 4_Semana_AI_20250828_2.pdf\n",
            "  - 8_Semana_AI_20250923_2.pdf\n",
            "  - 3_Semana_AI_20250819_1.pdf\n",
            "  - 9_Semana_AI_20251002_2.pdf\n",
            "  - 6_Semana_AI_20250909_1.pdf\n",
            "  - 9_SEMANA_AI_20251002_1.pdf\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 4) Listar los PDFs disponibles\n",
        "#    (esto confirma que Colab est√° viendo tu carpeta)\n",
        "# ============================================\n",
        "\n",
        "pdf_files = [f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")]\n",
        "print(f\"üìö PDFs encontrados: {len(pdf_files)}\")\n",
        "for f in pdf_files:\n",
        "    print(\"  -\", f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "z1k1mRxE0TsY",
        "outputId": "4037ee3b-aa81-45c8-d76f-d095f618275f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Metadata CSV cargada correctamente (46 filas) desde:\n",
            "/content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "\n",
            "üìë Columnas detectadas: ['id_doc', 'nombre_archivo', 'autor', 'fecha', 'tema']\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"    print(\\\"\\u26a0\\ufe0f No se encontr\\u00f3 el archivo 'metadata\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id_doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"DOC_009\",\n          \"DOC_002\",\n          \"DOC_006\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nombre_archivo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"3_Semana_AI_20250821_1.pdf\",\n          \"1_Semana_AI_20250807_2.pdf\",\n          \"2_Semana_AI_20250814_2.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"autor\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Julio Varela Venegas\",\n          \"Fernando Daniel Brenes Reyes\",\n          \"Jose Pablo Quesada Rodr\\u00edguez\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fecha\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-08-07 00:00:00\",\n        \"max\": \"2025-08-26 00:00:00\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"2025-08-07 00:00:00\",\n          \"2025-08-12 00:00:00\",\n          \"2025-08-26 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tema\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Aplicaci\\u00f3n del \\u00e1lgebra lineal y la programaci\\u00f3n vectorial en IA, con enfoque en aprendizaje supervisado, representaci\\u00f3n de vectores y uso de NumPy y Jupyter Notebook.\",\n          \"Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos aut\\u00f3nomos, con \\u00e9nfasis en el aprendizaje supervisado y no supervisado basado en datos.\",\n          \"Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de \\u00e1lgebra lineal y tensores en PyTorch.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f06b3f85-b117-4395-b1fc-dc13052cd70a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_doc</th>\n",
              "      <th>nombre_archivo</th>\n",
              "      <th>autor</th>\n",
              "      <th>fecha</th>\n",
              "      <th>tema</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DOC_001</td>\n",
              "      <td>1_SEMANA_AI_20250807_1.pdf</td>\n",
              "      <td>Rodolfo David Acu√±a L√≥pez</td>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>Principios fundamentales de la inteligencia ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DOC_002</td>\n",
              "      <td>1_Semana_AI_20250807_2.pdf</td>\n",
              "      <td>Fernando Daniel Brenes Reyes</td>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>Aplicaciones de la inteligencia artificial y m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DOC_003</td>\n",
              "      <td>2_SEMANA_AI_20250812_1.pdf</td>\n",
              "      <td>Priscilla Jim√©nez Salgado</td>\n",
              "      <td>2025-08-12</td>\n",
              "      <td>Introducci√≥n a machine learning y deep learnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DOC_004</td>\n",
              "      <td>2_Semana_AI_20250812_3.pdf</td>\n",
              "      <td>Luis Alfredo Gonz√°lez S√°nchez</td>\n",
              "      <td>2025-08-12</td>\n",
              "      <td>Resumen de conceptos clave de IA y enfoques de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DOC_005</td>\n",
              "      <td>2_Semana_AI_20250814_1.pdf</td>\n",
              "      <td>Kendall Rodr√≠guez Camacho</td>\n",
              "      <td>2025-08-14</td>\n",
              "      <td>Introducci√≥n a √°lgebra lineal aplicada con Pyt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DOC_006</td>\n",
              "      <td>2_Semana_AI_20250814_2.pdf</td>\n",
              "      <td>Jose Pablo Quesada Rodr√≠guez</td>\n",
              "      <td>2025-08-14</td>\n",
              "      <td>Resumen detallado sobre tipos de aprendizaje, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DOC_007</td>\n",
              "      <td>3_Semana_AI_20250819_1.pdf</td>\n",
              "      <td>Javier Rojas Rojas</td>\n",
              "      <td>2025-08-19</td>\n",
              "      <td>Revisi√≥n de √°lgebra lineal y aprendizaje super...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DOC_008</td>\n",
              "      <td>3_Semana_AI_20250819_2.pdf</td>\n",
              "      <td>Mariana Quesada S√°nchez</td>\n",
              "      <td>2025-08-19</td>\n",
              "      <td>Repaso de √°lgebra lineal y fundamentos del apr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>DOC_009</td>\n",
              "      <td>3_Semana_AI_20250821_1.pdf</td>\n",
              "      <td>Julio Varela Venegas</td>\n",
              "      <td>2025-08-21</td>\n",
              "      <td>Aplicaci√≥n del √°lgebra lineal y la programaci√≥...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DOC_010</td>\n",
              "      <td>4_SEMANA_AI_20250826_1.pdf</td>\n",
              "      <td>Andr√©s S√°nchez Rojas</td>\n",
              "      <td>2025-08-26</td>\n",
              "      <td>Implementaci√≥n del algoritmo KNN y fundamentos...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f06b3f85-b117-4395-b1fc-dc13052cd70a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f06b3f85-b117-4395-b1fc-dc13052cd70a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f06b3f85-b117-4395-b1fc-dc13052cd70a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8cbf63bd-6381-4642-91f3-c74d7258ebfa\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8cbf63bd-6381-4642-91f3-c74d7258ebfa')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8cbf63bd-6381-4642-91f3-c74d7258ebfa button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    id_doc              nombre_archivo                          autor  \\\n",
              "0  DOC_001  1_SEMANA_AI_20250807_1.pdf      Rodolfo David Acu√±a L√≥pez   \n",
              "1  DOC_002  1_Semana_AI_20250807_2.pdf   Fernando Daniel Brenes Reyes   \n",
              "2  DOC_003  2_SEMANA_AI_20250812_1.pdf      Priscilla Jim√©nez Salgado   \n",
              "3  DOC_004  2_Semana_AI_20250812_3.pdf  Luis Alfredo Gonz√°lez S√°nchez   \n",
              "4  DOC_005  2_Semana_AI_20250814_1.pdf      Kendall Rodr√≠guez Camacho   \n",
              "5  DOC_006  2_Semana_AI_20250814_2.pdf   Jose Pablo Quesada Rodr√≠guez   \n",
              "6  DOC_007  3_Semana_AI_20250819_1.pdf             Javier Rojas Rojas   \n",
              "7  DOC_008  3_Semana_AI_20250819_2.pdf        Mariana Quesada S√°nchez   \n",
              "8  DOC_009  3_Semana_AI_20250821_1.pdf           Julio Varela Venegas   \n",
              "9  DOC_010  4_SEMANA_AI_20250826_1.pdf           Andr√©s S√°nchez Rojas   \n",
              "\n",
              "       fecha                                               tema  \n",
              "0 2025-08-07  Principios fundamentales de la inteligencia ar...  \n",
              "1 2025-08-07  Aplicaciones de la inteligencia artificial y m...  \n",
              "2 2025-08-12  Introducci√≥n a machine learning y deep learnin...  \n",
              "3 2025-08-12  Resumen de conceptos clave de IA y enfoques de...  \n",
              "4 2025-08-14  Introducci√≥n a √°lgebra lineal aplicada con Pyt...  \n",
              "5 2025-08-14  Resumen detallado sobre tipos de aprendizaje, ...  \n",
              "6 2025-08-19  Revisi√≥n de √°lgebra lineal y aprendizaje super...  \n",
              "7 2025-08-19  Repaso de √°lgebra lineal y fundamentos del apr...  \n",
              "8 2025-08-21  Aplicaci√≥n del √°lgebra lineal y la programaci√≥...  \n",
              "9 2025-08-26  Implementaci√≥n del algoritmo KNN y fundamentos...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 5) Cargar archivo de metadata\n",
        "#    Columnas esperadas: id_doc, nombre_archivo, autor, fecha, tema\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "if os.path.exists(METADATA_FILE):\n",
        "    # Leer tal cual, como texto (sin parsear fechas)\n",
        "    df_meta = pd.read_csv(METADATA_FILE, dtype=str, keep_default_na=False)\n",
        "    print(f\"‚úÖ Metadata CSV cargada correctamente ({len(df_meta)} filas) desde:\\n{METADATA_FILE}\\n\")\n",
        "\n",
        "    # Chequeo suave de columnas (sin modificar nada)\n",
        "    expected = [\"id_doc\", \"nombre_archivo\", \"autor\", \"fecha\", \"tema\"]\n",
        "    missing = [c for c in expected if c not in df_meta.columns]\n",
        "    if missing:\n",
        "        print(\"‚ö†Ô∏è Faltan columnas esperadas:\", missing)\n",
        "    else:\n",
        "        print(\"üìë Columnas detectadas:\", list(df_meta.columns))\n",
        "\n",
        "        df_meta[\"fecha\"] = pd.to_datetime(df_meta[\"fecha\"], errors=\"coerce\")\n",
        "\n",
        "\n",
        "    # Preview\n",
        "    display(df_meta.head(10))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ el archivo 'metadata.csv' en la carpeta Metadata:\", METADATA_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLmQP8kv_Zkx",
        "outputId": "f25b7773-34af-43ed-92a4-e2d6be9958ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Extracci√≥n, normalizaci√≥n y segmentaci√≥n listas\n",
            "üßæ Base (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/base_documentos.jsonl\n",
            "üß± Base (Parquet): /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/base_documentos.parquet\n",
            "üîπ Seg A (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_a.jsonl\n",
            "üî∏ Seg B (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_b.jsonl\n",
            "üóíÔ∏è  Notas:          /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/preprocesamiento_decisiones.md\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# COMPA√ëERO 1 ‚Äì DATOS Y PREPROCESAMIENTO (PASO 2 COMPLETO)\n",
        "# Extraer texto, normalizar y segmentar (A: chunks fijos, B: encabezados)\n",
        "# Requiere:\n",
        "#  - Variables definidas antes: PROYECTO_DIR, METADATA_FILE, PDFS_DIR\n",
        "#  - METADATA_FILE con columnas: id_doc, nombre_archivo, autor, fecha, tema\n",
        "# Salidas (para Compa√±ero 2):\n",
        "#  - dataset/base_documentos.jsonl / .parquet\n",
        "#  - dataset/seg_a.jsonl / dataset/seg_b.jsonl\n",
        "#  - dataset/txt_por_doc/DOC_###.txt (opcional)\n",
        "#  - dataset/preprocesamiento_decisiones.md\n",
        "# ============================================================\n",
        "\n",
        "!pip install --quiet pdfplumber pandas pyarrow\n",
        "\n",
        "import os, re, json, unicodedata\n",
        "from pathlib import Path\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- CONFIGURACI√ìN ----------\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "TXT_DIR = os.path.join(OUT_DIR, \"txt_por_doc\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(TXT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "BASE_DOCS_JSONL   = os.path.join(OUT_DIR, \"base_documentos.jsonl\")\n",
        "BASE_DOCS_PARQUET = os.path.join(OUT_DIR, \"base_documentos.parquet\")\n",
        "SEG_A_JSONL       = os.path.join(OUT_DIR, \"seg_a.jsonl\")\n",
        "SEG_B_JSONL       = os.path.join(OUT_DIR, \"seg_b.jsonl\")\n",
        "NOTAS_PREPROC     = os.path.join(OUT_DIR, \"preprocesamiento_decisiones.md\")\n",
        "\n",
        "# (Ajusta si quer√©s otros tama√±os)\n",
        "CHUNK_WORDS   = 400    # tama√±o del chunk (en palabras aproximado)\n",
        "CHUNK_OVERLAP = 80     # solapamiento entre chunks\n",
        "\n",
        "# Si existen salidas previas, limpirlas (evita duplicados al re-ejecutar)\n",
        "for p in [BASE_DOCS_JSONL, BASE_DOCS_PARQUET, SEG_A_JSONL, SEG_B_JSONL, NOTAS_PREPROC]:\n",
        "    try:\n",
        "        if os.path.exists(p): os.remove(p)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------- NORMALIZACI√ìN ----------\n",
        "def normalize_unicode_nfc(text: str) -> str:\n",
        "    return unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "def strip_control_chars(text: str) -> str:\n",
        "    # Deja \\n y \\t; elimina otros de control\n",
        "    return \"\".join(ch for ch in text if ch in (\"\\n\",\"\\t\") or ord(ch) >= 32)\n",
        "\n",
        "def standardize_quotes_dashes(text: str) -> str:\n",
        "    repl = {\n",
        "        \"‚Äú\": \"\\\"\", \"‚Äù\": \"\\\"\", \"‚Äû\": \"\\\"\", \"¬´\": \"\\\"\", \"¬ª\": \"\\\"\",\n",
        "        \"‚Äô\": \"'\", \"¬¥\": \"'\", \"‚Äò\": \"'\",\n",
        "        \"‚Äê\": \"-\", \"‚Äì\": \"-\", \"‚Äî\": \"-\", \"‚àí\": \"-\",\n",
        "        \"‚Ä¶\": \"...\", \"‚Ä¢\": \"-\", \"¬∑\": \"-\"\n",
        "    }\n",
        "    for a, b in repl.items():\n",
        "        text = text.replace(a, b)\n",
        "    return text\n",
        "\n",
        "def remove_hyphen_linebreaks(text: str) -> str:\n",
        "    # Une palabras cortadas por guion al final de l√≠nea: \"infor-\\nmaci√≥n\" -> \"informaci√≥n\"\n",
        "    return re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
        "\n",
        "def collapse_whitespace(text: str) -> str:\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def to_lower(text: str) -> str:\n",
        "    return text.lower()\n",
        "\n",
        "def normalize_text_pipeline(raw: str) -> str:\n",
        "    if not raw: return \"\"\n",
        "    t = normalize_unicode_nfc(raw)\n",
        "    t = strip_control_chars(t)\n",
        "    t = standardize_quotes_dashes(t)\n",
        "    t = remove_hyphen_linebreaks(t)\n",
        "    t = collapse_whitespace(t)\n",
        "    t = to_lower(t)\n",
        "    return t\n",
        "\n",
        "# ---------- SEGMENTACI√ìN ----------\n",
        "def segment_fixed_overlap(text: str, words_per_chunk=CHUNK_WORDS, overlap=CHUNK_OVERLAP):\n",
        "    \"\"\"Segmentaci√≥n A: chunks por palabras con solapamiento.\"\"\"\n",
        "    words = text.split()\n",
        "    if not words: return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    idx = 0\n",
        "    while i < len(words):\n",
        "        chunk_words = words[i:i+words_per_chunk]\n",
        "        chunk_text = \" \".join(chunk_words).strip()\n",
        "        if chunk_text:\n",
        "            chunks.append((idx, chunk_text))\n",
        "            idx += 1\n",
        "        i += max(1, words_per_chunk - overlap)\n",
        "    return chunks\n",
        "\n",
        "_HEADING_RE = re.compile(\n",
        "    r\"\"\"^(\n",
        "        abstract\\b|\n",
        "        resumen\\b|\n",
        "        introducci[o√≥]n\\b|\n",
        "        conclusi[o√≥]n\\b|\n",
        "        referencias\\b|\n",
        "        agradecimientos\\b|\n",
        "        related\\ work\\b|\n",
        "        i{1,3}\\.|iv\\.|v\\.|vi\\.|vii\\.|viii\\.|ix\\.|x\\.|      # romanos\n",
        "        \\d+\\.\\s|                                          # 1. 2. 3.\n",
        "        [A-Z][A-Z0-9\\s\\-\\&]{3,}$                          # l√≠nea MAY√öSCULAS (t√≠tulo)\n",
        "    )\"\"\",\n",
        "    re.IGNORECASE | re.VERBOSE\n",
        ")\n",
        "\n",
        "def segment_by_headings(text: str, min_section_words=120):\n",
        "    \"\"\"Segmentaci√≥n B: por encabezados; fusiona secciones peque√±as.\"\"\"\n",
        "    lines = [l.strip() for l in text.splitlines()]\n",
        "    # Marcar √≠ndices de l√≠neas que parecen encabezado\n",
        "    header_idx = [i for i, line in enumerate(lines) if line and _HEADING_RE.match(line)]\n",
        "    # Siempre incluir inicio y fin\n",
        "    if 0 not in header_idx: header_idx = [0] + header_idx\n",
        "    if len(lines) - 1 not in header_idx: header_idx = header_idx + [len(lines) - 1]\n",
        "    header_idx = sorted(set(header_idx))\n",
        "\n",
        "    # Cortar por rangos\n",
        "    raw_sections = []\n",
        "    for a, b in zip(header_idx, header_idx[1:]):\n",
        "        sec = \"\\n\".join(lines[a:b]).strip()\n",
        "        if sec: raw_sections.append(sec)\n",
        "    # A√±adir √∫ltimo trozo\n",
        "    tail = \"\\n\".join(lines[header_idx[-1]:]).strip()\n",
        "    if tail: raw_sections.append(tail)\n",
        "\n",
        "    # Fusionar secciones demasiado peque√±as\n",
        "    merged = []\n",
        "    buff = []\n",
        "    wcount = 0\n",
        "    for sec in raw_sections:\n",
        "        wc = len(sec.split())\n",
        "        if wcount + wc < min_section_words:\n",
        "            buff.append(sec); wcount += wc\n",
        "        else:\n",
        "            if buff:\n",
        "                buff.append(sec)\n",
        "                merged.append(\"\\n\\n\".join(buff).strip())\n",
        "                buff, wcount = [], 0\n",
        "            else:\n",
        "                merged.append(sec)\n",
        "                buff, wcount = [], 0\n",
        "    if buff:\n",
        "        merged.append(\"\\n\\n\".join(buff).strip())\n",
        "\n",
        "    # Indexar\n",
        "    return [(i, s) for i, s in enumerate(merged)]\n",
        "\n",
        "# ---------- CARGA METADATA ----------\n",
        "assert os.path.exists(METADATA_FILE), f\"No existe METADATA_FILE: {METADATA_FILE}\"\n",
        "df_meta = pd.read_csv(METADATA_FILE, dtype=str, keep_default_na=False)\n",
        "for col in [\"id_doc\",\"nombre_archivo\",\"autor\",\"fecha\",\"tema\"]:\n",
        "    if col not in df_meta.columns:\n",
        "        raise ValueError(f\"Falta columna en metadata: {col}\")\n",
        "\n",
        "# √≠ndice r√°pido por nombre de archivo (case-insensitive)\n",
        "meta_idx = {str(n).strip().lower(): i for i, n in enumerate(df_meta[\"nombre_archivo\"])}\n",
        "\n",
        "# ---------- RECORRIDO PDFs ----------\n",
        "base_registros = []\n",
        "seg_a_registros = []\n",
        "seg_b_registros = []\n",
        "errores = []\n",
        "\n",
        "pdf_files_sorted = sorted([f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")])\n",
        "\n",
        "for fname in pdf_files_sorted:\n",
        "    key = fname.strip().lower()\n",
        "    if key not in meta_idx:\n",
        "        errores.append((fname, \"no_encontrado_en_metadata\"))\n",
        "        print(f\"‚ö†Ô∏è  {fname}: no aparece en 'nombre_archivo' de la metadata; se omite.\")\n",
        "        continue\n",
        "\n",
        "    row   = df_meta.iloc[meta_idx[key]]\n",
        "    iddoc = row[\"id_doc\"]; autor=row[\"autor\"]; fecha=row[\"fecha\"]; tema=row[\"tema\"]\n",
        "    pdf_path = os.path.join(PDFS_DIR, fname)\n",
        "\n",
        "    # Extraer texto del PDF\n",
        "    try:\n",
        "        pages = []\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for p in pdf.pages:\n",
        "                pages.append(p.extract_text() or \"\")\n",
        "        full_text = \"\\n\".join(pages)\n",
        "    except Exception as e:\n",
        "        errores.append((fname, f\"error_lectura_pdf:{e}\"))\n",
        "        print(f\"‚ùå Error leyendo {fname}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Normalizar\n",
        "    texto_limpio = normalize_text_pipeline(full_text)\n",
        "\n",
        "    # Guardar TXT por doc (√∫til para inspecci√≥n)\n",
        "    with open(os.path.join(TXT_DIR, f\"{iddoc}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(texto_limpio)\n",
        "\n",
        "    # Registro base (documento completo)\n",
        "    base_registros.append({\n",
        "        \"id_doc\": iddoc,\n",
        "        \"nombre_archivo\": fname,\n",
        "        \"autor\": autor,\n",
        "        \"fecha\": str(fecha),\n",
        "        \"tema\": tema,\n",
        "        \"texto_original\": full_text,\n",
        "        \"texto_limpio\": texto_limpio\n",
        "    })\n",
        "\n",
        "    # ---------- SEGMENTACI√ìN A: chunks fijos ----------\n",
        "    chunks_a = segment_fixed_overlap(texto_limpio, CHUNK_WORDS, CHUNK_OVERLAP)\n",
        "    for idx_chunk, chunk_text in chunks_a:\n",
        "        seg_a_registros.append({\n",
        "            \"id_doc\": iddoc,\n",
        "            \"segmentacion\": \"A\",\n",
        "            \"chunk_id\": f\"{iddoc}_A_{idx_chunk:03d}\",\n",
        "            \"idx\": idx_chunk,\n",
        "            \"autor\": autor,\n",
        "            \"fecha\": str(fecha),\n",
        "            \"tema\": tema,\n",
        "            \"texto\": chunk_text\n",
        "        })\n",
        "\n",
        "    # ---------- SEGMENTACI√ìN B: encabezados ----------\n",
        "    sections_b = segment_by_headings(texto_limpio, min_section_words=120)\n",
        "    for idx_sec, sec_text in sections_b:\n",
        "        seg_b_registros.append({\n",
        "            \"id_doc\": iddoc,\n",
        "            \"segmentacion\": \"B\",\n",
        "            \"chunk_id\": f\"{iddoc}_B_{idx_sec:03d}\",\n",
        "            \"idx\": idx_sec,\n",
        "            \"autor\": autor,\n",
        "            \"fecha\": str(fecha),\n",
        "            \"tema\": tema,\n",
        "            \"texto\": sec_text\n",
        "        })\n",
        "\n",
        "# ---------- GUARDAR SALIDAS ----------\n",
        "# Base documentos\n",
        "with open(BASE_DOCS_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in base_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "pd.DataFrame(base_registros).to_parquet(BASE_DOCS_PARQUET, index=False)\n",
        "\n",
        "# Segmentaciones\n",
        "with open(SEG_A_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in seg_a_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "with open(SEG_B_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in seg_b_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Mini-documentaci√≥n de decisiones para el informe\n",
        "notas = f\"\"\"# Preprocesamiento y Segmentaci√≥n (borrador)\n",
        "\n",
        "**Normalizaci√≥n aplicada**\n",
        "- Unicode NFC (mantener tildes correctas).\n",
        "- Limpieza de caracteres de control (excepto \\\\n y \\\\t).\n",
        "- Estandarizaci√≥n de comillas/guiones (‚Äú ‚Äù ‚Äò ‚Äô ‚Äî ‚Äì ‚Ä¶ ‚Üí \" ' - ...).\n",
        "- Uni√≥n de palabras cortadas por guion al fin de l√≠nea (e.g., \"infor-\\\\nmaci√≥n\" ‚Üí \"informaci√≥n\").\n",
        "- Colapso de espacios y saltos en blanco excesivos.\n",
        "- Conversi√≥n a min√∫sculas (para comparar segmentaciones bajo mismas condiciones).\n",
        "\n",
        "**Segmentaci√≥n A ‚Äì Chunks fijos**\n",
        "- Tama√±o ‚âà {CHUNK_WORDS} palabras, solapamiento ‚âà {CHUNK_OVERLAP}.\n",
        "- Ventajas: control de longitud, √∫til para evaluaci√≥n reproducible.\n",
        "- Desventajas: puede cortar ideas a mitad.\n",
        "\n",
        "**Segmentaci√≥n B ‚Äì Encabezados/Secciones**\n",
        "- Reglas: detec. de Abstract/Resumen/Introducci√≥n/Conclusi√≥n/Referencias, numerales (1., 2., ...), romanos (I., II., ...), t√≠tulos en MAY√öSCULAS.\n",
        "- Se fusionan secciones muy cortas (<120 palabras) con la siguiente para asegurar contexto m√≠nimo.\n",
        "- Ventajas: mantiene unidades sem√°nticas; Desventajas: depende de patrones editoriales.\n",
        "\n",
        "**Salidas**\n",
        "- base_documentos.jsonl / .parquet: texto completo normalizado por documento + metadata (autor/fecha/tema).\n",
        "- seg_a.jsonl / seg_b.jsonl: fragmentos con `id_doc`, `chunk_id`, `segmentacion`, `idx`, `texto`, y metadata.\n",
        "- txt_por_doc/: √∫til para inspecci√≥n r√°pida o depurar PDF problem√°ticos.\n",
        "\n",
        "**Razonamiento para comparaci√≥n**\n",
        "- A: garantiza tama√±o estable ‚Üí resultados de recuperaci√≥n comparables.\n",
        "- B: favorece coherencia sem√°ntica ‚Üí potencialmente mejor grounding.\n",
        "- Compa√±ero 2 debe crear dos √≠ndices (A y B) y comparar m√©tricas (recall@k, precisi√≥n manual, tiempo respuesta).\n",
        "\"\"\"\n",
        "with open(NOTAS_PREPROC, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(notas)\n",
        "\n",
        "print(\"‚úÖ Extracci√≥n, normalizaci√≥n y segmentaci√≥n listas\")\n",
        "print(f\"üßæ Base (JSONL):  {BASE_DOCS_JSONL}\")\n",
        "print(f\"üß± Base (Parquet): {BASE_DOCS_PARQUET}\")\n",
        "print(f\"üîπ Seg A (JSONL):  {SEG_A_JSONL}\")\n",
        "print(f\"üî∏ Seg B (JSONL):  {SEG_B_JSONL}\")\n",
        "print(f\"üóíÔ∏è  Notas:          {NOTAS_PREPROC}\")\n",
        "if errores:\n",
        "    print(f\"‚ö†Ô∏è Incidencias ({len(errores)}):\")\n",
        "    for e in errores[:12]:\n",
        "        print(\"   -\", e)\n",
        "    if len(errores) > 12:\n",
        "        print(\"   ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE9pHFW5QZ0P",
        "outputId": "4fcb921e-08ca-4810-e265-9c3e24915c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Verificando archivos generados...\n",
            "\n",
            "‚úÖ Metadata: encontrado (MetadataRAW.csv)\n",
            "‚úÖ Base documentos JSONL: encontrado (base_documentos.jsonl)\n",
            "‚úÖ Base documentos Parquet: encontrado (base_documentos.parquet)\n",
            "‚úÖ Segmentaci√≥n A (chunks fijos): encontrado (seg_a.jsonl)\n",
            "‚úÖ Segmentaci√≥n B (encabezados): encontrado (seg_b.jsonl)\n",
            "‚úÖ Notas de preprocesamiento: encontrado (preprocesamiento_decisiones.md)\n",
            "\n",
            "üìä PDFs reales: 46\n",
            "üìÑ Documentos procesados: 46\n",
            "\n",
            "‚úÖ Todos los documentos fueron procesados correctamente.\n",
            "\n",
            "üß≠ RESUMEN PARA David:\n",
            "\n",
            "Los datos est√°n listos para generar embeddings:\n",
            "\n",
            "- dataset/seg_a.jsonl ‚Üí fragmentos con segmentaci√≥n A (chunks fijos)\n",
            "- dataset/seg_b.jsonl ‚Üí fragmentos con segmentaci√≥n B (por encabezados)\n",
            "- MetadataRAW.csv (o metadata.csv) ‚Üí autor, fecha, tema por documento\n",
            "\n",
            "Archivos opcionales:\n",
            "- base_documentos.jsonl/.parquet ‚Üí textos completos normalizados\n",
            "- preprocesamiento_decisiones.md ‚Üí descripci√≥n de limpieza y segmentaci√≥n\n",
            "\n",
            "David debe usar seg_a.jsonl y seg_b.jsonl\n",
            "para crear las dos bases vectoriales y comparar su rendimiento.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# üîö CIERRE Priscilla ‚Äì VERIFICACI√ìN FINAL Y RESUMEN\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Rutas de salida esperadas\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "paths = {\n",
        "    \"Metadata\": METADATA_FILE,\n",
        "    \"Base documentos JSONL\": os.path.join(OUT_DIR, \"base_documentos.jsonl\"),\n",
        "    \"Base documentos Parquet\": os.path.join(OUT_DIR, \"base_documentos.parquet\"),\n",
        "    \"Segmentaci√≥n A (chunks fijos)\": os.path.join(OUT_DIR, \"seg_a.jsonl\"),\n",
        "    \"Segmentaci√≥n B (encabezados)\": os.path.join(OUT_DIR, \"seg_b.jsonl\"),\n",
        "    \"Notas de preprocesamiento\": os.path.join(OUT_DIR, \"preprocesamiento_decisiones.md\"),\n",
        "}\n",
        "\n",
        "print(\"üìÇ Verificando archivos generados...\\n\")\n",
        "for nombre, ruta in paths.items():\n",
        "    if os.path.exists(ruta):\n",
        "        print(f\"‚úÖ {nombre}: encontrado ({os.path.basename(ruta)})\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {nombre}: NO encontrado -> {ruta}\")\n",
        "\n",
        "# Conteo r√°pido de PDFs y documentos base\n",
        "pdfs = [f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")]\n",
        "pdf_count = len(pdfs)\n",
        "base_path = paths[\"Base documentos JSONL\"]\n",
        "base_count = sum(1 for _ in open(base_path, encoding=\"utf-8\")) if os.path.exists(base_path) else 0\n",
        "\n",
        "print(f\"\\nüìä PDFs reales: {pdf_count}\")\n",
        "print(f\"üìÑ Documentos procesados: {base_count}\")\n",
        "\n",
        "if pdf_count == base_count:\n",
        "    print(\"\\n‚úÖ Todos los documentos fueron procesados correctamente.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Hay diferencias entre PDFs y registros procesados. Revisa nombres o metadatos.\")\n",
        "\n",
        "print(\"\\nüß≠ RESUMEN PARA David:\")\n",
        "print(\"\"\"\n",
        "Los datos est√°n listos para generar embeddings:\n",
        "\n",
        "- dataset/seg_a.jsonl ‚Üí fragmentos con segmentaci√≥n A (chunks fijos)\n",
        "- dataset/seg_b.jsonl ‚Üí fragmentos con segmentaci√≥n B (por encabezados)\n",
        "- MetadataRAW.csv (o metadata.csv) ‚Üí autor, fecha, tema por documento\n",
        "\n",
        "Archivos opcionales:\n",
        "- base_documentos.jsonl/.parquet ‚Üí textos completos normalizados\n",
        "- preprocesamiento_decisiones.md ‚Üí descripci√≥n de limpieza y segmentaci√≥n\n",
        "\n",
        "David debe usar seg_a.jsonl y seg_b.jsonl\n",
        "para crear las dos bases vectoriales y comparar su rendimiento.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLJfwo7E--ol",
        "outputId": "458473ce-f335-4bad-fea6-1b1e04860ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos de segmentaci√≥n encontrados:\n",
            "- Segmentaci√≥n A: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_a.jsonl\n",
            "- Segmentaci√≥n B: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_b.jsonl\n",
            "\n",
            "Datos cargados:\n",
            "- Segmentaci√≥n A: 227 fragmentos\n",
            "- Segmentaci√≥n B: 349 fragmentos\n",
            "\n",
            "Ejemplo de fragmento (Segmentaci√≥n A):\n",
            "- chunk_id: DOC_033_A_000\n",
            "- id_doc: DOC_033\n",
            "- autor: Mar√≠a Jos√© Chac√≥n Rodr√≠guez\n",
            "- texto (primeros 100 chars): apuntes ia clase 7/10 gianmarco oporta pe'rez ingenier'ƒ±a en computacio'n instituto tecnolo'gico de ‚Ä¶\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 2: Configuraci√≥n y carga de datos segmentados (limpio)\n",
        "# ============================================================\n",
        "\n",
        "from typing import List, Dict\n",
        "import os, json\n",
        "\n",
        "# Rutas de datos (definidas por Compa√±ero 1)\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "SEG_A_JSONL = os.path.join(OUT_DIR, \"seg_a.jsonl\")\n",
        "SEG_B_JSONL = os.path.join(OUT_DIR, \"seg_b.jsonl\")\n",
        "\n",
        "# Verificar que los archivos existen\n",
        "if not os.path.exists(SEG_A_JSONL):\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ {SEG_A_JSONL}. Aseg√∫rate de que Compa√±ero 1 complet√≥ su parte.\")\n",
        "if not os.path.exists(SEG_B_JSONL):\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ {SEG_B_JSONL}. Aseg√∫rate de que Compa√±ero 1 complet√≥ su parte.\")\n",
        "\n",
        "print(\"Archivos de segmentaci√≥n encontrados:\")\n",
        "print(f\"- Segmentaci√≥n A: {SEG_A_JSONL}\")\n",
        "print(f\"- Segmentaci√≥n B: {SEG_B_JSONL}\")\n",
        "\n",
        "def load_segmented_data(jsonl_path: str) -> List[Dict]:\n",
        "    \"\"\"Carga los datos segmentados desde un archivo JSONL.\"\"\"\n",
        "    data = []\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Cargar datos\n",
        "seg_a_data = load_segmented_data(SEG_A_JSONL)\n",
        "seg_b_data = load_segmented_data(SEG_B_JSONL)\n",
        "\n",
        "print(\"\\nDatos cargados:\")\n",
        "print(f\"- Segmentaci√≥n A: {len(seg_a_data)} fragmentos\")\n",
        "print(f\"- Segmentaci√≥n B: {len(seg_b_data)} fragmentos\")\n",
        "\n",
        "# Mostrar ejemplo de un fragmento\n",
        "if seg_a_data:\n",
        "    ejemplo = seg_a_data[0]\n",
        "    print(\"\\nEjemplo de fragmento (Segmentaci√≥n A):\")\n",
        "    print(f\"- chunk_id: {ejemplo.get('chunk_id', 'N/A')}\")\n",
        "    print(f\"- id_doc: {ejemplo.get('id_doc', 'N/A')}\")\n",
        "    print(f\"- autor: {ejemplo.get('autor', 'N/A')}\")\n",
        "    print(f\"- texto (primeros 100 chars): {ejemplo.get('texto', '')[:100]}‚Ä¶\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJI3u-Sp_CBK",
        "outputId": "f3b41f09-dd8c-44da-f5c6-ee0bb50065a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configurando modelo de embeddings...\n",
            "Modelo objetivo: sentence-transformers/all-MiniLM-L6-v2\n",
            "Dimensi√≥n esperada: 384\n",
            "Dispositivo: CPU\n",
            "‚úÖ Embeddings v√≠a langchain_huggingface OK\n",
            "‚úÖ Embeddings listos en CPU\n",
            "\n",
            "Tokenizaci√≥n de ejemplo:\n",
            "- Longitud: 3170 chars  |  Tokens aprox: 822  |  L√≠mite ~8000\n",
            "\n",
            "Probando generaci√≥n de embedding...\n",
            "‚úÖ Embedding OK | Dimensi√≥n: 384\n",
            "Primeros 5 valores: [-0.02066517062485218, 0.02713960036635399, -0.03559556230902672, -0.028662530705332756, -0.03590256720781326]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 3: Tokenizaci√≥n y generaci√≥n de embeddings (estable)\n",
        "# Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
        "# ============================================================\n",
        "\n",
        "import warnings, sys\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tiktoken\n",
        "\n",
        "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
        "    try:\n",
        "        enc = tiktoken.encoding_for_model(model)\n",
        "        return len(enc.encode(text))\n",
        "    except Exception:\n",
        "        return max(1, len(text) // 4)\n",
        "\n",
        "print(\"Configurando modelo de embeddings...\")\n",
        "print(\"Modelo objetivo: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(\"Dimensi√≥n esperada: 384\")\n",
        "\n",
        "# Detectar dispositivo\n",
        "try:\n",
        "    import torch\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "except Exception:\n",
        "    device = \"cpu\"\n",
        "print(\"Dispositivo:\", device.upper())\n",
        "\n",
        "embeddings_model = None\n",
        "\n",
        "# 1) Intento: wrapper oficial de LangChain\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    embeddings_model = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={\"device\": device},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "    _ = embeddings_model.embed_query(\"probe\")\n",
        "    print(\"‚úÖ Embeddings v√≠a langchain_huggingface OK\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Falla en HuggingFaceEmbeddings:\", repr(e))\n",
        "    print(\"Activando fallback con SentenceTransformer...\")\n",
        "\n",
        "# 2) Fallback: usar SentenceTransformer directo y adaptarlo\n",
        "if embeddings_model is None:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    st_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
        "\n",
        "    class STEmbeddingsAdapter:\n",
        "        def __init__(self, model): self.model = model\n",
        "        def embed_query(self, text: str):\n",
        "            return self.model.encode([text], normalize_embeddings=True)[0].tolist()\n",
        "        def embed_documents(self, texts):\n",
        "            return self.model.encode(list(texts), normalize_embeddings=True).tolist()\n",
        "\n",
        "    embeddings_model = STEmbeddingsAdapter(st_model)\n",
        "    _ = embeddings_model.embed_query(\"probe\")\n",
        "    print(\"‚úÖ Embeddings v√≠a SentenceTransformer (fallback) OK\")\n",
        "\n",
        "print(\"‚úÖ Embeddings listos en\", device.upper())\n",
        "\n",
        "# Tokenizaci√≥n de ejemplo\n",
        "if 'seg_a_data' in globals() and seg_a_data:\n",
        "    ejemplo_texto = seg_a_data[0].get(\"texto\", \"\")\n",
        "    tk = count_tokens(ejemplo_texto)\n",
        "    print(\"\\nTokenizaci√≥n de ejemplo:\")\n",
        "    print(f\"- Longitud: {len(ejemplo_texto)} chars  |  Tokens aprox: {tk}  |  L√≠mite ~8000\")\n",
        "\n",
        "# Prueba de generaci√≥n de embedding\n",
        "print(\"\\nProbando generaci√≥n de embedding...\")\n",
        "test_text = \"Este es un texto de prueba para verificar que los embeddings funcionan correctamente.\"\n",
        "emb = embeddings_model.embed_query(test_text)\n",
        "print(\"‚úÖ Embedding OK | Dimensi√≥n:\", len(emb))\n",
        "print(\"Primeros 5 valores:\", emb[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq3O4s3cHlEn"
      },
      "source": [
        "**modulo conflictivo paso 4** error de incompatibilidad de librerias faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irw2GKdzzUa7",
        "outputId": "39afb371-abfd-4586-ff03-67100a5d422c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creando bases vectoriales‚Ä¶\n",
            "‚úì A creada y guardada (227 docs) | backend: numpy\n",
            "‚úì B creada y guardada (349 docs) | backend: numpy\n",
            "‚úì Ambos vectorstores listos.\n",
            "\n",
            "Consulta: ‚Äúinteligencia artificial y aprendizaje autom√°tico‚Äù\n",
            "Segmentaci√≥n A:\n",
            "  1. score=0.6387 | autor=Andrey Ure√±a Berm√∫dez | de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerz‚Ä¶\n",
            "  2. score=0.6069 | autor=Luis Alfredo Gonz√°lez S√°nchez | notas de clase inteligenciaartificial-12deagosto-semana2 luis alfredo gonza'lez sa'nchez escuela de ‚Ä¶\n",
            "  3. score=0.5983 | autor=Rodolfo David Acu√±a L√≥pez | - sistema mostrando comportamiento \"inteligente\" 1980s - algo \"inteligente\" que resuelva tareas comp‚Ä¶\n",
            "\n",
            "Segmentaci√≥n B:\n",
            "  1. score=0.6504 | autor=Andrey Ure√±a Berm√∫dez | vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosd‚Ä¶\n",
            "  2. score=0.6386 | autor=Rodolfo David Acu√±a L√≥pez | references [1] apuntes de la clase de inteligencia artificial, profesor steven andrey  pachecoportug‚Ä¶\n",
            "  3. score=0.6221 | autor=Priscilla Jim√©nez Salgado | la inteligencia artificialtiene aplicaciones en una gran variedad de a'reas. algunos ejemplos destac‚Ä¶\n",
            "\n",
            "Backends: A=numpy, B=numpy\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 4 (robusto, sin cambiar versiones): FAISS con fallback\n",
        "# Import seguro + salidas limpias (LangChain 1.x)\n",
        "# ============================================================\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "import numpy as np, os, shutil, json\n",
        "\n",
        "# LangChain 1.x: Document viene de langchain_core\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# --- Config de verbosidad (dejar en False para salidas limpias) ---\n",
        "VERBOSE = False\n",
        "TRY_FAISS = False  # dejar en False en este entorno (NumPy 2.x)\n",
        "\n",
        "# ---------- 0) Carga segura de FAISS y wrappers LC (condicional) ----------\n",
        "def _load_faiss(try_faiss: bool):\n",
        "    if not try_faiss:\n",
        "        return None\n",
        "    try:\n",
        "        import faiss as _faiss\n",
        "        _ = _faiss.IndexFlatIP(4)  # sanity check\n",
        "        return _faiss\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "_FAISS = _load_faiss(TRY_FAISS)\n",
        "\n",
        "# Importar wrappers de langchain_community solo si intentaremos FAISS\n",
        "LCFAISS = None\n",
        "InMemoryDocstore = None\n",
        "if _FAISS is not None:\n",
        "    try:\n",
        "        from langchain_community.vectorstores import FAISS as LCFAISS\n",
        "        from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "    except Exception:\n",
        "        LCFAISS = None\n",
        "        InMemoryDocstore = None\n",
        "\n",
        "# ---------- 1) Saneo de documentos ----------\n",
        "def create_documents_from_segments(segments: List[Dict[str, Any]]) -> List[Document]:\n",
        "    docs: List[Document] = []\n",
        "    for seg in segments:\n",
        "        txt = seg.get(\"texto\", \"\") or \"\"\n",
        "        if not isinstance(txt, str): txt = str(txt)\n",
        "        txt = txt.strip()\n",
        "        if not txt: continue\n",
        "        meta = {\n",
        "            \"id_doc\": seg.get(\"id_doc\", \"\"),\n",
        "            \"chunk_id\": seg.get(\"chunk_id\", \"\"),\n",
        "            \"segmentacion\": seg.get(\"segmentacion\", \"\"),\n",
        "            \"idx\": int(seg.get(\"idx\", 0) or 0),\n",
        "            \"autor\": seg.get(\"autor\", \"\"),\n",
        "            \"fecha\": seg.get(\"fecha\", \"\"),\n",
        "            \"tema\": seg.get(\"tema\", \"\"),\n",
        "            \"nombre_archivo\": seg.get(\"nombre_archivo\", \"\"),\n",
        "        }\n",
        "        docs.append(Document(page_content=txt, metadata=meta))\n",
        "    return docs\n",
        "\n",
        "# ---------- 2) Conversi√≥n robusta de embeddings ----------\n",
        "def to_float32_c_contiguous(vecs) -> np.ndarray:\n",
        "    import numpy as _np\n",
        "    try:\n",
        "        import torch as _torch\n",
        "    except Exception:\n",
        "        _torch = None\n",
        "\n",
        "    if vecs is None:\n",
        "        raise ValueError(\"Embedder devolvi√≥ None\")\n",
        "\n",
        "    if _torch is not None and isinstance(vecs, _torch.Tensor):\n",
        "        arr = vecs.detach().cpu().to(_torch.float32).numpy()\n",
        "        arr = _np.ascontiguousarray(arr, dtype=_np.float32)\n",
        "        if not _np.isfinite(arr).all(): raise ValueError(\"NaN/Inf en embeddings\")\n",
        "        return arr\n",
        "\n",
        "    if isinstance(vecs, (list, tuple)):\n",
        "        if len(vecs) == 0: raise ValueError(\"Lista de embeddings vac√≠a\")\n",
        "        rows, d = [], None\n",
        "        for i, v in enumerate(vecs):\n",
        "            if v is None: raise ValueError(f\"Fila {i} es None\")\n",
        "            if _torch is not None and hasattr(v, \"detach\") and hasattr(v, \"cpu\"):\n",
        "                v = v.detach().cpu().numpy()\n",
        "            v = _np.asarray(v, dtype=_np.float32).reshape(-1)\n",
        "            if d is None: d = v.shape[0]\n",
        "            elif v.shape[0] != d: raise ValueError(f\"Dimensiones inconsistentes en fila {i}: {v.shape[0]} vs {d}\")\n",
        "            rows.append(v)\n",
        "        arr = _np.vstack(rows).astype(_np.float32, copy=False)\n",
        "        arr = _np.ascontiguousarray(arr, dtype=_np.float32)\n",
        "        if not _np.isfinite(arr).all(): raise ValueError(\"NaN/Inf en embeddings\")\n",
        "        return arr\n",
        "\n",
        "    arr = _np.asarray(vecs, dtype=_np.float32)\n",
        "    arr = _np.ascontiguousarray(arr, dtype=_np.float32)\n",
        "    if arr.ndim != 2: raise ValueError(f\"Embeddings ndim={arr.ndim}, esperado 2\")\n",
        "    if not _np.isfinite(arr).all(): raise ValueError(\"NaN/Inf en embeddings\")\n",
        "    return arr\n",
        "\n",
        "# ---------- 3) Backend de respaldo (NumPy Cosine) ----------\n",
        "@dataclass\n",
        "class SimpleDoc:\n",
        "    text: str\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "class NumpyCosineVS:\n",
        "    \"\"\"VectorStore m√≠nimo compatible con similarity_search_with_score/save/load.\"\"\"\n",
        "    def __init__(self, embeddings, docs: List[Document], X: np.ndarray):\n",
        "        self.embeddings = embeddings\n",
        "        self.docs = [SimpleDoc(d.page_content, dict(d.metadata)) for d in docs]\n",
        "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1.0\n",
        "        self.X = (X / norms).astype(np.float32, copy=False)\n",
        "\n",
        "    def similarity_search_with_score(self, query: str, k: int = 5):\n",
        "        qv = self.embeddings.embed_query(query)\n",
        "        qv = np.asarray(qv, dtype=np.float32).reshape(1, -1)\n",
        "        qn = qv / max(np.linalg.norm(qv), 1e-12)\n",
        "        scores = (self.X @ qn.T).reshape(-1)\n",
        "        k = max(1, min(k, len(scores)))\n",
        "        idx = np.argpartition(-scores, kth=k-1)[:k]\n",
        "        idx = idx[np.argsort(-scores[idx])]\n",
        "        return [(Document(page_content=self.docs[i].text, metadata=self.docs[i].metadata),\n",
        "                 float(scores[i])) for i in idx]\n",
        "\n",
        "    def save_local(self, dirpath: str):\n",
        "        os.makedirs(dirpath, exist_ok=True)\n",
        "        np.save(os.path.join(dirpath, \"vectors.npy\"), self.X)\n",
        "        with open(os.path.join(dirpath, \"meta.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            for d in self.docs:\n",
        "                f.write(json.dumps({\"text\": d.text, \"metadata\": d.metadata}, ensure_ascii=False) + \"\\n\")\n",
        "        with open(os.path.join(dirpath, \"_backend.txt\"), \"w\") as f:\n",
        "            f.write(\"numpy\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_local(cls, dirpath: str, embeddings):\n",
        "        X = np.load(os.path.join(dirpath, \"vectors.npy\"))\n",
        "        docs = []\n",
        "        with open(os.path.join(dirpath, \"meta.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                docs.append(Document(page_content=obj[\"text\"], metadata=obj[\"metadata\"]))\n",
        "        return cls(embeddings=embeddings, docs=docs, X=X)\n",
        "\n",
        "# ---------- 4) Intento FAISS con fallback silencioso ----------\n",
        "def _try_faiss_index(X: np.ndarray, docs: List[Document], embedder, normalize: bool):\n",
        "    \"\"\"\n",
        "    Devuelve (vs, backend) usando FAISS si _FAISS y LCFAISS est√°n disponibles.\n",
        "    Si no lo est√°n, devuelve (None, None) para activar NumPy.\n",
        "    \"\"\"\n",
        "    if _FAISS is None or LCFAISS is None or InMemoryDocstore is None:\n",
        "        return None, None\n",
        "    try:\n",
        "        index = _FAISS.IndexFlatIP(X.shape[1])\n",
        "        Xreq = np.require(X, dtype=np.float32, requirements=[\"C\", \"A\", \"W\"])\n",
        "        try:\n",
        "            index.add(Xreq)\n",
        "        except Exception:\n",
        "            Xcopy = np.array(Xreq, dtype=np.float32, copy=True, order=\"C\")\n",
        "            index.add(Xcopy)\n",
        "\n",
        "        id_map = {str(i): doc for i, doc in enumerate(docs)}\n",
        "        docstore = InMemoryDocstore(id_map)\n",
        "        index_to_docstore_id = {i: str(i) for i in range(X.shape[0])}\n",
        "        vs = LCFAISS(\n",
        "            embedding_function=embedder,\n",
        "            index=index,\n",
        "            docstore=docstore,\n",
        "            index_to_docstore_id=index_to_docstore_id,\n",
        "            normalize_L2=normalize,\n",
        "        )\n",
        "        return vs, \"faiss\"\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def build_vs_with_faiss_or_fallback(docs: List[Document], embedder, normalize=True):\n",
        "    if not docs:\n",
        "        raise ValueError(\"No hay documentos para indexar.\")\n",
        "    texts = [d.page_content for d in docs]\n",
        "    vecs = embedder.embed_documents(texts)\n",
        "    X = to_float32_c_contiguous(vecs)\n",
        "    if normalize:\n",
        "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1.0\n",
        "        X = (X / norms).astype(np.float32, copy=False)\n",
        "\n",
        "    vs, backend = _try_faiss_index(X, docs, embedder, normalize)\n",
        "    if vs is not None:\n",
        "        return vs, backend\n",
        "    return NumpyCosineVS(embeddings=embedder, docs=docs, X=X), \"numpy\"\n",
        "\n",
        "# ---------- 5) Creaci√≥n y guardado de A/B ----------\n",
        "VECTORSTORE_DIR_A = os.path.join(OUT_DIR, \"vectorstore_a\")\n",
        "VECTORSTORE_DIR_B = os.path.join(OUT_DIR, \"vectorstore_b\")\n",
        "for p in [VECTORSTORE_DIR_A, VECTORSTORE_DIR_B]:\n",
        "    if os.path.exists(p): shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "print(\"Creando bases vectoriales‚Ä¶\")\n",
        "try:\n",
        "    docs_a = create_documents_from_segments(seg_a_data)\n",
        "    vectorstore_a, backend_a = build_vs_with_faiss_or_fallback(docs_a, embeddings_model, normalize=True)\n",
        "    vectorstore_a.save_local(VECTORSTORE_DIR_A)\n",
        "    print(f\"‚úì A creada y guardada ({len(docs_a)} docs) | backend: {backend_a}\")\n",
        "\n",
        "    docs_b = create_documents_from_segments(seg_b_data)\n",
        "    vectorstore_b, backend_b = build_vs_with_faiss_or_fallback(docs_b, embeddings_model, normalize=True)\n",
        "    vectorstore_b.save_local(VECTORSTORE_DIR_B)\n",
        "    print(f\"‚úì B creada y guardada ({len(docs_b)} docs) | backend: {backend_b}\")\n",
        "\n",
        "    print(\"‚úì Ambos vectorstores listos.\")\n",
        "except Exception as e:\n",
        "    print(\"‚úó Error creando vectorstores:\", e)\n",
        "    raise\n",
        "\n",
        "# ---------- 6) Prueba de b√∫squeda (salida breve) ----------\n",
        "test_query = \"inteligencia artificial y aprendizaje autom√°tico\"\n",
        "try:\n",
        "    res_a = vectorstore_a.similarity_search_with_score(test_query, k=3)\n",
        "    res_b = vectorstore_b.similarity_search_with_score(test_query, k=3)\n",
        "\n",
        "    print(f\"\\nConsulta: ‚Äú{test_query}‚Äù\")\n",
        "    print(\"Segmentaci√≥n A:\")\n",
        "    for i, (doc, score) in enumerate(res_a, 1):\n",
        "        autor = doc.metadata.get(\"autor\", \"N/A\")\n",
        "        resumen = (doc.page_content[:100].replace(\"\\n\",\" \") + \"‚Ä¶\") if len(doc.page_content) > 100 else doc.page_content\n",
        "        print(f\"  {i}. score={score:.4f} | autor={autor} | {resumen}\")\n",
        "\n",
        "    print(\"\\nSegmentaci√≥n B:\")\n",
        "    for i, (doc, score) in enumerate(res_b, 1):\n",
        "        autor = doc.metadata.get(\"autor\", \"N/A\")\n",
        "        resumen = (doc.page_content[:100].replace(\"\\n\",\" \") + \"‚Ä¶\") if len(doc.page_content) > 100 else doc.page_content\n",
        "        print(f\"  {i}. score={score:.4f} | autor={autor} | {resumen}\")\n",
        "\n",
        "    print(f\"\\nBackends: A={backend_a}, B={backend_b}\")\n",
        "except Exception as e:\n",
        "    print(\"‚úó Error en b√∫squeda:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LneHSDcq_G3J"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 5: Crear RAG Tool en LangChain\n",
        "# La herramienta consulta la base vectorial y devuelve:\n",
        "# - fragmento (texto)\n",
        "# - documento de origen (id_doc, nombre_archivo)\n",
        "# - autor\n",
        "# ============================================================\n",
        "\n",
        "from typing import Any, List, Tuple, Dict, Optional\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import inspect\n",
        "from langchain_core.tools import Tool  # Para LangChain 0.2; si usas 0.1, cambia a langchain.tools\n",
        "\n",
        "# ---- Helpers de parsing ------------------------------------------------------\n",
        "\n",
        "def _parse_query_and_filters(raw_query: str) -> tuple[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Extrae filtros de la query y devuelve (texto_libre, filtros_dict).\n",
        "    Formatos admitidos (espacios opcionales):\n",
        "      - autor:\"Alan Turing\"   |  autor: Alan Turing\n",
        "      - fecha>=2024-01-01     |  fecha<=2025-12-31 (ISO YYYY-MM-DD)\n",
        "      - tema:\"redes neuronales\" | tema: redes neuronales\n",
        "      - tags: audio, cnn\n",
        "    \"\"\"\n",
        "    q = raw_query.strip()\n",
        "\n",
        "    # Mapa de alias -> campo real de metadata (ajusta seg√∫n tus metadatos)\n",
        "    keymap = {\n",
        "        \"autor\": \"autor\",\n",
        "        \"author\": \"autor\",\n",
        "        \"fecha\": \"fecha\",\n",
        "        \"date\": \"fecha\",\n",
        "        \"tema\": \"tema\",\n",
        "        \"topic\": \"tema\",\n",
        "        \"tags\": \"tags\",\n",
        "        \"etiquetas\": \"tags\"\n",
        "    }\n",
        "\n",
        "    # Captura patrones key op value (op: :, =, >=, <=, >, <)\n",
        "    # Ej.: autor:\"Alan Turing\", fecha>=2024-01-01, tema: redes neuronales\n",
        "    token_pat = re.compile(\n",
        "        r'(?P<k>\\w+)\\s*(?P<op>:|=|>=|<=|>|<)\\s*(?P<v>\"[^\"]+\"|\\'[^\\']+\\'|[^,;]+)'\n",
        "    )\n",
        "\n",
        "    filters: Dict[str, Any] = {}\n",
        "    consumed_spans = []\n",
        "\n",
        "    for m in token_pat.finditer(q):\n",
        "        k_raw = m.group(\"k\").lower()\n",
        "        op = m.group(\"op\")\n",
        "        v_raw = m.group(\"v\").strip()\n",
        "\n",
        "        # Limpia comillas\n",
        "        if len(v_raw) >= 2 and ((v_raw[0] == '\"' and v_raw[-1] == '\"') or (v_raw[0] == \"'\" and v_raw[-1] == \"'\")):\n",
        "            v = v_raw[1:-1].strip()\n",
        "        else:\n",
        "            v = v_raw.strip()\n",
        "\n",
        "        # Normaliza clave\n",
        "        k = keymap.get(k_raw)\n",
        "        if not k:\n",
        "            continue  # clave desconocida -> la ignoramos\n",
        "\n",
        "        # Guardamos como lista de condiciones por clave (p.ej., m√∫ltiples tags)\n",
        "        if k not in filters:\n",
        "            filters[k] = []\n",
        "\n",
        "        # Procesamos valores especiales\n",
        "        if k == \"tags\":\n",
        "            # Separar por comas y limpiar\n",
        "            tags = [t.strip() for t in re.split(r'[;,]', v) if t.strip()]\n",
        "            filters[k].append((\"in\", tags))\n",
        "        elif k == \"fecha\":\n",
        "            # Intentamos parsear fecha ISO\n",
        "            try:\n",
        "                _ = datetime.fromisoformat(v)\n",
        "            except Exception:\n",
        "                # si no es ISO, lo tratamos como string literal\n",
        "                pass\n",
        "            filters[k].append((op, v))\n",
        "        else:\n",
        "            # texto exacto/contiene (para op \":\" interpretamos \"contiene\")\n",
        "            if op == \":\":\n",
        "                filters[k].append((\"contains\", v))\n",
        "            elif op == \"=\":\n",
        "                filters[k].append((\"eq\", v))\n",
        "            else:\n",
        "                # Para autor/tema normalmente no tiene sentido > <, pero lo admitimos como \"eq\" por compatibilidad\n",
        "                filters[k].append((\"eq\", v))\n",
        "\n",
        "        consumed_spans.append(m.span())\n",
        "\n",
        "    # Remueve las partes ‚Äúconsumidas‚Äù de la query para dejar el texto libre\n",
        "    free_text_parts = []\n",
        "    last_end = 0\n",
        "    for (start, end) in consumed_spans:\n",
        "        # a√±ade el texto entre patrones\n",
        "        free_text_parts.append(q[last_end:start])\n",
        "        last_end = end\n",
        "    free_text_parts.append(q[last_end:])\n",
        "    free_text = \" \".join(p.strip() for p in free_text_parts).strip()\n",
        "    return (free_text, filters)\n",
        "\n",
        "\n",
        "def _meta_match(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Aplica filtros a un diccionario de metadata.\n",
        "    Soporta:\n",
        "      - autor: (\"contains\", \"Turing\") | (\"eq\",\"Alan Turing\")\n",
        "      - tema:  idem\n",
        "      - fecha: (\">=\", \"2024-01-01\") etc. (si ISO; si no, compara string)\n",
        "      - tags:  (\"in\", [\"audio\",\"cnn\"]) => al menos uno debe estar en meta['tags']\n",
        "    \"\"\"\n",
        "    def _as_str(x): return \"\" if x is None else str(x)\n",
        "\n",
        "    for key, conds in filters.items():\n",
        "        val = meta.get(key)\n",
        "        # Normaliza tipo\n",
        "        if key == \"tags\":\n",
        "            # meta puede ser str \"a,b,c\" o lista\n",
        "            if isinstance(val, str):\n",
        "                meta_tags = [t.strip().lower() for t in re.split(r'[;,]', val) if t.strip()]\n",
        "            elif isinstance(val, list):\n",
        "                meta_tags = [str(t).strip().lower() for t in val]\n",
        "            else:\n",
        "                meta_tags = []\n",
        "        elif key == \"fecha\":\n",
        "            # Mantener string original; si es ISO podemos parsear\n",
        "            meta_fecha = _as_str(val)\n",
        "        else:\n",
        "            meta_text = _as_str(val)\n",
        "\n",
        "        for (op, expect) in conds:\n",
        "            if key == \"tags\" and op == \"in\":\n",
        "                wanted = [t.lower() for t in expect]\n",
        "                if not any(w in meta_tags for w in wanted):\n",
        "                    return False\n",
        "            elif key in (\"autor\", \"tema\"):\n",
        "                if op == \"contains\":\n",
        "                    if expect.lower() not in meta_text.lower():\n",
        "                        return False\n",
        "                elif op == \"eq\":\n",
        "                    if meta_text.lower() != expect.lower():\n",
        "                        return False\n",
        "                else:\n",
        "                    # Otros ops no aplican; consideramos fallo\n",
        "                    return False\n",
        "            elif key == \"fecha\":\n",
        "                # intentamos comparaci√≥n temporal si ambas son ISO (YYYY-MM-DD)\n",
        "                lhs, rhs = meta_fecha, str(expect)\n",
        "                try:\n",
        "                    d_lhs = datetime.fromisoformat(lhs)\n",
        "                    d_rhs = datetime.fromisoformat(rhs)\n",
        "                    if op == \">=\" and not (d_lhs >= d_rhs): return False\n",
        "                    if op == \"<=\" and not (d_lhs <= d_rhs): return False\n",
        "                    if op == \">\"  and not (d_lhs >  d_rhs): return False\n",
        "                    if op == \"<\"  and not (d_lhs <  d_rhs): return False\n",
        "                    if op in (\":\", \"=\", \"eq\") and not (d_lhs == d_rhs): return False\n",
        "                except Exception:\n",
        "                    # Si no son ISO, compara como string\n",
        "                    if op in (\":\", \"=\", \"eq\") and not (lhs == rhs): return False\n",
        "                    if op == \">=\" and not (lhs >= rhs): return False\n",
        "                    if op == \"<=\" and not (lhs <= rhs): return False\n",
        "                    if op == \">\"  and not (lhs >  rhs): return False\n",
        "                    if op == \"<\"  and not (lhs <  rhs): return False\n",
        "            else:\n",
        "                # clave no soportada\n",
        "                return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def _format_result(i: int, score: Optional[float], seg: str, doc: Any) -> str:\n",
        "    try:\n",
        "        score_str = f\"{float(score):.4f}\" if score is not None else \"N/A\"\n",
        "    except Exception:\n",
        "        score_str = str(score) if score is not None else \"N/A\"\n",
        "\n",
        "    meta = getattr(doc, \"metadata\", {}) or {}\n",
        "    fragmento = getattr(doc, \"page_content\", \"\") or \"\"\n",
        "    preview = fragmento[:500] + (\"...\" if len(fragmento) > 500 else \"\")\n",
        "\n",
        "    id_doc         = str(meta.get(\"id_doc\", \"N/A\"))\n",
        "    nombre_archivo = str(meta.get(\"nombre_archivo\", \"N/A\"))\n",
        "    autor          = str(meta.get(\"autor\", \"N/A\"))\n",
        "    chunk_id       = str(meta.get(\"chunk_id\", \"N/A\"))\n",
        "\n",
        "    source_line = f\"Documento: {id_doc}\"\n",
        "    if nombre_archivo and nombre_archivo != \"N/A\":\n",
        "        source_line += f\" ¬∑ archivo: {nombre_archivo}\"\n",
        "    source_line += f\" ¬∑ chunk: {chunk_id}\"\n",
        "\n",
        "    return (\n",
        "        f\"[{seg} ¬∑ Resultado {i} ¬∑ score/distancia: {score_str}]\\n\"\n",
        "        f\"Fragmento:\\n{preview}\\n\"\n",
        "        f\"{source_line}\\n\"\n",
        "        f\"Autor: {autor}\"\n",
        "    )\n",
        "\n",
        "# ---- Factory del Tool con filtros -------------------------------------------\n",
        "\n",
        "def create_rag_tool(\n",
        "    vectorstore: Any,\n",
        "    segmentacion_name: str,\n",
        "    default_top_k: int = 5,\n",
        "    name_prefix: str = \"rag_search\"\n",
        ") -> Tool:\n",
        "    \"\"\"\n",
        "    Tool RAG que soporta filtros por metadata embebidos en la query:\n",
        "      - autor:\"Nombre\"\n",
        "      - fecha>=YYYY-MM-DD\n",
        "      - tema: algo\n",
        "      - tags: a, b, c\n",
        "    \"\"\"\n",
        "\n",
        "    def rag_search(user_query: str) -> str:\n",
        "        query_text, filters = _parse_query_and_filters(user_query or \"\")\n",
        "\n",
        "        if not query_text and not filters:\n",
        "            return \"No se proporcion√≥ una consulta.\"\n",
        "\n",
        "        # --- Intento 1: usar filtro nativo si el vectorstore lo admite ---\n",
        "        results: List[Tuple[Any, Optional[float]]] = []\n",
        "        used_native_filter = False\n",
        "        k_request = default_top_k\n",
        "\n",
        "        # ¬øsimilarity_search_with_score acepta 'filter'?\n",
        "        sig = None\n",
        "        try:\n",
        "            sig = inspect.signature(vectorstore.similarity_search_with_score)\n",
        "        except Exception:\n",
        "            sig = None\n",
        "\n",
        "        if sig and \"filter\" in sig.parameters and filters:\n",
        "            # Convertimos nuestros filtros a un dict simple de igualdad/contiene\n",
        "            # Nota: distintos backends aceptan diferentes operadores;\n",
        "            # aqu√≠ probamos un mapeo b√°sico (autor/tema eq|contains, tags in).\n",
        "            backend_filter = {}\n",
        "\n",
        "            # Igualdades simples (preferimos eq a contains si hay solo eq)\n",
        "            for k, conds in filters.items():\n",
        "                if k == \"tags\":\n",
        "                    # algunos backends aceptan {\"tags\": {\"$in\": [...]}}\n",
        "                    # aqu√≠ probamos un formato directo (depende del backend)\n",
        "                    values = []\n",
        "                    for (op, v) in conds:\n",
        "                        if op == \"in\":\n",
        "                            values.extend(v)\n",
        "                    if values:\n",
        "                        backend_filter[k] = values  # puede requerir adaptaci√≥n\n",
        "                elif k in (\"autor\", \"tema\"):\n",
        "                    # prioriza \"eq\" si existe, si no \"contains\"\n",
        "                    eq_val = next((v for (op, v) in conds if op == \"eq\"), None)\n",
        "                    contains_val = next((v for (op, v) in conds if op == \"contains\"), None)\n",
        "                    if eq_val is not None:\n",
        "                        backend_filter[k] = eq_val\n",
        "                    elif contains_val is not None:\n",
        "                        backend_filter[k] = contains_val\n",
        "                elif k == \"fecha\":\n",
        "                    # muchos backends no soportan rangos; lo haremos en fallback\n",
        "                    pass\n",
        "\n",
        "            try:\n",
        "                raw = vectorstore.similarity_search_with_score(\n",
        "                    query_text if query_text else \"\", k=max(k_request, 10),\n",
        "                    filter=backend_filter if backend_filter else None\n",
        "                )\n",
        "                # Normaliza a [(doc, score)]\n",
        "                if raw and isinstance(raw[0], tuple) and len(raw[0]) == 2:\n",
        "                    results = [(doc, score) for doc, score in raw]\n",
        "                elif raw:\n",
        "                    results = [(doc, None) for doc in raw]\n",
        "                used_native_filter = True\n",
        "            except Exception:\n",
        "                used_native_filter = False\n",
        "                results = []\n",
        "\n",
        "        # --- Intento 2: fallback universal (k grande + filtrado en Python) ---\n",
        "        if not results:\n",
        "            try:\n",
        "                raw = vectorstore.similarity_search_with_score(\n",
        "                    query_text if query_text else \"\", k=max(default_top_k * 10, 50)\n",
        "                )\n",
        "                if raw and isinstance(raw[0], tuple) and len(raw[0]) == 2:\n",
        "                    results = [(doc, score) for doc, score in raw]\n",
        "                elif raw:\n",
        "                    results = [(doc, None) for doc in raw]\n",
        "            except Exception:\n",
        "                # fallback extra: similarity_search sin score\n",
        "                try:\n",
        "                    raw = vectorstore.similarity_search(query_text if query_text else \"\", k=max(default_top_k * 10, 50))\n",
        "                    results = [(doc, None) for doc in raw]\n",
        "                except Exception as e:\n",
        "                    return f\"Error en la b√∫squeda RAG: {str(e)}\"\n",
        "\n",
        "            # si tenemos filtros, aplicarlos en memoria\n",
        "            if filters:\n",
        "                filtered = []\n",
        "                for (doc, sc) in results:\n",
        "                    meta = getattr(doc, \"metadata\", {}) or {}\n",
        "                    if _meta_match(meta, filters):\n",
        "                        filtered.append((doc, sc))\n",
        "                results = filtered\n",
        "\n",
        "        if not results:\n",
        "            return \"No se encontraron fragmentos relevantes para la consulta.\"\n",
        "\n",
        "        # Limitar a top-k finales\n",
        "        results = results[:default_top_k]\n",
        "\n",
        "        # Formato de salida\n",
        "        lines = []\n",
        "        for i, (doc, score) in enumerate(results, 1):\n",
        "            lines.append(_format_result(i, score, segmentacion_name, doc))\n",
        "\n",
        "        # Nota: ‚Äúscore‚Äù puede ser distancia: menor = mejor (FAISS).\n",
        "        # No asumimos que mayor sea mejor universalmente.\n",
        "        # Si usaste filtro nativo, parte del trabajo ya se hizo ‚Äúaguas arriba‚Äù.\n",
        "        return \"\\n\\n\".join(lines)\n",
        "\n",
        "    tool_name = f\"{name_prefix}_{segmentacion_name}\"\n",
        "    tool_desc = (\n",
        "        f\"B√∫squeda RAG en segmentaci√≥n {segmentacion_name} con filtros por metadata. \"\n",
        "        f\"Admite en la query: autor:\\\"Nombre\\\", fecha>=YYYY-MM-DD, tema: algo, tags: a,b,c. \"\n",
        "        f\"Devuelve fragmentos + fuente (id_doc, archivo, chunk) + autor.\"\n",
        "    )\n",
        "\n",
        "    return Tool(\n",
        "        name=tool_name,\n",
        "        description=tool_desc,\n",
        "        func=rag_search,\n",
        "        return_direct=False\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehh92iT6nybV",
        "outputId": "0226ef98-c3c0-4bdb-f3f6-ce17a58f97b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creando herramientas RAG...\n",
            "Herramientas RAG creadas:\n",
            "   - rag_search_A: B√∫squeda RAG en segmentaci√≥n A con filtros por metadata. Admite en la query: autor:\"Nombre\", fecha>=YYYY-MM-DD, tema: algo, tags: a,b,c. Devuelve fragmentos + fuente (id_doc, archivo, chunk) + autor....\n",
            "   - rag_search_B: B√∫squeda RAG en segmentaci√≥n B con filtros por metadata. Admite en la query: autor:\"Nombre\", fecha>=YYYY-MM-DD, tema: algo, tags: a,b,c. Devuelve fragmentos + fuente (id_doc, archivo, chunk) + autor....\n",
            "\n",
            "========================================================================\n",
            "PRUEBA EMP√çRICA A vs B (sin overlap)\n",
            "========================================================================\n",
            "\n",
            "Filtro: autor=\"Priscilla\" + tema=\"machine learning\"\n",
            "A: segmentaci√≥n por chunks fijos | B: por encabezados/secciones\n",
            "\n",
            "\n",
            "========================================================================\n",
            "RESULTADOS ‚Äî Segmento A (chunks fijos)\n",
            "========================================================================\n",
            "\n",
            "Query: autor:\"Priscilla\" tema:\"machine learning\"\n",
            "M√©tricas A: items=2 | docs_unicos=1 | chunks_unicos=2 | chars=794 | tiempo=21.5 ms\n",
            "\n",
            "Tabla A (resumen):\n",
            " resultado_idx  id_doc      chunk_id archivo                     autor score_ou_distancia  chars_fragmento\n",
            "             1 DOC_003 DOC_003_A_005     N/A Priscilla Jim√©nez Salgado             0.1345              280\n",
            "             2 DOC_003 DOC_003_A_004     N/A Priscilla Jim√©nez Salgado             0.0459              514\n",
            "\n",
            "Vista previa A (Top-1 fragmento, truncado):\n",
            "Fragmento:\n",
            "ma's eficiente hacia una solucio'n, representando la mejor la pro'xima semana la modalidad sera' virtual. opcio'ncomoelcaminodemenorcosto. hasta el martes 26 de agosto las clases sera'n presencialesyesemarteshabra' quiz acumulativo. fig.11. ejemplodelprocesodebu'squeda\n",
            "\n",
            "========================================================================\n",
            "RESULTADOS ‚Äî Segmento B (encabezados/secciones)\n",
            "========================================================================\n",
            "\n",
            "Query: autor:\"Priscilla\" tema:\"machine learning\"\n",
            "M√©tricas B: items=1 | docs_unicos=1 | chunks_unicos=1 | chars=514 | tiempo=15.5 ms\n",
            "\n",
            "Tabla B (resumen):\n",
            " resultado_idx  id_doc      chunk_id archivo                     autor score_ou_distancia  chars_fragmento\n",
            "             1 DOC_003 DOC_003_B_002     N/A Priscilla Jim√©nez Salgado             0.0664              514\n",
            "\n",
            "Vista previa B (Top-1 fragmento, truncado):\n",
            "Fragmento:\n",
            "ii. machinelearning\n",
            "machine learning. esto incluye experimentar\n",
            "conaprendizajenosupervisado.\n",
            "\n",
            "el concepto de machine learning consiste en\n",
            "disenÀúar ma'quinas capaces de realizar tareas sin estar\n",
            "programadasdeformaexpl'ƒ±cita,extrayendolalo'gica\n",
            "directamentedelosdatos.\n",
            "por ejemplo, no se le indicara' a la computadora\n",
            "que' es un perro ni las reglas que lo definen (cola, fig.4. descripcio'ngeneraldelusodemachinelearningenlaciencia.\n",
            "ojos, raza, etc.); en su lugar, se le proporcionara' una\n",
            "imagen y ret...\n",
            "\n",
            "[Nota metodol√≥gica]\n",
            "Dise√±o: comparaci√≥n de recuperaci√≥n RAG en dos segmentaciones independientes: A (chunks fijos) y B (encabezados/secciones). La consulta restringe por metadata (Filtro: autor=\"Priscilla\" + tema=\"machine learning\"). Se reportan, por segmento: n√∫mero de items, documentos √∫nicos, chunks √∫nicos, caracteres agregados y latencia. Los resultados se presentan por separado (sin superposiciones), priorizando claridad y concisi√≥n.\n",
            "\n",
            "[CSV generados]\n",
            " - rag_eval/segmento_A_resultados.csv\n",
            " - rag_eval/segmento_B_resultados.csv\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Comparaci√≥n RAG A vs B (paper-friendly, sin overlap)\n",
        "# A = chunks fijos | B = encabezados/secciones\n",
        "# Filtros: autor=\"Priscilla\" + (opcional) tema\n",
        "# Produce: m√©tricas concisas, tablas por segmento y CSV separados.\n",
        "# ============================================================\n",
        "\n",
        "import time, re, os\n",
        "from typing import List, Dict, Tuple, Any, Optional\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Crear tools por segmentaci√≥n\n",
        "# -----------------------------\n",
        "print(\"Creando herramientas RAG...\")\n",
        "rag_tool_a = create_rag_tool(vectorstore_a, segmentacion_name=\"A\", default_top_k=5)\n",
        "rag_tool_b = create_rag_tool(vectorstore_b, segmentacion_name=\"B\", default_top_k=5)\n",
        "\n",
        "print(\"Herramientas RAG creadas:\")\n",
        "print(f\"   - {rag_tool_a.name}: {rag_tool_a.description[:200]}...\")\n",
        "print(f\"   - {rag_tool_b.name}: {rag_tool_b.description[:200]}...\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Configuraci√≥n del experimento\n",
        "# -----------------------------\n",
        "AUTOR = \"Priscilla\"      # filtro obligatorio por metadata\n",
        "TEMA  = \"machine learning\"  # <-- pon None si no quieres tema (o cambia el t√©rmino)\n",
        "\n",
        "# Construimos la query (autor + tema opcional)\n",
        "if TEMA and TEMA.strip():\n",
        "    test_query = f'autor:\"{AUTOR}\" tema:\"{TEMA.strip()}\"'\n",
        "    subtitulo  = f'Filtro: autor=\"{AUTOR}\" + tema=\"{TEMA.strip()}\"'\n",
        "else:\n",
        "    test_query = f'autor:\"{AUTOR}\"'\n",
        "    subtitulo  = f'Filtro: autor=\"{AUTOR}\"'\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Helpers de ejecuci√≥n y parsing\n",
        "# -----------------------------\n",
        "def _safe_invoke(tool, query: str) -> Tuple[str, float]:\n",
        "    \"\"\"Invoca el tool (invoke/run) y mide tiempo.\"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    try:\n",
        "        out = tool.invoke(query) if hasattr(tool, \"invoke\") else tool.run(query)\n",
        "    except Exception as e:\n",
        "        out = f\"[ERROR] {e}\"\n",
        "    dt = time.perf_counter() - t0\n",
        "    return out, dt\n",
        "\n",
        "_result_header_pat = re.compile(\n",
        "    r'^\\[(?P<seg>[^|\\]]+?)\\s*¬∑\\s*Resultado\\s+(?P<idx>\\d+)\\s*¬∑\\s*score/distancia:\\s*(?P<score>[^\\]]+)\\]',\n",
        "    re.MULTILINE\n",
        ")\n",
        "_doc_line_pat = re.compile(\n",
        "    r'(?im)^Documento:\\s*(?P<id_doc>[^\\n¬∑]+)'\n",
        "    r'(?:\\s*¬∑\\s*archivo:\\s*(?P<archivo>[^\\n¬∑]+))?'\n",
        "    r'\\s*¬∑\\s*chunk:\\s*(?P<chunk_id>[^\\n]+)'\n",
        ")\n",
        "_author_line_pat = re.compile(r'(?im)^Autor:\\s*(?P<autor>.+)$')\n",
        "\n",
        "def _parse_results(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parsea salida del Tool en √≠tems estructurados:\n",
        "    segmento, resultado_idx, score/distancia, id_doc, archivo, chunk_id, autor, fragmento\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip() or text.strip().startswith(\"[ERROR]\"):\n",
        "        return []\n",
        "    items = []\n",
        "    headers = list(_result_header_pat.finditer(text))\n",
        "    if not headers:\n",
        "        return []\n",
        "    for i, m in enumerate(headers):\n",
        "        start = m.end()\n",
        "        end = headers[i + 1].start() if i + 1 < len(headers) else len(text)\n",
        "        block = text[start:end].strip()\n",
        "\n",
        "        seg   = m.group(\"seg\").strip()\n",
        "        idx   = int(m.group(\"idx\"))\n",
        "        score = m.group(\"score\").strip()\n",
        "\n",
        "        doc_m  = _doc_line_pat.search(block)\n",
        "        auth_m = _author_line_pat.search(block)\n",
        "\n",
        "        id_doc   = doc_m.group(\"id_doc\").strip() if doc_m else \"N/A\"\n",
        "        archivo  = doc_m.group(\"archivo\").strip() if (doc_m and doc_m.group(\"archivo\")) else \"N/A\"\n",
        "        chunk_id = doc_m.group(\"chunk_id\").strip() if doc_m else \"N/A\"\n",
        "        autor    = auth_m.group(\"autor\").strip() if auth_m else \"N/A\"\n",
        "\n",
        "        # fragmento sin las l√≠neas de Documento/Autor\n",
        "        frag = _doc_line_pat.sub(\"\", block)\n",
        "        frag = _author_line_pat.sub(\"\", frag)\n",
        "        frag = frag.strip()\n",
        "\n",
        "        items.append({\n",
        "            \"segmento\": seg,\n",
        "            \"resultado_idx\": idx,\n",
        "            \"score_ou_distancia\": score,\n",
        "            \"id_doc\": id_doc,\n",
        "            \"archivo\": archivo,\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"autor\": autor,\n",
        "            \"chars_fragmento\": len(frag),\n",
        "            \"fragmento\": frag\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def _metrics(items: List[Dict[str, Any]], elapsed_s: float) -> Dict[str, Any]:\n",
        "    \"\"\"M√©tricas concisas para el paper.\"\"\"\n",
        "    chars_total = sum(it.get(\"chars_fragmento\", 0) for it in items)\n",
        "    docs = {(it[\"id_doc\"] or \"N/A\").strip() for it in items}\n",
        "    chunks = {((it[\"id_doc\"] or \"N/A\").strip(), (it[\"chunk_id\"] or \"N/A\").strip()) for it in items}\n",
        "    return {\n",
        "        \"items\": len(items),\n",
        "        \"docs_unicos\": len(docs),\n",
        "        \"chunks_unicos\": len(chunks),\n",
        "        \"chars\": chars_total,\n",
        "        \"tiempo_ms\": round(elapsed_s * 1000.0, 1)\n",
        "    }\n",
        "\n",
        "def _to_df(items: List[Dict[str, Any]]) -> pd.DataFrame:\n",
        "    cols = [\"segmento\",\"resultado_idx\",\"id_doc\",\"chunk_id\",\"archivo\",\"autor\",\"score_ou_distancia\",\"chars_fragmento\",\"fragmento\"]\n",
        "    df = pd.DataFrame(items)\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(columns=cols)\n",
        "    return df[cols].sort_values([\"resultado_idx\"]).reset_index(drop=True)\n",
        "\n",
        "def _print_section_title(title: str):\n",
        "    print(\"\\n\" + \"=\"*72)\n",
        "    print(title)\n",
        "    print(\"=\"*72 + \"\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Ejecutar A y B (por separado)\n",
        "# -----------------------------\n",
        "_print_section_title(\"PRUEBA EMP√çRICA A vs B (sin overlap)\")\n",
        "print(subtitulo)\n",
        "print(\"A: segmentaci√≥n por chunks fijos | B: por encabezados/secciones\\n\")\n",
        "\n",
        "raw_a, t_a = _safe_invoke(rag_tool_a, test_query)\n",
        "raw_b, t_b = _safe_invoke(rag_tool_b, test_query)\n",
        "\n",
        "items_a = _parse_results(raw_a)\n",
        "items_b = _parse_results(raw_b)\n",
        "\n",
        "metrics_a = _metrics(items_a, t_a)\n",
        "metrics_b = _metrics(items_b, t_b)\n",
        "\n",
        "df_a = _to_df(items_a)\n",
        "df_b = _to_df(items_b)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Exportar CSV por segmento\n",
        "# -----------------------------\n",
        "os.makedirs(\"rag_eval\", exist_ok=True)\n",
        "df_a.to_csv(\"rag_eval/segmento_A_resultados.csv\", index=False)\n",
        "df_b.to_csv(\"rag_eval/segmento_B_resultados.csv\", index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Reporte por segmento (paper-friendly)\n",
        "# -----------------------------\n",
        "# A)\n",
        "_print_section_title(\"RESULTADOS ‚Äî Segmento A (chunks fijos)\")\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"M√©tricas A: items={metrics_a['items']} | docs_unicos={metrics_a['docs_unicos']} | \"\n",
        "      f\"chunks_unicos={metrics_a['chunks_unicos']} | chars={metrics_a['chars']} | \"\n",
        "      f\"tiempo={metrics_a['tiempo_ms']} ms\")\n",
        "\n",
        "# Vista tabular resumida (sin mostrar fragmento completo para ser conciso)\n",
        "cols_summary = [\"resultado_idx\",\"id_doc\",\"chunk_id\",\"archivo\",\"autor\",\"score_ou_distancia\",\"chars_fragmento\"]\n",
        "print(\"\\nTabla A (resumen):\")\n",
        "print(df_a[cols_summary].to_string(index=False) if not df_a.empty else \"(sin resultados)\")\n",
        "\n",
        "# Vista corta (primer fragmento) ‚Äî √∫til para el paper (cita breve)\n",
        "if not df_a.empty:\n",
        "    fragA = df_a.loc[0, \"fragmento\"]\n",
        "    previewA = fragA[:700] + (\"...\" if len(fragA) > 700 else \"\")\n",
        "    print(\"\\nVista previa A (Top-1 fragmento, truncado):\")\n",
        "    print(previewA)\n",
        "else:\n",
        "    print(\"\\nVista previa A: (sin resultados)\")\n",
        "\n",
        "# B)\n",
        "_print_section_title(\"RESULTADOS ‚Äî Segmento B (encabezados/secciones)\")\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"M√©tricas B: items={metrics_b['items']} | docs_unicos={metrics_b['docs_unicos']} | \"\n",
        "      f\"chunks_unicos={metrics_b['chunks_unicos']} | chars={metrics_b['chars']} | \"\n",
        "      f\"tiempo={metrics_b['tiempo_ms']} ms\")\n",
        "\n",
        "print(\"\\nTabla B (resumen):\")\n",
        "print(df_b[cols_summary].to_string(index=False) if not df_b.empty else \"(sin resultados)\")\n",
        "\n",
        "if not df_b.empty:\n",
        "    fragB = df_b.loc[0, \"fragmento\"]\n",
        "    previewB = fragB[:700] + (\"...\" if len(fragB) > 700 else \"\")\n",
        "    print(\"\\nVista previa B (Top-1 fragmento, truncado):\")\n",
        "    print(previewB)\n",
        "else:\n",
        "    print(\"\\nVista previa B: (sin resultados)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Nota metodol√≥gica concisa (lista para el paper)\n",
        "# -----------------------------\n",
        "print(\"\\n[Nota metodol√≥gica]\")\n",
        "print(\n",
        "    \"Dise√±o: comparaci√≥n de recuperaci√≥n RAG en dos segmentaciones independientes: \"\n",
        "    \"A (chunks fijos) y B (encabezados/secciones). La consulta restringe por metadata \"\n",
        "    f\"({subtitulo}). Se reportan, por segmento: n√∫mero de items, documentos √∫nicos, \"\n",
        "    \"chunks √∫nicos, caracteres agregados y latencia. Los resultados se presentan por \"\n",
        "    \"separado (sin superposiciones), priorizando claridad y concisi√≥n.\"\n",
        ")\n",
        "\n",
        "print(\"\\n[CSV generados]\")\n",
        "print(\" - rag_eval/segmento_A_resultados.csv\")\n",
        "print(\" - rag_eval/segmento_B_resultados.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knyTiCAV_LJi",
        "outputId": "2bd1636f-1d88-451d-d0cd-40361d2da6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 6: Crear WebSearch Tool (DuckDuckGo)\n",
        "# Solo debe usarse cuando el usuario lo pida expl√≠citamente\n",
        "# ============================================================\n",
        "\n",
        "%pip install -q ddgs  # descomenta si a√∫n no lo tienes\n",
        "\n",
        "from langchain_core.tools import Tool\n",
        "from ddgs import DDGS\n",
        "import re, time\n",
        "\n",
        "def _normalize_query(q: str) -> str:\n",
        "    q = q.strip()\n",
        "    m = re.match(r'^(site:[^\\s]+)\\s+(.+)$', q)\n",
        "    if m:\n",
        "        site, rest = m.group(1), m.group(2).strip()\n",
        "        if not (rest.startswith('\"') and rest.endswith('\"')) and \" \" in rest:\n",
        "            rest = f'\"{rest}\"'\n",
        "        return f\"{site} {rest}\"\n",
        "    return q\n",
        "\n",
        "def _ddg_text_search(ddgs_obj: DDGS, q: str, region: str, max_results: int):\n",
        "    \"\"\"Compatibilidad entre versiones de ddgs.\"\"\"\n",
        "    try:\n",
        "        # Firmas nuevas (query como primer posicional o keyword)\n",
        "        return list(ddgs_obj.text(q, region=region, max_results=max_results))\n",
        "    except TypeError:\n",
        "        # Firmas antiguas (keywords=...)\n",
        "        return list(ddgs_obj.text(keywords=q, region=region, max_results=max_results))\n",
        "\n",
        "def create_web_search_tool() -> Tool:\n",
        "    def web_search_func(query: str, max_results: int = 6, region: str = \"wt-wt\") -> str:\n",
        "        if not isinstance(query, str) or not query.strip():\n",
        "            return \"Consulta inv√°lida para web_search.\"\n",
        "        q = _normalize_query(query)\n",
        "        last_err = None\n",
        "        for t in range(2):  # reintentos suaves\n",
        "            try:\n",
        "                with DDGS() as ddgs_obj:\n",
        "                    results = _ddg_text_search(ddgs_obj, q, region, max_results)\n",
        "                if results:\n",
        "                    lines = []\n",
        "                    for r in results[:max_results]:\n",
        "                        title = r.get(\"title\") or \"(sin t√≠tulo)\"\n",
        "                        href = r.get(\"href\") or \"\"\n",
        "                        snippet = (r.get(\"body\") or \"\").replace(\"\\n\", \" \")\n",
        "                        if len(snippet) > 150:\n",
        "                            snippet = snippet[:150] + \"‚Ä¶\"\n",
        "                        lines.append(f\"- {title}\\n  {href}\\n  {snippet}\")\n",
        "                    return \"\\n\".join(lines)\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "            time.sleep(0.6 + 0.4 * t)\n",
        "        return (\"Sin resultados (o bloqueado por el buscador).\"\n",
        "                if last_err is None else f\"Error en b√∫squeda: {type(last_err).__name__}: {last_err}\")\n",
        "    return Tool(\n",
        "        name=\"web_search\",\n",
        "        description=(\"B√∫squeda web (DuckDuckGo v√≠a ddgs). √ösala s√≥lo cuando el usuario lo pida \"\n",
        "                     \"o si se necesita informaci√≥n externa.\"),\n",
        "        func=web_search_func,\n",
        "    )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X69YgP_p4Anq",
        "outputId": "ae85f1d0-48d3-4244-a52e-ea3dd84768ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WebSearch Tool: B√∫squeda web (DuckDuckGo v√≠a ddgs). √ösala s√≥lo cuando el usuario lo pida o si se necesita informaci√≥n externa.\n",
            "- Aprendizaje por refuerzo - Wikipedia, la enciclopedia libre\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo\n",
            "  El aprendizaje por refuerzo o aprendizaje reforzado (en ingl√©s: reinforcement learning) es un √°rea del aprendizaje autom√°tico (AA) inspirada en la psi‚Ä¶\n",
            "- Aprendizaje de refuerzo profundo - Wikipedia, la enciclopedia ...\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_de_refuerzo_profundo\n",
            "  Con esta capa de abstracci√≥n, los algoritmos de aprendizaje por refuerzo profundo pueden dise√±arse de forma que se generalicen y el mismo modelo pueda‚Ä¶\n",
            "- Aprendizaje por refuerzo a partir de retroalimentaci√≥n humana\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo_a_partir_de_retroalimentaci√≥n_humana\n",
            "  Visi√≥n general del aprendizaje por refuerzo a partir de retroalimentaci√≥n humana La retroalimentaci√≥n humana se suele recoger pidiendo a los humanos q‚Ä¶\n",
            "- Aprendizaje por refuerzo multiagente - Wikipedia, la ...\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo_multiagente\n",
            "  El aprendizaje por refuerzo de multiagente es un subcampo del aprendizaje por refuerzo . Se centra en estudiar el comportamiento de m√∫ltiples agentes ‚Ä¶\n",
            "- Wikipedia Aprendizaje por imitaci√≥n (inteligencia artificial) - Wikipedia, la enciclopedia libre\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_por_imitaci√≥n_(inteligencia_artificial)\n",
            "  January 3, 2025 - Mientras que el \" aprendizaje por refuerzo \" ordinario implica el uso de recompensas y castigos para aprender un comportamiento, en ‚Ä¶\n",
            "- Wikipedia Datos biol√≥gicos - Wikipedia, la enciclopedia libre\n",
            "  https://es.wikipedia.org/wiki/Datos_biol√≥gicos\n",
            "  May 10, 2025 - El aprendizaje por refuerzo, t√©rmino procedente de la psicolog√≠a conductista, es un m√©todo de resoluci√≥n de problemas mediante el apren‚Ä¶\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "## Prueba paso 6 Web Search\n",
        "web_search_tool = create_web_search_tool()\n",
        "print(\"WebSearch Tool:\", web_search_tool.description)\n",
        "\n",
        "web = create_web_search_tool()\n",
        "print(web.invoke(\"site:wikipedia.org aprendizaje por refuerzo\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eBQS4WCfkkCM"
      },
      "outputs": [],
      "source": [
        "# Instanciar la tool web\n",
        "web_search_tool = create_web_search_tool()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHP7IO-2_OUw",
        "outputId": "d5c67b8e-2cc6-42c5-c720-3257e89cf202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RESUMEN DAVID\n",
            "======================================================================\n",
            "\n",
            "Datos procesados:\n",
            " - Fragmentos Segmentaci√≥n A: 227\n",
            " - Fragmentos Segmentaci√≥n B: 349\n",
            "\n",
            " Bases vectoriales creadas:\n",
            " - Vectorstore A: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/vectorstore_a\n",
            " - Vectorstore B: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/vectorstore_b\n",
            "\n",
            " Herramientas disponibles:\n",
            " 1. rag_search_A: B√∫squeda RAG con segmentaci√≥n A (chunks fijos)\n",
            " 2. rag_search_B: B√∫squeda RAG con segmentaci√≥n B (encabezados)\n",
            " 3. web_search: B√∫squeda web (solo cuando se solicite expl√≠citamente)\n",
            "\n",
            "üìù Flujo implementado:\n",
            " 1. Tokenizaci√≥n: Verificaci√≥n de tokens con tiktoken\n",
            " 2. Embeddings: Generaci√≥n con sentence-transformers/all-MiniLM-L6-v2 (modelo local, gratuito)\n",
            " 3. Almacenamiento: Dos vectorstores FAISS (uno por segmentaci√≥n)\n",
            " 4. Consulta: Herramientas RAG que retornan fragmento, documento y autor\n",
            " 5. WebSearch: Herramienta disponible con restricci√≥n de uso\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 7: Resumen y verificaci√≥n final\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RESUMEN DAVID\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nDatos procesados:\")\n",
        "print(f\" - Fragmentos Segmentaci√≥n A: {len(seg_a_data)}\")\n",
        "print(f\" - Fragmentos Segmentaci√≥n B: {len(seg_b_data)}\")\n",
        "\n",
        "print(\"\\n Bases vectoriales creadas:\")\n",
        "print(f\" - Vectorstore A: {VECTORSTORE_DIR_A}\")\n",
        "print(f\" - Vectorstore B: {VECTORSTORE_DIR_B}\")\n",
        "\n",
        "print(\"\\n Herramientas disponibles:\")\n",
        "print(f\" 1. {rag_tool_a.name}: B√∫squeda RAG con segmentaci√≥n A (chunks fijos)\")\n",
        "print(f\" 2. {rag_tool_b.name}: B√∫squeda RAG con segmentaci√≥n B (encabezados)\")\n",
        "print(f\" 3. {web_search_tool.name}: B√∫squeda web (solo cuando se solicite expl√≠citamente)\")\n",
        "\n",
        "print(\"\\nüìù Flujo implementado:\")\n",
        "print(\" 1. Tokenizaci√≥n: Verificaci√≥n de tokens con tiktoken\")\n",
        "print(\" 2. Embeddings: Generaci√≥n con sentence-transformers/all-MiniLM-L6-v2 (modelo local, gratuito)\")\n",
        "print(\" 3. Almacenamiento: Dos vectorstores FAISS (uno por segmentaci√≥n)\")\n",
        "print(\" 4. Consulta: Herramientas RAG que retornan fragmento, documento y autor\")\n",
        "print(\" 5. WebSearch: Herramienta disponible con restricci√≥n de uso\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlDBm2Il_S8X",
        "outputId": "b9a87e78-f25d-4447-9d16-5b3bfccc057c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Streamlit y streamlit-chat instalados correctamente\n",
            "üì¶ Listos para la aplicaci√≥n web\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPA√ëERO 3 ‚Äì AGENTE, ORQUESTACI√ìN, MEMORIA Y APP\n",
        "# ============================================================\n",
        "# Paso 1: Instalaci√≥n de dependencias adicionales para Streamlit\n",
        "# ============================================================\n",
        "# NOTA: Las dependencias principales (LangChain, OpenAI, etc.) ya est√°n\n",
        "# instaladas en la Celda 2. Esta celda solo instala lo necesario para Streamlit.\n",
        "# ============================================================\n",
        "\n",
        "# Instalar librer√≠as adicionales para interfaz web\n",
        "%pip install --quiet streamlit streamlit-chat\n",
        "\n",
        "print(\"‚úÖ Streamlit y streamlit-chat instalados correctamente\")\n",
        "print(\"üì¶ Listos para la aplicaci√≥n web\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whA2LHY9_T8T",
        "outputId": "afd16672-4809-4e50-bd2a-92e0efa1cb40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API Key configurada correctamente\n",
            "‚úÖ Prompts definidos para pruebas emp√≠ricas A/B\n",
            "   - AGENT_PROMPT_A ‚Üí Prioriza SOLO rag_search_A (chunks fijos). web_search: solo si el usuario lo pide y est√° disponible.\n",
            "   - AGENT_PROMPT_B ‚Üí Prioriza SOLO rag_search_B (encabezados). web_search: solo si el usuario lo pide y est√° disponible.\n",
            "\n",
            "üìå Recuerda: al crear los agentes, pasa √∫nicamente su tool de RAG correspondiente; a√±ade `web_search` SOLO si planeas permitirlo en esa ejecuci√≥n.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 2: Configuraci√≥n del modelo OpenAI y definici√≥n de prompts\n",
        "# (dos variantes A/B para pruebas emp√≠ricas)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from langchain.agents import AgentExecutor, create_react_agent\n",
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.memory import ConversationBufferWindowMemory\n",
        "from google.colab import userdata\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURACI√ìN DE API KEY DE OPENAI\n",
        "# ============================================================\n",
        "# IMPORTANTE: Configura OPENAI_API_KEY en Colab Secrets o como variable de entorno\n",
        "#\n",
        "# OPCI√ìN 1 (Recomendada): Usar Colab Secrets\n",
        "#   1. Haz clic en el icono üîí (candado) en la barra lateral de Colab\n",
        "#   2. Haz clic en \"Add new secret\"\n",
        "#   3. Nombre: OPENAI_API_KEY\n",
        "#   4. Valor: tu-api-key-de-openai (formato: sk-...)\n",
        "#\n",
        "# OPCI√ìN 2: Configurar directamente aqu√≠ (descomenta y pega tu key):\n",
        "#   OPENAI_API_KEY = 'sk-tu-api-key-aqui'\n",
        "# ============================================================\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Si no est√° en Secrets, puedes configurarla aqu√≠ directamente:\n",
        "# if not OPENAI_API_KEY:\n",
        "#     OPENAI_API_KEY = 'sk-tu-api-key-aqui'  # <-- Pega tu API key aqu√≠\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  OPENAI_API_KEY no configurada.\")\n",
        "    print(\"\\nüìã INSTRUCCIONES:\")\n",
        "    print(\"   1. Ve a Colab Secrets (icono üîí en la barra lateral)\")\n",
        "    print(\"   2. Agrega una nueva clave: OPENAI_API_KEY\")\n",
        "    print(\"   3. O configura la key directamente en el c√≥digo (ver comentarios arriba)\")\n",
        "    print(\"\\n   Obt√©n tu API key en: https://platform.openai.com/api-keys\")\n",
        "else:\n",
        "    # üëâ Exportar al entorno para que los clientes la detecten autom√°ticamente\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "    print(\"‚úÖ OpenAI API Key configurada correctamente\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# PROMPT VARIANTE A (prioriza rag_search_A; web_search solo si el usuario lo pide expl√≠citamente)\n",
        "# ------------------------------------------------------------\n",
        "AGENT_PROMPT_A = \"\"\"Eres un asistente acad√©mico especializado en el curso de Inteligencia Artificial.\n",
        "Tu nombre es AsistenteIA y tu rol es ayudar a los estudiantes a encontrar informaci√≥n en los apuntes del curso.\n",
        "\n",
        "**INSTRUCCIONES IMPORTANTES (VARIANTE A):**\n",
        "1. Antes de responder, consulta SIEMPRE los apuntes usando EXCLUSIVAMENTE la herramienta `rag_search_A`.\n",
        "2. Cita SIEMPRE el documento de origen y el autor cuando uses informaci√≥n de los apuntes.\n",
        "3. Solo usa la b√∫squeda en la web (`web_search`) si el usuario lo solicita expl√≠citamente (por ejemplo: \"busca en la web\", \"revisa en internet\", \"fuentes externas\") **y solo si la herramienta `web_search` est√° disponible en esta ejecuci√≥n**. Si no est√° disponible, ind√≠calo claramente y contin√∫a con `rag_search_A`.\n",
        "\n",
        "**ESTILO DE RESPUESTA:**\n",
        "- S√© claro, conciso y educativo.\n",
        "- Explica conceptos de manera accesible.\n",
        "- Usa ejemplos cuando sea √∫til.\n",
        "- Cita siempre tus fuentes: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "**HERRAMIENTAS DISPONIBLES EN ESTA VARIANTE:**\n",
        "- rag_search_A: Busca en apuntes usando segmentaci√≥n por chunks fijos (m√°s preciso para fragmentos espec√≠ficos).\n",
        "- (Opcional en tiempo de ejecuci√≥n) web_search: solo si el usuario lo pide expl√≠citamente y la herramienta est√° habilitada.\n",
        "\n",
        "**EJEMPLO DE USO:**\n",
        "Usuario: \"¬øQu√© es el aprendizaje supervisado?\"\n",
        "1. Usa `rag_search_A` para buscar en los apuntes.\n",
        "2. Responde bas√°ndote en los resultados encontrados.\n",
        "3. Cita: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "{history}\n",
        "\n",
        "Pregunta: {input}\n",
        "Piensa paso a paso y decide qu√© informaci√≥n recuperar con `rag_search_A`. Solo usa `web_search` si el usuario lo pide expl√≠citamente y la herramienta est√° disponible.\n",
        "\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# PROMPT VARIANTE B (prioriza rag_search_B; web_search solo si el usuario lo pide expl√≠citamente)\n",
        "# ------------------------------------------------------------\n",
        "AGENT_PROMPT_B = \"\"\"Eres un asistente acad√©mico especializado en el curso de Inteligencia Artificial.\n",
        "Tu nombre es AsistenteIA y tu rol es ayudar a los estudiantes a encontrar informaci√≥n en los apuntes del curso.\n",
        "\n",
        "**INSTRUCCIONES IMPORTANTES (VARIANTE B):**\n",
        "1. Antes de responder, consulta SIEMPRE los apuntes usando EXCLUSIVAMENTE la herramienta `rag_search_B`.\n",
        "2. Cita SIEMPRE el documento de origen y el autor cuando uses informaci√≥n de los apuntes.\n",
        "3. Solo usa la b√∫squeda en la web (`web_search`) si el usuario lo solicita expl√≠citamente (por ejemplo: \"busca en la web\", \"revisa en internet\", \"fuentes externas\") **y solo si la herramienta `web_search` est√° disponible en esta ejecuci√≥n**. Si no est√° disponible, ind√≠calo claramente y contin√∫a con `rag_search_B`.\n",
        "\n",
        "**ESTILO DE RESPUENSA:**\n",
        "- S√© claro, conciso y educativo.\n",
        "- Explica conceptos de manera accesible.\n",
        "- Usa ejemplos cuando sea √∫til.\n",
        "- Cita siempre tus fuentes: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "**HERRAMIENTAS DISPONIBLES EN ESTA VARIANTE:**\n",
        "- rag_search_B: Busca en apuntes usando segmentaci√≥n por encabezados (mejor para temas completos).\n",
        "- (Opcional en tiempo de ejecuci√≥n) web_search: solo si el usuario lo pide expl√≠citamente y la herramienta est√° habilitada.\n",
        "\n",
        "**EJEMPLO DE USO:**\n",
        "Usuario: \"¬øQu√© es el aprendizaje supervisado?\"\n",
        "1. Usa `rag_search_B` para buscar en los apuntes.\n",
        "2. Responde bas√°ndote en los resultados encontrados.\n",
        "3. Cita: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "{history}\n",
        "\n",
        "Pregunta: {input}\n",
        "Piensa paso a paso y decide qu√© informaci√≥n recuperar con `rag_search_B`. Solo usa `web_search` si el usuario lo pide expl√≠citamente y la herramienta est√° disponible.\n",
        "\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "print(\"‚úÖ Prompts definidos para pruebas emp√≠ricas A/B\")\n",
        "print(\"   - AGENT_PROMPT_A ‚Üí Prioriza SOLO rag_search_A (chunks fijos). web_search: solo si el usuario lo pide y est√° disponible.\")\n",
        "print(\"   - AGENT_PROMPT_B ‚Üí Prioriza SOLO rag_search_B (encabezados). web_search: solo si el usuario lo pide y est√° disponible.\")\n",
        "print(\"\\nüìå Recuerda: al crear los agentes, pasa √∫nicamente su tool de RAG correspondiente; a√±ade `web_search` SOLO si planeas permitirlo en esa ejecuci√≥n.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZbx8hyT_WRz",
        "outputId": "c8d6eb00-89ed-44c9-8006-e319b6336662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Configurando modelo OpenAI (con fallbacks)...\n",
            "‚úÖ Modelo configurado: gpt-4o\n",
            "\n",
            "üîß Estado de herramientas/prompts detectadas:\n",
            "   - rag_tool_a: s√≠\n",
            "   - rag_tool_b: s√≠\n",
            "   - web_search_tool (opcional): s√≠\n",
            "   - AGENT_PROMPT_A: s√≠\n",
            "   - AGENT_PROMPT_B: s√≠\n",
            "\n",
            "ü§ñ Creando agente A (rag_search_A) ...\n",
            "‚úÖ Agente A listo (rag_search_A + web opcional si el usuario lo pide)\n",
            "\n",
            "ü§ñ Creando agente B (rag_search_B) ...\n",
            "‚úÖ Agente B listo (rag_search_B + web opcional si el usuario lo pide)\n",
            "\n",
            "üìå Uso:\n",
            "  respA = agent_executor_A.invoke({'input': 'tu pregunta', 'session_id': 'A'})\n",
            "  respB = agent_executor_B.invoke({'input': 'tu pregunta', 'session_id': 'B'})\n",
            "  # Activa DEBUG=True (arriba) para ver tool_calls y args en la consola.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# - Normaliza nombres de tools\n",
        "# - DEBUG opcional con impresi√≥n de tool_calls\n",
        "# - Fallback: si no hay tool_calls ni texto, fuerza 1 llamada RAG\n",
        "# - Fix: ToolMessage siempre con tool_call_id string (LangChain 1.x)\n",
        "# - Mejora: prioridad web cuando el usuario lo pide\n",
        "# - Mejora: manejo 429 proactivo + s√≠ntesis inmediata (sin 2¬™ vuelta del LLM)\n",
        "# ============================================================\n",
        "\n",
        "import os, json, textwrap\n",
        "from uuid import uuid4\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# --- Mensajes / memoria ---\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "DEBUG = False  # pon True para ver trazas de tool_calls\n",
        "\n",
        "# ============================================================\n",
        "# 0) API Key\n",
        "# ============================================================\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\") or userdata.get(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  No se puede continuar sin OPENAI_API_KEY.\")\n",
        "    llm = None\n",
        "else:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# ============================================================\n",
        "# 3A) Configurar LLM con fallback ordenado\n",
        "# ============================================================\n",
        "def configurar_llm(preferidos=None, temperature=0.1):\n",
        "    if preferidos is None:\n",
        "        preferidos = [\n",
        "            \"gpt-4o\",\n",
        "            \"gpt-4-turbo\",\n",
        "            \"gpt-4o-mini\",\n",
        "            \"gpt-4\",\n",
        "            \"gpt-3.5-turbo\",\n",
        "        ]\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        raise RuntimeError(\"OPENAI_API_KEY no est√° disponible.\")\n",
        "\n",
        "    print(\"üîß Configurando modelo OpenAI (con fallbacks)...\")\n",
        "    last_err = None\n",
        "    for name in preferidos:\n",
        "        try:\n",
        "            _llm = ChatOpenAI(\n",
        "                model=name,\n",
        "                openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            # ping m√≠nimo\n",
        "            _ = _llm.invoke(\"ping\")\n",
        "            print(f\"‚úÖ Modelo configurado: {name}\")\n",
        "            return _llm\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Fall√≥ {name}: {e}\")\n",
        "            last_err = e\n",
        "    raise RuntimeError(f\"No se pudo configurar ning√∫n modelo. √öltimo error: {last_err}\")\n",
        "\n",
        "llm = configurar_llm() if OPENAI_API_KEY else None\n",
        "\n",
        "# ============================================================\n",
        "# 3B) Agentes A/B con bucle de tool-calling robustecido\n",
        "# ============================================================\n",
        "\n",
        "_SESSION_STORES = {}\n",
        "_SYSTEM_SET = set()\n",
        "\n",
        "def get_history(session_id: str) -> ChatMessageHistory:\n",
        "    hist = _SESSION_STORES.get(session_id)\n",
        "    if hist is None:\n",
        "        hist = ChatMessageHistory()\n",
        "        _SESSION_STORES[session_id] = hist\n",
        "    return hist\n",
        "\n",
        "def ensure_system_prompt(session_id: str, system_text: str):\n",
        "    if session_id not in _SYSTEM_SET:\n",
        "        get_history(session_id).add_message(SystemMessage(content=system_text))\n",
        "        _SYSTEM_SET.add(session_id)\n",
        "\n",
        "def _usuario_pide_web(texto: str) -> bool:\n",
        "    if not isinstance(texto, str):\n",
        "        return False\n",
        "    t = texto.lower()\n",
        "    gatillos = [\n",
        "        \"busca en la web\", \"buscar en la web\", \"revisa en la web\",\n",
        "        \"en internet\", \"busca en internet\", \"buscar en internet\",\n",
        "        \"web_search\", \"fuentes externas\", \"googlealo\", \"googlea esto\",\n",
        "        \"consulta la web\", \"haz una b√∫squeda web\",\n",
        "    ]\n",
        "    return any(g in t for g in gatillos)\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "def _map_tools_por_nombre(tools: list):\n",
        "    return {_norm(getattr(t, \"name\", \"\")): t for t in tools if getattr(t, \"name\", \"\")}\n",
        "\n",
        "# ============================================================\n",
        "# HOTFIX tool_calls: normalizador y extractor robustos\n",
        "#  - Soporta:\n",
        "#    A) {\"function\": {\"name\": \"...\", \"arguments\": \"...|dict\"}, \"id\": \"...\"}   (OpenAI-like)\n",
        "#    C) {\"name\": \"...\", \"arguments\": {...}, \"id\": \"...\"}                       (variantes planas)\n",
        "# ============================================================\n",
        "def _normalize_call_dict(call):\n",
        "    \"\"\"Devuelve un dict unificado: {\"name\": str|None, \"arguments\": dict|str|None, \"id\": str|None}\"\"\"\n",
        "    if not isinstance(call, dict):\n",
        "        return {\"name\": None, \"arguments\": None, \"id\": None}\n",
        "    cid = call.get(\"id\") or call.get(\"tool_call_id\") or f\"auto-{uuid4().hex[:8]}\"\n",
        "\n",
        "    # Caso A (OpenAI-like)\n",
        "    fn = call.get(\"function\")\n",
        "    if isinstance(fn, dict) and (\"name\" in fn or \"arguments\" in fn):\n",
        "        nm = fn.get(\"name\")\n",
        "        args = fn.get(\"arguments\")\n",
        "        return {\"name\": nm, \"arguments\": args, \"id\": cid}\n",
        "\n",
        "    if \"name\" in call and (\"args\" in call or \"arguments\" in call):\n",
        "        nm = call.get(\"name\")\n",
        "        args = call.get(\"args\", call.get(\"arguments\"))\n",
        "        return {\"name\": nm, \"arguments\": args, \"id\": cid}\n",
        "\n",
        "    # √öltimo intento: llaves planas\n",
        "    nm = call.get(\"name\")\n",
        "    args = call.get(\"arguments\") or call.get(\"args\")\n",
        "    return {\"name\": nm, \"arguments\": args, \"id\": cid}\n",
        "\n",
        "def _extraer_tool_calls(ai_msg: AIMessage):\n",
        "    calls = getattr(ai_msg, \"tool_calls\", None)\n",
        "    kw = getattr(ai_msg, \"additional_kwargs\", {}) or {}\n",
        "    if not calls:\n",
        "        calls = kw.get(\"tool_calls\") or kw.get(\"tool_calls_json\") or []\n",
        "    if DEBUG:\n",
        "        print(\"üß© tool_calls crudos:\", calls if calls else \"(ninguno)\")\n",
        "        if kw:\n",
        "            snippet = textwrap.shorten(str(kw), width=400, placeholder=\" ‚Ä¶\")\n",
        "            print(\"üß© additional_kwargs:\", snippet)\n",
        "    norm = []\n",
        "    for c in (calls or []):\n",
        "        norm.append(_normalize_call_dict(c))\n",
        "    return norm\n",
        "\n",
        "def _ejecutar_tool_call(tool, call_norm):\n",
        "    # call_norm viene normalizado por _extraer_tool_calls\n",
        "    fn = getattr(tool, \"invoke\", None) or getattr(tool, \"run\", None) or getattr(tool, \"func\", None)\n",
        "    if fn is None:\n",
        "        return \"[Error: herramienta no es invocable]\"\n",
        "    args = call_norm.get(\"arguments\", {})\n",
        "    # Decodifica si vino como JSON string\n",
        "    if isinstance(args, str):\n",
        "        try:\n",
        "            args = json.loads(args or \"{}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        if isinstance(args, dict):\n",
        "            if \"query\" in args:\n",
        "                try:\n",
        "                    return fn(args[\"query\"])\n",
        "                except TypeError:\n",
        "                    return fn(args)\n",
        "            return fn(args)\n",
        "        else:\n",
        "            return fn(args)\n",
        "    except Exception as e:\n",
        "        return f\"[Error ejecutando tool: {e}]\"\n",
        "\n",
        "# --- Helpers para ToolMessage con ID v√°lido (requisito en LC 1.x) ---\n",
        "def _mk_tool_msg(name: str, content: str, tool_call_id: str | None):\n",
        "    tid = tool_call_id if isinstance(tool_call_id, str) and tool_call_id else f\"auto-{_norm(name)}-{uuid4().hex[:8]}\"\n",
        "    nm = name if (name and str(name).strip()) else \"tool\"\n",
        "    return ToolMessage(content=str(content), name=nm, tool_call_id=tid)\n",
        "\n",
        "def _patch_calls_into_history(history, raw_name, tool_out, t_id):\n",
        "    history.add_message(_mk_tool_msg(raw_name or \"tool\", tool_out, t_id))\n",
        "\n",
        "def _patch_forced_rag_into_history(history, principal, tool_out):\n",
        "    history.add_message(_mk_tool_msg(getattr(principal, \"name\", \"rag\"), tool_out, f\"forced-{uuid4().hex[:8]}\"))\n",
        "\n",
        "# ====== S√çNTESIS INMEDIATA + 429 PROACTIVO ======\n",
        "ALWAYS_SYNTHESIZE_AFTER_TOOLS = True\n",
        "MAX_TOOL_MSGS_FOR_SYNTHESIS = 2\n",
        "LOG_429_BANNER = True\n",
        "\n",
        "def _sintetizar_desde_tools(history, max_msgs: int = MAX_TOOL_MSGS_FOR_SYNTHESIS) -> str:\n",
        "    tmsgs = [m for m in history.messages if isinstance(m, ToolMessage)]\n",
        "    if not tmsgs:\n",
        "        return \"\"\n",
        "    rec = tmsgs[-max_msgs:]\n",
        "    partes = []\n",
        "    for m in rec:\n",
        "        nombre = getattr(m, \"name\", \"tool\")\n",
        "        contenido = (m.content or \"\").strip()\n",
        "        if not contenido:\n",
        "            continue\n",
        "        if len(contenido) > 1200:\n",
        "            contenido = contenido[:1200] + \" ...\"\n",
        "        partes.append(f\"‚Ä¢ {nombre}\\n{contenido}\")\n",
        "    if not partes:\n",
        "        return \"\"\n",
        "    return \"S√≠ntesis basada en herramientas:\\n\\n\" + \"\\n\\n\".join(partes)\n",
        "\n",
        "class SimpleAgentExecutor:\n",
        "    def __init__(self, llm_base, system_text: str, tools: list, variante_label: str, max_iters: int = 2):\n",
        "        # ‚Üì max_iters=2 para no golpear la cuota\n",
        "        self.llm = llm_base.bind_tools(tools) if tools else llm_base\n",
        "        self.system_text = system_text\n",
        "        self.tools = tools\n",
        "        self.tools_by_name = _map_tools_por_nombre(tools)  # normalizados\n",
        "        self.variante_label = variante_label\n",
        "        self.max_iters = max_iters\n",
        "        if DEBUG:\n",
        "            print(f\"[DEBUG] Tools registradas ({variante_label}):\", list(self.tools_by_name.keys()))\n",
        "\n",
        "    def invoke(self, payload: dict, config: dict = None):\n",
        "        cfg = config or {}\n",
        "        sid = payload.get(\"session_id\") or (cfg.get(\"configurable\", {}) or {}).get(\"session_id\") or \"default\"\n",
        "\n",
        "        ensure_system_prompt(sid, self.system_text)\n",
        "        history = get_history(sid)\n",
        "\n",
        "        user_input = payload.get(\"input\") or \"\"\n",
        "        history.add_message(HumanMessage(content=user_input))\n",
        "        mensajes = history.messages[:]\n",
        "        explicito_web = _usuario_pide_web(user_input)\n",
        "\n",
        "        forced_rag_used = False\n",
        "        forced_web_used = False\n",
        "\n",
        "        for it in range(1, self.max_iters + 1):\n",
        "            try:\n",
        "                ai_msg = self.llm.invoke(mensajes)\n",
        "            except Exception as e:\n",
        "                msg = str(e)\n",
        "\n",
        "                # === 429 PROACTIVO: si el LLM falla por cuota ANTES de tool_calls ===\n",
        "                if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
        "                    if LOG_429_BANNER:\n",
        "                        print(\"‚ö†Ô∏è [MODO 429 PROACTIVO] LLM fall√≥ por cuota; forzando tools y sintetizando‚Ä¶\")\n",
        "                    did_tool = False\n",
        "\n",
        "                    # 1) Si el usuario pidi√≥ web y existe la tool, forzar web primero\n",
        "                    if explicito_web and \"web_search\" in self.tools_by_name:\n",
        "                        web_tool = self.tools_by_name[\"web_search\"]\n",
        "                        forced_call = {\"name\": web_tool.name,\n",
        "                                       \"arguments\": {\"query\": user_input},\n",
        "                                       \"id\": f\"forced-web-{uuid4().hex[:8]}\"}\n",
        "                        tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                        _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "                        did_tool = True\n",
        "\n",
        "                    # 2) Si no hicimos web, forzar RAG principal de la variante\n",
        "                    if not did_tool:\n",
        "                        principal = None\n",
        "                        for key in [\"rag_search_a\", \"rag_search_b\"]:\n",
        "                            if key in self.tools_by_name:\n",
        "                                principal = self.tools_by_name[key]; break\n",
        "                        if principal is not None:\n",
        "                            forced_call = {\"name\": principal.name,\n",
        "                                           \"arguments\": {\"query\": user_input},\n",
        "                                           \"id\": f\"forced-{uuid4().hex[:8]}\"}\n",
        "                            tool_out = _ejecutar_tool_call(principal, forced_call)\n",
        "                            _patch_forced_rag_into_history(history, principal, tool_out)\n",
        "                            did_tool = True\n",
        "\n",
        "                    # 3) Sintetizar con lo que haya\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                    # 4) No hubo tool posible\n",
        "                    return {\n",
        "                        \"output\": (\"Estoy temporalmente limitado por cuota (429) y no logr√© ejecutar \"\n",
        "                                   \"una herramienta de respaldo. Intenta de nuevo o cambia de modelo.\"),\n",
        "                        \"iterations\": it\n",
        "                    }\n",
        "\n",
        "                # Otros errores\n",
        "                return {\"output\": f\"[Error del modelo: {e}]\", \"iterations\": it}\n",
        "\n",
        "            history.add_message(ai_msg)\n",
        "            calls = _extraer_tool_calls(ai_msg)\n",
        "\n",
        "            if DEBUG:\n",
        "                clen = len(ai_msg.content or \"\")\n",
        "                print(f\"[DEBUG] iter {it} | content len={clen} | tool_calls={bool(calls)}\")\n",
        "                if calls:\n",
        "                    for c in calls:\n",
        "                        print(\"   call(norm):\", c.get(\"name\"), c.get(\"arguments\"))\n",
        "\n",
        "            # === Cuando hay tool_calls, las ejecutamos ===\n",
        "            if calls:\n",
        "                # PRIORIDAD WEB: si el usuario pidi√≥ web y el LLM NO la incluy√≥, la forzamos primero\n",
        "                if explicito_web and \"web_search\" in self.tools_by_name:\n",
        "                    has_web = False\n",
        "                    for c in calls:\n",
        "                        nm = (c.get(\"name\") or \"\").strip().lower()\n",
        "                        if nm == \"web_search\":\n",
        "                            has_web = True\n",
        "                            break\n",
        "                    if not has_web:\n",
        "                        web_tool = self.tools_by_name[\"web_search\"]\n",
        "                        forced_call = {\n",
        "                            \"name\": web_tool.name,\n",
        "                            \"arguments\": {\"query\": user_input},\n",
        "                            \"id\": f\"forced-web-{uuid4().hex[:8]}\",\n",
        "                        }\n",
        "                        tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                        _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "\n",
        "                # Ejecutar los calls (respetando la pol√≠tica web)\n",
        "                for call_norm in calls:\n",
        "                    raw_name = (call_norm.get(\"name\") or \"\").strip()\n",
        "                    t_name = _norm(raw_name)\n",
        "                    t_id = call_norm.get(\"id\", \"\")\n",
        "\n",
        "                    if t_name == \"web_search\" and not explicito_web:\n",
        "                        tool_out = (\n",
        "                            \"Solicitud de web detectada, pero esta variante solo usa web_search si el usuario \"\n",
        "                            \"lo pide expl√≠citamente. Contin√∫o con los apuntes.\"\n",
        "                        )\n",
        "                    else:\n",
        "                        tool = self.tools_by_name.get(t_name)\n",
        "                        tool_out = (f\"[Tool '{raw_name}' no disponible en esta variante]\"\n",
        "                                    if tool is None else _ejecutar_tool_call(tool, call_norm))\n",
        "\n",
        "                    _patch_calls_into_history(history, raw_name or \"tool\", tool_out, t_id)\n",
        "\n",
        "                # S√çNTESIS INMEDIATA\n",
        "                if ALWAYS_SYNTHESIZE_AFTER_TOOLS:\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                mensajes = history.messages[:]\n",
        "                continue  # siguiente iteraci√≥n tras observar las tools\n",
        "\n",
        "            # === Si NO hay tool_calls: intentar forzar lo necesario ===\n",
        "            txt = (ai_msg.content or \"\").strip()\n",
        "\n",
        "            # (A) Si el usuario pidi√≥ WEB expl√≠cita y no hubo tool_calls, forzar web una vez\n",
        "            if explicito_web and not forced_web_used and \"web_search\" in self.tools_by_name:\n",
        "                web_tool = self.tools_by_name[\"web_search\"]\n",
        "                forced_call = {\"name\": getattr(web_tool, \"name\", \"web_search\"),\n",
        "                               \"arguments\": {\"query\": user_input},\n",
        "                               \"id\": f\"forced-web-{uuid4().hex[:8]}\"}\n",
        "                tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "\n",
        "                # S√≠ntesis inmediata tras forzar web\n",
        "                if ALWAYS_SYNTHESIZE_AFTER_TOOLS:\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                mensajes = history.messages[:]\n",
        "                forced_web_used = True\n",
        "                continue\n",
        "\n",
        "            # (B) Si no pidi√≥ web, forzar el RAG principal una vez\n",
        "            if not forced_rag_used:\n",
        "                principal = None\n",
        "                for key in [\"rag_search_a\", \"rag_search_b\"]:\n",
        "                    if key in self.tools_by_name:\n",
        "                        principal = self.tools_by_name[key]; break\n",
        "                if principal is not None:\n",
        "                    forced_call = {\"name\": principal.name,\n",
        "                                   \"arguments\": {\"query\": user_input},\n",
        "                                   \"id\": f\"forced-{uuid4().hex[:8]}\"}\n",
        "                    tool_out = _ejecutar_tool_call(principal, forced_call)\n",
        "                    _patch_forced_rag_into_history(history, principal, tool_out)\n",
        "\n",
        "                    # S√≠ntesis inmediata tras forzar RAG\n",
        "                    if ALWAYS_SYNTHESIZE_AFTER_TOOLS:\n",
        "                        sintetico = _sintetizar_desde_tools(history)\n",
        "                        if sintetico:\n",
        "                            return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                    mensajes = history.messages[:]\n",
        "                    forced_rag_used = True\n",
        "                    continue\n",
        "\n",
        "            # (C) Si el modelo s√≠ dio texto, devu√©lvelo\n",
        "            if txt:\n",
        "                return {\"output\": txt, \"iterations\": it}\n",
        "\n",
        "            # (D) √öltimo recurso: s√≠ntesis si hay herramientas en historial\n",
        "            sintetico = _sintetizar_desde_tools(history)\n",
        "            if sintetico:\n",
        "                return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "            return {\"output\": \"No fue posible recuperar informaci√≥n en este momento.\", \"iterations\": it}\n",
        "\n",
        "        # max_iters alcanzado: intenta s√≠ntesis antes de rendirte\n",
        "        sintetico = _sintetizar_desde_tools(history)\n",
        "        if sintetico:\n",
        "            return {\"output\": sintetico, \"iterations\": self.max_iters}\n",
        "        return {\"output\": \"Se alcanz√≥ el m√°ximo de iteraciones sin respuesta final.\", \"iterations\": self.max_iters}\n",
        "\n",
        "def build_agent_executor(llm_base, system_text: str, tools: list, variante_label: str, max_iters: int = 2):\n",
        "    print(f\"\\nü§ñ Creando agente {variante_label} ...\")\n",
        "    return SimpleAgentExecutor(\n",
        "        llm_base=llm_base,\n",
        "        system_text=system_text,\n",
        "        tools=tools,\n",
        "        variante_label=variante_label,\n",
        "        max_iters=max_iters,\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# 3B-bis) Construcci√≥n concreta de A y B (usa tus prompts del Paso 2)\n",
        "# ============================================================\n",
        "\n",
        "agent_executor_A = None\n",
        "agent_executor_B = None\n",
        "\n",
        "if llm is not None:\n",
        "    tiene_a = \"rag_tool_a\" in globals()\n",
        "    tiene_b = \"rag_tool_b\" in globals()\n",
        "    tiene_web = \"web_search_tool\" in globals()\n",
        "    tiene_promptA = \"AGENT_PROMPT_A\" in globals()\n",
        "    tiene_promptB = \"AGENT_PROMPT_B\" in globals()\n",
        "\n",
        "    print(\"\\nüîß Estado de herramientas/prompts detectadas:\")\n",
        "    print(f\"   - rag_tool_a: {'s√≠' if tiene_a else 'no'}\")\n",
        "    print(f\"   - rag_tool_b: {'s√≠' if tiene_b else 'no'}\")\n",
        "    print(f\"   - web_search_tool (opcional): {'s√≠' if tiene_web else 'no'}\")\n",
        "    print(f\"   - AGENT_PROMPT_A: {'s√≠' if tiene_promptA else 'no'}\")\n",
        "    print(f\"   - AGENT_PROMPT_B: {'s√≠' if tiene_promptB else 'no'}\")\n",
        "\n",
        "    if tiene_a and tiene_promptA:\n",
        "        tools_A = [rag_tool_a] + ([web_search_tool] if tiene_web else [])\n",
        "        agent_executor_A = build_agent_executor(\n",
        "            llm_base=llm,\n",
        "            system_text=AGENT_PROMPT_A,\n",
        "            tools=tools_A,\n",
        "            variante_label=\"A (rag_search_A)\",\n",
        "            max_iters=2,\n",
        "        )\n",
        "        print(\"‚úÖ Agente A listo (rag_search_A + web opcional si el usuario lo pide)\")\n",
        "    else:\n",
        "        print(\"‚è≥ Agente A pendiente (falta rag_tool_a o AGENT_PROMPT_A)\")\n",
        "\n",
        "    if tiene_b and tiene_promptB:\n",
        "        tools_B = [rag_tool_b] + ([web_search_tool] if tiene_web else [])\n",
        "        agent_executor_B = build_agent_executor(\n",
        "            llm_base=llm,\n",
        "            system_text=AGENT_PROMPT_B,\n",
        "            tools=tools_B,\n",
        "            variante_label=\"B (rag_search_B)\",\n",
        "            max_iters=2,\n",
        "        )\n",
        "        print(\"‚úÖ Agente B listo (rag_search_B + web opcional si el usuario lo pide)\")\n",
        "    else:\n",
        "        print(\"‚è≥ Agente B pendiente (falta rag_tool_b o AGENT_PROMPT_B)\")\n",
        "\n",
        "print(\"\\nüìå Uso:\")\n",
        "print(\"  respA = agent_executor_A.invoke({'input': 'tu pregunta', 'session_id': 'A'})\")\n",
        "print(\"  respB = agent_executor_B.invoke({'input': 'tu pregunta', 'session_id': 'B'})\")\n",
        "print(\"  # Activa DEBUG=True (arriba) para ver tool_calls y args en la consola.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5kVqDH1VI-6S"
      },
      "outputs": [],
      "source": [
        "# @title üîß Parche: s√≠ntesis inmediata + manejo 429 proactivo + menos iteraciones\n",
        "import os, json\n",
        "from uuid import uuid4\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "# === 1) Sintetizar SIEMPRE tras usar tools (sin esperar otra vuelta del LLM) ===\n",
        "ALWAYS_SYNTHESIZE_AFTER_TOOLS = True   # evita depender del 2¬∫ turno del modelo\n",
        "MAX_TOOL_MSGS_FOR_SYNTHESIS = 2        # usa las √∫ltimas N salidas de herramientas\n",
        "LOG_429_BANNER = True                  # imprime un banner cuando entra al modo 429 proactivo\n",
        "\n",
        "def _sintetizar_desde_tools(history, max_msgs: int = MAX_TOOL_MSGS_FOR_SYNTHESIS) -> str:\n",
        "    from langchain_core.messages import ToolMessage\n",
        "    tmsgs = [m for m in history.messages if isinstance(m, ToolMessage)]\n",
        "    if not tmsgs:\n",
        "        return \"\"\n",
        "    rec = tmsgs[-max_msgs:]\n",
        "    partes = []\n",
        "    for m in rec:\n",
        "        nombre = getattr(m, \"name\", \"tool\")\n",
        "        contenido = (m.content or \"\").strip()\n",
        "        if not contenido:\n",
        "            continue\n",
        "        if len(contenido) > 1200:\n",
        "            contenido = contenido[:1200] + \" ...\"\n",
        "        partes.append(f\"‚Ä¢ {nombre}\\n{contenido}\")\n",
        "    if not partes:\n",
        "        return \"\"\n",
        "    return \"S√≠ntesis basada en herramientas:\\n\\n\" + \"\\n\\n\".join(partes)\n",
        "\n",
        "# === 2) Reemplazo del ejecutor: menos iteraciones, s√≠ntesis inmediata y 429 proactivo ===\n",
        "class SimpleAgentExecutor:\n",
        "    def __init__(self, llm_base, system_text: str, tools: list, variante_label: str, max_iters: int = 2):\n",
        "        # ‚Üì max_iters bajo para no quemar cuota\n",
        "        self.llm = llm_base.bind_tools(tools) if tools else llm_base\n",
        "        self.system_text = system_text\n",
        "        self.tools = tools\n",
        "        self.tools_by_name = _map_tools_por_nombre(tools)\n",
        "        self.variante_label = variante_label\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def invoke(self, payload: dict, config: dict = None):\n",
        "        cfg = config or {}\n",
        "        sid = payload.get(\"session_id\") or (cfg.get(\"configurable\", {}) or {}).get(\"session_id\") or \"default\"\n",
        "\n",
        "        ensure_system_prompt(sid, self.system_text)\n",
        "        history = get_history(sid)\n",
        "\n",
        "        user_input = payload.get(\"input\") or \"\"\n",
        "        history.add_message(HumanMessage(content=user_input))\n",
        "        mensajes = history.messages[:]\n",
        "        explicito_web = _usuario_pide_web(user_input)\n",
        "\n",
        "        forced_rag_used = False\n",
        "        forced_web_used = False\n",
        "\n",
        "        for it in range(1, self.max_iters + 1):\n",
        "            try:\n",
        "                ai_msg = self.llm.invoke(mensajes)\n",
        "            except Exception as e:\n",
        "                msg = str(e)\n",
        "\n",
        "                # ‚Äî‚Äî‚Äî PARCHE 429 PROACTIVO ‚Äî‚Äî‚Äî\n",
        "                if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
        "                    if LOG_429_BANNER:\n",
        "                        print(\"‚ö†Ô∏è [MODO 429 PROACTIVO] LLM fall√≥ por cuota; forzando tools y sintetizando‚Ä¶\")\n",
        "                    did_tool = False\n",
        "\n",
        "                    # 1) Si el usuario pidi√≥ web y existe la tool, la forzamos\n",
        "                    if explicito_web and \"web_search\" in self.tools_by_name:\n",
        "                        web_tool = self.tools_by_name[\"web_search\"]\n",
        "                        forced_call = {\"name\": web_tool.name,\n",
        "                                       \"arguments\": {\"query\": user_input},\n",
        "                                       \"id\": f\"forced-web-{uuid4().hex[:8]}\"}\n",
        "                        tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                        _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "                        did_tool = True\n",
        "\n",
        "                    # 2) Si no hicimos web, forzamos el RAG principal de la variante\n",
        "                    if not did_tool:\n",
        "                        principal = None\n",
        "                        for key in [\"rag_search_a\", \"rag_search_b\"]:\n",
        "                            if key in self.tools_by_name:\n",
        "                                principal = self.tools_by_name[key]; break\n",
        "                        if principal is not None:\n",
        "                            forced_call = {\"name\": principal.name,\n",
        "                                           \"arguments\": {\"query\": user_input},\n",
        "                                           \"id\": f\"forced-{uuid4().hex[:8]}\"}\n",
        "                            tool_out = _ejecutar_tool_call(principal, forced_call)\n",
        "                            _patch_forced_rag_into_history(history, principal, tool_out)\n",
        "                            did_tool = True\n",
        "\n",
        "                    # 3) Sintetizamos con lo que haya salido de las tools forzadas\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                    # 4) Si por alguna raz√≥n no hubo tool posible, mensaje claro\n",
        "                    return {\n",
        "                        \"output\": (\"Estoy temporalmente limitado por cuota (429) y no logr√© ejecutar \"\n",
        "                                   \"una herramienta de respaldo. Intenta de nuevo o cambia de modelo.\"),\n",
        "                        \"iterations\": it\n",
        "                    }\n",
        "                # ‚Äî‚Äî‚Äî FIN PARCHE 429 PROACTIVO ‚Äî‚Äî‚Äî\n",
        "\n",
        "                # Otros errores no relacionados con cuota\n",
        "                return {\"output\": f\"[Error del modelo: {e}]\", \"iterations\": it}\n",
        "\n",
        "            history.add_message(ai_msg)\n",
        "            calls = _extraer_tool_calls(ai_msg)\n",
        "\n",
        "            # === Cuando hay tool_calls, las ejecutamos ===\n",
        "            if calls:\n",
        "                for call in calls:\n",
        "                    raw_name = (call.get(\"name\") or \"\").strip()\n",
        "                    t_name = _norm(raw_name)\n",
        "                    t_id = call.get(\"id\", \"\")\n",
        "                    if t_name == \"web_search\" and not explicito_web:\n",
        "                        tool_out = (\"Solicitud de web detectada, pero esta variante solo usa web_search si el usuario \"\n",
        "                                    \"lo pide expl√≠citamente. Contin√∫o con los apuntes.\")\n",
        "                    else:\n",
        "                        tool = self.tools_by_name.get(t_name)\n",
        "                        tool_out = (f\"[Tool '{raw_name}' no disponible en esta variante]\"\n",
        "                                    if tool is None else _ejecutar_tool_call(tool, call))\n",
        "                    _patch_calls_into_history(history, raw_name, tool_out, t_id)\n",
        "\n",
        "                # ‚ö†Ô∏è S√çNTESIS INMEDIATA: evita requerir 2¬∫ turno del LLM\n",
        "                if ALWAYS_SYNTHESIZE_AFTER_TOOLS:\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                # Si quisieras dar oportunidad al LLM, comenta el return de arriba\n",
        "                mensajes = history.messages[:]\n",
        "                continue\n",
        "\n",
        "            # === Sin tool_calls: intentamos forzar lo necesario ===\n",
        "            txt = (ai_msg.content or \"\").strip()\n",
        "\n",
        "            # Forzar web si el usuario lo pidi√≥ expl√≠citamente\n",
        "            if explicito_web and not forced_web_used and \"web_search\" in self.tools_by_name:\n",
        "                web_tool = self.tools_by_name[\"web_search\"]\n",
        "                forced_call = {\"name\": web_tool.name, \"arguments\": {\"query\": user_input}, \"id\": f\"forced-web-{uuid4().hex[:8]}\"}\n",
        "                tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "                # S√≠ntesis inmediata tras forzar web\n",
        "                sintetico = _sintetizar_desde_tools(history)\n",
        "                if sintetico:\n",
        "                    return {\"output\": sintetico, \"iterations\": it}\n",
        "                mensajes = history.messages[:]\n",
        "                forced_web_used = True\n",
        "                continue\n",
        "\n",
        "            # Forzar RAG una vez\n",
        "            if not forced_rag_used:\n",
        "                principal = None\n",
        "                for key in [\"rag_search_a\", \"rag_search_b\"]:\n",
        "                    if key in self.tools_by_name:\n",
        "                        principal = self.tools_by_name[key]; break\n",
        "                if principal is not None:\n",
        "                    forced_call = {\"name\": principal.name, \"arguments\": {\"query\": user_input}, \"id\": f\"forced-{uuid4().hex[:8]}\"}\n",
        "                    tool_out = _ejecutar_tool_call(principal, forced_call)\n",
        "                    _patch_forced_rag_into_history(history, principal, tool_out)\n",
        "                    # S√≠ntesis inmediata tras forzar RAG\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "                    mensajes = history.messages[:]\n",
        "                    forced_rag_used = True\n",
        "                    continue\n",
        "\n",
        "            # Si el modelo dio texto, devu√©lvelo\n",
        "            if txt:\n",
        "                return {\"output\": txt, \"iterations\": it}\n",
        "\n",
        "            # √öltimo recurso: s√≠ntesis si hay herramientas\n",
        "            sintetico = _sintetizar_desde_tools(history)\n",
        "            if sintetico:\n",
        "                return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "            return {\"output\": \"No fue posible recuperar informaci√≥n en este momento.\", \"iterations\": it}\n",
        "\n",
        "        # max_iters alcanzado: intenta s√≠ntesis antes de rendirte\n",
        "        sintetico = _sintetizar_desde_tools(history)\n",
        "        if sintetico:\n",
        "            return {\"output\": sintetico, \"iterations\": self.max_iters}\n",
        "        return {\"output\": \"Se alcanz√≥ el m√°ximo de iteraciones sin respuesta final.\", \"iterations\": self.max_iters}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVcJeGK2sO6-",
        "outputId": "8e5c9d35-4dab-491f-9bb8-49690e8a23a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Creando agente A (rag_search_a) ...\n",
            "\n",
            "ü§ñ Creando agente B (rag_search_b) ...\n",
            "A keys: ['rag_search_a', 'web_search']\n",
            "B keys: ['rag_search_b', 'web_search']\n"
          ]
        }
      ],
      "source": [
        "_SESSION_STORES.clear(); _SYSTEM_SET.clear()\n",
        "tools_A = [rag_tool_a, web_search_tool]\n",
        "tools_B = [rag_tool_b, web_search_tool]\n",
        "agent_executor_A = build_agent_executor(llm, AGENT_PROMPT_A, tools_A, \"A (rag_search_a)\", max_iters=2)\n",
        "agent_executor_B = build_agent_executor(llm, AGENT_PROMPT_B, tools_B, \"B (rag_search_b)\", max_iters=2)\n",
        "print(\"A keys:\", list(agent_executor_A.tools_by_name.keys()))\n",
        "print(\"B keys:\", list(agent_executor_B.tools_by_name.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XauI6YzAzmPv",
        "outputId": "bdd3149d-3ca4-4a44-8180-185aa19dcd8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[A¬∑RAG(mismo prompt)] iters=1 ¬∑ salida:\n",
            "  S√≠ntesis basada en herramientas:  ‚Ä¢ rag_search_A [A ¬∑ Resultado 1 ¬∑ score/distancia: 0.5599] Fragmento: de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligen\n",
            "   tools usadas: rag_search_A\n",
            "\n",
            "[B¬∑RAG(mismo prompt)] iters=1 ¬∑ salida:\n",
            "  S√≠ntesis basada en herramientas:  ‚Ä¢ rag_search_B [B ¬∑ Resultado 1 ¬∑ score/distancia: 0.7283] Fragmento: references [1] apuntes de la clase de inteligencia artificial, profesor steven andrey  pachecoportuguez,institutotecnolo'gicodecostarica,2025. Documento: DOC_001 ¬∑ chunk: DOC_001_B_006 Autor: Rodolfo David Acu√±a L√≥pez  [B ¬∑ Resultado 2 ¬∑ score/distancia: 0.6157] Fragmento: vii. conclusio'n los t\n",
            "   tools usadas: rag_search_B\n",
            "‚ùå A usa SOLO rag_search_a (sin web_search ni rag_search_b)\n",
            "‚ùå B usa SOLO rag_search_b (sin web_search ni rag_search_a)\n",
            "\n",
            "[A¬∑WEB(mismo prompt)] iters=1 ¬∑ salida:\n",
            "  S√≠ntesis basada en herramientas:  ‚Ä¢ rag_search_A [A ¬∑ Resultado 1 ¬∑ score/distancia: 0.6600] Fragmento: de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligen\n",
            "   tools usadas: rag_search_A, web_search\n",
            "\n",
            "[B¬∑WEB(mismo prompt)] iters=1 ¬∑ salida:\n",
            "  S√≠ntesis basada en herramientas:  ‚Ä¢ rag_search_B [B ¬∑ Resultado 1 ¬∑ score/distancia: 0.7283] Fragmento: references [1] apuntes de la clase de inteligencia artificial, profesor steven andrey  pachecoportuguez,institutotecnolo'gicodecostarica,2025. Documento: DOC_001 ¬∑ chunk: DOC_001_B_006 Autor: Rodolfo David Acu√±a L√≥pez  [B ¬∑ Resultado 2 ¬∑ score/distancia: 0.6157] Fragmento: vii. conclusio'n los t\n",
            "   tools usadas: rag_search_B, web_search\n",
            "‚úÖ A¬∑WEB invoca web_search\n",
            "‚úÖ B¬∑WEB invoca web_search\n",
            "\n",
            "‚Äî RESUMEN ‚Äî\n",
            "A¬∑RAG -> FAIL   | tools: ['rag_search_A']\n",
            "B¬∑RAG -> FAIL   | tools: ['rag_search_B']\n",
            "A¬∑WEB -> OK | tools: ['rag_search_A', 'web_search']\n",
            "B¬∑WEB -> OK | tools: ['rag_search_B', 'web_search']\n"
          ]
        }
      ],
      "source": [
        "# @title üî¨ Tests concisos A/B (mismo prompt) + Web\n",
        "import uuid\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "def _sid(tag):\n",
        "    return f\"{tag}-{uuid.uuid4().hex[:6]}\"\n",
        "\n",
        "def _tool_names_for(session_id):\n",
        "    hist = _SESSION_STORES.get(session_id)\n",
        "    if not hist:\n",
        "        return []\n",
        "    return [m.name for m in hist.messages if isinstance(m, ToolMessage)]\n",
        "\n",
        "def _run(agent, tag, prompt):\n",
        "    sid = _sid(tag)\n",
        "    resp = agent.invoke({\"input\": prompt, \"session_id\": sid})\n",
        "    out = (resp.get(\"output\",\"\") or \"\")[:400].replace(\"\\n\", \" \")\n",
        "    tools = _tool_names_for(sid)\n",
        "    used = \", \".join(tools) if tools else \"(ninguna)\"\n",
        "    print(f\"\\n[{tag}] iters={resp.get('iterations')} ¬∑ salida:\\n  {out}\")\n",
        "    print(f\"   tools usadas: {used}\")\n",
        "    return sid, tools\n",
        "\n",
        "def _ok(msg, cond):\n",
        "    print((\"‚úÖ \" if cond else \"‚ùå \") + msg)\n",
        "\n",
        "assert agent_executor_A and agent_executor_B, \"A/B no est√°n construidos.\"\n",
        "\n",
        "# ----------------- 1) MISMO PROMPT: RAG A vs RAG B -----------------\n",
        "base_prompt = \"¬øQu√© es inteligencia artificial? Cita autor y documento de los apuntes.\"\n",
        "\n",
        "sidA_rag, toolsA_rag = _run(agent_executor_A, \"A¬∑RAG(mismo prompt)\", base_prompt)\n",
        "sidB_rag, toolsB_rag = _run(agent_executor_B, \"B¬∑RAG(mismo prompt)\", base_prompt)\n",
        "\n",
        "setA = set(toolsA_rag)\n",
        "setB = set(toolsB_rag)\n",
        "\n",
        "# Exclusividad: solo su RAG (permitimos llamadas repetidas, por eso usamos set)\n",
        "_ok(\"A usa SOLO rag_search_a (sin web_search ni rag_search_b)\",\n",
        "    setA == {\"rag_search_a\"})\n",
        "_ok(\"B usa SOLO rag_search_b (sin web_search ni rag_search_a)\",\n",
        "    setB == {\"rag_search_b\"})\n",
        "\n",
        "# ----------------- 2) MISMO PROMPT + WEB EXPL√çCITO -----------------\n",
        "web_prompt = base_prompt + \" Por favor, busca en la web internet 2 novedades recientes y resume en 2 l√≠neas.\"\n",
        "\n",
        "sidA_web, toolsA_web = _run(agent_executor_A, \"A¬∑WEB(mismo prompt)\", web_prompt)\n",
        "sidB_web, toolsB_web = _run(agent_executor_B, \"B¬∑WEB(mismo prompt)\", web_prompt)\n",
        "\n",
        "setA_web = set(toolsA_web)\n",
        "setB_web = set(toolsB_web)\n",
        "\n",
        "# Debe invocar web_search (si adem√°s cae a RAG, no es problema)\n",
        "_ok(\"A¬∑WEB invoca web_search\", \"web_search\" in setA_web)\n",
        "_ok(\"B¬∑WEB invoca web_search\", \"web_search\" in setB_web)\n",
        "\n",
        "# ----------------- RESUMEN -----------------\n",
        "print(\"\\n‚Äî RESUMEN ‚Äî\")\n",
        "print(f\"A¬∑RAG -> {'OK' if setA == {'rag_search_a'} else 'FAIL'}   | tools: {toolsA_rag}\")\n",
        "print(f\"B¬∑RAG -> {'OK' if setB == {'rag_search_b'} else 'FAIL'}   | tools: {toolsB_rag}\")\n",
        "print(f\"A¬∑WEB -> {'OK' if 'web_search' in setA_web else 'FAIL'} | tools: {toolsA_web}\")\n",
        "print(f\"B¬∑WEB -> {'OK' if 'web_search' in setB_web else 'FAIL'} | tools: {toolsB_web}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCPYcKRNly1P",
        "outputId": "7a69328a-c02f-4e4b-ee5e-77f742973e3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Creando agente A (rag_search_a) ...\n",
            "\n",
            "ü§ñ Creando agente B (rag_search_b) ...\n",
            "A keys: ['rag_search_a', 'web_search']\n",
            "B keys: ['rag_search_b', 'web_search']\n"
          ]
        }
      ],
      "source": [
        "# Aseg√∫rate de tener la instancia creada en una celda previa:\n",
        "# web_search_tool = create_web_search_tool()\n",
        "\n",
        "# 1) Limpiar sesiones/historial para que no arrastren estado viejo\n",
        "_SESSION_STORES.clear()\n",
        "_SYSTEM_SET.clear()\n",
        "\n",
        "# 2) Reconstruir ambos agentes *incluyendo* la tool web\n",
        "tools_A = [rag_tool_a, web_search_tool]\n",
        "tools_B = [rag_tool_b, web_search_tool]\n",
        "\n",
        "agent_executor_A = build_agent_executor(llm, AGENT_PROMPT_A, tools_A, \"A (rag_search_a)\", max_iters=2)\n",
        "agent_executor_B = build_agent_executor(llm, AGENT_PROMPT_B, tools_B, \"B (rag_search_b)\", max_iters=2)\n",
        "\n",
        "# 3) Verificar que ambos ven 'web_search'\n",
        "print(\"A keys:\", list(agent_executor_A.tools_by_name.keys()))\n",
        "print(\"B keys:\", list(agent_executor_B.tools_by_name.keys()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSjoa9ko_ag7",
        "outputId": "80778354-2fe1-4c55-95c3-bc35f35d7526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Probando el agente A (rag_search_A) con ejemplos...\n",
            "\n",
            "======================================================================\n",
            "Ejemplo 1 (Agente A): Consulta sobre los apuntes\n",
            "======================================================================\n",
            "\n",
            "‚ùì Pregunta: ¬øQu√© es la inteligencia artificial seg√∫n los apuntes del curso?\n",
            "\n",
            "\n",
            "‚úÖ Respuesta del agente A:\n",
            "S√≠ntesis basada en herramientas:\n",
            "\n",
            "‚Ä¢ rag_search_A\n",
            "[A ¬∑ Resultado 1 ¬∑ score/distancia: 0.6600]\n",
            "Fragmento:\n",
            "de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligentes. estas herramientas representan un paso clave hacia una inteligencia artificial ma's contextual,...\n",
            "\n",
            "üîß Iteraciones: 1\n",
            "\n",
            "======================================================================\n",
            "Ejemplo 2 (Agente A): Consulta espec√≠fica\n",
            "======================================================================\n",
            "\n",
            "‚ùì Pregunta: Expl√≠came sobre aprendizaje supervisado\n",
            "\n",
            "\n",
            "‚úÖ Respuesta del agente A:\n",
            "S√≠ntesis basada en herramientas:\n",
            "\n",
            "‚Ä¢ rag_search_A\n",
            "[A ¬∑ Resultado 1 ¬∑ score/distancia: 0.6600]\n",
            "Fragmento:\n",
            "de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligentes. estas herramientas representan un paso clave hacia una inteligencia artificial ma's contextual,...\n",
            "\n",
            "üîß Iteraciones: 1\n",
            "\n",
            "======================================================================\n",
            "üß™ Probando el agente B (rag_search_B) con ejemplos...\n",
            "\n",
            "======================================================================\n",
            "Ejemplo 1 (Agente B): Consulta sobre los apuntes\n",
            "======================================================================\n",
            "\n",
            "‚ùì Pregunta: ¬øQu√© es la inteligencia artificial seg√∫n los apuntes del curso?\n",
            "\n",
            "\n",
            "‚úÖ Respuesta del agente B:\n",
            "S√≠ntesis basada en herramientas:\n",
            "\n",
            "‚Ä¢ rag_search_B\n",
            "[B ¬∑ Resultado 1 ¬∑ score/distancia: 0.7092]\n",
            "Fragmento:\n",
            "references\n",
            "[1] apuntes de la clase de inteligencia artificial, profesor steven andrey\n",
            "\n",
            "pachecoportuguez,institutotecnolo'gicodecostarica,2025.\n",
            "Documento: DOC_001 ¬∑ chunk: DOC_001_B_006\n",
            "Autor: Rodolfo David Acu√±a L√≥pez\n",
            "\n",
            "[B ¬∑ Resultado 2 ¬∑ score/distancia: 0.6796]\n",
            "Fragmento:\n",
            "vii. conclusio'n\n",
            "los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosproce...\n",
            "\n",
            "üîß Iteraciones: 1\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 4: Probar el agente con ejemplos\n",
        "# ============================================================\n",
        "\n",
        "# Probar agente A\n",
        "if agent_executor_A:\n",
        "    print(\"üß™ Probando el agente A (rag_search_A) con ejemplos...\\n\")\n",
        "\n",
        "    # ---------- Ejemplo 1 con Agente A ----------\n",
        "    print(\"=\"*70)\n",
        "    print(\"Ejemplo 1 (Agente A): Consulta sobre los apuntes\")\n",
        "    print(\"=\"*70)\n",
        "    test_query_1 = \"¬øQu√© es la inteligencia artificial seg√∫n los apuntes del curso?\"\n",
        "    print(f\"\\n‚ùì Pregunta: {test_query_1}\\n\")\n",
        "\n",
        "    try:\n",
        "        result_1 = agent_executor_A.invoke({\"input\": test_query_1, \"session_id\": \"test_A\"})\n",
        "        output_1 = result_1.get(\"output\", \"\")\n",
        "        print(\"\\n‚úÖ Respuesta del agente A:\")\n",
        "        print(output_1[:500] + \"...\" if len(output_1) > 500 else output_1)\n",
        "        print(f\"\\nüîß Iteraciones: {result_1.get('iterations', 'N/A')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "    # ---------- Ejemplo 2 con Agente A ----------\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Ejemplo 2 (Agente A): Consulta espec√≠fica\")\n",
        "    print(\"=\"*70)\n",
        "    test_query_2 = \"Expl√≠came sobre aprendizaje supervisado\"\n",
        "    print(f\"\\n‚ùì Pregunta: {test_query_2}\\n\")\n",
        "\n",
        "    try:\n",
        "        result_2 = agent_executor_A.invoke({\"input\": test_query_2, \"session_id\": \"test_A\"})\n",
        "        output_2 = result_2.get(\"output\", \"\")\n",
        "        print(\"\\n‚úÖ Respuesta del agente A:\")\n",
        "        print(output_2[:500] + \"...\" if len(output_2) > 500 else output_2)\n",
        "        print(f\"\\nüîß Iteraciones: {result_2.get('iterations', 'N/A')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  El agente A no est√° configurado. Configura OPENAI_API_KEY primero.\")\n",
        "\n",
        "# Probar agente B\n",
        "if agent_executor_B:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üß™ Probando el agente B (rag_search_B) con ejemplos...\\n\")\n",
        "\n",
        "    # ---------- Ejemplo 1 con Agente B ----------\n",
        "    print(\"=\"*70)\n",
        "    print(\"Ejemplo 1 (Agente B): Consulta sobre los apuntes\")\n",
        "    print(\"=\"*70)\n",
        "    test_query_1 = \"¬øQu√© es la inteligencia artificial seg√∫n los apuntes del curso?\"\n",
        "    print(f\"\\n‚ùì Pregunta: {test_query_1}\\n\")\n",
        "\n",
        "    try:\n",
        "        result_1 = agent_executor_B.invoke({\"input\": test_query_1, \"session_id\": \"test_B\"})\n",
        "        output_1 = result_1.get(\"output\", \"\")\n",
        "        print(\"\\n‚úÖ Respuesta del agente B:\")\n",
        "        print(output_1[:500] + \"...\" if len(output_1) > 500 else output_1)\n",
        "        print(f\"\\nüîß Iteraciones: {result_1.get('iterations', 'N/A')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  El agente B no est√° configurado. Configura OPENAI_API_KEY primero.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "VV3it8BI_c_P",
        "outputId": "0f8d8dad-fa3a-47cd-84ea-da4c990ec90c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.prompts'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4051264360.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#%pip install PromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationBufferWindowMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.prompts'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 5: App Streamlit con ‚ÄúWeb solo si el usuario lo pide‚Äù\n",
        "# ============================================================\n",
        "\n",
        "# NOTA: Esta celda usa c√≥digo antiguo de LangChain que NO es compatible\n",
        "# con la versi√≥n actual instalada (0.3.27). Se comenta para evitar errores.\n",
        "# Si necesitas Streamlit, deber√°s adaptar el c√≥digo al SimpleAgentExecutor\n",
        "# que ya est√° implementado en las celdas anteriores.\n",
        "\n",
        "# C√ìDIGO COMENTADO - INCOMPATIBLE CON LANGCHAIN 0.3.27\n",
        "\"\"\"\n",
        "import os, pickle, re\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from langchain.agents import AgentExecutor, create_react_agent  # NO DISPONIBLE EN 0.3.27\n",
        "\"\"\"\n",
        "# ‚úÖ Instalaci√≥n de dependencias necesarias para LangChain 0.3.27\n",
        "%pip install --quiet langchain langchain-core langchain-community langchain-openai\n",
        "\n",
        "# ‚úÖ Imports actualizados para LangChain 0.3.27\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# ConversationBufferWindowMemory - intentar m√∫ltiples ubicaciones\n",
        "ConversationBufferWindowMemory = None\n",
        "try:\n",
        "    from langchain.memory import ConversationBufferWindowMemory\n",
        "    print(\"‚úÖ ConversationBufferWindowMemory importado desde langchain.memory\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from langchain_core.memory import ConversationBufferWindowMemory\n",
        "        print(\"‚úÖ ConversationBufferWindowMemory importado desde langchain_core.memory\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            # Intentar importar desde langchain.chains si est√° disponible\n",
        "            from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "            print(\"‚úÖ ConversationBufferWindowMemory importado desde langchain.chains.conversation.memory\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è ConversationBufferWindowMemory no encontrado en ninguna ubicaci√≥n est√°ndar\")\n",
        "            print(\"   Verificando m√≥dulos disponibles...\")\n",
        "            try:\n",
        "                import langchain.memory as mem\n",
        "                print(f\"   M√≥dulos en langchain.memory: {dir(mem)}\")\n",
        "            except:\n",
        "                pass\n",
        "            # Si no est√° disponible, el c√≥digo fallar√° m√°s adelante cuando se intente usar\n",
        "            # El usuario necesitar√° adaptar el c√≥digo o usar una alternativa\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.tools.render import render_text_description\n",
        "from langchain_core.tools import Tool\n",
        "\n",
        "# --------- guardar config para la app ----------\n",
        "# Verificar que las variables necesarias est√©n definidas\n",
        "required_vars = {\n",
        "    \"OUT_DIR\": OUT_DIR if 'OUT_DIR' in globals() else None,\n",
        "    \"VECTORSTORE_DIR_A\": VECTORSTORE_DIR_A if 'VECTORSTORE_DIR_A' in globals() else None,\n",
        "    \"VECTORSTORE_DIR_B\": VECTORSTORE_DIR_B if 'VECTORSTORE_DIR_B' in globals() else None,\n",
        "    \"PROYECTO_DIR\": PROYECTO_DIR if 'PROYECTO_DIR' in globals() else None,\n",
        "}\n",
        "\n",
        "# Determinar qu√© prompt usar (AGENT_PROMPT, AGENT_PROMPT_A, o crear uno gen√©rico)\n",
        "if 'AGENT_PROMPT' in globals():\n",
        "    agent_prompt = AGENT_PROMPT\n",
        "elif 'AGENT_PROMPT_A' in globals():\n",
        "    agent_prompt = AGENT_PROMPT_A\n",
        "    print(\"‚ÑπÔ∏è Usando AGENT_PROMPT_A como prompt del agente\")\n",
        "elif 'AGENT_PROMPT_B' in globals():\n",
        "    agent_prompt = AGENT_PROMPT_B\n",
        "    print(\"‚ÑπÔ∏è Usando AGENT_PROMPT_B como prompt del agente\")\n",
        "else:\n",
        "    agent_prompt = \"Eres un asistente acad√©mico especializado en el curso de Inteligencia Artificial.\"\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ AGENT_PROMPT, AGENT_PROMPT_A ni AGENT_PROMPT_B. Usando prompt gen√©rico.\")\n",
        "\n",
        "# Verificar que las variables requeridas est√©n disponibles\n",
        "missing_vars = [var for var, value in required_vars.items() if value is None]\n",
        "if missing_vars:\n",
        "    print(f\"‚ö†Ô∏è Variables faltantes: {', '.join(missing_vars)}\")\n",
        "    print(\"   Aseg√∫rate de ejecutar las celdas anteriores que definen estas variables.\")\n",
        "    print(\"   El c√≥digo de Streamlit no se generar√° hasta que todas las variables est√©n definidas.\")\n",
        "else:\n",
        "    STREAMLIT_DATA_PATH = os.path.join(required_vars[\"OUT_DIR\"], \"streamlit_data.pkl\")\n",
        "    with open(STREAMLIT_DATA_PATH, \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"agent_prompt\": agent_prompt,\n",
        "            \"vectorstore_a_path\": required_vars[\"VECTORSTORE_DIR_A\"],\n",
        "            \"vectorstore_b_path\": required_vars[\"VECTORSTORE_DIR_B\"],\n",
        "            \"proyecto_dir\": required_vars[\"PROYECTO_DIR\"],\n",
        "        }, f)\n",
        "    print(\"‚úÖ Configuraci√≥n guardada para Streamlit:\", STREAMLIT_DATA_PATH)\n",
        "\n",
        "# --------- archivo de la app ----------\n",
        "STREAMLIT_APP_CODE = f'''import streamlit as st\n",
        "import os, re, pickle\n",
        "from typing import List\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "# ‚ö†Ô∏è NOTA: AgentExecutor y create_react_agent no est√°n disponibles en LangChain 0.3.27\n",
        "# Necesitar√°s usar una alternativa como SimpleAgentExecutor o actualizar a una versi√≥n compatible\n",
        "# from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "# ConversationBufferWindowMemory - intentar m√∫ltiples ubicaciones\n",
        "ConversationBufferWindowMemory = None\n",
        "try:\n",
        "    from langchain.memory import ConversationBufferWindowMemory\n",
        "    print(\"‚úÖ ConversationBufferWindowMemory importado desde langchain.memory\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        from langchain_core.memory import ConversationBufferWindowMemory\n",
        "        print(\"‚úÖ ConversationBufferWindowMemory importado desde langchain_core.memory\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "            print(\"‚úÖ ConversationBufferWindowMemory importado desde langchain.chains.conversation.memory\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è ConversationBufferWindowMemory no encontrado en ninguna ubicaci√≥n est√°ndar\")\n",
        "            print(\"   El c√≥digo de Streamlit puede no funcionar correctamente sin esta clase.\")\n",
        "            print(\"   Considera usar una alternativa o actualizar LangChain a una versi√≥n compatible.\")\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_core.tools.render import render_text_description\n",
        "\n",
        "# DuckDuckGo (opcional)\n",
        "try:\n",
        "    from langchain_community.tools import DuckDuckGoSearchRun\n",
        "    _HAS_DDG = True\n",
        "except Exception:\n",
        "    _HAS_DDG = False\n",
        "\n",
        "st.set_page_config(page_title=\"AsistenteIA - Curso de IA\", page_icon=\"ü§ñ\", layout=\"wide\")\n",
        "st.title(\"ü§ñ AsistenteIA - Curso de Inteligencia Artificial\")\n",
        "st.caption(\"Prioriza apuntes (RAG). Usa la web **solo** si lo pides expl√≠citamente.\")\n",
        "\n",
        "BASE_DIR = \"{PROYECTO_DIR}\"\n",
        "OUT_DIR = os.path.join(BASE_DIR, \"dataset\")\n",
        "CFG_PATH = os.path.join(OUT_DIR, \"streamlit_data.pkl\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_cfg():\n",
        "    with open(CFG_PATH, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_embeddings():\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={{\"device\": \"cpu\"}},\n",
        "        encode_kwargs={{\"normalize_embeddings\": True}}\n",
        "    )\n",
        "\n",
        "@st.cache_resource\n",
        "def load_vectorstores():\n",
        "    cfg = load_cfg()\n",
        "    emb = load_embeddings()\n",
        "    vs_a = FAISS.load_local(cfg[\"vectorstore_a_path\"], emb, allow_dangerous_deserialization=True)\n",
        "    vs_b = FAISS.load_local(cfg[\"vectorstore_b_path\"], emb, allow_dangerous_deserialization=True)\n",
        "    return vs_a, vs_b\n",
        "\n",
        "def create_rag_tool(vs, name: str) -> Tool:\n",
        "    def rag_search(query: str, k: int = 5) -> str:\n",
        "        try:\n",
        "            results = vs.similarity_search_with_score(query, k=k)\n",
        "            if not results:\n",
        "                return \"No se encontraron fragmentos relevantes.\"\n",
        "            out = []\n",
        "            for i, (doc, score) in enumerate(results, 1):\n",
        "                autor = doc.metadata.get(\"autor\", \"N/A\")\n",
        "                id_doc = doc.metadata.get(\"id_doc\", \"N/A\")\n",
        "                chunk_id = doc.metadata.get(\"chunk_id\", \"N/A\")\n",
        "                frag = (doc.page_content or \"\")[:700].replace(\"\\\\n\",\" \")\n",
        "                out.append(\n",
        "                    f\"[Resultado {{i}} ¬∑ Score: {{score:.4f}}]\\\\n\"\n",
        "                    f\"Fragmento: {{frag}}...\\\\n\"\n",
        "                    f\"Seg√∫n [{{autor}}] en [{{id_doc}}] (chunk {{chunk_id}}).\"\n",
        "                )\n",
        "            return \"\\\\n\\\\n\".join(out)\n",
        "        except Exception as e:\n",
        "            return f\"Error en RAG {{name}}: {{e}}\"\n",
        "    return Tool(\n",
        "        name=f\"rag_search_{{name}}\",\n",
        "        description=f\"Busca en apuntes (segmentaci√≥n {{name}}). Devuelve fragmentos con cita.\",\n",
        "        func=rag_search\n",
        "    )\n",
        "\n",
        "def create_web_tool() -> Tool:\n",
        "    if not _HAS_DDG:\n",
        "        return Tool(\n",
        "            name=\"web_search\",\n",
        "            description=\"B√∫squeda web (stub). √ösala solo si la pides expl√≠citamente.\",\n",
        "            func=lambda q: \"WebSearch no disponible (falta DuckDuckGoSearchRun).\"\n",
        "        )\n",
        "    search = DuckDuckGoSearchRun()\n",
        "    def _web(q: str) -> str:\n",
        "        try:\n",
        "            r = search.run(q)\n",
        "            return f\"Resultados web para '{{q}}'\\\\n\\\\n{{r}}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error en b√∫squeda web: {{e}}\"\n",
        "    return Tool(\n",
        "        name=\"web_search\",\n",
        "        description=\"B√∫squeda en internet (DuckDuckGo). Solo cuando lo pidas expl√≠citamente.\",\n",
        "        func=_web\n",
        "    )\n",
        "\n",
        "# --------- Estado ----------\n",
        "if \"messages\" not in st.session_state: st.session_state.messages = []\n",
        "if \"agent_rag\" not in st.session_state: st.session_state.agent_rag = None\n",
        "if \"agent_rag_web\" not in st.session_state: st.session_state.agent_rag_web = None\n",
        "\n",
        "# --------- Sidebar ----------\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configuraci√≥n\")\n",
        "    gkey = st.text_input(\"OpenAI API Key\", type=\"password\", value=os.getenv(\"OPENAI_API_KEY\",\"\"))\n",
        "    if gkey: os.environ[\"OPENAI_API_KEY\"] = gkey\n",
        "\n",
        "    if gkey and (st.session_state.agent_rag is None or st.session_state.agent_rag_web is None):\n",
        "        with st.spinner(\"Inicializando agentes...\"):\n",
        "            cfg = load_cfg()\n",
        "            vs_a, vs_b = load_vectorstores()\n",
        "\n",
        "            ragA = create_rag_tool(vs_a, \"A\")\n",
        "            ragB = create_rag_tool(vs_b, \"B\")\n",
        "            web  = create_web_tool()\n",
        "\n",
        "            def make_agent(tools):\n",
        "                llm = ChatOpenAI(\n",
        "                    model=\"gpt-4-turbo\",\n",
        "                    openai_api_key=gkey,\n",
        "                    temperature=0.1\n",
        "                )\n",
        "                tool_str = render_text_description(tools)\n",
        "                tool_names = \", \".join([t.name for t in tools])\n",
        "\n",
        "                # ‚úÖ Incluimos {{history}} y {{agent_scratchpad}} en el template\n",
        "                prompt = PromptTemplate(\n",
        "                    template=(\n",
        "                        \"{{agent_profile}}\\\\n\\\\n\"\n",
        "                        \"Tienes acceso a estas herramientas:\\\\n{{tools}}\\\\n\\\\n\"\n",
        "                        \"Sigue este formato EXACTO (sin bloques de c√≥digo):\\\\n\\\\n\"\n",
        "                        \"Question: {{input}}\\\\n\"\n",
        "                        \"Thought: razona brevemente el siguiente paso\\\\n\"\n",
        "                        \"Action: una de [{{tool_names}}]\\\\n\"\n",
        "                        \"Action Input: el input para la acci√≥n\\\\n\"\n",
        "                        \"Observation: el resultado de la acci√≥n\\\\n\"\n",
        "                        \"... (repite Thought/Action/Action Input/Observation si hace falta) ...\\\\n\"\n",
        "                        \"Thought: I now know the final answer\\\\n\"\n",
        "                        \"Final Answer: tu respuesta final en espa√±ol, citando autor y documento si aplica\\\\n\\\\n\"\n",
        "                        \"Historial:\\\\n{{history}}\\\\n\\\\n\"\n",
        "                        \"Razonamiento y pasos previos:\\\\n{{agent_scratchpad}}\\\\n\"\n",
        "                    ),\n",
        "                    input_variables=[\"history\",\"input\",\"agent_scratchpad\",\"tools\",\"tool_names\"]\n",
        "                ).partial(\n",
        "                    agent_profile=cfg[\"agent_prompt\"],\n",
        "                    tools=tool_str,\n",
        "                    tool_names=tool_names\n",
        "                )\n",
        "\n",
        "                memory = ConversationBufferWindowMemory(\n",
        "                    k=5,\n",
        "                    memory_key=\"history\",\n",
        "                    return_messages=True,\n",
        "                    output_key=\"output\"\n",
        "                )\n",
        "\n",
        "                agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "                return AgentExecutor(\n",
        "                    agent=agent,\n",
        "                    tools=tools,\n",
        "                    memory=memory,\n",
        "                    verbose=True,\n",
        "                    handle_parsing_errors=True,\n",
        "                    max_iterations=8,\n",
        "                    return_intermediate_steps=True\n",
        "                )\n",
        "\n",
        "            # agente SOLO RAG\n",
        "            st.session_state.agent_rag = make_agent([ragA, ragB])\n",
        "            # agente RAG + Web\n",
        "            st.session_state.agent_rag_web = make_agent([ragA, ragB, web])\n",
        "\n",
        "            st.success(\"‚úÖ Agentes listos: RAG y RAG+Web\")\n",
        "\n",
        "# --------- Render previo ----------\n",
        "for m in st.session_state.messages:\n",
        "    with st.chat_message(m[\"role\"]):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "# --------- Chat ----------\n",
        "if user_prompt := st.chat_input(\"Pregunta algo sobre los apuntes de IA...\"):\n",
        "    st.session_state.messages.append({{\"role\":\"user\",\"content\":user_prompt}})\n",
        "    with st.chat_message(\"user\"): st.markdown(user_prompt)\n",
        "\n",
        "    # ¬øEl usuario pidi√≥ expl√≠citamente web?\n",
        "    wants_web = re.search(r\"\\\\b(web|internet|google|websearch|buscar en (la )?web)\\\\b\", user_prompt, re.I)\n",
        "\n",
        "    executor = st.session_state.agent_rag_web if wants_web else st.session_state.agent_rag\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        if executor is None:\n",
        "            st.warning(\"Configura la API key en el sidebar.\")\n",
        "        else:\n",
        "            with st.spinner(\"Pensando...\"):\n",
        "                try:\n",
        "                    result = executor.invoke({{\"input\": user_prompt}})\n",
        "                    resp = result.get(\"output\",\"\")\n",
        "                    st.markdown(resp)\n",
        "                    st.session_state.messages.append({{\"role\":\"assistant\",\"content\":resp}})\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error: {{e}}\")\n",
        "'''\n",
        "\n",
        "STREAMLIT_APP_PATH = os.path.join(PROYECTO_DIR, \"streamlit_app.py\")\n",
        "with open(STREAMLIT_APP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(STREAMLIT_APP_CODE)\n",
        "\n",
        "print(\"‚úÖ App Streamlit escrita:\")\n",
        "print(\"   \", STREAMLIT_APP_PATH)\n",
        "print(\"üöÄ Ejecuta:  streamlit run streamlit_app.py  (o tu bloque con ngrok)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuG_0BzK_f-p"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 6: Resumen final y verificaci√≥n de Compa√±ero 3 (actualizado)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Intentar recuperar rutas ya definidas; si no existen, usar valores por defecto seguros\n",
        "try:\n",
        "    app_path = STREAMLIT_APP_PATH\n",
        "except NameError:\n",
        "    try:\n",
        "        app_path = os.path.join(PROYECTO_DIR, \"streamlit_app.py\")\n",
        "    except NameError:\n",
        "        app_path = \"streamlit_app.py\"\n",
        "\n",
        "try:\n",
        "    proyecto_dir = PROYECTO_DIR\n",
        "except NameError:\n",
        "    proyecto_dir = \".\"\n",
        "\n",
        "try:\n",
        "    streamlit_data_path = STREAMLIT_DATA_PATH\n",
        "except NameError:\n",
        "    try:\n",
        "        streamlit_data_path = os.path.join(os.path.join(proyecto_dir, \"dataset\"), \"streamlit_data.pkl\")\n",
        "    except Exception:\n",
        "        streamlit_data_path = \"dataset/streamlit_data.pkl\"\n",
        "\n",
        "try:\n",
        "    vectorstore_a_path = VECTORSTORE_DIR_A\n",
        "except NameError:\n",
        "    vectorstore_a_path = \"<VECTORSTORE_DIR_A>\"\n",
        "\n",
        "try:\n",
        "    vectorstore_b_path = VECTORSTORE_DIR_B\n",
        "except NameError:\n",
        "    vectorstore_b_path = \"<VECTORSTORE_DIR_B>\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ RESUMEN DE LA PARTE DEL COMPA√ëERO 3\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìã Componentes implementados:\")\n",
        "\n",
        "print(\"\\n1. ‚úÖ Prompt base/perfil del agente:\")\n",
        "print(\"   - Nombre: AsistenteIA\")\n",
        "print(\"   - Rol: Asistente acad√©mico especializado en IA\")\n",
        "print(\"   - Estilo: Claro, educativo, con citas\")\n",
        "print(\"   - Restricci√≥n: Primero apuntes (RAG), luego web (solo si el usuario lo solicita o no hay cobertura)\")\n",
        "\n",
        "print(\"\\n2. ‚úÖ Agente orquestador:\")\n",
        "print(\"   - Modelos preferidos (fallback en cascada):\")\n",
        "print(\"       gpt-4-turbo ‚Üí gpt-4o-mini ‚Üí gpt-4 ‚Üí gpt-3.5-turbo ‚Üí gpt-4o ‚Üí gpt-3.5-turbo\")\n",
        "print(\"   - Par√°metros: temperature=0.1, max_output_tokens=1024\")\n",
        "print(\"   - Patr√≥n: ReAct ESTRICTO (Reasoning + Acting) con formato impuesto\")\n",
        "print(\"   - Post-procesado: verificaci√≥n de cita con ensure_citation\")\n",
        "print(\"   - Decisi√≥n: Entre rag_search_A, rag_search_B, web_search (stub) o cierre directo\")\n",
        "print(\"   - Max iteraciones: 8\")\n",
        "print(\"   - early_stopping_method: generate\")\n",
        "print(\"   - return_intermediate_steps: True\")\n",
        "\n",
        "print(\"\\n3. ‚úÖ Memoria conversacional:\")\n",
        "print(\"   - Tipo: ConversationBufferWindowMemory\")\n",
        "print(\"   - Ventana: √öltimas 5 interacciones (k=5)\")\n",
        "print(\"   - Caracter√≠stica: No guarda historial permanente\")\n",
        "\n",
        "print(\"\\n4. ‚úÖ Interfaz Streamlit:\")\n",
        "print(\"   - Aplicaci√≥n web completa con chat en tiempo real\")\n",
        "print(\"   - Indica herramientas usadas por turno\")\n",
        "print(\"   - Sidebar para configurar OPENAI_API_KEY y ver √∫ltimas herramientas\")\n",
        "print(f\"   - Archivo de aplicaci√≥n: {app_path}\")\n",
        "print(f\"   - Configuraci√≥n guardada: {streamlit_data_path}\")\n",
        "\n",
        "print(\"\\n5. ‚úÖ Integraci√≥n completa:\")\n",
        "print(\"   - Agente + Tools + Memoria + Interfaz integrados\")\n",
        "print(\"   - Listo para demostraci√≥n en vivo con tus vectorstores\")\n",
        "\n",
        "print(\"\\nüìä Herramientas disponibles para el agente:\")\n",
        "print(\"   1) rag_search_A: B√∫squeda en apuntes (segmentaci√≥n A - chunks fijos) con cita obligatoria\")\n",
        "print(\"   2) rag_search_B: B√∫squeda en apuntes (segmentaci√≥n B - encabezados) con cita obligatoria\")\n",
        "print(\"   3) web_search: B√∫squeda web (stub en esta demo; usar solo si se solicita)\")\n",
        "\n",
        "print(\"\\nüìÅ Rutas de vectorstores (FAISS):\")\n",
        "print(f\"   - VECTORSTORE_DIR_A: {vectorstore_a_path}\")\n",
        "print(f\"   - VECTORSTORE_DIR_B: {vectorstore_b_path}\")\n",
        "\n",
        "print(\"\\nüöÄ Para ejecutar la aplicaci√≥n:\")\n",
        "print(\"   1) Asegura que los vectorstores (A y B) existan en las rutas anteriores.\")\n",
        "print(\"   2) Configura la variable de entorno OPENAI_API_KEY (o en el sidebar de la app).\")\n",
        "print(f\"   3) cd {proyecto_dir}\")\n",
        "print(f\"   4) streamlit run {os.path.basename(app_path)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ COMPA√ëERO 3 - TAREA COMPLETADA\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüéØ El sistema est√° listo para:\")\n",
        "print(\"   - Comparar el comportamiento de ambas segmentaciones (A vs B)\")\n",
        "print(\"   - Observar herramientas usadas, trazas ReAct e impacto de la memoria\")\n",
        "print(\"   - Demostraci√≥n presencial con consultas reales del curso\")\n",
        "print(\"=\"*70)\n",
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-6Z4ZGXv-V7"
      },
      "outputs": [],
      "source": [
        "# Instalar duckduckgo-search para b√∫squeda web\n",
        "# NOTA: langchain_community ya est√° instalado en la Celda 2 (versi√≥n >=0.2.15)\n",
        "# Solo necesitamos instalar duckduckgo-search\n",
        "%pip install --quiet duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5GWdnK3yW3p"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip -q install pyngrok streamlit requests\n",
        "\n",
        "import os, time, shlex, subprocess, requests\n",
        "from pyngrok import ngrok\n",
        "\n",
        "APP_PATH = \"/content/drive/MyDrive/Colab Notebooks/Tarea3-IA/streamlit_app.py\"\n",
        "PORT = 8502\n",
        "\n",
        "# 1) Arranca Streamlit\n",
        "!pkill -f \"streamlit run\" || true\n",
        "cmd = f'streamlit run \"{APP_PATH}\" --server.port {PORT} --server.address 0.0.0.0 --server.headless true --browser.gatherUsageStats false'\n",
        "sp = subprocess.Popen(shlex.split(cmd))\n",
        "\n",
        "# 2) Espera a que est√© saludable\n",
        "import time\n",
        "ok = False\n",
        "for _ in range(40):\n",
        "    time.sleep(0.5)\n",
        "    try:\n",
        "        r = requests.get(f\"http://localhost:{PORT}/_stcore/health\", timeout=1)\n",
        "        if r.ok:\n",
        "            ok = True\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "if not ok:\n",
        "    raise SystemExit(\"Streamlit no respondi√≥; revisa logs con: !pkill -f 'streamlit run'; !tail -n 120 /tmp/streamlit.log\")\n",
        "\n",
        "# 3) Configura authtoken (desde Secrets o pegado)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    token = userdata.get(\"NGROK_AUTHTOKEN\")\n",
        "except Exception:\n",
        "    token = None\n",
        "# token = \"PEGAR_AQUI_TU_TOKEN\"  # <-- alternativa manual\n",
        "if not token:\n",
        "    raise SystemExit(\"Falta NGROK_AUTHTOKEN (ponlo en Colab Secrets o p√©galo en la variable 'token').\")\n",
        "\n",
        "ngrok.set_auth_token(token)\n",
        "\n",
        "# 4) Cierra t√∫neles previos y abre uno nuevo\n",
        "for t in ngrok.get_tunnels():\n",
        "    ngrok.disconnect(t.public_url)\n",
        "\n",
        "public_url = ngrok.connect(PORT, \"http\").public_url\n",
        "print(\"üåê URL p√∫blica:\", public_url)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02446af37b9a43a6a18648ce8a88a595": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03335e7e3afc415cb0aa7d6f11db5dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "055542ecb0e04ef49714c32001c19330": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06dd0ed90ec5487d9eedd7b1d7ad03fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f42949df0bdd42dd8b8dcc5c70043aac",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70bc013675c14e57be0a023c33a1b555",
            "value": 349
          }
        },
        "0f587fb5340646c09be2da571b6bcb25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "105b408281fd414daec81817fa60b1bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "152558aa9fe74f93835d54ff8a8f10c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165d48948e184f909a4dde05f0647b98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1973786fee7e41daba689e40a6183d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19ec7be049df4b7b93507d8112bba69b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a84fa9fd87b43669eb6748e9dbb97d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bf187771aa845a28e82ee311ac07a9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c0f7d99607f4710b9beb6987b32ead5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1c2db6efc1bb4977bf29890d2b61cb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ed86fc53e2e418da663b48b5c3fd351": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_105b408281fd414daec81817fa60b1bf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f08921db6e934d4eb31dfad55c5f51ef",
            "value": 1
          }
        },
        "22774cc56b9443df868b10ab4c169c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23291c78b4424f229278e05cac25c8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9896a774ff0047b48b0ad7a5276384f9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f0b25f15ecbd46a5a9b9eb57346d488c",
            "value": "‚Äá612/612‚Äá[00:00&lt;00:00,‚Äá14.5kB/s]"
          }
        },
        "233b2f7e5eaa480492f0473623c70134": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fa876ad54dd42399e54684f5dfca8e0",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d189e8ca580d4137bd87289646c9e26c",
            "value": 116
          }
        },
        "2542d4f04c7544278e5993633bb5b9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2816d71c2b914b8f82a170b7c52cbcb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee2ccd835534163851a7aba0aa2cbef",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d4c5eb2e034445eb891eb9a97fb7958e",
            "value": "sentence_bert_config.json:‚Äá100%"
          }
        },
        "2926b3da7a1a4b5b978976cc5d510ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29e4d75a2b3d4032aa3dbc4f2c9975d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a2d268e5e564b5d92898682df4b8624": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6609691e167a428ab1011f72510f3b57",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bbfc023c2e44320862c131bb2dce916",
            "value": 53
          }
        },
        "2a53c4b274b04e7eb8672c5ac616185c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "310b273527d34127abf31a1a285e68c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c6166df49149bea00d1a5b9769bba3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e5cbdcad04429f87d34797225b372d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bb8fdec708940eda4b54188e06439d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eceadeb84e244a3882de6a07566b2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf8cfd738d244596b3f00b9e1fd548e7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b2e1f2cbffdb46369143fc659885ec50",
            "value": "‚Äá53.0/53.0‚Äá[00:00&lt;00:00,‚Äá2.25kB/s]"
          }
        },
        "3fa876ad54dd42399e54684f5dfca8e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40688bb881974f058d1d99d29b138a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a86b1ae42084e439959ac6b6fa46163",
              "IPY_MODEL_233b2f7e5eaa480492f0473623c70134",
              "IPY_MODEL_44bd266a2f2248858cffcbbbdb802120"
            ],
            "layout": "IPY_MODEL_152558aa9fe74f93835d54ff8a8f10c2"
          }
        },
        "44bd266a2f2248858cffcbbbdb802120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a84fa9fd87b43669eb6748e9dbb97d3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_22774cc56b9443df868b10ab4c169c18",
            "value": "‚Äá116/116‚Äá[00:00&lt;00:00,‚Äá2.88kB/s]"
          }
        },
        "4b10b489a3d34c469996cf9a326c0097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4db1b7c585024fc1a745a4e731c70474": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f12f92037df4b4faa89c2502f9002a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9523a16e02214039924e5613276a1b45",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc211367670446008bf1f27636ffc9b1",
            "value": "vocab.txt:‚Äá"
          }
        },
        "54b6e834f6ca4076b92d167396df2eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f415be8be994eb3a1c1aab955484ebf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4b10b489a3d34c469996cf9a326c0097",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "54bc6f6575b64de7af146a097c3e47fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "55018d28d5c64215bcbc820f9a68914d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c629f04e8e2444b68052e373de38a725",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9de40b988e5c478a9be83ab2a2b49f52",
            "value": 90868376
          }
        },
        "550d10d3e0d04fa0bd2baba0a2a69b58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5625cd5c38ec41cd9b626a89f7715d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b06c4a31fee472fa5895fffc9077dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d555664d92aa4427a91f8091a82cf097",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8faea8294be0492d8e1dafae8325d759",
            "value": 190
          }
        },
        "5bbfc023c2e44320862c131bb2dce916": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "646c5b0f5a9344ddb8204ceae976037a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2816d71c2b914b8f82a170b7c52cbcb5",
              "IPY_MODEL_2a2d268e5e564b5d92898682df4b8624",
              "IPY_MODEL_3eceadeb84e244a3882de6a07566b2cb"
            ],
            "layout": "IPY_MODEL_31c6166df49149bea00d1a5b9769bba3"
          }
        },
        "654eb87658c04a3387ec3972ecd2974f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6609691e167a428ab1011f72510f3b57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f7490d4724c46fd8e06cbb3848f887c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70bc013675c14e57be0a023c33a1b555": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71c39c8de1ff405b8ce43b3f457db8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73a8c317825143afbf48114d8b32d04b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "748ef51e7ac14f3184373c35baa887ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "767716710f274910a87179e2040059a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76bf6cc8c7fe4166b679f34de0bee2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a77711102d342f99afb5e9249a754fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef0545f26b214dd3a56295d423d2c4a3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1c2db6efc1bb4977bf29890d2b61cb15",
            "value": "modules.json:‚Äá100%"
          }
        },
        "7cc441931d6d469c86f63ddd556f376b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec59a972e7141408fc60928b4141d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84e725bc6cd54c7c9983b7d57804842a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "874e51415ca349469bb564e684f5d339": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a53c4b274b04e7eb8672c5ac616185c",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76bf6cc8c7fe4166b679f34de0bee2ae",
            "value": 612
          }
        },
        "8a2935b7b7b94791b8cfeaaead422724": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5f5924a3f0b4956bb896e82722856dd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc68c7da6f514ddfa4489bad6949406b",
            "value": "‚Äá90.9M/90.9M‚Äá[00:01&lt;00:00,‚Äá73.4MB/s]"
          }
        },
        "8bbb01eb45864b1f95532c2eee3407b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b98d460e08de465ebcb31c684f871583",
              "IPY_MODEL_1ed86fc53e2e418da663b48b5c3fd351",
              "IPY_MODEL_bf25cdf5389b49a39f276642599e82b3"
            ],
            "layout": "IPY_MODEL_73a8c317825143afbf48114d8b32d04b"
          }
        },
        "8ee2ccd835534163851a7aba0aa2cbef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f415be8be994eb3a1c1aab955484ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8faea8294be0492d8e1dafae8325d759": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fb3a9985a9f413b97a708795806207e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90f60ecb21164af592148bffc8f50750": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02446af37b9a43a6a18648ce8a88a595",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7ec59a972e7141408fc60928b4141d33",
            "value": "config.json:‚Äá100%"
          }
        },
        "9523a16e02214039924e5613276a1b45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97933dd8beb1467fb8c7899a7bb38318": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e70c6b9dbd8142a7819452e1a3f57d8c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2926b3da7a1a4b5b978976cc5d510ac0",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "980fa204feaa4382bfef0b05cd6ddd29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "981549a57226444c97eb4a50cdc3ff98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84e725bc6cd54c7c9983b7d57804842a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c4321fe496014372ab9fe341fc795c67",
            "value": "config.json:‚Äá100%"
          }
        },
        "9896a774ff0047b48b0ad7a5276384f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a86b1ae42084e439959ac6b6fa46163": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e627fcd229aa43a5986092860fc993f5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ae06b35563e24235be4c8abf75a7b4b3",
            "value": "config_sentence_transformers.json:‚Äá100%"
          }
        },
        "9d99f1eae99446ff914b802fe878d264": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_748ef51e7ac14f3184373c35baa887ae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_055542ecb0e04ef49714c32001c19330",
            "value": "‚Äá190/190‚Äá[00:00&lt;00:00,‚Äá2.02kB/s]"
          }
        },
        "9de40b988e5c478a9be83ab2a2b49f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1333f4a234b44c999128226cfb61b86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f63ff1845644e9be2f87fcedc722b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_165d48948e184f909a4dde05f0647b98",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fae1806f351f42d997bc9a614e1f68a9",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "a68bcc3ce0cb43149ef067b917eb9fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2e31ab141554b25b02b20a8d1a80110",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_71c39c8de1ff405b8ce43b3f457db8cd",
            "value": "‚Äá232k/?‚Äá[00:00&lt;00:00,‚Äá2.21MB/s]"
          }
        },
        "a80e3b57b4484cb19be43ebcf3cf98d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8ad90def4eb4adeb20c8fb50d937beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97933dd8beb1467fb8c7899a7bb38318",
              "IPY_MODEL_fd5304d2a2dd453cb90cf830f25dbbe2",
              "IPY_MODEL_eb01ab1e60af487d83ba8aeddcf98bdc"
            ],
            "layout": "IPY_MODEL_3bb8fdec708940eda4b54188e06439d5"
          }
        },
        "a97f3324b844472da11fd5fc012559fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae06b35563e24235be4c8abf75a7b4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b10c889d6280498ca29d31d17f78e55f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2e1f2cbffdb46369143fc659885ec50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8a916060e1d48acb9d759dea8ea38d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7146e50e280480d85aa541f0a39e93c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0f587fb5340646c09be2da571b6bcb25",
            "value": "README.md:‚Äá"
          }
        },
        "b98d460e08de465ebcb31c684f871583": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b10c889d6280498ca29d31d17f78e55f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4db1b7c585024fc1a745a4e731c70474",
            "value": "tokenizer.json:‚Äá"
          }
        },
        "bb80de04d5c940bdaac1ffc40c109d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2542d4f04c7544278e5993633bb5b9f6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1973786fee7e41daba689e40a6183d73",
            "value": "‚Äá350/350‚Äá[00:00&lt;00:00,‚Äá6.68kB/s]"
          }
        },
        "bec1b02ba4054950ab2d6a5b0d7c784d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a77711102d342f99afb5e9249a754fd",
              "IPY_MODEL_06dd0ed90ec5487d9eedd7b1d7ad03fa",
              "IPY_MODEL_fe81263a3a19462494df2d76da2d1af5"
            ],
            "layout": "IPY_MODEL_7cc441931d6d469c86f63ddd556f376b"
          }
        },
        "bf25cdf5389b49a39f276642599e82b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19ec7be049df4b7b93507d8112bba69b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_35e5cbdcad04429f87d34797225b372d",
            "value": "‚Äá466k/?‚Äá[00:00&lt;00:00,‚Äá5.34MB/s]"
          }
        },
        "bf8cfd738d244596b3f00b9e1fd548e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfa8a5811cfc43d9acfb30a6ac1eac28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29e4d75a2b3d4032aa3dbc4f2c9975d4",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03335e7e3afc415cb0aa7d6f11db5dfb",
            "value": 350
          }
        },
        "c158bb21101d433a9a569018c5acd637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54b6e834f6ca4076b92d167396df2eb8",
              "IPY_MODEL_bfa8a5811cfc43d9acfb30a6ac1eac28",
              "IPY_MODEL_bb80de04d5c940bdaac1ffc40c109d25"
            ],
            "layout": "IPY_MODEL_8fb3a9985a9f413b97a708795806207e"
          }
        },
        "c4321fe496014372ab9fe341fc795c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c629f04e8e2444b68052e373de38a725": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc6ea8c18cda44d197c2c2475485252c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c0f7d99607f4710b9beb6987b32ead5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5625cd5c38ec41cd9b626a89f7715d04",
            "value": 1
          }
        },
        "cccb761dd3724110854fad7435c75d87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccef152d9aba444ea986551f4326cfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90f60ecb21164af592148bffc8f50750",
              "IPY_MODEL_5b06c4a31fee472fa5895fffc9077dd6",
              "IPY_MODEL_9d99f1eae99446ff914b802fe878d264"
            ],
            "layout": "IPY_MODEL_ee0b1717bc9642d6a935e3d259d6d947"
          }
        },
        "d1410f247e294852a0455bc8f6d65e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3f63ff1845644e9be2f87fcedc722b2",
              "IPY_MODEL_55018d28d5c64215bcbc820f9a68914d",
              "IPY_MODEL_8a2935b7b7b94791b8cfeaaead422724"
            ],
            "layout": "IPY_MODEL_310b273527d34127abf31a1a285e68c4"
          }
        },
        "d189e8ca580d4137bd87289646c9e26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3299261ba0c43ee82633ba43ff73a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f12f92037df4b4faa89c2502f9002a1",
              "IPY_MODEL_cc6ea8c18cda44d197c2c2475485252c",
              "IPY_MODEL_a68bcc3ce0cb43149ef067b917eb9fa6"
            ],
            "layout": "IPY_MODEL_550d10d3e0d04fa0bd2baba0a2a69b58"
          }
        },
        "d4c5eb2e034445eb891eb9a97fb7958e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d555664d92aa4427a91f8091a82cf097": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d98d8fa4bf504991ac5ec3d9aa2c39d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc211367670446008bf1f27636ffc9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc68c7da6f514ddfa4489bad6949406b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0c8b570190e471a8190170f49402f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54bc6f6575b64de7af146a097c3e47fb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_980fa204feaa4382bfef0b05cd6ddd29",
            "value": 1
          }
        },
        "e2e31ab141554b25b02b20a8d1a80110": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5f5924a3f0b4956bb896e82722856dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e627fcd229aa43a5986092860fc993f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e70c6b9dbd8142a7819452e1a3f57d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7146e50e280480d85aa541f0a39e93c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e91931f8d815462d814b72c2862b380a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_767716710f274910a87179e2040059a9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6f7490d4724c46fd8e06cbb3848f887c",
            "value": "‚Äá10.5k/?‚Äá[00:00&lt;00:00,‚Äá482kB/s]"
          }
        },
        "e956d5f41a474bb29ff1c68cb1d6eeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_981549a57226444c97eb4a50cdc3ff98",
              "IPY_MODEL_874e51415ca349469bb564e684f5d339",
              "IPY_MODEL_23291c78b4424f229278e05cac25c8e1"
            ],
            "layout": "IPY_MODEL_cccb761dd3724110854fad7435c75d87"
          }
        },
        "eb01ab1e60af487d83ba8aeddcf98bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a97f3324b844472da11fd5fc012559fd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d98d8fa4bf504991ac5ec3d9aa2c39d3",
            "value": "‚Äá112/112‚Äá[00:00&lt;00:00,‚Äá1.45kB/s]"
          }
        },
        "ee0b1717bc9642d6a935e3d259d6d947": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef0545f26b214dd3a56295d423d2c4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08921db6e934d4eb31dfad55c5f51ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0b25f15ecbd46a5a9b9eb57346d488c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f42949df0bdd42dd8b8dcc5c70043aac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7e7c1402dac4b8fa2dfc925d56b67dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fae1806f351f42d997bc9a614e1f68a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd5304d2a2dd453cb90cf830f25dbbe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf187771aa845a28e82ee311ac07a9d",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_654eb87658c04a3387ec3972ecd2974f",
            "value": 112
          }
        },
        "fe81263a3a19462494df2d76da2d1af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1333f4a234b44c999128226cfb61b86",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a80e3b57b4484cb19be43ebcf3cf98d1",
            "value": "‚Äá349/349‚Äá[00:00&lt;00:00,‚Äá10.7kB/s]"
          }
        },
        "ff41fe863a394246ad3f9b15febef8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8a916060e1d48acb9d759dea8ea38d9",
              "IPY_MODEL_e0c8b570190e471a8190170f49402f51",
              "IPY_MODEL_e91931f8d815462d814b72c2862b380a"
            ],
            "layout": "IPY_MODEL_f7e7c1402dac4b8fa2dfc925d56b67dc"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
