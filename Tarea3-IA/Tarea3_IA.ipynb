{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9504BR1-w8t",
        "outputId": "6e2fd4f4-9136-4adf-f1b7-c35bc669b30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Drive montado.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 1) Montar Google Drive\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive montado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwb41D3L_FuD",
        "outputId": "669fd430-7274-4859-c5a3-f1b46bb27d9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Proyecto: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA\n",
            "üìÅ Metadata: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "üìÅ PDFs: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/RepositorioApuntesPdf\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 2) Definir las rutas base en Drive\n",
        "#    Ajustadas a tu estructura:\n",
        "#    Mi unidad / Colab Notebooks / Tarea3-IA / ...\n",
        "# ============================================\n",
        "import os\n",
        "\n",
        "# Ruta base a \"Mi unidad\"\n",
        "BASE_DRIVE = \"/content/drive/MyDrive\"\n",
        "\n",
        "# Carpeta ra√≠z del proyecto de la tarea\n",
        "PROYECTO_DIR = os.path.join(BASE_DRIVE, \"Colab Notebooks\", \"Tarea3-IA\")\n",
        "\n",
        "# Carpetas espec√≠ficas\n",
        "METADATA_FILE = os.path.join(PROYECTO_DIR, \"MetadataRAW.csv\")\n",
        "PDFS_DIR = os.path.join(PROYECTO_DIR, \"RepositorioApuntesPdf\")\n",
        "\n",
        "print(\"üìÅ Proyecto:\", PROYECTO_DIR)\n",
        "print(\"üìÅ Metadata:\", METADATA_FILE)\n",
        "print(\"üìÅ PDFs:\", PDFS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH70dK4RDelg",
        "outputId": "3e40ec49-92a4-43f2-ab1e-dee03a576e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Carpeta del proyecto encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA\n",
            "‚úÖ Archivo de metadata encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "‚úÖ Carpeta de PDFs encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/RepositorioApuntesPdf\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 3) Verificar que las carpetas existen\n",
        "#    (esto ayuda a detectar typos en el nombre)\n",
        "# ============================================\n",
        "\n",
        "def check_dir(path, name):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"‚úÖ {name} encontrada: {path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {name} NO encontrada. Revisa el nombre en Drive: {path}\")\n",
        "\n",
        "check_dir(PROYECTO_DIR, \"Carpeta del proyecto\")\n",
        "check_dir(METADATA_FILE, \"Archivo de metadata\")\n",
        "check_dir(PDFS_DIR, \"Carpeta de PDFs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBcFaeTIkRSM",
        "outputId": "6349a204-7329-4e32-ff5a-01dc69a4a7d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö PDFs encontrados: 46\n",
            "  - 5_Semana_AI_20250904_2.pdf\n",
            "  - 6_Semana_AI_20250911_2.pdf\n",
            "  - 11_Semana_AI_20251016_4.pdf\n",
            "  - 11_Semana_AI_20251014_3.pdf\n",
            "  - 10_SEMANA_AI_20251007_1-222887296.pdf\n",
            "  - 7_Semana_AI_20250916_2.pdf\n",
            "  - 12_SEMANA_AI_20251021_3.pdf\n",
            "  - 2_Semana_AI_20250812_3.pdf\n",
            "  - 11_Semana_AI_20251014_2.pdf\n",
            "  - 8_Semana_AI_20250925_2.pdf\n",
            "  - 11_Semana_AI_20251014_1.pdf\n",
            "  - 5_Semana_AI_20250904_1.pdf\n",
            "  - 10_SEMANA_AI_20251009_1.pdf\n",
            "  - 12_SEMANA_AI_20251021_4.pdf\n",
            "  - 8_Semana_AI_20250923_1.pdf\n",
            "  - 1_Semana_AI_20250807_2.pdf\n",
            "  - 2_Semana_AI_20250814_1.pdf\n",
            "  - 2_Semana_AI_20250814_2.pdf\n",
            "  - 5_Semana_AI_20250902_2.pdf\n",
            "  - 6_Semana_AI_20250911_1.pdf\n",
            "  - 5_Semana_AI_20250902_1.pdf\n",
            "  - 11_SEMANA_AI_20251016_2.pdf\n",
            "  - 4_SEMANA_AI_20250826_1.pdf\n",
            "  - 6_Semana_AI_20250909_2-220676337.pdf\n",
            "  - 8_Semana_AI_20250925_1.pdf\n",
            "  - 7_Semana_AI_20250918_1.pdf\n",
            "  - 12_SEMANA_AI_20251021_1.pdf\n",
            "  - 3_Semana_AI_20250819_2.pdf\n",
            "  - 4_Semana_AI_20250828_1.pdf\n",
            "  - 7_Semana_AI_20250918_2.pdf\n",
            "  - 4_SEMANA_AI_20250826_2.pdf\n",
            "  - 3_Semana_AI_20250821_1.pdf\n",
            "  - 2_SEMANA_AI_20250812_1.pdf\n",
            "  - 12_Semana_AI_20251021_2.pdf\n",
            "  - 12_SEMANA_AL_20251023_2.pdf\n",
            "  - 7_Semana_AI_20250916_1.pdf\n",
            "  - 1_SEMANA_AI_20250807_1.pdf\n",
            "  - 12_SEMANA_AI_20251023_1.pdf\n",
            "  - 10_SEMANA_AI_20251007_1.pdf\n",
            "  - 12_Semana_AI_20251023_3.pdf\n",
            "  - 4_Semana_AI_20250828_2.pdf\n",
            "  - 8_Semana_AI_20250923_2.pdf\n",
            "  - 3_Semana_AI_20250819_1.pdf\n",
            "  - 9_Semana_AI_20251002_2.pdf\n",
            "  - 6_Semana_AI_20250909_1.pdf\n",
            "  - 9_SEMANA_AI_20251002_1.pdf\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# 4) Listar los PDFs disponibles\n",
        "#    (esto confirma que Colab est√° viendo tu carpeta)\n",
        "# ============================================\n",
        "\n",
        "pdf_files = [f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")]\n",
        "print(f\"üìö PDFs encontrados: {len(pdf_files)}\")\n",
        "for f in pdf_files:\n",
        "    print(\"  -\", f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "z1k1mRxE0TsY",
        "outputId": "b569247e-c316-4230-baf6-1b75507cbc69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Metadata CSV cargada correctamente (46 filas) desde:\n",
            "/content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "\n",
            "üìë Columnas detectadas: ['id_doc', 'nombre_archivo', 'autor', 'fecha', 'tema']\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"    print(\\\"\\u26a0\\ufe0f No se encontr\\u00f3 el archivo 'metadata\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id_doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"DOC_009\",\n          \"DOC_002\",\n          \"DOC_006\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nombre_archivo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"3_Semana_AI_20250821_1.pdf\",\n          \"1_Semana_AI_20250807_2.pdf\",\n          \"2_Semana_AI_20250814_2.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"autor\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Julio Varela Venegas\",\n          \"Fernando Daniel Brenes Reyes\",\n          \"Jose Pablo Quesada Rodr\\u00edguez\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fecha\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-08-07 00:00:00\",\n        \"max\": \"2025-08-26 00:00:00\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"2025-08-07 00:00:00\",\n          \"2025-08-12 00:00:00\",\n          \"2025-08-26 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tema\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Aplicaci\\u00f3n del \\u00e1lgebra lineal y la programaci\\u00f3n vectorial en IA, con enfoque en aprendizaje supervisado, representaci\\u00f3n de vectores y uso de NumPy y Jupyter Notebook.\",\n          \"Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos aut\\u00f3nomos, con \\u00e9nfasis en el aprendizaje supervisado y no supervisado basado en datos.\",\n          \"Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de \\u00e1lgebra lineal y tensores en PyTorch.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3e517003-894a-4bb3-a469-1d54d531f0af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_doc</th>\n",
              "      <th>nombre_archivo</th>\n",
              "      <th>autor</th>\n",
              "      <th>fecha</th>\n",
              "      <th>tema</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DOC_001</td>\n",
              "      <td>1_SEMANA_AI_20250807_1.pdf</td>\n",
              "      <td>Rodolfo David Acu√±a L√≥pez</td>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>Principios fundamentales de la inteligencia ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DOC_002</td>\n",
              "      <td>1_Semana_AI_20250807_2.pdf</td>\n",
              "      <td>Fernando Daniel Brenes Reyes</td>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>Aplicaciones de la inteligencia artificial y m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DOC_003</td>\n",
              "      <td>2_SEMANA_AI_20250812_1.pdf</td>\n",
              "      <td>Priscilla Jim√©nez Salgado</td>\n",
              "      <td>2025-08-12</td>\n",
              "      <td>Introducci√≥n a machine learning y deep learnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DOC_004</td>\n",
              "      <td>2_Semana_AI_20250812_3.pdf</td>\n",
              "      <td>Luis Alfredo Gonz√°lez S√°nchez</td>\n",
              "      <td>2025-08-12</td>\n",
              "      <td>Resumen de conceptos clave de IA y enfoques de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DOC_005</td>\n",
              "      <td>2_Semana_AI_20250814_1.pdf</td>\n",
              "      <td>Kendall Rodr√≠guez Camacho</td>\n",
              "      <td>2025-08-14</td>\n",
              "      <td>Introducci√≥n a √°lgebra lineal aplicada con Pyt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DOC_006</td>\n",
              "      <td>2_Semana_AI_20250814_2.pdf</td>\n",
              "      <td>Jose Pablo Quesada Rodr√≠guez</td>\n",
              "      <td>2025-08-14</td>\n",
              "      <td>Resumen detallado sobre tipos de aprendizaje, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DOC_007</td>\n",
              "      <td>3_Semana_AI_20250819_1.pdf</td>\n",
              "      <td>Javier Rojas Rojas</td>\n",
              "      <td>2025-08-19</td>\n",
              "      <td>Revisi√≥n de √°lgebra lineal y aprendizaje super...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DOC_008</td>\n",
              "      <td>3_Semana_AI_20250819_2.pdf</td>\n",
              "      <td>Mariana Quesada S√°nchez</td>\n",
              "      <td>2025-08-19</td>\n",
              "      <td>Repaso de √°lgebra lineal y fundamentos del apr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>DOC_009</td>\n",
              "      <td>3_Semana_AI_20250821_1.pdf</td>\n",
              "      <td>Julio Varela Venegas</td>\n",
              "      <td>2025-08-21</td>\n",
              "      <td>Aplicaci√≥n del √°lgebra lineal y la programaci√≥...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DOC_010</td>\n",
              "      <td>4_SEMANA_AI_20250826_1.pdf</td>\n",
              "      <td>Andr√©s S√°nchez Rojas</td>\n",
              "      <td>2025-08-26</td>\n",
              "      <td>Implementaci√≥n del algoritmo KNN y fundamentos...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e517003-894a-4bb3-a469-1d54d531f0af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3e517003-894a-4bb3-a469-1d54d531f0af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3e517003-894a-4bb3-a469-1d54d531f0af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1637f250-0107-4434-b523-7ce013be0b5c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1637f250-0107-4434-b523-7ce013be0b5c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1637f250-0107-4434-b523-7ce013be0b5c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    id_doc              nombre_archivo                          autor  \\\n",
              "0  DOC_001  1_SEMANA_AI_20250807_1.pdf      Rodolfo David Acu√±a L√≥pez   \n",
              "1  DOC_002  1_Semana_AI_20250807_2.pdf   Fernando Daniel Brenes Reyes   \n",
              "2  DOC_003  2_SEMANA_AI_20250812_1.pdf      Priscilla Jim√©nez Salgado   \n",
              "3  DOC_004  2_Semana_AI_20250812_3.pdf  Luis Alfredo Gonz√°lez S√°nchez   \n",
              "4  DOC_005  2_Semana_AI_20250814_1.pdf      Kendall Rodr√≠guez Camacho   \n",
              "5  DOC_006  2_Semana_AI_20250814_2.pdf   Jose Pablo Quesada Rodr√≠guez   \n",
              "6  DOC_007  3_Semana_AI_20250819_1.pdf             Javier Rojas Rojas   \n",
              "7  DOC_008  3_Semana_AI_20250819_2.pdf        Mariana Quesada S√°nchez   \n",
              "8  DOC_009  3_Semana_AI_20250821_1.pdf           Julio Varela Venegas   \n",
              "9  DOC_010  4_SEMANA_AI_20250826_1.pdf           Andr√©s S√°nchez Rojas   \n",
              "\n",
              "       fecha                                               tema  \n",
              "0 2025-08-07  Principios fundamentales de la inteligencia ar...  \n",
              "1 2025-08-07  Aplicaciones de la inteligencia artificial y m...  \n",
              "2 2025-08-12  Introducci√≥n a machine learning y deep learnin...  \n",
              "3 2025-08-12  Resumen de conceptos clave de IA y enfoques de...  \n",
              "4 2025-08-14  Introducci√≥n a √°lgebra lineal aplicada con Pyt...  \n",
              "5 2025-08-14  Resumen detallado sobre tipos de aprendizaje, ...  \n",
              "6 2025-08-19  Revisi√≥n de √°lgebra lineal y aprendizaje super...  \n",
              "7 2025-08-19  Repaso de √°lgebra lineal y fundamentos del apr...  \n",
              "8 2025-08-21  Aplicaci√≥n del √°lgebra lineal y la programaci√≥...  \n",
              "9 2025-08-26  Implementaci√≥n del algoritmo KNN y fundamentos...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ============================================\n",
        "# 5) Cargar archivo de metadata\n",
        "#    Columnas esperadas: id_doc, nombre_archivo, autor, fecha, tema\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "if os.path.exists(METADATA_FILE):\n",
        "    # Leer tal cual, como texto (sin parsear fechas)\n",
        "    df_meta = pd.read_csv(METADATA_FILE, dtype=str, keep_default_na=False)\n",
        "    print(f\"‚úÖ Metadata CSV cargada correctamente ({len(df_meta)} filas) desde:\\n{METADATA_FILE}\\n\")\n",
        "\n",
        "    # Chequeo suave de columnas (sin modificar nada)\n",
        "    expected = [\"id_doc\", \"nombre_archivo\", \"autor\", \"fecha\", \"tema\"]\n",
        "    missing = [c for c in expected if c not in df_meta.columns]\n",
        "    if missing:\n",
        "        print(\"‚ö†Ô∏è Faltan columnas esperadas:\", missing)\n",
        "    else:\n",
        "        print(\"üìë Columnas detectadas:\", list(df_meta.columns))\n",
        "\n",
        "        df_meta[\"fecha\"] = pd.to_datetime(df_meta[\"fecha\"], errors=\"coerce\")\n",
        "\n",
        "\n",
        "    # Preview\n",
        "    display(df_meta.head(10))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ el archivo 'metadata.csv' en la carpeta Metadata:\", METADATA_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLmQP8kv_Zkx",
        "outputId": "7fbb8976-63d4-4bff-9e07-91b1b45a0a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Extracci√≥n, normalizaci√≥n y segmentaci√≥n listas\n",
            "üßæ Base (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/base_documentos.jsonl\n",
            "üß± Base (Parquet): /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/base_documentos.parquet\n",
            "üîπ Seg A (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_a.jsonl\n",
            "üî∏ Seg B (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_b.jsonl\n",
            "üóíÔ∏è  Notas:          /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/preprocesamiento_decisiones.md\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPA√ëERO 1 ‚Äì DATOS Y PREPROCESAMIENTO (PASO 2 COMPLETO)\n",
        "# Extraer texto, normalizar y segmentar (A: chunks fijos, B: encabezados)\n",
        "# Requiere:\n",
        "#  - Variables definidas antes: PROYECTO_DIR, METADATA_FILE, PDFS_DIR\n",
        "#  - METADATA_FILE con columnas: id_doc, nombre_archivo, autor, fecha, tema\n",
        "# Salidas (para Compa√±ero 2):\n",
        "#  - dataset/base_documentos.jsonl / .parquet\n",
        "#  - dataset/seg_a.jsonl / dataset/seg_b.jsonl\n",
        "#  - dataset/txt_por_doc/DOC_###.txt (opcional)\n",
        "#  - dataset/preprocesamiento_decisiones.md\n",
        "# ============================================================\n",
        "\n",
        "!pip install --quiet pdfplumber pandas pyarrow\n",
        "\n",
        "import os, re, json, unicodedata\n",
        "from pathlib import Path\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- CONFIGURACI√ìN ----------\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "TXT_DIR = os.path.join(OUT_DIR, \"txt_por_doc\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(TXT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "BASE_DOCS_JSONL   = os.path.join(OUT_DIR, \"base_documentos.jsonl\")\n",
        "BASE_DOCS_PARQUET = os.path.join(OUT_DIR, \"base_documentos.parquet\")\n",
        "SEG_A_JSONL       = os.path.join(OUT_DIR, \"seg_a.jsonl\")\n",
        "SEG_B_JSONL       = os.path.join(OUT_DIR, \"seg_b.jsonl\")\n",
        "NOTAS_PREPROC     = os.path.join(OUT_DIR, \"preprocesamiento_decisiones.md\")\n",
        "\n",
        "# (Ajusta si quer√©s otros tama√±os)\n",
        "CHUNK_WORDS   = 400    # tama√±o del chunk (en palabras aproximado)\n",
        "CHUNK_OVERLAP = 80     # solapamiento entre chunks\n",
        "\n",
        "# Si existen salidas previas, limpirlas (evita duplicados al re-ejecutar)\n",
        "for p in [BASE_DOCS_JSONL, BASE_DOCS_PARQUET, SEG_A_JSONL, SEG_B_JSONL, NOTAS_PREPROC]:\n",
        "    try:\n",
        "        if os.path.exists(p): os.remove(p)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------- NORMALIZACI√ìN ----------\n",
        "def normalize_unicode_nfc(text: str) -> str:\n",
        "    return unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "def strip_control_chars(text: str) -> str:\n",
        "    # Deja \\n y \\t; elimina otros de control\n",
        "    return \"\".join(ch for ch in text if ch in (\"\\n\",\"\\t\") or ord(ch) >= 32)\n",
        "\n",
        "def standardize_quotes_dashes(text: str) -> str:\n",
        "    repl = {\n",
        "        \"‚Äú\": \"\\\"\", \"‚Äù\": \"\\\"\", \"‚Äû\": \"\\\"\", \"¬´\": \"\\\"\", \"¬ª\": \"\\\"\",\n",
        "        \"‚Äô\": \"'\", \"¬¥\": \"'\", \"‚Äò\": \"'\",\n",
        "        \"‚Äê\": \"-\", \"‚Äì\": \"-\", \"‚Äî\": \"-\", \"‚àí\": \"-\",\n",
        "        \"‚Ä¶\": \"...\", \"‚Ä¢\": \"-\", \"¬∑\": \"-\"\n",
        "    }\n",
        "    for a, b in repl.items():\n",
        "        text = text.replace(a, b)\n",
        "    return text\n",
        "\n",
        "def remove_hyphen_linebreaks(text: str) -> str:\n",
        "    # Une palabras cortadas por guion al final de l√≠nea: \"infor-\\nmaci√≥n\" -> \"informaci√≥n\"\n",
        "    return re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
        "\n",
        "def collapse_whitespace(text: str) -> str:\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def to_lower(text: str) -> str:\n",
        "    return text.lower()\n",
        "\n",
        "def normalize_text_pipeline(raw: str) -> str:\n",
        "    if not raw: return \"\"\n",
        "    t = normalize_unicode_nfc(raw)\n",
        "    t = strip_control_chars(t)\n",
        "    t = standardize_quotes_dashes(t)\n",
        "    t = remove_hyphen_linebreaks(t)\n",
        "    t = collapse_whitespace(t)\n",
        "    t = to_lower(t)\n",
        "    return t\n",
        "\n",
        "# ---------- SEGMENTACI√ìN ----------\n",
        "def segment_fixed_overlap(text: str, words_per_chunk=CHUNK_WORDS, overlap=CHUNK_OVERLAP):\n",
        "    \"\"\"Segmentaci√≥n A: chunks por palabras con solapamiento.\"\"\"\n",
        "    words = text.split()\n",
        "    if not words: return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    idx = 0\n",
        "    while i < len(words):\n",
        "        chunk_words = words[i:i+words_per_chunk]\n",
        "        chunk_text = \" \".join(chunk_words).strip()\n",
        "        if chunk_text:\n",
        "            chunks.append((idx, chunk_text))\n",
        "            idx += 1\n",
        "        i += max(1, words_per_chunk - overlap)\n",
        "    return chunks\n",
        "\n",
        "_HEADING_RE = re.compile(\n",
        "    r\"\"\"^(\n",
        "        abstract\\b|\n",
        "        resumen\\b|\n",
        "        introducci[o√≥]n\\b|\n",
        "        conclusi[o√≥]n\\b|\n",
        "        referencias\\b|\n",
        "        agradecimientos\\b|\n",
        "        related\\ work\\b|\n",
        "        i{1,3}\\.|iv\\.|v\\.|vi\\.|vii\\.|viii\\.|ix\\.|x\\.|      # romanos\n",
        "        \\d+\\.\\s|                                          # 1. 2. 3.\n",
        "        [A-Z][A-Z0-9\\s\\-\\&]{3,}$                          # l√≠nea MAY√öSCULAS (t√≠tulo)\n",
        "    )\"\"\",\n",
        "    re.IGNORECASE | re.VERBOSE\n",
        ")\n",
        "\n",
        "def segment_by_headings(text: str, min_section_words=120):\n",
        "    \"\"\"Segmentaci√≥n B: por encabezados; fusiona secciones peque√±as.\"\"\"\n",
        "    lines = [l.strip() for l in text.splitlines()]\n",
        "    # Marcar √≠ndices de l√≠neas que parecen encabezado\n",
        "    header_idx = [i for i, line in enumerate(lines) if line and _HEADING_RE.match(line)]\n",
        "    # Siempre incluir inicio y fin\n",
        "    if 0 not in header_idx: header_idx = [0] + header_idx\n",
        "    if len(lines) - 1 not in header_idx: header_idx = header_idx + [len(lines) - 1]\n",
        "    header_idx = sorted(set(header_idx))\n",
        "\n",
        "    # Cortar por rangos\n",
        "    raw_sections = []\n",
        "    for a, b in zip(header_idx, header_idx[1:]):\n",
        "        sec = \"\\n\".join(lines[a:b]).strip()\n",
        "        if sec: raw_sections.append(sec)\n",
        "    # A√±adir √∫ltimo trozo\n",
        "    tail = \"\\n\".join(lines[header_idx[-1]:]).strip()\n",
        "    if tail: raw_sections.append(tail)\n",
        "\n",
        "    # Fusionar secciones demasiado peque√±as\n",
        "    merged = []\n",
        "    buff = []\n",
        "    wcount = 0\n",
        "    for sec in raw_sections:\n",
        "        wc = len(sec.split())\n",
        "        if wcount + wc < min_section_words:\n",
        "            buff.append(sec); wcount += wc\n",
        "        else:\n",
        "            if buff:\n",
        "                buff.append(sec)\n",
        "                merged.append(\"\\n\\n\".join(buff).strip())\n",
        "                buff, wcount = [], 0\n",
        "            else:\n",
        "                merged.append(sec)\n",
        "                buff, wcount = [], 0\n",
        "    if buff:\n",
        "        merged.append(\"\\n\\n\".join(buff).strip())\n",
        "\n",
        "    # Indexar\n",
        "    return [(i, s) for i, s in enumerate(merged)]\n",
        "\n",
        "# ---------- CARGA METADATA ----------\n",
        "assert os.path.exists(METADATA_FILE), f\"No existe METADATA_FILE: {METADATA_FILE}\"\n",
        "df_meta = pd.read_csv(METADATA_FILE, dtype=str, keep_default_na=False)\n",
        "for col in [\"id_doc\",\"nombre_archivo\",\"autor\",\"fecha\",\"tema\"]:\n",
        "    if col not in df_meta.columns:\n",
        "        raise ValueError(f\"Falta columna en metadata: {col}\")\n",
        "\n",
        "# √≠ndice r√°pido por nombre de archivo (case-insensitive)\n",
        "meta_idx = {str(n).strip().lower(): i for i, n in enumerate(df_meta[\"nombre_archivo\"])}\n",
        "\n",
        "# ---------- RECORRIDO PDFs ----------\n",
        "base_registros = []\n",
        "seg_a_registros = []\n",
        "seg_b_registros = []\n",
        "errores = []\n",
        "\n",
        "pdf_files_sorted = sorted([f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")])\n",
        "\n",
        "for fname in pdf_files_sorted:\n",
        "    key = fname.strip().lower()\n",
        "    if key not in meta_idx:\n",
        "        errores.append((fname, \"no_encontrado_en_metadata\"))\n",
        "        print(f\"‚ö†Ô∏è  {fname}: no aparece en 'nombre_archivo' de la metadata; se omite.\")\n",
        "        continue\n",
        "\n",
        "    row   = df_meta.iloc[meta_idx[key]]\n",
        "    iddoc = row[\"id_doc\"]; autor=row[\"autor\"]; fecha=row[\"fecha\"]; tema=row[\"tema\"]\n",
        "    pdf_path = os.path.join(PDFS_DIR, fname)\n",
        "\n",
        "    # Extraer texto del PDF\n",
        "    try:\n",
        "        pages = []\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for p in pdf.pages:\n",
        "                pages.append(p.extract_text() or \"\")\n",
        "        full_text = \"\\n\".join(pages)\n",
        "    except Exception as e:\n",
        "        errores.append((fname, f\"error_lectura_pdf:{e}\"))\n",
        "        print(f\"‚ùå Error leyendo {fname}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Normalizar\n",
        "    texto_limpio = normalize_text_pipeline(full_text)\n",
        "\n",
        "    # Guardar TXT por doc (√∫til para inspecci√≥n)\n",
        "    with open(os.path.join(TXT_DIR, f\"{iddoc}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(texto_limpio)\n",
        "\n",
        "    # Registro base (documento completo)\n",
        "    base_registros.append({\n",
        "        \"id_doc\": iddoc,\n",
        "        \"nombre_archivo\": fname,\n",
        "        \"autor\": autor,\n",
        "        \"fecha\": str(fecha),\n",
        "        \"tema\": tema,\n",
        "        \"texto_original\": full_text,\n",
        "        \"texto_limpio\": texto_limpio\n",
        "    })\n",
        "\n",
        "    # ---------- SEGMENTACI√ìN A: chunks fijos ----------\n",
        "    chunks_a = segment_fixed_overlap(texto_limpio, CHUNK_WORDS, CHUNK_OVERLAP)\n",
        "    for idx_chunk, chunk_text in chunks_a:\n",
        "        seg_a_registros.append({\n",
        "            \"id_doc\": iddoc,\n",
        "            \"segmentacion\": \"A\",\n",
        "            \"chunk_id\": f\"{iddoc}_A_{idx_chunk:03d}\",\n",
        "            \"idx\": idx_chunk,\n",
        "            \"autor\": autor,\n",
        "            \"fecha\": str(fecha),\n",
        "            \"tema\": tema,\n",
        "            \"texto\": chunk_text\n",
        "        })\n",
        "\n",
        "    # ---------- SEGMENTACI√ìN B: encabezados ----------\n",
        "    sections_b = segment_by_headings(texto_limpio, min_section_words=120)\n",
        "    for idx_sec, sec_text in sections_b:\n",
        "        seg_b_registros.append({\n",
        "            \"id_doc\": iddoc,\n",
        "            \"segmentacion\": \"B\",\n",
        "            \"chunk_id\": f\"{iddoc}_B_{idx_sec:03d}\",\n",
        "            \"idx\": idx_sec,\n",
        "            \"autor\": autor,\n",
        "            \"fecha\": str(fecha),\n",
        "            \"tema\": tema,\n",
        "            \"texto\": sec_text\n",
        "        })\n",
        "\n",
        "# ---------- GUARDAR SALIDAS ----------\n",
        "# Base documentos\n",
        "with open(BASE_DOCS_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in base_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "pd.DataFrame(base_registros).to_parquet(BASE_DOCS_PARQUET, index=False)\n",
        "\n",
        "# Segmentaciones\n",
        "with open(SEG_A_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in seg_a_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "with open(SEG_B_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in seg_b_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Mini-documentaci√≥n de decisiones para el informe\n",
        "notas = f\"\"\"# Preprocesamiento y Segmentaci√≥n (borrador)\n",
        "\n",
        "**Normalizaci√≥n aplicada**\n",
        "- Unicode NFC (mantener tildes correctas).\n",
        "- Limpieza de caracteres de control (excepto \\\\n y \\\\t).\n",
        "- Estandarizaci√≥n de comillas/guiones (‚Äú ‚Äù ‚Äò ‚Äô ‚Äî ‚Äì ‚Ä¶ ‚Üí \" ' - ...).\n",
        "- Uni√≥n de palabras cortadas por guion al fin de l√≠nea (e.g., \"infor-\\\\nmaci√≥n\" ‚Üí \"informaci√≥n\").\n",
        "- Colapso de espacios y saltos en blanco excesivos.\n",
        "- Conversi√≥n a min√∫sculas (para comparar segmentaciones bajo mismas condiciones).\n",
        "\n",
        "**Segmentaci√≥n A ‚Äì Chunks fijos**\n",
        "- Tama√±o ‚âà {CHUNK_WORDS} palabras, solapamiento ‚âà {CHUNK_OVERLAP}.\n",
        "- Ventajas: control de longitud, √∫til para evaluaci√≥n reproducible.\n",
        "- Desventajas: puede cortar ideas a mitad.\n",
        "\n",
        "**Segmentaci√≥n B ‚Äì Encabezados/Secciones**\n",
        "- Reglas: detec. de Abstract/Resumen/Introducci√≥n/Conclusi√≥n/Referencias, numerales (1., 2., ...), romanos (I., II., ...), t√≠tulos en MAY√öSCULAS.\n",
        "- Se fusionan secciones muy cortas (<120 palabras) con la siguiente para asegurar contexto m√≠nimo.\n",
        "- Ventajas: mantiene unidades sem√°nticas; Desventajas: depende de patrones editoriales.\n",
        "\n",
        "**Salidas**\n",
        "- base_documentos.jsonl / .parquet: texto completo normalizado por documento + metadata (autor/fecha/tema).\n",
        "- seg_a.jsonl / seg_b.jsonl: fragmentos con `id_doc`, `chunk_id`, `segmentacion`, `idx`, `texto`, y metadata.\n",
        "- txt_por_doc/: √∫til para inspecci√≥n r√°pida o depurar PDF problem√°ticos.\n",
        "\n",
        "**Razonamiento para comparaci√≥n**\n",
        "- A: garantiza tama√±o estable ‚Üí resultados de recuperaci√≥n comparables.\n",
        "- B: favorece coherencia sem√°ntica ‚Üí potencialmente mejor grounding.\n",
        "- Compa√±ero 2 debe crear dos √≠ndices (A y B) y comparar m√©tricas (recall@k, precisi√≥n manual, tiempo respuesta).\n",
        "\"\"\"\n",
        "with open(NOTAS_PREPROC, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(notas)\n",
        "\n",
        "print(\"‚úÖ Extracci√≥n, normalizaci√≥n y segmentaci√≥n listas\")\n",
        "print(f\"üßæ Base (JSONL):  {BASE_DOCS_JSONL}\")\n",
        "print(f\"üß± Base (Parquet): {BASE_DOCS_PARQUET}\")\n",
        "print(f\"üîπ Seg A (JSONL):  {SEG_A_JSONL}\")\n",
        "print(f\"üî∏ Seg B (JSONL):  {SEG_B_JSONL}\")\n",
        "print(f\"üóíÔ∏è  Notas:          {NOTAS_PREPROC}\")\n",
        "if errores:\n",
        "    print(f\"‚ö†Ô∏è Incidencias ({len(errores)}):\")\n",
        "    for e in errores[:12]:\n",
        "        print(\"   -\", e)\n",
        "    if len(errores) > 12:\n",
        "        print(\"   ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE9pHFW5QZ0P",
        "outputId": "1cbea5b6-b713-44b7-e0bc-bae9c902214c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Verificando archivos generados...\n",
            "\n",
            "‚úÖ Metadata: encontrado (MetadataRAW.csv)\n",
            "‚úÖ Base documentos JSONL: encontrado (base_documentos.jsonl)\n",
            "‚úÖ Base documentos Parquet: encontrado (base_documentos.parquet)\n",
            "‚úÖ Segmentaci√≥n A (chunks fijos): encontrado (seg_a.jsonl)\n",
            "‚úÖ Segmentaci√≥n B (encabezados): encontrado (seg_b.jsonl)\n",
            "‚úÖ Notas de preprocesamiento: encontrado (preprocesamiento_decisiones.md)\n",
            "\n",
            "üìä PDFs reales: 46\n",
            "üìÑ Documentos procesados: 46\n",
            "\n",
            "‚úÖ Todos los documentos fueron procesados correctamente.\n",
            "\n",
            "üß≠ RESUMEN PARA COMPA√ëERO 2:\n",
            "\n",
            "Los datos est√°n listos para generar embeddings:\n",
            "\n",
            "- dataset/seg_a.jsonl ‚Üí fragmentos con segmentaci√≥n A (chunks fijos)\n",
            "- dataset/seg_b.jsonl ‚Üí fragmentos con segmentaci√≥n B (por encabezados)\n",
            "- MetadataRAW.csv (o metadata.csv) ‚Üí autor, fecha, tema por documento\n",
            "\n",
            "Archivos opcionales:\n",
            "- base_documentos.jsonl/.parquet ‚Üí textos completos normalizados\n",
            "- preprocesamiento_decisiones.md ‚Üí descripci√≥n de limpieza y segmentaci√≥n\n",
            "\n",
            "El Compa√±ero 2 debe usar seg_a.jsonl y seg_b.jsonl\n",
            "para crear las dos bases vectoriales y comparar su rendimiento.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# üîö CIERRE DEL COMPA√ëERO 1 ‚Äì VERIFICACI√ìN FINAL Y RESUMEN\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Rutas de salida esperadas\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "paths = {\n",
        "    \"Metadata\": METADATA_FILE,\n",
        "    \"Base documentos JSONL\": os.path.join(OUT_DIR, \"base_documentos.jsonl\"),\n",
        "    \"Base documentos Parquet\": os.path.join(OUT_DIR, \"base_documentos.parquet\"),\n",
        "    \"Segmentaci√≥n A (chunks fijos)\": os.path.join(OUT_DIR, \"seg_a.jsonl\"),\n",
        "    \"Segmentaci√≥n B (encabezados)\": os.path.join(OUT_DIR, \"seg_b.jsonl\"),\n",
        "    \"Notas de preprocesamiento\": os.path.join(OUT_DIR, \"preprocesamiento_decisiones.md\"),\n",
        "}\n",
        "\n",
        "print(\"üìÇ Verificando archivos generados...\\n\")\n",
        "for nombre, ruta in paths.items():\n",
        "    if os.path.exists(ruta):\n",
        "        print(f\"‚úÖ {nombre}: encontrado ({os.path.basename(ruta)})\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {nombre}: NO encontrado -> {ruta}\")\n",
        "\n",
        "# Conteo r√°pido de PDFs y documentos base\n",
        "pdfs = [f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")]\n",
        "pdf_count = len(pdfs)\n",
        "base_path = paths[\"Base documentos JSONL\"]\n",
        "base_count = sum(1 for _ in open(base_path, encoding=\"utf-8\")) if os.path.exists(base_path) else 0\n",
        "\n",
        "print(f\"\\nüìä PDFs reales: {pdf_count}\")\n",
        "print(f\"üìÑ Documentos procesados: {base_count}\")\n",
        "\n",
        "if pdf_count == base_count:\n",
        "    print(\"\\n‚úÖ Todos los documentos fueron procesados correctamente.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Hay diferencias entre PDFs y registros procesados. Revisa nombres o metadatos.\")\n",
        "\n",
        "print(\"\\nüß≠ RESUMEN PARA COMPA√ëERO 2:\")\n",
        "print(\"\"\"\n",
        "Los datos est√°n listos para generar embeddings:\n",
        "\n",
        "- dataset/seg_a.jsonl ‚Üí fragmentos con segmentaci√≥n A (chunks fijos)\n",
        "- dataset/seg_b.jsonl ‚Üí fragmentos con segmentaci√≥n B (por encabezados)\n",
        "- MetadataRAW.csv (o metadata.csv) ‚Üí autor, fecha, tema por documento\n",
        "\n",
        "Archivos opcionales:\n",
        "- base_documentos.jsonl/.parquet ‚Üí textos completos normalizados\n",
        "- preprocesamiento_decisiones.md ‚Üí descripci√≥n de limpieza y segmentaci√≥n\n",
        "\n",
        "El Compa√±ero 2 debe usar seg_a.jsonl y seg_b.jsonl\n",
        "para crear las dos bases vectoriales y comparar su rendimiento.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYTYbYkNSjTm",
        "outputId": "02e6af24-2131-4e52-af2e-0c4c6753d752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.3/3.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/3.3 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Dependencias instaladas\n",
            "üì¶ sentence-transformers incluye PyTorch para acelerar embeddings en GPU (si est√° disponible en Colab)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPA√ëERO 2 ‚Äì EMBEDDINGS, VECTOR DB Y TOOLS DE RAG/WEB\n",
        "# ============================================================\n",
        "# Paso 1: Instalaci√≥n de dependencias necesarias\n",
        "# ============================================================\n",
        "\n",
        "!pip install --quiet langchain langchain-community faiss-cpu tiktoken sentence-transformers duckduckgo-search torch ddgs\n",
        "\n",
        "print(\"‚úÖ Dependencias instaladas\")\n",
        "print(\"üì¶ sentence-transformers incluye PyTorch para acelerar embeddings en GPU (si est√° disponible en Colab)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwKURxqNM_pp",
        "outputId": "649495d1-f088-47eb-f751-9a533f877e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Importaciones completadas\n",
            "üì¶ Usando modelo local (sentence-transformers/all-MiniLM-L6-v2) - sin necesidad de API keys\n",
            "‚úÖ Archivos de segmentaci√≥n encontrados:\n",
            "   - Segmentaci√≥n A: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_a.jsonl\n",
            "   - Segmentaci√≥n B: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_b.jsonl\n",
            "\n",
            "üìä Datos cargados:\n",
            "   - Segmentaci√≥n A: 227 fragmentos\n",
            "   - Segmentaci√≥n B: 349 fragmentos\n",
            "\n",
            "üìÑ Ejemplo de fragmento (Segmentaci√≥n A):\n",
            "   - chunk_id: DOC_033_A_000\n",
            "   - id_doc: DOC_033\n",
            "   - autor: Mar√≠a Jos√© Chac√≥n Rodr√≠guez\n",
            "   - texto (primeros 100 chars): apuntes ia clase 7/10 gianmarco oporta pe'rez ingenier'ƒ±a en computacio'n instituto tecnolo'gico de ...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 2: Configuraci√≥n y carga de datos segmentados\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain.tools import Tool\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "print(\"‚úÖ Importaciones completadas\")\n",
        "print(\"üì¶ Usando modelo local (sentence-transformers/all-MiniLM-L6-v2) - sin necesidad de API keys\")\n",
        "\n",
        "# Rutas de datos (definidas por Compa√±ero 1)\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "SEG_A_JSONL = os.path.join(OUT_DIR, \"seg_a.jsonl\")\n",
        "SEG_B_JSONL = os.path.join(OUT_DIR, \"seg_b.jsonl\")\n",
        "\n",
        "# Verificar que los archivos existen\n",
        "if not os.path.exists(SEG_A_JSONL):\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ {SEG_A_JSONL}. Aseg√∫rate de que Compa√±ero 1 complet√≥ su parte.\")\n",
        "if not os.path.exists(SEG_B_JSONL):\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ {SEG_B_JSONL}. Aseg√∫rate de que Compa√±ero 1 complet√≥ su parte.\")\n",
        "\n",
        "print(f\"‚úÖ Archivos de segmentaci√≥n encontrados:\")\n",
        "print(f\"   - Segmentaci√≥n A: {SEG_A_JSONL}\")\n",
        "print(f\"   - Segmentaci√≥n B: {SEG_B_JSONL}\")\n",
        "\n",
        "# Funci√≥n para cargar datos segmentados\n",
        "def load_segmented_data(jsonl_path: str) -> List[Dict]:\n",
        "    \"\"\"Carga los datos segmentados desde un archivo JSONL.\"\"\"\n",
        "    data = []\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Cargar datos\n",
        "seg_a_data = load_segmented_data(SEG_A_JSONL)\n",
        "seg_b_data = load_segmented_data(SEG_B_JSONL)\n",
        "\n",
        "print(f\"\\nüìä Datos cargados:\")\n",
        "print(f\"   - Segmentaci√≥n A: {len(seg_a_data)} fragmentos\")\n",
        "print(f\"   - Segmentaci√≥n B: {len(seg_b_data)} fragmentos\")\n",
        "\n",
        "# Mostrar ejemplo de un fragmento\n",
        "if seg_a_data:\n",
        "    print(f\"\\nüìÑ Ejemplo de fragmento (Segmentaci√≥n A):\")\n",
        "    ejemplo = seg_a_data[0]\n",
        "    print(f\"   - chunk_id: {ejemplo.get('chunk_id', 'N/A')}\")\n",
        "    print(f\"   - id_doc: {ejemplo.get('id_doc', 'N/A')}\")\n",
        "    print(f\"   - autor: {ejemplo.get('autor', 'N/A')}\")\n",
        "    print(f\"   - texto (primeros 100 chars): {ejemplo.get('texto', '')[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf8GR75iNUaX",
        "outputId": "5e53e086-a244-4a0d-8806-4c1ca822d346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Configurando modelo de embeddings...\n",
            "   Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
            "   Dimensi√≥n: 384\n",
            "   Ventajas: Gratuito, local, r√°pido, buen rendimiento\n",
            "   Dispositivo: CPU\n",
            "‚úÖ Modelo de embeddings configurado\n",
            "\n",
            "üìù Tokenizaci√≥n de ejemplo:\n",
            "   - Texto: 3170 caracteres\n",
            "   - Tokens aproximados: 822\n",
            "   - L√≠mite recomendado para embeddings: 8000 tokens\n",
            "\n",
            "üî¨ Probando generaci√≥n de embedding...\n",
            "‚úÖ Embedding generado exitosamente\n",
            "   - Dimensi√≥n del embedding: 384\n",
            "   - Primeros 5 valores: [-0.02066517062485218, 0.02713960036635399, -0.03559556230902672, -0.028662530705332756, -0.03590256720781326]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 3: Tokenizaci√≥n y generaci√≥n de embeddings\n",
        "# ============================================================\n",
        "# Usaremos sentence-transformers/all-MiniLM-L6-v2 (modelo local, gratuito)\n",
        "# ============================================================\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# Configurar el modelo de embeddings\n",
        "# Usando sentence-transformers/all-MiniLM-L6-v2 (gratuito, no requiere API key)\n",
        "print(\"üîß Configurando modelo de embeddings...\")\n",
        "print(\"   Modelo: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(\"   Dimensi√≥n: 384\")\n",
        "print(\"   Ventajas: Gratuito, local, r√°pido, buen rendimiento\")\n",
        "\n",
        "# Detectar si hay GPU disponible en Colab\n",
        "try:\n",
        "    import torch\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"   Dispositivo: {device.upper()}\")\n",
        "except:\n",
        "    device = 'cpu'\n",
        "    print(f\"   Dispositivo: CPU (PyTorch no disponible, usando CPU)\")\n",
        "\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': device},  # Usar GPU si est√° disponible en Colab\n",
        "    encode_kwargs={'normalize_embeddings': True}  # Normalizar embeddings para mejor b√∫squeda\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo de embeddings configurado\")\n",
        "\n",
        "# Funci√≥n para tokenizar y verificar longitud\n",
        "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
        "    \"\"\"Cuenta tokens aproximados usando tiktoken.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "        return len(encoding.encode(text))\n",
        "    except:\n",
        "        # Fallback simple: ~4 chars por token\n",
        "        return len(text) // 4\n",
        "\n",
        "# Probar con un fragmento de ejemplo\n",
        "if seg_a_data:\n",
        "    ejemplo_texto = seg_a_data[0].get(\"texto\", \"\")\n",
        "    tokens = count_tokens(ejemplo_texto)\n",
        "    print(f\"\\nüìù Tokenizaci√≥n de ejemplo:\")\n",
        "    print(f\"   - Texto: {len(ejemplo_texto)} caracteres\")\n",
        "    print(f\"   - Tokens aproximados: {tokens}\")\n",
        "    print(f\"   - L√≠mite recomendado para embeddings: 8000 tokens\")\n",
        "\n",
        "# Probar generaci√≥n de embedding\n",
        "print(\"\\nüî¨ Probando generaci√≥n de embedding...\")\n",
        "test_text = \"Este es un texto de prueba para verificar que los embeddings funcionan correctamente.\"\n",
        "try:\n",
        "    test_embedding = embeddings_model.embed_query(test_text)\n",
        "    print(f\"‚úÖ Embedding generado exitosamente\")\n",
        "    print(f\"   - Dimensi√≥n del embedding: {len(test_embedding)}\")\n",
        "    print(f\"   - Primeros 5 valores: {test_embedding[:5]}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error generando embedding: {e}\")\n",
        "    print(\"   Verifica tu API key o conexi√≥n a internet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy3sn9DeNVtf",
        "outputId": "842819fd-379a-4e38-bc86-d5c5b13a43c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö Creando documentos de LangChain...\n",
            "‚úÖ Documentos creados:\n",
            "   - Segmentaci√≥n A: 227 documentos\n",
            "   - Segmentaci√≥n B: 349 documentos\n",
            "\n",
            "üî® Creando bases vectoriales (esto puede tomar unos minutos)...\n",
            "   üìÇ Cargando vectorstore A existente desde /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/vectorstore_a...\n",
            "   ‚úÖ Vectorstore A cargado\n",
            "   üìÇ Cargando vectorstore B existente desde /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/vectorstore_b...\n",
            "   ‚úÖ Vectorstore B cargado\n",
            "\n",
            "‚úÖ Ambos vectorstores creados/cargados exitosamente\n",
            "\n",
            "üîç Probando b√∫squeda sem√°ntica...\n",
            "\n",
            "üìä Resultados para query: 'inteligencia artificial y aprendizaje autom√°tico'\n",
            "\n",
            "   Segmentaci√≥n A (top 3):\n",
            "      1. Score: 0.7227 | Autor: Andrey Ure√±a Berm√∫dez\n",
            "         Texto: de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante...\n",
            "      2. Score: 0.7863 | Autor: Luis Alfredo Gonz√°lez S√°nchez\n",
            "         Texto: notas de clase inteligenciaartificial-12deagosto-semana2 luis alfredo gonza'lez ...\n",
            "      3. Score: 0.8033 | Autor: Rodolfo David Acu√±a L√≥pez\n",
            "         Texto: - sistema mostrando comportamiento \"inteligente\" 1980s - algo \"inteligente\" que ...\n",
            "\n",
            "   Segmentaci√≥n B (top 3):\n",
            "      1. Score: 0.6992 | Autor: Andrey Ure√±a Berm√∫dez\n",
            "         Texto: vii. conclusio'n\n",
            "los temas revisados durante esta semana refuerzan la comprensio...\n",
            "      2. Score: 0.7228 | Autor: Rodolfo David Acu√±a L√≥pez\n",
            "         Texto: references\n",
            "[1] apuntes de la clase de inteligencia artificial, profesor steven a...\n",
            "      3. Score: 0.7557 | Autor: Priscilla Jim√©nez Salgado\n",
            "         Texto: la inteligencia artificialtiene aplicaciones en una\n",
            "gran variedad de a'reas. alg...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 4: Almacenamiento en base de datos vectorial\n",
        "# Crear dos √≠ndices/vectorstores (uno por cada segmentaci√≥n)\n",
        "# ============================================================\n",
        "\n",
        "# Funci√≥n para convertir datos segmentados a documentos de LangChain\n",
        "def create_documents_from_segments(segments: List[Dict]) -> List[Document]:\n",
        "    \"\"\"Convierte fragmentos segmentados a Documentos de LangChain con metadata.\"\"\"\n",
        "    documents = []\n",
        "    for seg in segments:\n",
        "        doc = Document(\n",
        "            page_content=seg.get(\"texto\", \"\"),\n",
        "            metadata={\n",
        "                \"id_doc\": seg.get(\"id_doc\", \"\"),\n",
        "                \"chunk_id\": seg.get(\"chunk_id\", \"\"),\n",
        "                \"segmentacion\": seg.get(\"segmentacion\", \"\"),\n",
        "                \"idx\": seg.get(\"idx\", 0),\n",
        "                \"autor\": seg.get(\"autor\", \"\"),\n",
        "                \"fecha\": seg.get(\"fecha\", \"\"),\n",
        "                \"tema\": seg.get(\"tema\", \"\")\n",
        "            }\n",
        "        )\n",
        "        documents.append(doc)\n",
        "    return documents\n",
        "\n",
        "# Crear documentos para ambas segmentaciones\n",
        "print(\"üìö Creando documentos de LangChain...\")\n",
        "docs_a = create_documents_from_segments(seg_a_data)\n",
        "docs_b = create_documents_from_segments(seg_b_data)\n",
        "\n",
        "print(f\"‚úÖ Documentos creados:\")\n",
        "print(f\"   - Segmentaci√≥n A: {len(docs_a)} documentos\")\n",
        "print(f\"   - Segmentaci√≥n B: {len(docs_b)} documentos\")\n",
        "\n",
        "# Crear vectorstores usando FAISS\n",
        "print(\"\\nüî® Creando bases vectoriales (esto puede tomar unos minutos)...\")\n",
        "\n",
        "VECTORSTORE_DIR_A = os.path.join(OUT_DIR, \"vectorstore_a\")\n",
        "VECTORSTORE_DIR_B = os.path.join(OUT_DIR, \"vectorstore_b\")\n",
        "\n",
        "try:\n",
        "    # Vectorstore A (chunks fijos)\n",
        "    if os.path.exists(VECTORSTORE_DIR_A):\n",
        "        print(f\"   üìÇ Cargando vectorstore A existente desde {VECTORSTORE_DIR_A}...\")\n",
        "        vectorstore_a = FAISS.load_local(\n",
        "            VECTORSTORE_DIR_A,\n",
        "            embeddings_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        print(\"   ‚úÖ Vectorstore A cargado\")\n",
        "    else:\n",
        "        print(f\"   üî® Creando vectorstore A desde {len(docs_a)} documentos...\")\n",
        "        vectorstore_a = FAISS.from_documents(docs_a, embeddings_model)\n",
        "        vectorstore_a.save_local(VECTORSTORE_DIR_A)\n",
        "        print(f\"   ‚úÖ Vectorstore A creado y guardado en {VECTORSTORE_DIR_A}\")\n",
        "\n",
        "    # Vectorstore B (encabezados)\n",
        "    if os.path.exists(VECTORSTORE_DIR_B):\n",
        "        print(f\"   üìÇ Cargando vectorstore B existente desde {VECTORSTORE_DIR_B}...\")\n",
        "        vectorstore_b = FAISS.load_local(\n",
        "            VECTORSTORE_DIR_B,\n",
        "            embeddings_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        print(\"   ‚úÖ Vectorstore B cargado\")\n",
        "    else:\n",
        "        print(f\"   üî® Creando vectorstore B desde {len(docs_b)} documentos...\")\n",
        "        vectorstore_b = FAISS.from_documents(docs_b, embeddings_model)\n",
        "        vectorstore_b.save_local(VECTORSTORE_DIR_B)\n",
        "        print(f\"   ‚úÖ Vectorstore B creado y guardado en {VECTORSTORE_DIR_B}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Ambos vectorstores creados/cargados exitosamente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creando vectorstores: {e}\")\n",
        "    raise\n",
        "\n",
        "# Probar b√∫squeda en ambos vectorstores\n",
        "print(\"\\nüîç Probando b√∫squeda sem√°ntica...\")\n",
        "test_query = \"inteligencia artificial y aprendizaje autom√°tico\"\n",
        "\n",
        "try:\n",
        "    results_a = vectorstore_a.similarity_search_with_score(test_query, k=3)\n",
        "    results_b = vectorstore_b.similarity_search_with_score(test_query, k=3)\n",
        "\n",
        "    print(f\"\\nüìä Resultados para query: '{test_query}'\")\n",
        "    print(f\"\\n   Segmentaci√≥n A (top 3):\")\n",
        "    for i, (doc, score) in enumerate(results_a, 1):\n",
        "        print(f\"      {i}. Score: {score:.4f} | Autor: {doc.metadata.get('autor', 'N/A')}\")\n",
        "        print(f\"         Texto: {doc.page_content[:80]}...\")\n",
        "\n",
        "    print(f\"\\n   Segmentaci√≥n B (top 3):\")\n",
        "    for i, (doc, score) in enumerate(results_b, 1):\n",
        "        print(f\"      {i}. Score: {score:.4f} | Autor: {doc.metadata.get('autor', 'N/A')}\")\n",
        "        print(f\"         Texto: {doc.page_content[:80]}...\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error en b√∫squeda de prueba: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z5HUNJtNZBW",
        "outputId": "e346d44f-55c6-44b7-df37-abee14ed9ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creando herramientas RAG...\n",
            "‚úÖ Herramientas RAG creadas:\n",
            "   - rag_search_A: Busca informaci√≥n relevante en la base de documentos usando segmentaci√≥n A. Reto...\n",
            "   - rag_search_B: Busca informaci√≥n relevante en la base de documentos usando segmentaci√≥n B. Reto...\n",
            "\n",
            "üß™ Probando herramienta RAG...\n",
            "\n",
            "üìã Resultado de RAG para: '¬øQu√© es la inteligencia artificial?'\n",
            "[Resultado 1 - Score: 0.6305]\n",
            "Fragmento: de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligentes. estas herramientas representan un paso clave hacia una inteligencia artificial ma's contextual, adaptable y responsable. referencia pacheco portuguez, s. (202...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 5: Crear RAG Tool en LangChain\n",
        "# La herramienta consulta la base vectorial y devuelve:\n",
        "# - fragmento (texto)\n",
        "# - documento de origen (id_doc, nombre_archivo)\n",
        "# - autor\n",
        "# ============================================================\n",
        "\n",
        "def create_rag_tool(vectorstore, segmentacion_name: str):\n",
        "    \"\"\"\n",
        "    Crea una herramienta RAG que consulta la base vectorial.\n",
        "\n",
        "    Args:\n",
        "        vectorstore: Base vectorial (FAISS)\n",
        "        segmentacion_name: Nombre de la segmentaci√≥n (\"A\" o \"B\")\n",
        "\n",
        "    Returns:\n",
        "        Tool de LangChain\n",
        "    \"\"\"\n",
        "    def rag_search(query: str, k: int = 5) -> str:\n",
        "        \"\"\"\n",
        "        Busca fragmentos relevantes en la base vectorial.\n",
        "\n",
        "        Args:\n",
        "            query: Consulta del usuario\n",
        "            k: N√∫mero de resultados a retornar (default: 5)\n",
        "\n",
        "        Returns:\n",
        "            String formateado con fragmentos, documentos de origen y autores\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # B√∫squeda sem√°ntica\n",
        "            results = vectorstore.similarity_search_with_score(query, k=k)\n",
        "\n",
        "            if not results:\n",
        "                return \"No se encontraron fragmentos relevantes para la consulta.\"\n",
        "\n",
        "            # Formatear resultados\n",
        "            formatted_results = []\n",
        "            for i, (doc, score) in enumerate(results, 1):\n",
        "                fragmento = doc.page_content\n",
        "                id_doc = doc.metadata.get(\"id_doc\", \"N/A\")\n",
        "                autor = doc.metadata.get(\"autor\", \"N/A\")\n",
        "                chunk_id = doc.metadata.get(\"chunk_id\", \"N/A\")\n",
        "\n",
        "                formatted_results.append(\n",
        "                    f\"[Resultado {i} - Score: {score:.4f}]\\n\"\n",
        "                    f\"Fragmento: {fragmento[:500]}{'...' if len(fragmento) > 500 else ''}\\n\"\n",
        "                    f\"Documento de origen: {id_doc} (chunk: {chunk_id})\\n\"\n",
        "                    f\"Autor: {autor}\\n\"\n",
        "                )\n",
        "\n",
        "            return \"\\n\\n\".join(formatted_results)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error en la b√∫squeda RAG: {str(e)}\"\n",
        "\n",
        "    return Tool(\n",
        "        name=f\"rag_search_{segmentacion_name}\",\n",
        "        description=f\"Busca informaci√≥n relevante en la base de documentos usando segmentaci√≥n {segmentacion_name}. \"\n",
        "                   f\"Retorna fragmentos de texto, documento de origen y autor. \"\n",
        "                   f\"√ösala cuando necesites buscar informaci√≥n en los apuntes del curso.\",\n",
        "        func=rag_search\n",
        "    )\n",
        "\n",
        "# Crear herramientas RAG para ambas segmentaciones\n",
        "print(\"üîß Creando herramientas RAG...\")\n",
        "rag_tool_a = create_rag_tool(vectorstore_a, \"A\")\n",
        "rag_tool_b = create_rag_tool(vectorstore_b, \"B\")\n",
        "\n",
        "print(\"‚úÖ Herramientas RAG creadas:\")\n",
        "print(f\"   - {rag_tool_a.name}: {rag_tool_a.description[:80]}...\")\n",
        "print(f\"   - {rag_tool_b.name}: {rag_tool_b.description[:80]}...\")\n",
        "\n",
        "# Probar herramienta RAG\n",
        "print(\"\\nüß™ Probando herramienta RAG...\")\n",
        "test_query_rag = \"¬øQu√© es la inteligencia artificial?\"\n",
        "result_rag = rag_tool_a.run(test_query_rag)\n",
        "print(f\"\\nüìã Resultado de RAG para: '{test_query_rag}'\")\n",
        "print(result_rag[:500] + \"...\" if len(result_rag) > 500 else result_rag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwBvSEAvNdS4",
        "outputId": "37f6a42d-811b-48cc-c1fe-f5629701fce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Error creando herramienta de b√∫squeda web: Could not import ddgs python package. Please install it with `pip install -U ddgs`.\n",
            "   Creando herramienta stub (sin funcionalidad)\n",
            "\n",
            "üì° Herramienta de b√∫squeda web:\n",
            "   - Nombre: web_search\n",
            "   - Descripci√≥n: Busca informaci√≥n en internet. IMPORTANTE: Solo usar cuando el usuario expl√≠citamente lo solicite....\n",
            "\n",
            "======================================================================\n",
            "‚ö†Ô∏è  RESTRICCI√ìN DE USO DE WEB SEARCH:\n",
            "======================================================================\n",
            "\n",
            "La herramienta web_search solo debe usarse cuando:\n",
            "1. El usuario expl√≠citamente solicita buscar informaci√≥n en internet\n",
            "2. La informaci√≥n no est√° disponible en la base de documentos del curso\n",
            "3. El usuario pregunta sobre informaci√≥n actual o externa al curso\n",
            "\n",
            "Para consultas sobre el contenido del curso, SIEMPRE usar primero\n",
            "las herramientas RAG (rag_search_A o rag_search_B).\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 6: Crear WebSearch Tool en LangChain\n",
        "# IMPORTANTE: Solo se debe usar cuando el usuario lo pida expl√≠citamente\n",
        "# ============================================================\n",
        "\n",
        "def create_web_search_tool():\n",
        "    \"\"\"\n",
        "    Crea una herramienta de b√∫squeda web.\n",
        "\n",
        "    IMPORTANTE: Esta herramienta solo debe usarse cuando el usuario\n",
        "    expl√≠citamente solicite buscar informaci√≥n en la web o cuando\n",
        "    la informaci√≥n no est√© disponible en la base de documentos.\n",
        "\n",
        "    Returns:\n",
        "        Tool de LangChain para b√∫squeda web\n",
        "    \"\"\"\n",
        "    # Opci√≥n 1: DuckDuckGo (no requiere API key)\n",
        "    try:\n",
        "        search = DuckDuckGoSearchRun()\n",
        "\n",
        "        def web_search_func(query: str) -> str:\n",
        "            \"\"\"\n",
        "            Busca informaci√≥n en la web usando DuckDuckGo.\n",
        "\n",
        "            IMPORTANTE: Solo usar cuando el usuario expl√≠citamente lo solicite\n",
        "            o cuando la informaci√≥n no est√© disponible en los documentos.\n",
        "\n",
        "            Args:\n",
        "                query: Consulta de b√∫squeda\n",
        "\n",
        "            Returns:\n",
        "                Resultados de la b√∫squeda web\n",
        "            \"\"\"\n",
        "            try:\n",
        "                results = search.run(query)\n",
        "                return f\"Resultados de b√∫squeda web para '{query}':\\n\\n{results}\"\n",
        "            except Exception as e:\n",
        "                return f\"Error en b√∫squeda web: {str(e)}\"\n",
        "\n",
        "        web_tool = Tool(\n",
        "            name=\"web_search\",\n",
        "            description=\"Busca informaci√≥n en internet usando DuckDuckGo. \"\n",
        "                       \"IMPORTANTE: Solo usar cuando el usuario expl√≠citamente solicite \"\n",
        "                       \"buscar informaci√≥n en la web o cuando la informaci√≥n no est√© \"\n",
        "                       \"disponible en la base de documentos del curso. \"\n",
        "                       \"No usar por defecto para consultas sobre el contenido del curso.\",\n",
        "            func=web_search_func\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Herramienta de b√∫squeda web creada (DuckDuckGo)\")\n",
        "        return web_tool\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error creando herramienta de b√∫squeda web: {e}\")\n",
        "        print(\"   Creando herramienta stub (sin funcionalidad)\")\n",
        "\n",
        "        # Tool stub si hay error\n",
        "        def web_search_stub(query: str) -> str:\n",
        "            return \"Herramienta de b√∫squeda web no disponible. Por favor, usa la herramienta RAG para buscar en los documentos.\"\n",
        "\n",
        "        return Tool(\n",
        "            name=\"web_search\",\n",
        "            description=\"Busca informaci√≥n en internet. \"\n",
        "                       \"IMPORTANTE: Solo usar cuando el usuario expl√≠citamente lo solicite.\",\n",
        "            func=web_search_stub\n",
        "        )\n",
        "\n",
        "# Crear herramienta de b√∫squeda web\n",
        "web_search_tool = create_web_search_tool()\n",
        "\n",
        "print(f\"\\nüì° Herramienta de b√∫squeda web:\")\n",
        "print(f\"   - Nombre: {web_search_tool.name}\")\n",
        "print(f\"   - Descripci√≥n: {web_search_tool.description[:100]}...\")\n",
        "\n",
        "# Documentar la restricci√≥n de uso\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚ö†Ô∏è  RESTRICCI√ìN DE USO DE WEB SEARCH:\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "La herramienta web_search solo debe usarse cuando:\n",
        "1. El usuario expl√≠citamente solicita buscar informaci√≥n en internet\n",
        "2. La informaci√≥n no est√° disponible en la base de documentos del curso\n",
        "3. El usuario pregunta sobre informaci√≥n actual o externa al curso\n",
        "\n",
        "Para consultas sobre el contenido del curso, SIEMPRE usar primero\n",
        "las herramientas RAG (rag_search_A o rag_search_B).\n",
        "\"\"\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg3_84WsNhmh",
        "outputId": "7acde0c4-fc4f-4260-bd87-d1ac630fec3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "‚úÖ RESUMEN DE LA PARTE DEL COMPA√ëERO 2\n",
            "======================================================================\n",
            "\n",
            "üìä Datos procesados:\n",
            "   - Fragmentos Segmentaci√≥n A: 227\n",
            "   - Fragmentos Segmentaci√≥n B: 349\n",
            "\n",
            "üî® Bases vectoriales creadas:\n",
            "   - Vectorstore A: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/vectorstore_a\n",
            "   - Vectorstore B: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/vectorstore_b\n",
            "\n",
            "üîß Herramientas disponibles:\n",
            "   1. rag_search_A: B√∫squeda RAG con segmentaci√≥n A (chunks fijos)\n",
            "   2. rag_search_B: B√∫squeda RAG con segmentaci√≥n B (encabezados)\n",
            "   3. web_search: B√∫squeda web (solo cuando se solicite expl√≠citamente)\n",
            "\n",
            "üìù Flujo implementado:\n",
            "   1. ‚úÖ Tokenizaci√≥n: Verificaci√≥n de tokens con tiktoken\n",
            "   2. ‚úÖ Embeddings: Generaci√≥n con sentence-transformers/all-MiniLM-L6-v2 (modelo local, gratuito)\n",
            "   3. ‚úÖ Almacenamiento: Dos vectorstores FAISS (uno por segmentaci√≥n)\n",
            "   4. ‚úÖ Consulta: Herramientas RAG que retornan fragmento, documento y autor\n",
            "   5. ‚úÖ WebSearch: Herramienta disponible con restricci√≥n de uso\n",
            "\n",
            "üéØ Pr√≥ximos pasos (Compa√±ero 3):\n",
            "   - Integrar herramientas en un agente\n",
            "   - Comparar rendimiento de ambas segmentaciones\n",
            "   - Evaluar m√©tricas (recall@k, precisi√≥n, tiempo de respuesta)\n",
            "\n",
            "======================================================================\n",
            "‚úÖ COMPA√ëERO 2 - TAREA COMPLETADA\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Paso 7: Resumen y verificaci√≥n final\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ RESUMEN DE LA PARTE DEL COMPA√ëERO 2\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä Datos procesados:\")\n",
        "print(f\"   - Fragmentos Segmentaci√≥n A: {len(seg_a_data)}\")\n",
        "print(f\"   - Fragmentos Segmentaci√≥n B: {len(seg_b_data)}\")\n",
        "\n",
        "print(\"\\nüî® Bases vectoriales creadas:\")\n",
        "print(f\"   - Vectorstore A: {VECTORSTORE_DIR_A}\")\n",
        "print(f\"   - Vectorstore B: {VECTORSTORE_DIR_B}\")\n",
        "\n",
        "print(\"\\nüîß Herramientas disponibles:\")\n",
        "print(f\"   1. {rag_tool_a.name}: B√∫squeda RAG con segmentaci√≥n A (chunks fijos)\")\n",
        "print(f\"   2. {rag_tool_b.name}: B√∫squeda RAG con segmentaci√≥n B (encabezados)\")\n",
        "print(f\"   3. {web_search_tool.name}: B√∫squeda web (solo cuando se solicite expl√≠citamente)\")\n",
        "\n",
        "print(\"\\nüìù Flujo implementado:\")\n",
        "print(\"   1. ‚úÖ Tokenizaci√≥n: Verificaci√≥n de tokens con tiktoken\")\n",
        "print(\"   2. ‚úÖ Embeddings: Generaci√≥n con sentence-transformers/all-MiniLM-L6-v2 (modelo local, gratuito)\")\n",
        "print(\"   3. ‚úÖ Almacenamiento: Dos vectorstores FAISS (uno por segmentaci√≥n)\")\n",
        "print(\"   4. ‚úÖ Consulta: Herramientas RAG que retornan fragmento, documento y autor\")\n",
        "print(\"   5. ‚úÖ WebSearch: Herramienta disponible con restricci√≥n de uso\")\n",
        "\n",
        "print(\"\\nüéØ Pr√≥ximos pasos (Compa√±ero 3):\")\n",
        "print(\"   - Integrar herramientas en un agente\")\n",
        "print(\"   - Comparar rendimiento de ambas segmentaciones\")\n",
        "print(\"   - Evaluar m√©tricas (recall@k, precisi√≥n, tiempo de respuesta)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ COMPA√ëERO 2 - TAREA COMPLETADA\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQq2odVGNmSk",
        "outputId": "7f49c5e8-231a-4b1a-939c-77fff4e4b3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö EJEMPLO DE USO DE HERRAMIENTAS\n",
            "\n",
            "======================================================================\n",
            "Ejemplo 1: B√∫squeda RAG con segmentaci√≥n A (chunks fijos)\n",
            "======================================================================\n",
            "\n",
            "üîç Consulta: 'aprendizaje supervisado'\n",
            "\n",
            "üìã Resultados:\n",
            "[Resultado 1 - Score: 0.0806]\n",
            "Fragmento: en aprendizaje supervisado.\n",
            "Documento de origen: DOC_019 (chunk: DOC_019_A_003)\n",
            "Autor: Ashley V√°squez\n",
            "\n",
            "\n",
            "[Resultado 2 - Score: 0.8452]\n",
            "Fragmento: supervisado: el modelo aprende a partir de obtencio'n inicial de los datos hasta la puesta en datos que incluyen etiquetas, las cuales sirven marcha del sistema en produccio'n. como referencia durante el entrenamiento. un ejemplocomu'neslaclasificacio'ndeima'genes. iii. jerarqu'ƒ±adeconceptosenia - no supervisado:elmodelotrabajacondatossin etiquetas y se encarga de encontrar patrones en los datos ocultos. un ejemplo claro de esto son losclusters. - semi-supervisado: combina datos etiquetados y no...\n",
            "Documento de origen: DOC_003 (chunk: DOC_003_A_003)\n",
            "Autor: Priscilla Jim√©nez Salgado\n",
            "\n",
            "\n",
            "[Resultado 3 - Score: 1...\n",
            "\n",
            "======================================================================\n",
            "Ejemplo 2: B√∫squeda RAG con segmentaci√≥n B (encabezados)\n",
            "======================================================================\n",
            "\n",
            "üîç Consulta: 'aprendizaje supervisado'\n",
            "\n",
            "üìã Resultados:\n",
            "[Resultado 1 - Score: 0.5927]\n",
            "Fragmento: a produccio'n en donde se debe de supervisar para garantizar un correcto funcionamiento.\n",
            "Documento de origen: DOC_004 (chunk: DOC_004_B_008)\n",
            "Autor: Luis Alfredo Gonz√°lez S√°nchez\n",
            "\n",
            "\n",
            "[Resultado 2 - Score: 0.9669]\n",
            "Fragmento: tablei\n",
            "\n",
            "tiposdeaprendizajel masivos\n",
            "- transformar el modelo: busca la optimizacio'n de este\n",
            "tipo descripcio'n\n",
            "supervisado conjunto de datos que tienen una etiqueta, la cual - onnx:seencargadehacertransformacionesenmodelos\n",
            "supervisaelaprendizajeydeterminaquetanbieno para optimizar los modelos.\n",
            "malseencuentraladescripcion. - mlops: debe pensar como tomar un modelo masivo y\n",
            "no-supervisado norequiereetiquetas,utilizaalgoritmosdemachine\n",
            "\n",
            "disponibilizarlo para los usuarios\n",
            "learning disenÀúados para des...\n",
            "Documento de origen: DOC_006 (chun...\n",
            "\n",
            "======================================================================\n",
            "üí° Nota: Compara los resultados de ambas segmentaciones\n",
            "   para evaluar cu√°l proporciona mejor contexto.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BONUS: Ejemplo de uso de las herramientas\n",
        "# ============================================================\n",
        "\n",
        "print(\"üìö EJEMPLO DE USO DE HERRAMIENTAS\\n\")\n",
        "\n",
        "# Ejemplo 1: B√∫squeda RAG con segmentaci√≥n A\n",
        "print(\"=\"*70)\n",
        "print(\"Ejemplo 1: B√∫squeda RAG con segmentaci√≥n A (chunks fijos)\")\n",
        "print(\"=\"*70)\n",
        "query1 = \"aprendizaje supervisado\"\n",
        "print(f\"\\nüîç Consulta: '{query1}'\")\n",
        "print(f\"\\nüìã Resultados:\")\n",
        "result1 = rag_tool_a.run(query1)\n",
        "print(result1[:800] + \"...\" if len(result1) > 800 else result1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Ejemplo 2: B√∫squeda RAG con segmentaci√≥n B (encabezados)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüîç Consulta: '{query1}'\")\n",
        "print(f\"\\nüìã Resultados:\")\n",
        "result2 = rag_tool_b.run(query1)\n",
        "print(result2[:800] + \"...\" if len(result2) > 800 else result2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° Nota: Compara los resultados de ambas segmentaciones\")\n",
        "print(\"   para evaluar cu√°l proporciona mejor contexto.\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPA√ëERO 3 ‚Äì AGENTE, ORQUESTACI√ìN, MEMORIA Y APP\n",
        "# ============================================================\n",
        "# Paso 1: Instalaci√≥n de dependencias necesarias\n",
        "# ============================================================\n",
        "\n",
        "# Resolver conflictos de dependencias: actualizar todas las versiones de LangChain\n",
        "print(\"üîß Resolviendo conflictos de dependencias...\")\n",
        "!pip install --quiet --upgrade langchain langchain-core langchain-community langchain-openai\n",
        "\n",
        "# Instalar dependencias del Compa√±ero 3\n",
        "print(\"üì¶ Instalando dependencias del Compa√±ero 3...\")\n",
        "!pip install --quiet langchain-google-genai streamlit streamlit-chat\n",
        "\n",
        "print(\"\\n‚úÖ Dependencias instaladas correctamente\")\n",
        "print(\"üì¶ Gemini Flash (2.0 experimental o 1.5) para orquestaci√≥n del agente\")\n",
        "print(\"üì¶ Streamlit para interfaz web\")\n",
        "print(\"\\nüí° Todas las versiones de LangChain est√°n actualizadas y compatibles\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Paso 2: Configuraci√≥n del modelo Gemini y definici√≥n del prompt base\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Configurar API Key de Google (Gemini)\n",
        "# IMPORTANTE: Configura GOOGLE_API_KEY en Colab Secrets o como variable de entorno\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  GOOGLE_API_KEY no configurada.\")\n",
        "    print(\"   Config√∫rala en Colab Secrets o como variable de entorno:\")\n",
        "    print(\"   - Ve a Colab Secrets (icono de candado en la barra lateral)\")\n",
        "    print(\"   - Agrega una nueva clave: GOOGLE_API_KEY\")\n",
        "    print(\"   - O usa: os.environ['GOOGLE_API_KEY'] = 'tu-api-key'\")\n",
        "else:\n",
        "    print(\"‚úÖ Google API Key configurada\")\n",
        "\n",
        "# Definir el prompt base/perfil del agente\n",
        "AGENT_PROMPT = \"\"\"Eres un asistente acad√©mico especializado en el curso de Inteligencia Artificial.\n",
        "Tu nombre es AsistenteIA y tu rol es ayudar a los estudiantes a encontrar informaci√≥n en los apuntes del curso.\n",
        "\n",
        "**INSTRUCCIONES IMPORTANTES:**\n",
        "1. SIEMPRE consulta primero los apuntes del curso usando las herramientas RAG disponibles antes de responder.\n",
        "2. SIEMPRE cita el documento de origen y el autor cuando uses informaci√≥n de los apuntes.\n",
        "3. Solo usa la b√∫squeda web (web_search) cuando:\n",
        "   - El usuario expl√≠citamente lo solicite\n",
        "   - La informaci√≥n no est√© disponible en los apuntes del curso\n",
        "   - Sea necesario buscar informaci√≥n actual o externa al curso\n",
        "\n",
        "**ESTILO DE RESPUESTA:**\n",
        "- S√© claro, conciso y educativo\n",
        "- Explica conceptos de manera accesible\n",
        "- Usa ejemplos cuando sea √∫til\n",
        "- Cita siempre tus fuentes: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "**HERRAMIENTAS DISPONIBLES:**\n",
        "- rag_search_A: Busca en apuntes usando segmentaci√≥n por chunks fijos (m√°s preciso para fragmentos espec√≠ficos)\n",
        "- rag_search_B: Busca en apuntes usando segmentaci√≥n por encabezados (mejor para temas completos)\n",
        "- web_search: Busca en internet (solo usar cuando se solicite expl√≠citamente)\n",
        "\n",
        "**EJEMPLO DE USO:**\n",
        "Usuario: \"¬øQu√© es el aprendizaje supervisado?\"\n",
        "1. Usa rag_search_A o rag_search_B para buscar en los apuntes\n",
        "2. Responde bas√°ndote en los resultados encontrados\n",
        "3. Cita: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "{history}\n",
        "\n",
        "Pregunta: {input}\n",
        "Piensa paso a paso y decide qu√© herramienta usar.\n",
        "\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "print(\"‚úÖ Prompt base del agente definido\")\n",
        "print(\"\\nüìã Perfil del agente:\")\n",
        "print(\"   - Nombre: AsistenteIA\")\n",
        "print(\"   - Rol: Asistente acad√©mico especializado en IA\")\n",
        "print(\"   - Estilo: Claro, educativo, con citas\")\n",
        "print(\"   - Prioridad: Primero apuntes, luego web (si se solicita)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Paso 3: Configurar modelo Gemini 2.5 Flash y crear el agente\n",
        "# ============================================================\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  No se puede crear el agente sin GOOGLE_API_KEY\")\n",
        "    print(\"   Configura la API key antes de continuar\")\n",
        "    llm = None\n",
        "    agent = None\n",
        "    agent_executor = None\n",
        "else:\n",
        "    # Configurar modelo Gemini 2.5 Flash\n",
        "    print(\"üîß Configurando Gemini 2.5 Flash...\")\n",
        "    # Intentar usar gemini-2.0-flash-exp (m√°s reciente) o gemini-1.5-flash como fallback\n",
        "    try:\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-2.0-flash-exp\",  # Modelo experimental m√°s reciente\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            temperature=0.3,  # M√°s determinista para b√∫squedas\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "        print(\"   Usando: gemini-2.0-flash-exp\")\n",
        "    except:\n",
        "        # Fallback a gemini-1.5-flash si el experimental no est√° disponible\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            temperature=0.3,\n",
        "            convert_system_message_to_human=True\n",
        "        )\n",
        "        print(\"   Usando: gemini-1.5-flash (fallback)\")\n",
        "    print(\"‚úÖ Modelo Gemini 2.5 Flash configurado\")\n",
        "    \n",
        "    # Preparar herramientas para el agente\n",
        "    tools = [rag_tool_a, rag_tool_b, web_search_tool]\n",
        "    \n",
        "    print(f\"\\nüîß Herramientas disponibles para el agente:\")\n",
        "    for tool in tools:\n",
        "        print(f\"   - {tool.name}\")\n",
        "    \n",
        "    # Crear prompt template\n",
        "    prompt = PromptTemplate(\n",
        "        template=AGENT_PROMPT,\n",
        "        input_variables=[\"history\", \"input\", \"agent_scratchpad\"]\n",
        "    )\n",
        "    \n",
        "    # Crear memoria conversacional (ventana de contexto)\n",
        "    # Mantiene las √∫ltimas 5 interacciones para coherencia\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        k=5,  # √öltimas 5 interacciones\n",
        "        memory_key=\"history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"output\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\nüíæ Memoria conversacional configurada:\")\n",
        "    print(\"   - Tipo: Ventana de contexto (√∫ltimas 5 interacciones)\")\n",
        "    print(\"   - No se guarda historial permanente\")\n",
        "    \n",
        "    # Crear agente usando ReAct pattern\n",
        "    print(\"\\nü§ñ Creando agente orquestador...\")\n",
        "    agent = create_react_agent(\n",
        "        llm=llm,\n",
        "        tools=tools,\n",
        "        prompt=prompt\n",
        "    )\n",
        "    \n",
        "    # Crear ejecutor del agente con memoria\n",
        "    agent_executor = AgentExecutor(\n",
        "        agent=agent,\n",
        "        tools=tools,\n",
        "        memory=memory,\n",
        "        verbose=True,\n",
        "        handle_parsing_errors=True,\n",
        "        max_iterations=5,\n",
        "        return_intermediate_steps=True\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Agente creado exitosamente\")\n",
        "    print(\"\\nüìä Configuraci√≥n del agente:\")\n",
        "    print(\"   - Modelo: Gemini 2.5 Flash\")\n",
        "    print(\"   - Patr√≥n: ReAct (Reasoning + Acting)\")\n",
        "    print(\"   - Memoria: Ventana de 5 interacciones\")\n",
        "    print(\"   - Max iteraciones: 5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Paso 4: Probar el agente con ejemplos\n",
        "# ============================================================\n",
        "\n",
        "if agent_executor:\n",
        "    print(\"üß™ Probando el agente con ejemplos...\\n\")\n",
        "    \n",
        "    # Ejemplo 1: Consulta sobre apuntes\n",
        "    print(\"=\"*70)\n",
        "    print(\"Ejemplo 1: Consulta sobre los apuntes\")\n",
        "    print(\"=\"*70)\n",
        "    test_query_1 = \"¬øQu√© es la inteligencia artificial seg√∫n los apuntes del curso?\"\n",
        "    print(f\"\\n‚ùì Pregunta: {test_query_1}\\n\")\n",
        "    \n",
        "    try:\n",
        "        result_1 = agent_executor.invoke({\"input\": test_query_1})\n",
        "        print(f\"\\n‚úÖ Respuesta del agente:\")\n",
        "        print(result_1[\"output\"][:500] + \"...\" if len(result_1[\"output\"]) > 500 else result_1[\"output\"])\n",
        "        print(f\"\\nüîß Herramientas usadas: {[step[0].tool for step in result_1.get('intermediate_steps', [])]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Ejemplo 2: Consulta espec√≠fica\")\n",
        "    print(\"=\"*70)\n",
        "    test_query_2 = \"Expl√≠came sobre aprendizaje supervisado\"\n",
        "    print(f\"\\n‚ùì Pregunta: {test_query_2}\\n\")\n",
        "    \n",
        "    try:\n",
        "        result_2 = agent_executor.invoke({\"input\": test_query_2})\n",
        "        print(f\"\\n‚úÖ Respuesta del agente:\")\n",
        "        print(result_2[\"output\"][:500] + \"...\" if len(result_2[\"output\"]) > 500 else result_2[\"output\"])\n",
        "        print(f\"\\nüîß Herramientas usadas: {[step[0].tool for step in result_2.get('intermediate_steps', [])]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  El agente no est√° configurado. Configura GOOGLE_API_KEY primero.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Paso 5: Crear aplicaci√≥n Streamlit\n",
        "# ============================================================\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Guardar objetos necesarios para Streamlit\n",
        "STREAMLIT_DATA_PATH = os.path.join(OUT_DIR, \"streamlit_data.pkl\")\n",
        "\n",
        "streamlit_data = {\n",
        "    \"agent_prompt\": AGENT_PROMPT,\n",
        "    \"vectorstore_a_path\": VECTORSTORE_DIR_A,\n",
        "    \"vectorstore_b_path\": VECTORSTORE_DIR_B,\n",
        "    \"seg_a_path\": SEG_A_JSONL,\n",
        "    \"seg_b_path\": SEG_B_JSONL,\n",
        "    \"proyecto_dir\": PROYECTO_DIR,\n",
        "}\n",
        "\n",
        "# Guardar configuraci√≥n\n",
        "with open(STREAMLIT_DATA_PATH, \"wb\") as f:\n",
        "    pickle.dump(streamlit_data, f)\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n guardada para Streamlit\")\n",
        "print(f\"üìÑ Archivo: {STREAMLIT_DATA_PATH}\")\n",
        "\n",
        "# Crear script Streamlit completo\n",
        "STREAMLIT_APP_CODE = f'''import streamlit as st\n",
        "import os\n",
        "import pickle\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.tools import Tool\n",
        "\n",
        "# Configuraci√≥n de la p√°gina\n",
        "st.set_page_config(\n",
        "    page_title=\"AsistenteIA - Curso de IA\",\n",
        "    page_icon=\"ü§ñ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"ü§ñ AsistenteIA - Curso de Inteligencia Artificial\")\n",
        "st.markdown(\"Asistente acad√©mico especializado en apuntes del curso de IA\")\n",
        "\n",
        "# Rutas\n",
        "BASE_DIR = \"{PROYECTO_DIR}\"\n",
        "OUT_DIR = os.path.join(BASE_DIR, \"dataset\")\n",
        "STREAMLIT_DATA_PATH = os.path.join(OUT_DIR, \"streamlit_data.pkl\")\n",
        "\n",
        "# Cargar configuraci√≥n\n",
        "@st.cache_resource\n",
        "def load_config():\n",
        "    try:\n",
        "        with open(STREAMLIT_DATA_PATH, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Inicializar recursos\n",
        "@st.cache_resource\n",
        "def load_embeddings():\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={{'device': 'cpu'}},\n",
        "        encode_kwargs={{'normalize_embeddings': True}}\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "@st.cache_resource\n",
        "def load_vectorstores():\n",
        "    embeddings = load_embeddings()\n",
        "    config = load_config()\n",
        "    if not config:\n",
        "        return None, None\n",
        "    \n",
        "    try:\n",
        "        vs_a = FAISS.load_local(\n",
        "            config[\"vectorstore_a_path\"],\n",
        "            embeddings,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        vs_b = FAISS.load_local(\n",
        "            config[\"vectorstore_b_path\"],\n",
        "            embeddings,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        return vs_a, vs_b\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error cargando vectorstores: {{e}}\")\n",
        "        return None, None\n",
        "\n",
        "# Crear herramientas RAG\n",
        "def create_rag_tool(vectorstore, name):\n",
        "    def rag_search(query: str, k: int = 5) -> str:\n",
        "        try:\n",
        "            results = vectorstore.similarity_search_with_score(query, k=k)\n",
        "            if not results:\n",
        "                return \"No se encontraron fragmentos relevantes.\"\n",
        "            \n",
        "            formatted = []\n",
        "            for i, (doc, score) in enumerate(results, 1):\n",
        "                formatted.append(\n",
        "                    f\"[Resultado {{i}} - Score: {{score:.4f}}]\\\\n\"\n",
        "                    f\"Fragmento: {{doc.page_content[:500]}}...\\\\n\"\n",
        "                    f\"Documento: {{doc.metadata.get('id_doc', 'N/A')}} ({{doc.metadata.get('chunk_id', 'N/A')}})\\\\n\"\n",
        "                    f\"Autor: {{doc.metadata.get('autor', 'N/A')}}\\\\n\"\n",
        "                )\n",
        "            return \"\\\\n\\\\n\".join(formatted)\n",
        "        except Exception as e:\n",
        "            return f\"Error: {{str(e)}}\"\n",
        "    \n",
        "    return Tool(\n",
        "        name=f\"rag_search_{{name}}\",\n",
        "        description=f\"Busca informaci√≥n en apuntes usando segmentaci√≥n {{name}}.\",\n",
        "        func=rag_search\n",
        "    )\n",
        "\n",
        "# Inicializar sesi√≥n\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"agent_executor\" not in st.session_state:\n",
        "    st.session_state.agent_executor = None\n",
        "if \"tools_used\" not in st.session_state:\n",
        "    st.session_state.tools_used = []\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configuraci√≥n\")\n",
        "    api_key = st.text_input(\n",
        "        \"Google API Key (Gemini)\",\n",
        "        type=\"password\",\n",
        "        value=os.getenv(\"GOOGLE_API_KEY\", \"\"),\n",
        "        help=\"Necesitas una API key de Google para usar Gemini\"\n",
        "    )\n",
        "    \n",
        "    if api_key:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "        \n",
        "        if st.session_state.agent_executor is None:\n",
        "            with st.spinner(\"Inicializando agente...\"):\n",
        "                try:\n",
        "                    vs_a, vs_b = load_vectorstores()\n",
        "                    if vs_a and vs_b:\n",
        "                        # Crear herramientas\n",
        "                        rag_tool_a = create_rag_tool(vs_a, \"A\")\n",
        "                        rag_tool_b = create_rag_tool(vs_b, \"B\")\n",
        "                        \n",
        "                        # Tool web stub\n",
        "                        web_tool = Tool(\n",
        "                            name=\"web_search\",\n",
        "                            description=\"Busca en internet (solo cuando se solicite expl√≠citamente).\",\n",
        "                            func=lambda q: \"B√∫squeda web no disponible en esta demo.\"\n",
        "                        )\n",
        "                        \n",
        "                        tools = [rag_tool_a, rag_tool_b, web_tool]\n",
        "                        \n",
        "                                                 # LLM\n",
        "                         # Intentar usar gemini-2.0-flash-exp o gemini-1.5-flash como fallback\n",
        "                         try:\n",
        "                             llm = ChatGoogleGenerativeAI(\n",
        "                                 model=\"gemini-2.0-flash-exp\",\n",
        "                                 google_api_key=api_key,\n",
        "                                 temperature=0.3,\n",
        "                                 convert_system_message_to_human=True\n",
        "                             )\n",
        "                         except:\n",
        "                             llm = ChatGoogleGenerativeAI(\n",
        "                                 model=\"gemini-1.5-flash\",\n",
        "                                 google_api_key=api_key,\n",
        "                                 temperature=0.3,\n",
        "                                 convert_system_message_to_human=True\n",
        "                             )\n",
        "                        \n",
        "                        # Memoria\n",
        "                        memory = ConversationBufferWindowMemory(\n",
        "                            k=5,\n",
        "                            memory_key=\"history\",\n",
        "                            return_messages=True,\n",
        "                            output_key=\"output\"\n",
        "                        )\n",
        "                        \n",
        "                        config = load_config()\n",
        "                        prompt = PromptTemplate(\n",
        "                            template=config[\"agent_prompt\"] if config else \"\",\n",
        "                            input_variables=[\"history\", \"input\", \"agent_scratchpad\"]\n",
        "                        )\n",
        "                        \n",
        "                        # Agente\n",
        "                        agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "                        st.session_state.agent_executor = AgentExecutor(\n",
        "                            agent=agent,\n",
        "                            tools=tools,\n",
        "                            memory=memory,\n",
        "                            verbose=True,\n",
        "                            handle_parsing_errors=True,\n",
        "                            max_iterations=5\n",
        "                        )\n",
        "                        \n",
        "                        st.success(\"‚úÖ Agente inicializado\")\n",
        "                    else:\n",
        "                        st.error(\"Error cargando vectorstores\")\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error: {{e}}\")\n",
        "    \n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### üìä √öltimas herramientas\")\n",
        "    if st.session_state.tools_used:\n",
        "        for tool in st.session_state.tools_used[-5:]:\n",
        "            st.text(f\"‚Ä¢ {{tool}}\")\n",
        "\n",
        "# Chat\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "        if \"tools\" in message:\n",
        "            st.caption(f\"üîß {{', '.join(message['tools'])}}\")\n",
        "\n",
        "if prompt := st.chat_input(\"Haz una pregunta sobre el curso de IA...\"):\n",
        "    st.session_state.messages.append({{\"role\": \"user\", \"content\": prompt}})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    \n",
        "    if st.session_state.agent_executor:\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Pensando...\"):\n",
        "                try:\n",
        "                    result = st.session_state.agent_executor.invoke({{\"input\": prompt}})\n",
        "                    response = result[\"output\"]\n",
        "                    tools_used = [step[0].tool for step in result.get(\"intermediate_steps\", [])]\n",
        "                    st.session_state.tools_used.extend(tools_used)\n",
        "                    \n",
        "                    st.markdown(response)\n",
        "                    if tools_used:\n",
        "                        st.caption(f\"üîß {{', '.join(tools_used)}}\")\n",
        "                    \n",
        "                    st.session_state.messages.append({{\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": response,\n",
        "                        \"tools\": tools_used\n",
        "                    }})\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error: {{e}}\")\n",
        "    else:\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.warning(\"‚ö†Ô∏è Configura la API key en el sidebar\")\n",
        "'''\n",
        "\n",
        "# Guardar script Streamlit\n",
        "STREAMLIT_APP_PATH = os.path.join(PROYECTO_DIR, \"streamlit_app.py\")\n",
        "with open(STREAMLIT_APP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(STREAMLIT_APP_CODE)\n",
        "\n",
        "print(f\"\\n‚úÖ Aplicaci√≥n Streamlit creada\")\n",
        "print(f\"üìÑ Archivo: {STREAMLIT_APP_PATH}\")\n",
        "print(f\"\\nüöÄ Para ejecutar:\")\n",
        "print(f\"   cd {PROYECTO_DIR}\")\n",
        "print(f\"   streamlit run streamlit_app.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Paso 6: Resumen final y verificaci√≥n de Compa√±ero 3\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ RESUMEN DE LA PARTE DEL COMPA√ëERO 3\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìã Componentes implementados:\")\n",
        "\n",
        "print(\"\\n1. ‚úÖ Prompt base/perfil del agente:\")\n",
        "print(\"   - Nombre: AsistenteIA\")\n",
        "print(\"   - Rol: Asistente acad√©mico especializado en IA\")\n",
        "print(\"   - Estilo: Claro, educativo, con citas\")\n",
        "print(\"   - Restricci√≥n: Primero apuntes, luego web (solo si se solicita)\")\n",
        "\n",
        "print(\"\\n2. ‚úÖ Agente orquestador:\")\n",
        "print(\"   - Modelo: Gemini 2.0 Flash Experimental (gemini-2.0-flash-exp) o Gemini 1.5 Flash\")\n",
        "print(\"   - Patr√≥n: ReAct (Reasoning + Acting)\")\n",
        "print(\"   - Decisi√≥n: Entre RAG Tool A, RAG Tool B, WebSearch, o respuesta directa\")\n",
        "print(\"   - Max iteraciones: 5\")\n",
        "\n",
        "print(\"\\n3. ‚úÖ Memoria conversacional:\")\n",
        "print(\"   - Tipo: ConversationBufferWindowMemory\")\n",
        "print(\"   - Ventana: √öltimas 5 interacciones\")\n",
        "print(\"   - Caracter√≠stica: No guarda historial permanente\")\n",
        "\n",
        "print(\"\\n4. ‚úÖ Interfaz Streamlit:\")\n",
        "print(\"   - Aplicaci√≥n web completa\")\n",
        "print(\"   - Muestra conversaci√≥n en tiempo real\")\n",
        "print(\"   - Indica qu√© herramienta se us√≥\")\n",
        "print(\"   - Sidebar con configuraci√≥n y herramientas usadas\")\n",
        "print(f\"   - Archivo: streamlit_app.py\")\n",
        "\n",
        "print(\"\\n5. ‚úÖ Integraci√≥n completa:\")\n",
        "print(\"   - Agente + Tools + Memoria + Interfaz\")\n",
        "print(\"   - Listo para demostraci√≥n presencial\")\n",
        "\n",
        "print(\"\\nüìä Herramientas disponibles para el agente:\")\n",
        "print(\"   1. rag_search_A: B√∫squeda en apuntes (segmentaci√≥n A - chunks fijos)\")\n",
        "print(\"   2. rag_search_B: B√∫squeda en apuntes (segmentaci√≥n B - encabezados)\")\n",
        "print(\"   3. web_search: B√∫squeda web (solo cuando se solicite expl√≠citamente)\")\n",
        "\n",
        "print(\"\\nüöÄ Para ejecutar la aplicaci√≥n:\")\n",
        "print(f\"   1. Configura GOOGLE_API_KEY\")\n",
        "print(f\"   2. cd {PROYECTO_DIR}\")\n",
        "print(f\"   3. streamlit run streamlit_app.py\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ COMPA√ëERO 3 - TAREA COMPLETADA\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüéØ El sistema est√° listo para:\")\n",
        "print(\"   - Comparar rendimiento de ambas segmentaciones\")\n",
        "print(\"   - Evaluar m√©tricas (recall@k, precisi√≥n, tiempo de respuesta)\")\n",
        "print(\"   - Demostraci√≥n presencial\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
