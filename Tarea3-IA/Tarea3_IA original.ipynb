{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54537164e752449e89cd89935cbfd2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6636b41034204c13941e63a9e5571d06",
              "IPY_MODEL_f3bd9f0aa771435fa8ddc81b7431721f",
              "IPY_MODEL_c3b8e5f1ad5048a29b044ade2ff3a0df"
            ],
            "layout": "IPY_MODEL_2464640081c54ec19af615d180c61062"
          }
        },
        "6636b41034204c13941e63a9e5571d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_711664f141754098a4b120390a4e6c2e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_35d1b814dff1457d9ab59f8b893c2ded",
            "value": ""
          }
        },
        "f3bd9f0aa771435fa8ddc81b7431721f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99f1f933004b40c599475fb2866c8b32",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07836632ff774e0397fcfd3cd3a06531",
            "value": 0
          }
        },
        "c3b8e5f1ad5048a29b044ade2ff3a0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7153b5d1ce3549448fc758032e34a539",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1a11062b4417457bb04a810e8dde581c",
            "value": "‚Äá0/0‚Äá[00:00&lt;?,‚Äá?it/s]"
          }
        },
        "2464640081c54ec19af615d180c61062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711664f141754098a4b120390a4e6c2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d1b814dff1457d9ab59f8b893c2ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99f1f933004b40c599475fb2866c8b32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "07836632ff774e0397fcfd3cd3a06531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7153b5d1ce3549448fc758032e34a539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a11062b4417457bb04a810e8dde581c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "# === Celda -Limpieza: Limpieza segura del entorno antes del setup ===\n",
        "import os, shutil, glob, sys, subprocess\n",
        "\n",
        "print(\"üßπ Iniciando limpieza del entorno de Colab...\\n\")\n",
        "\n",
        "# 1Ô∏è‚É£ Eliminar cach√©s comunes (Hugging Face, Torch, etc.)\n",
        "for cache_dir in [\n",
        "    \"/root/.cache/huggingface\",\n",
        "    \"/root/.cache/torch/sentence_transformers\",\n",
        "    \"/root/.cache/torch/transformers\",\n",
        "    \"/content/hf_cache\"\n",
        "]:\n",
        "    if os.path.exists(cache_dir):\n",
        "        print(\" - Borrando cach√©:\", cache_dir)\n",
        "        shutil.rmtree(cache_dir, ignore_errors=True)\n",
        "\n",
        "# 2Ô∏è‚É£ Desinstalar posibles restos conflictivos\n",
        "subprocess.run([\n",
        "    sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\",\n",
        "    \"transformers\", \"tokenizers\", \"huggingface-hub\", \"sentence-transformers\"\n",
        "], check=False)\n",
        "\n",
        "# 3Ô∏è‚É£ Borrar distribuciones da√±adas (casos como \"~cipy\")\n",
        "for p in glob.glob(\"/usr/local/lib/python3.12/dist-packages/~cipy*\"):\n",
        "    print(\" - Borrando resto inv√°lido:\", p)\n",
        "    shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "# 4Ô∏è‚É£ Mostrar versiones base del entorno\n",
        "print(\"\\nüîç Versi√≥n de Python:\", sys.version)\n",
        "print(\"üì¶ Paquetes relevantes actualmente instalados:\\n\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"list\"], check=False)\n",
        "\n",
        "print(\"\\n‚úÖ Limpieza completada. Ahora ejecuta la Celda 0 (diagn√≥stico).\")\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "oped9o_Fomkf",
        "outputId": "7c4edae1-e273-4060-f2d5-52d3e95ec31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# === Celda -Limpieza: Limpieza segura del entorno antes del setup ===\\nimport os, shutil, glob, sys, subprocess\\n\\nprint(\"üßπ Iniciando limpieza del entorno de Colab...\\n\")\\n\\n# 1Ô∏è‚É£ Eliminar cach√©s comunes (Hugging Face, Torch, etc.)\\nfor cache_dir in [\\n    \"/root/.cache/huggingface\",\\n    \"/root/.cache/torch/sentence_transformers\",\\n    \"/root/.cache/torch/transformers\",\\n    \"/content/hf_cache\"\\n]:\\n    if os.path.exists(cache_dir):\\n        print(\" - Borrando cach√©:\", cache_dir)\\n        shutil.rmtree(cache_dir, ignore_errors=True)\\n\\n# 2Ô∏è‚É£ Desinstalar posibles restos conflictivos\\nsubprocess.run([\\n    sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\",\\n    \"transformers\", \"tokenizers\", \"huggingface-hub\", \"sentence-transformers\"\\n], check=False)\\n\\n# 3Ô∏è‚É£ Borrar distribuciones da√±adas (casos como \"~cipy\")\\nfor p in glob.glob(\"/usr/local/lib/python3.12/dist-packages/~cipy*\"):\\n    print(\" - Borrando resto inv√°lido:\", p)\\n    shutil.rmtree(p, ignore_errors=True)\\n\\n# 4Ô∏è‚É£ Mostrar versiones base del entorno\\nprint(\"\\nüîç Versi√≥n de Python:\", sys.version)\\nprint(\"üì¶ Paquetes relevantes actualmente instalados:\\n\")\\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"list\"], check=False)\\n\\nprint(\"\\n‚úÖ Limpieza completada. Ahora ejecuta la Celda 0 (diagn√≥stico).\")\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Celda 0: Diagn√≥stico completo del entorno (LangChain + HF)\n",
        "# ============================================================\n",
        "\n",
        "import importlib, pkgutil, sys\n",
        "\n",
        "def ver(mod):\n",
        "    \"\"\"Imprime versi√≥n del m√≥dulo si est√° instalado.\"\"\"\n",
        "    try:\n",
        "        m = importlib.import_module(mod)\n",
        "        print(f\"{mod:28s}\", getattr(m, \"__version__\", \"(sin __version__)\"))\n",
        "    except Exception as e:\n",
        "        print(f\"{mod:28s}\", \"‚Äî no instalado ‚Äî\", \"|\", e)\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"\\nüì¶ Versiones detectadas:\")\n",
        "for mod in [\n",
        "    \"numpy\", \"scipy\", \"sklearn\", \"torch\",\n",
        "    \"transformers\", \"tokenizers\",\n",
        "    \"huggingface_hub\", \"sentence_transformers\",\n",
        "    \"langchain\", \"langchain_community\", \"langchain_huggingface\"\n",
        "]:\n",
        "    ver(mod)\n",
        "\n",
        "# ============================================================\n",
        "# Comprobaci√≥n autom√°tica de HuggingFaceEmbeddings disponible\n",
        "# ============================================================\n",
        "print(\"\\nüîç Comprobando integraci√≥n de LangChain + HuggingFace...\")\n",
        "\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    origen = \"langchain_huggingface (moderno ‚úÖ)\"\n",
        "except ModuleNotFoundError:\n",
        "    try:\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "        origen = \"langchain_community.embeddings (cl√°sico ‚öôÔ∏è)\"\n",
        "    except ModuleNotFoundError:\n",
        "        HuggingFaceEmbeddings = None\n",
        "        origen = \"‚ùå Ning√∫n m√≥dulo de HuggingFaceEmbeddings disponible\"\n",
        "\n",
        "print(\"Origen del wrapper:\", origen)\n",
        "\n",
        "# ============================================================\n",
        "# Prueba funcional (si existe HuggingFaceEmbeddings)\n",
        "# ============================================================\n",
        "if HuggingFaceEmbeddings is not None:\n",
        "    import torch\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Dispositivo:\", device)\n",
        "\n",
        "    embeddings_model = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={\"device\": device},\n",
        "        encode_kwargs={\"normalize_embeddings\": True},\n",
        "    )\n",
        "\n",
        "    test_text = \"La inteligencia artificial aprende patrones del lenguaje humano.\"\n",
        "    vec = embeddings_model.embed_query(test_text)\n",
        "    print(\"\\n‚úÖ Embeddings funcionando correctamente\")\n",
        "    print(\"Dimensi√≥n del embedding:\", len(vec))\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No se pudo inicializar HuggingFaceEmbeddings.\")\n",
        "    print(\"Ejecuta esto si quieres instalar el paquete moderno:\")\n",
        "    print(\"!pip install -U langchain-huggingface\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555,
          "referenced_widgets": [
            "54537164e752449e89cd89935cbfd2d8",
            "6636b41034204c13941e63a9e5571d06",
            "f3bd9f0aa771435fa8ddc81b7431721f",
            "c3b8e5f1ad5048a29b044ade2ff3a0df",
            "2464640081c54ec19af615d180c61062",
            "711664f141754098a4b120390a4e6c2e",
            "35d1b814dff1457d9ab59f8b893c2ded",
            "99f1f933004b40c599475fb2866c8b32",
            "07836632ff774e0397fcfd3cd3a06531",
            "7153b5d1ce3549448fc758032e34a539",
            "1a11062b4417457bb04a810e8dde581c"
          ]
        },
        "id": "JSe9ewE6l7KG",
        "outputId": "d2ccc1f6-772d-4465-b07d-fb646c0a38a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "\n",
            "üì¶ Versiones detectadas:\n",
            "numpy                        2.3.4\n",
            "scipy                        1.16.3\n",
            "sklearn                      1.6.1\n",
            "torch                        2.8.0+cu126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54537164e752449e89cd89935cbfd2d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers                 4.45.2\n",
            "tokenizers                   0.20.1\n",
            "huggingface_hub              0.36.0\n",
            "sentence_transformers        2.7.0\n",
            "langchain                    1.0.5\n",
            "langchain_community          0.4.1\n",
            "langchain_huggingface        (sin __version__)\n",
            "\n",
            "üîç Comprobando integraci√≥n de LangChain + HuggingFace...\n",
            "Origen del wrapper: langchain_huggingface (moderno ‚úÖ)\n",
            "Dispositivo: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Embeddings funcionando correctamente\n",
            "Dimensi√≥n del embedding: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "# CELDA 1\n",
        "# 1) Reinstala versiones estables y compatibles (sin cach√© y forzando)\n",
        "%pip uninstall -y transformers tokenizers\n",
        "\n",
        "%pip install --no-cache-dir --force-reinstall \\\n",
        "  \"transformers==4.45.2\" \\\n",
        "  \"tokenizers==0.20.1\"\n",
        "\n",
        "%pip install --no-cache-dir -U \\\n",
        "  \"sentence-transformers==2.7.0\" \\\n",
        "  \"huggingface-hub==0.36.0\"\n",
        "\n",
        "# 2) Instala LangChain y el conector moderno HuggingFace\n",
        "%pip install --no-cache-dir -U \"langchain>=0.2.15\" \"langchain-huggingface>=0.0.3\"\n",
        "\n",
        "# ‚úÖ A√±ade soporte Gemini sobre LangChain 1.x (AI Studio)\n",
        "%pip install -U \"langchain-google-genai>=1.0.0\" \"google-generativeai>=0.8.0\"\n",
        "\n",
        "# (Opcional pero recomendado si sigues usando integraciones de la comunidad:\n",
        "#  vectorstores, tools, etc.)\n",
        "%pip install --no-cache-dir -U \"langchain-community>=0.2.15\"\n",
        "\n",
        "# 3) Verificaci√≥n r√°pida\n",
        "import importlib.util, transformers, sentence_transformers, huggingface_hub, sys\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"Tokenizers  :\", __import__(\"tokenizers\").__version__)\n",
        "print(\"Sentence-Transformers:\", sentence_transformers.__version__)\n",
        "print(\"HF Hub:\", huggingface_hub.__version__)\n",
        "\n",
        "print(\"BERT module ->\",\n",
        "      importlib.util.find_spec(\"transformers.models.bert.modeling_bert\") is not None)\n",
        "\n",
        "# Verificaci√≥n de que el wrapper moderno est√° disponible\n",
        "try:\n",
        "    import langchain_huggingface\n",
        "    print(\"langchain_huggingface disponible ‚úÖ\")\n",
        "except Exception as e:\n",
        "    print(\"langchain_huggingface NO disponible ‚ùå:\", e)\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "b86Ow0ynmAtU",
        "outputId": "2d31f5dc-fc7c-4e37-f97a-fdfb1115475b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# CELDA 1\\n# 1) Reinstala versiones estables y compatibles (sin cach√© y forzando)\\n%pip uninstall -y transformers tokenizers\\n\\n%pip install --no-cache-dir --force-reinstall   \"transformers==4.45.2\"   \"tokenizers==0.20.1\"\\n\\n%pip install --no-cache-dir -U   \"sentence-transformers==2.7.0\"   \"huggingface-hub==0.36.0\"\\n\\n# 2) Instala LangChain y el conector moderno HuggingFace\\n%pip install --no-cache-dir -U \"langchain>=0.2.15\" \"langchain-huggingface>=0.0.3\"\\n\\n# ‚úÖ A√±ade soporte Gemini sobre LangChain 1.x (AI Studio)\\n%pip install -U \"langchain-google-genai>=1.0.0\" \"google-generativeai>=0.8.0\"\\n\\n# (Opcional pero recomendado si sigues usando integraciones de la comunidad:\\n#  vectorstores, tools, etc.)\\n%pip install --no-cache-dir -U \"langchain-community>=0.2.15\"\\n\\n# 3) Verificaci√≥n r√°pida\\nimport importlib.util, transformers, sentence_transformers, huggingface_hub, sys\\n\\nprint(\"Python:\", sys.version.split()[0])\\nprint(\"Transformers:\", transformers.__version__)\\nprint(\"Tokenizers  :\", __import__(\"tokenizers\").__version__)\\nprint(\"Sentence-Transformers:\", sentence_transformers.__version__)\\nprint(\"HF Hub:\", huggingface_hub.__version__)\\n\\nprint(\"BERT module ->\",\\n      importlib.util.find_spec(\"transformers.models.bert.modeling_bert\") is not None)\\n\\n# Verificaci√≥n de que el wrapper moderno est√° disponible\\ntry:\\n    import langchain_huggingface\\n    print(\"langchain_huggingface disponible ‚úÖ\")\\nexcept Exception as e:\\n    print(\"langchain_huggingface NO disponible ‚ùå:\", e)\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2: confirmar que todo qued√≥ consistente\n",
        "import importlib, importlib.util\n",
        "\n",
        "def ver(mod):\n",
        "    m = importlib.import_module(mod)\n",
        "    print(f\"{mod:22s}\", getattr(m, \"__version__\", \"(sin __version__)\"))\n",
        "\n",
        "for mod in [\"transformers\",\"tokenizers\",\"huggingface_hub\",\"sentence_transformers\"]:\n",
        "    ver(mod)\n",
        "\n",
        "# Chequeo de BERT presente\n",
        "import transformers, importlib.util\n",
        "print(\"BERT presente ->\", importlib.util.find_spec(\"transformers.models.bert\") is not None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_AGoVpHmCNd",
        "outputId": "cbcbd9bb-9639-4898-d4e3-51281db5851a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers           4.45.2\n",
            "tokenizers             0.20.1\n",
            "huggingface_hub        0.36.0\n",
            "sentence_transformers  2.7.0\n",
            "BERT presente -> True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 3: prueba de humo; si falla, limpia cach√© y reintenta una vez\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os, shutil\n",
        "\n",
        "def try_load(clean_cache=False):\n",
        "    cache_dir = \"/content/hf_cache\" if clean_cache else None\n",
        "    if clean_cache:\n",
        "        for p in [\"/content/hf_cache\", os.path.expanduser(\"~/.cache/huggingface\")]:\n",
        "            if os.path.exists(p):\n",
        "                print(\"Limpiando cach√©:\", p); shutil.rmtree(p, ignore_errors=True)\n",
        "    m = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                            cache_folder=cache_dir, trust_remote_code=False)\n",
        "    v = m.encode([\"hola\",\"mundo\"], normalize_embeddings=True)\n",
        "    print(\"OK; dimensi√≥n:\", len(v[0]))\n",
        "\n",
        "try:\n",
        "    try_load(clean_cache=False)\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Falla inicial:\", e)\n",
        "    print(\"‚Üí Reintentando con cach√© limpia‚Ä¶\")\n",
        "    try_load(clean_cache=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCDWMcXdmEqF",
        "outputId": "88fc799f-4bb2-406c-acc6-8bdf1785f8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK; dimensi√≥n: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Prueba de que las librerias quedaron bien instaladas y funcionan respecto a langchain\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Versi√≥n sin cache_dir (usa la ruta por defecto)\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": device},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# Prueba simple\n",
        "test_text = \"La inteligencia artificial aprende patrones del lenguaje humano.\"\n",
        "vec = embeddings_model.embed_query(test_text)\n",
        "print(\"‚úÖ LangChain conectado con √©xito.\")\n",
        "print(\"Dimensi√≥n del embedding:\", len(vec))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O1b3oG0mGZE",
        "outputId": "4176f0dc-00ef-46c1-9701-f306a6fd781d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n",
            "‚úÖ LangChain conectado con √©xito.\n",
            "Dimensi√≥n del embedding: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9504BR1-w8t",
        "outputId": "0fedcbdf-8a24-4d8c-d9c1-f03882747bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Drive montado.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 1) Montar Google Drive\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive montado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 2) Definir las rutas base en Drive\n",
        "#    Ajustadas a tu estructura:\n",
        "#    Mi unidad / Colab Notebooks / Tarea3-IA / ...\n",
        "# ============================================\n",
        "import os\n",
        "\n",
        "# Ruta base a \"Mi unidad\"\n",
        "BASE_DRIVE = \"/content/drive/MyDrive\"\n",
        "\n",
        "# Carpeta ra√≠z del proyecto de la tarea\n",
        "PROYECTO_DIR = os.path.join(BASE_DRIVE, \"Colab Notebooks\", \"Tarea3-IA\")\n",
        "\n",
        "# Carpetas espec√≠ficas\n",
        "METADATA_FILE = os.path.join(PROYECTO_DIR, \"MetadataRAW.csv\")\n",
        "PDFS_DIR = os.path.join(PROYECTO_DIR, \"RepositorioApuntesPdf\")\n",
        "\n",
        "print(\"üìÅ Proyecto:\", PROYECTO_DIR)\n",
        "print(\"üìÅ Metadata:\", METADATA_FILE)\n",
        "print(\"üìÅ PDFs:\", PDFS_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwb41D3L_FuD",
        "outputId": "c50d8918-482e-4431-cc05-bf3ce78c9bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Proyecto: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA\n",
            "üìÅ Metadata: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "üìÅ PDFs: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/RepositorioApuntesPdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 3) Verificar que las carpetas existen\n",
        "#    (esto ayuda a detectar typos en el nombre)\n",
        "# ============================================\n",
        "\n",
        "def check_dir(path, name):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"‚úÖ {name} encontrada: {path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {name} NO encontrada. Revisa el nombre en Drive: {path}\")\n",
        "\n",
        "check_dir(PROYECTO_DIR, \"Carpeta del proyecto\")\n",
        "check_dir(METADATA_FILE, \"Archivo de metadata\")\n",
        "check_dir(PDFS_DIR, \"Carpeta de PDFs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH70dK4RDelg",
        "outputId": "64dfc36d-11a0-49a5-a64f-6c1a25d883c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Carpeta del proyecto encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA\n",
            "‚úÖ Archivo de metadata encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "‚úÖ Carpeta de PDFs encontrada: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/RepositorioApuntesPdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 4) Listar los PDFs disponibles\n",
        "#    (esto confirma que Colab est√° viendo tu carpeta)\n",
        "# ============================================\n",
        "\n",
        "pdf_files = [f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")]\n",
        "print(f\"üìö PDFs encontrados: {len(pdf_files)}\")\n",
        "for f in pdf_files:\n",
        "    print(\"  -\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBcFaeTIkRSM",
        "outputId": "3087b171-4b4f-4d80-fc45-5f3b03e3a7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö PDFs encontrados: 46\n",
            "  - 10_SEMANA_AI_20251009_1.pdf\n",
            "  - 11_Semana_AI_20251014_3.pdf\n",
            "  - 12_SEMANA_AI_20251023_1.pdf\n",
            "  - 5_Semana_AI_20250904_2.pdf\n",
            "  - 11_Semana_AI_20251014_1.pdf\n",
            "  - 5_Semana_AI_20250902_2.pdf\n",
            "  - 4_Semana_AI_20250828_2.pdf\n",
            "  - 3_Semana_AI_20250819_2.pdf\n",
            "  - 8_Semana_AI_20250925_1.pdf\n",
            "  - 2_SEMANA_AI_20250812_1.pdf\n",
            "  - 12_Semana_AI_20251023_3.pdf\n",
            "  - 4_Semana_AI_20250828_1.pdf\n",
            "  - 11_Semana_AI_20251014_2.pdf\n",
            "  - 7_Semana_AI_20250916_2.pdf\n",
            "  - 12_SEMANA_AI_20251021_3.pdf\n",
            "  - 11_SEMANA_AI_20251016_2.pdf\n",
            "  - 4_SEMANA_AI_20250826_1.pdf\n",
            "  - 2_Semana_AI_20250812_3.pdf\n",
            "  - 2_Semana_AI_20250814_2.pdf\n",
            "  - 6_Semana_AI_20250911_1.pdf\n",
            "  - 12_SEMANA_AI_20251021_1.pdf\n",
            "  - 10_SEMANA_AI_20251007_1-222887296.pdf\n",
            "  - 8_Semana_AI_20250923_2.pdf\n",
            "  - 1_Semana_AI_20250807_2.pdf\n",
            "  - 7_Semana_AI_20250918_1.pdf\n",
            "  - 6_Semana_AI_20250911_2.pdf\n",
            "  - 6_Semana_AI_20250909_2-220676337.pdf\n",
            "  - 8_Semana_AI_20250925_2.pdf\n",
            "  - 3_Semana_AI_20250821_1.pdf\n",
            "  - 8_Semana_AI_20250923_1.pdf\n",
            "  - 12_SEMANA_AL_20251023_2.pdf\n",
            "  - 6_Semana_AI_20250909_1.pdf\n",
            "  - 12_Semana_AI_20251021_2.pdf\n",
            "  - 1_SEMANA_AI_20250807_1.pdf\n",
            "  - 4_SEMANA_AI_20250826_2.pdf\n",
            "  - 10_SEMANA_AI_20251007_1.pdf\n",
            "  - 9_SEMANA_AI_20251002_1.pdf\n",
            "  - 3_Semana_AI_20250819_1.pdf\n",
            "  - 11_Semana_AI_20251016_4.pdf\n",
            "  - 7_Semana_AI_20250918_2.pdf\n",
            "  - 5_Semana_AI_20250902_1.pdf\n",
            "  - 12_SEMANA_AI_20251021_4.pdf\n",
            "  - 9_Semana_AI_20251002_2.pdf\n",
            "  - 7_Semana_AI_20250916_1.pdf\n",
            "  - 5_Semana_AI_20250904_1.pdf\n",
            "  - 2_Semana_AI_20250814_1.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================\n",
        "# 5) Cargar archivo de metadata\n",
        "#    Columnas esperadas: id_doc, nombre_archivo, autor, fecha, tema\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "if os.path.exists(METADATA_FILE):\n",
        "    # Leer tal cual, como texto (sin parsear fechas)\n",
        "    df_meta = pd.read_csv(METADATA_FILE, dtype=str, keep_default_na=False)\n",
        "    print(f\"‚úÖ Metadata CSV cargada correctamente ({len(df_meta)} filas) desde:\\n{METADATA_FILE}\\n\")\n",
        "\n",
        "    # Chequeo suave de columnas (sin modificar nada)\n",
        "    expected = [\"id_doc\", \"nombre_archivo\", \"autor\", \"fecha\", \"tema\"]\n",
        "    missing = [c for c in expected if c not in df_meta.columns]\n",
        "    if missing:\n",
        "        print(\"‚ö†Ô∏è Faltan columnas esperadas:\", missing)\n",
        "    else:\n",
        "        print(\"üìë Columnas detectadas:\", list(df_meta.columns))\n",
        "\n",
        "        df_meta[\"fecha\"] = pd.to_datetime(df_meta[\"fecha\"], errors=\"coerce\")\n",
        "\n",
        "\n",
        "    # Preview\n",
        "    display(df_meta.head(10))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ el archivo 'metadata.csv' en la carpeta Metadata:\", METADATA_FILE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "z1k1mRxE0TsY",
        "outputId": "74892b3f-66d3-479c-d01d-547f6deeefa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Metadata CSV cargada correctamente (46 filas) desde:\n",
            "/content/drive/MyDrive/Colab Notebooks/Tarea3-IA/MetadataRAW.csv\n",
            "\n",
            "üìë Columnas detectadas: ['id_doc', 'nombre_archivo', 'autor', 'fecha', 'tema']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    id_doc              nombre_archivo                          autor  \\\n",
              "0  DOC_001  1_SEMANA_AI_20250807_1.pdf      Rodolfo David Acu√±a L√≥pez   \n",
              "1  DOC_002  1_Semana_AI_20250807_2.pdf   Fernando Daniel Brenes Reyes   \n",
              "2  DOC_003  2_SEMANA_AI_20250812_1.pdf      Priscilla Jim√©nez Salgado   \n",
              "3  DOC_004  2_Semana_AI_20250812_3.pdf  Luis Alfredo Gonz√°lez S√°nchez   \n",
              "4  DOC_005  2_Semana_AI_20250814_1.pdf      Kendall Rodr√≠guez Camacho   \n",
              "5  DOC_006  2_Semana_AI_20250814_2.pdf   Jose Pablo Quesada Rodr√≠guez   \n",
              "6  DOC_007  3_Semana_AI_20250819_1.pdf             Javier Rojas Rojas   \n",
              "7  DOC_008  3_Semana_AI_20250819_2.pdf        Mariana Quesada S√°nchez   \n",
              "8  DOC_009  3_Semana_AI_20250821_1.pdf           Julio Varela Venegas   \n",
              "9  DOC_010  4_SEMANA_AI_20250826_1.pdf           Andr√©s S√°nchez Rojas   \n",
              "\n",
              "       fecha                                               tema  \n",
              "0 2025-08-07  Principios fundamentales de la inteligencia ar...  \n",
              "1 2025-08-07  Aplicaciones de la inteligencia artificial y m...  \n",
              "2 2025-08-12  Introducci√≥n a machine learning y deep learnin...  \n",
              "3 2025-08-12  Resumen de conceptos clave de IA y enfoques de...  \n",
              "4 2025-08-14  Introducci√≥n a √°lgebra lineal aplicada con Pyt...  \n",
              "5 2025-08-14  Resumen detallado sobre tipos de aprendizaje, ...  \n",
              "6 2025-08-19  Revisi√≥n de √°lgebra lineal y aprendizaje super...  \n",
              "7 2025-08-19  Repaso de √°lgebra lineal y fundamentos del apr...  \n",
              "8 2025-08-21  Aplicaci√≥n del √°lgebra lineal y la programaci√≥...  \n",
              "9 2025-08-26  Implementaci√≥n del algoritmo KNN y fundamentos...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-181b729d-8893-498f-b029-6b18606ee26e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_doc</th>\n",
              "      <th>nombre_archivo</th>\n",
              "      <th>autor</th>\n",
              "      <th>fecha</th>\n",
              "      <th>tema</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DOC_001</td>\n",
              "      <td>1_SEMANA_AI_20250807_1.pdf</td>\n",
              "      <td>Rodolfo David Acu√±a L√≥pez</td>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>Principios fundamentales de la inteligencia ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DOC_002</td>\n",
              "      <td>1_Semana_AI_20250807_2.pdf</td>\n",
              "      <td>Fernando Daniel Brenes Reyes</td>\n",
              "      <td>2025-08-07</td>\n",
              "      <td>Aplicaciones de la inteligencia artificial y m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DOC_003</td>\n",
              "      <td>2_SEMANA_AI_20250812_1.pdf</td>\n",
              "      <td>Priscilla Jim√©nez Salgado</td>\n",
              "      <td>2025-08-12</td>\n",
              "      <td>Introducci√≥n a machine learning y deep learnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DOC_004</td>\n",
              "      <td>2_Semana_AI_20250812_3.pdf</td>\n",
              "      <td>Luis Alfredo Gonz√°lez S√°nchez</td>\n",
              "      <td>2025-08-12</td>\n",
              "      <td>Resumen de conceptos clave de IA y enfoques de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DOC_005</td>\n",
              "      <td>2_Semana_AI_20250814_1.pdf</td>\n",
              "      <td>Kendall Rodr√≠guez Camacho</td>\n",
              "      <td>2025-08-14</td>\n",
              "      <td>Introducci√≥n a √°lgebra lineal aplicada con Pyt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DOC_006</td>\n",
              "      <td>2_Semana_AI_20250814_2.pdf</td>\n",
              "      <td>Jose Pablo Quesada Rodr√≠guez</td>\n",
              "      <td>2025-08-14</td>\n",
              "      <td>Resumen detallado sobre tipos de aprendizaje, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DOC_007</td>\n",
              "      <td>3_Semana_AI_20250819_1.pdf</td>\n",
              "      <td>Javier Rojas Rojas</td>\n",
              "      <td>2025-08-19</td>\n",
              "      <td>Revisi√≥n de √°lgebra lineal y aprendizaje super...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DOC_008</td>\n",
              "      <td>3_Semana_AI_20250819_2.pdf</td>\n",
              "      <td>Mariana Quesada S√°nchez</td>\n",
              "      <td>2025-08-19</td>\n",
              "      <td>Repaso de √°lgebra lineal y fundamentos del apr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>DOC_009</td>\n",
              "      <td>3_Semana_AI_20250821_1.pdf</td>\n",
              "      <td>Julio Varela Venegas</td>\n",
              "      <td>2025-08-21</td>\n",
              "      <td>Aplicaci√≥n del √°lgebra lineal y la programaci√≥...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DOC_010</td>\n",
              "      <td>4_SEMANA_AI_20250826_1.pdf</td>\n",
              "      <td>Andr√©s S√°nchez Rojas</td>\n",
              "      <td>2025-08-26</td>\n",
              "      <td>Implementaci√≥n del algoritmo KNN y fundamentos...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-181b729d-8893-498f-b029-6b18606ee26e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-181b729d-8893-498f-b029-6b18606ee26e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-181b729d-8893-498f-b029-6b18606ee26e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6f21a845-f413-4c95-8443-6aeacdec8d0f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f21a845-f413-4c95-8443-6aeacdec8d0f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6f21a845-f413-4c95-8443-6aeacdec8d0f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"\\u26a0\\ufe0f No se encontr\\u00f3 el archivo 'metadata\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id_doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"DOC_009\",\n          \"DOC_002\",\n          \"DOC_006\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nombre_archivo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"3_Semana_AI_20250821_1.pdf\",\n          \"1_Semana_AI_20250807_2.pdf\",\n          \"2_Semana_AI_20250814_2.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"autor\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Julio Varela Venegas\",\n          \"Fernando Daniel Brenes Reyes\",\n          \"Jose Pablo Quesada Rodr\\u00edguez\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fecha\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-08-07 00:00:00\",\n        \"max\": \"2025-08-26 00:00:00\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"2025-08-07 00:00:00\",\n          \"2025-08-12 00:00:00\",\n          \"2025-08-26 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tema\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Aplicaci\\u00f3n del \\u00e1lgebra lineal y la programaci\\u00f3n vectorial en IA, con enfoque en aprendizaje supervisado, representaci\\u00f3n de vectores y uso de NumPy y Jupyter Notebook.\",\n          \"Aplicaciones de la inteligencia artificial y modelos GPT-5 en autos aut\\u00f3nomos, con \\u00e9nfasis en el aprendizaje supervisado y no supervisado basado en datos.\",\n          \"Resumen detallado sobre tipos de aprendizaje, pipeline de machine learning y fundamentos de \\u00e1lgebra lineal y tensores en PyTorch.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# COMPA√ëERO 1 ‚Äì DATOS Y PREPROCESAMIENTO (PASO 2 COMPLETO)\n",
        "# Extraer texto, normalizar y segmentar (A: chunks fijos, B: encabezados)\n",
        "# Requiere:\n",
        "#  - Variables definidas antes: PROYECTO_DIR, METADATA_FILE, PDFS_DIR\n",
        "#  - METADATA_FILE con columnas: id_doc, nombre_archivo, autor, fecha, tema\n",
        "# Salidas (para Compa√±ero 2):\n",
        "#  - dataset/base_documentos.jsonl / .parquet\n",
        "#  - dataset/seg_a.jsonl / dataset/seg_b.jsonl\n",
        "#  - dataset/txt_por_doc/DOC_###.txt (opcional)\n",
        "#  - dataset/preprocesamiento_decisiones.md\n",
        "# ============================================================\n",
        "\n",
        "!pip install --quiet pdfplumber pandas pyarrow\n",
        "\n",
        "import os, re, json, unicodedata\n",
        "from pathlib import Path\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- CONFIGURACI√ìN ----------\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "TXT_DIR = os.path.join(OUT_DIR, \"txt_por_doc\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(TXT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "BASE_DOCS_JSONL   = os.path.join(OUT_DIR, \"base_documentos.jsonl\")\n",
        "BASE_DOCS_PARQUET = os.path.join(OUT_DIR, \"base_documentos.parquet\")\n",
        "SEG_A_JSONL       = os.path.join(OUT_DIR, \"seg_a.jsonl\")\n",
        "SEG_B_JSONL       = os.path.join(OUT_DIR, \"seg_b.jsonl\")\n",
        "NOTAS_PREPROC     = os.path.join(OUT_DIR, \"preprocesamiento_decisiones.md\")\n",
        "\n",
        "# (Ajusta si quer√©s otros tama√±os)\n",
        "CHUNK_WORDS   = 400    # tama√±o del chunk (en palabras aproximado)\n",
        "CHUNK_OVERLAP = 80     # solapamiento entre chunks\n",
        "\n",
        "# Si existen salidas previas, limpirlas (evita duplicados al re-ejecutar)\n",
        "for p in [BASE_DOCS_JSONL, BASE_DOCS_PARQUET, SEG_A_JSONL, SEG_B_JSONL, NOTAS_PREPROC]:\n",
        "    try:\n",
        "        if os.path.exists(p): os.remove(p)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------- NORMALIZACI√ìN ----------\n",
        "def normalize_unicode_nfc(text: str) -> str:\n",
        "    return unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "def strip_control_chars(text: str) -> str:\n",
        "    # Deja \\n y \\t; elimina otros de control\n",
        "    return \"\".join(ch for ch in text if ch in (\"\\n\",\"\\t\") or ord(ch) >= 32)\n",
        "\n",
        "def standardize_quotes_dashes(text: str) -> str:\n",
        "    repl = {\n",
        "        \"‚Äú\": \"\\\"\", \"‚Äù\": \"\\\"\", \"‚Äû\": \"\\\"\", \"¬´\": \"\\\"\", \"¬ª\": \"\\\"\",\n",
        "        \"‚Äô\": \"'\", \"¬¥\": \"'\", \"‚Äò\": \"'\",\n",
        "        \"‚Äê\": \"-\", \"‚Äì\": \"-\", \"‚Äî\": \"-\", \"‚àí\": \"-\",\n",
        "        \"‚Ä¶\": \"...\", \"‚Ä¢\": \"-\", \"¬∑\": \"-\"\n",
        "    }\n",
        "    for a, b in repl.items():\n",
        "        text = text.replace(a, b)\n",
        "    return text\n",
        "\n",
        "def remove_hyphen_linebreaks(text: str) -> str:\n",
        "    # Une palabras cortadas por guion al final de l√≠nea: \"infor-\\nmaci√≥n\" -> \"informaci√≥n\"\n",
        "    return re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
        "\n",
        "def collapse_whitespace(text: str) -> str:\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def to_lower(text: str) -> str:\n",
        "    return text.lower()\n",
        "\n",
        "def normalize_text_pipeline(raw: str) -> str:\n",
        "    if not raw: return \"\"\n",
        "    t = normalize_unicode_nfc(raw)\n",
        "    t = strip_control_chars(t)\n",
        "    t = standardize_quotes_dashes(t)\n",
        "    t = remove_hyphen_linebreaks(t)\n",
        "    t = collapse_whitespace(t)\n",
        "    t = to_lower(t)\n",
        "    return t\n",
        "\n",
        "# ---------- SEGMENTACI√ìN ----------\n",
        "def segment_fixed_overlap(text: str, words_per_chunk=CHUNK_WORDS, overlap=CHUNK_OVERLAP):\n",
        "    \"\"\"Segmentaci√≥n A: chunks por palabras con solapamiento.\"\"\"\n",
        "    words = text.split()\n",
        "    if not words: return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    idx = 0\n",
        "    while i < len(words):\n",
        "        chunk_words = words[i:i+words_per_chunk]\n",
        "        chunk_text = \" \".join(chunk_words).strip()\n",
        "        if chunk_text:\n",
        "            chunks.append((idx, chunk_text))\n",
        "            idx += 1\n",
        "        i += max(1, words_per_chunk - overlap)\n",
        "    return chunks\n",
        "\n",
        "_HEADING_RE = re.compile(\n",
        "    r\"\"\"^(\n",
        "        abstract\\b|\n",
        "        resumen\\b|\n",
        "        introducci[o√≥]n\\b|\n",
        "        conclusi[o√≥]n\\b|\n",
        "        referencias\\b|\n",
        "        agradecimientos\\b|\n",
        "        related\\ work\\b|\n",
        "        i{1,3}\\.|iv\\.|v\\.|vi\\.|vii\\.|viii\\.|ix\\.|x\\.|      # romanos\n",
        "        \\d+\\.\\s|                                          # 1. 2. 3.\n",
        "        [A-Z][A-Z0-9\\s\\-\\&]{3,}$                          # l√≠nea MAY√öSCULAS (t√≠tulo)\n",
        "    )\"\"\",\n",
        "    re.IGNORECASE | re.VERBOSE\n",
        ")\n",
        "\n",
        "def segment_by_headings(text: str, min_section_words=120):\n",
        "    \"\"\"Segmentaci√≥n B: por encabezados; fusiona secciones peque√±as.\"\"\"\n",
        "    lines = [l.strip() for l in text.splitlines()]\n",
        "    # Marcar √≠ndices de l√≠neas que parecen encabezado\n",
        "    header_idx = [i for i, line in enumerate(lines) if line and _HEADING_RE.match(line)]\n",
        "    # Siempre incluir inicio y fin\n",
        "    if 0 not in header_idx: header_idx = [0] + header_idx\n",
        "    if len(lines) - 1 not in header_idx: header_idx = header_idx + [len(lines) - 1]\n",
        "    header_idx = sorted(set(header_idx))\n",
        "\n",
        "    # Cortar por rangos\n",
        "    raw_sections = []\n",
        "    for a, b in zip(header_idx, header_idx[1:]):\n",
        "        sec = \"\\n\".join(lines[a:b]).strip()\n",
        "        if sec: raw_sections.append(sec)\n",
        "    # A√±adir √∫ltimo trozo\n",
        "    tail = \"\\n\".join(lines[header_idx[-1]:]).strip()\n",
        "    if tail: raw_sections.append(tail)\n",
        "\n",
        "    # Fusionar secciones demasiado peque√±as\n",
        "    merged = []\n",
        "    buff = []\n",
        "    wcount = 0\n",
        "    for sec in raw_sections:\n",
        "        wc = len(sec.split())\n",
        "        if wcount + wc < min_section_words:\n",
        "            buff.append(sec); wcount += wc\n",
        "        else:\n",
        "            if buff:\n",
        "                buff.append(sec)\n",
        "                merged.append(\"\\n\\n\".join(buff).strip())\n",
        "                buff, wcount = [], 0\n",
        "            else:\n",
        "                merged.append(sec)\n",
        "                buff, wcount = [], 0\n",
        "    if buff:\n",
        "        merged.append(\"\\n\\n\".join(buff).strip())\n",
        "\n",
        "    # Indexar\n",
        "    return [(i, s) for i, s in enumerate(merged)]\n",
        "\n",
        "# ---------- CARGA METADATA ----------\n",
        "assert os.path.exists(METADATA_FILE), f\"No existe METADATA_FILE: {METADATA_FILE}\"\n",
        "df_meta = pd.read_csv(METADATA_FILE, dtype=str, keep_default_na=False)\n",
        "for col in [\"id_doc\",\"nombre_archivo\",\"autor\",\"fecha\",\"tema\"]:\n",
        "    if col not in df_meta.columns:\n",
        "        raise ValueError(f\"Falta columna en metadata: {col}\")\n",
        "\n",
        "# √≠ndice r√°pido por nombre de archivo (case-insensitive)\n",
        "meta_idx = {str(n).strip().lower(): i for i, n in enumerate(df_meta[\"nombre_archivo\"])}\n",
        "\n",
        "# ---------- RECORRIDO PDFs ----------\n",
        "base_registros = []\n",
        "seg_a_registros = []\n",
        "seg_b_registros = []\n",
        "errores = []\n",
        "\n",
        "pdf_files_sorted = sorted([f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")])\n",
        "\n",
        "for fname in pdf_files_sorted:\n",
        "    key = fname.strip().lower()\n",
        "    if key not in meta_idx:\n",
        "        errores.append((fname, \"no_encontrado_en_metadata\"))\n",
        "        print(f\"‚ö†Ô∏è  {fname}: no aparece en 'nombre_archivo' de la metadata; se omite.\")\n",
        "        continue\n",
        "\n",
        "    row   = df_meta.iloc[meta_idx[key]]\n",
        "    iddoc = row[\"id_doc\"]; autor=row[\"autor\"]; fecha=row[\"fecha\"]; tema=row[\"tema\"]\n",
        "    pdf_path = os.path.join(PDFS_DIR, fname)\n",
        "\n",
        "    # Extraer texto del PDF\n",
        "    try:\n",
        "        pages = []\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for p in pdf.pages:\n",
        "                pages.append(p.extract_text() or \"\")\n",
        "        full_text = \"\\n\".join(pages)\n",
        "    except Exception as e:\n",
        "        errores.append((fname, f\"error_lectura_pdf:{e}\"))\n",
        "        print(f\"‚ùå Error leyendo {fname}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Normalizar\n",
        "    texto_limpio = normalize_text_pipeline(full_text)\n",
        "\n",
        "    # Guardar TXT por doc (√∫til para inspecci√≥n)\n",
        "    with open(os.path.join(TXT_DIR, f\"{iddoc}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(texto_limpio)\n",
        "\n",
        "    # Registro base (documento completo)\n",
        "    base_registros.append({\n",
        "        \"id_doc\": iddoc,\n",
        "        \"nombre_archivo\": fname,\n",
        "        \"autor\": autor,\n",
        "        \"fecha\": str(fecha),\n",
        "        \"tema\": tema,\n",
        "        \"texto_original\": full_text,\n",
        "        \"texto_limpio\": texto_limpio\n",
        "    })\n",
        "\n",
        "    # ---------- SEGMENTACI√ìN A: chunks fijos ----------\n",
        "    chunks_a = segment_fixed_overlap(texto_limpio, CHUNK_WORDS, CHUNK_OVERLAP)\n",
        "    for idx_chunk, chunk_text in chunks_a:\n",
        "        seg_a_registros.append({\n",
        "            \"id_doc\": iddoc,\n",
        "            \"segmentacion\": \"A\",\n",
        "            \"chunk_id\": f\"{iddoc}_A_{idx_chunk:03d}\",\n",
        "            \"idx\": idx_chunk,\n",
        "            \"autor\": autor,\n",
        "            \"fecha\": str(fecha),\n",
        "            \"tema\": tema,\n",
        "            \"texto\": chunk_text\n",
        "        })\n",
        "\n",
        "    # ---------- SEGMENTACI√ìN B: encabezados ----------\n",
        "    sections_b = segment_by_headings(texto_limpio, min_section_words=120)\n",
        "    for idx_sec, sec_text in sections_b:\n",
        "        seg_b_registros.append({\n",
        "            \"id_doc\": iddoc,\n",
        "            \"segmentacion\": \"B\",\n",
        "            \"chunk_id\": f\"{iddoc}_B_{idx_sec:03d}\",\n",
        "            \"idx\": idx_sec,\n",
        "            \"autor\": autor,\n",
        "            \"fecha\": str(fecha),\n",
        "            \"tema\": tema,\n",
        "            \"texto\": sec_text\n",
        "        })\n",
        "\n",
        "# ---------- GUARDAR SALIDAS ----------\n",
        "# Base documentos\n",
        "with open(BASE_DOCS_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in base_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "pd.DataFrame(base_registros).to_parquet(BASE_DOCS_PARQUET, index=False)\n",
        "\n",
        "# Segmentaciones\n",
        "with open(SEG_A_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in seg_a_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "with open(SEG_B_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
        "    for r in seg_b_registros:\n",
        "        jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Mini-documentaci√≥n de decisiones para el informe\n",
        "notas = f\"\"\"# Preprocesamiento y Segmentaci√≥n (borrador)\n",
        "\n",
        "**Normalizaci√≥n aplicada**\n",
        "- Unicode NFC (mantener tildes correctas).\n",
        "- Limpieza de caracteres de control (excepto \\\\n y \\\\t).\n",
        "- Estandarizaci√≥n de comillas/guiones (‚Äú ‚Äù ‚Äò ‚Äô ‚Äî ‚Äì ‚Ä¶ ‚Üí \" ' - ...).\n",
        "- Uni√≥n de palabras cortadas por guion al fin de l√≠nea (e.g., \"infor-\\\\nmaci√≥n\" ‚Üí \"informaci√≥n\").\n",
        "- Colapso de espacios y saltos en blanco excesivos.\n",
        "- Conversi√≥n a min√∫sculas (para comparar segmentaciones bajo mismas condiciones).\n",
        "\n",
        "**Segmentaci√≥n A ‚Äì Chunks fijos**\n",
        "- Tama√±o ‚âà {CHUNK_WORDS} palabras, solapamiento ‚âà {CHUNK_OVERLAP}.\n",
        "- Ventajas: control de longitud, √∫til para evaluaci√≥n reproducible.\n",
        "- Desventajas: puede cortar ideas a mitad.\n",
        "\n",
        "**Segmentaci√≥n B ‚Äì Encabezados/Secciones**\n",
        "- Reglas: detec. de Abstract/Resumen/Introducci√≥n/Conclusi√≥n/Referencias, numerales (1., 2., ...), romanos (I., II., ...), t√≠tulos en MAY√öSCULAS.\n",
        "- Se fusionan secciones muy cortas (<120 palabras) con la siguiente para asegurar contexto m√≠nimo.\n",
        "- Ventajas: mantiene unidades sem√°nticas; Desventajas: depende de patrones editoriales.\n",
        "\n",
        "**Salidas**\n",
        "- base_documentos.jsonl / .parquet: texto completo normalizado por documento + metadata (autor/fecha/tema).\n",
        "- seg_a.jsonl / seg_b.jsonl: fragmentos con `id_doc`, `chunk_id`, `segmentacion`, `idx`, `texto`, y metadata.\n",
        "- txt_por_doc/: √∫til para inspecci√≥n r√°pida o depurar PDF problem√°ticos.\n",
        "\n",
        "**Razonamiento para comparaci√≥n**\n",
        "- A: garantiza tama√±o estable ‚Üí resultados de recuperaci√≥n comparables.\n",
        "- B: favorece coherencia sem√°ntica ‚Üí potencialmente mejor grounding.\n",
        "- Compa√±ero 2 debe crear dos √≠ndices (A y B) y comparar m√©tricas (recall@k, precisi√≥n manual, tiempo respuesta).\n",
        "\"\"\"\n",
        "with open(NOTAS_PREPROC, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(notas)\n",
        "\n",
        "print(\"‚úÖ Extracci√≥n, normalizaci√≥n y segmentaci√≥n listas\")\n",
        "print(f\"üßæ Base (JSONL):  {BASE_DOCS_JSONL}\")\n",
        "print(f\"üß± Base (Parquet): {BASE_DOCS_PARQUET}\")\n",
        "print(f\"üîπ Seg A (JSONL):  {SEG_A_JSONL}\")\n",
        "print(f\"üî∏ Seg B (JSONL):  {SEG_B_JSONL}\")\n",
        "print(f\"üóíÔ∏è  Notas:          {NOTAS_PREPROC}\")\n",
        "if errores:\n",
        "    print(f\"‚ö†Ô∏è Incidencias ({len(errores)}):\")\n",
        "    for e in errores[:12]:\n",
        "        print(\"   -\", e)\n",
        "    if len(errores) > 12:\n",
        "        print(\"   ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLmQP8kv_Zkx",
        "outputId": "237d82d3-ae09-4bd8-d663-a797aafb4a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extracci√≥n, normalizaci√≥n y segmentaci√≥n listas\n",
            "üßæ Base (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/base_documentos.jsonl\n",
            "üß± Base (Parquet): /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/base_documentos.parquet\n",
            "üîπ Seg A (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_a.jsonl\n",
            "üî∏ Seg B (JSONL):  /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_b.jsonl\n",
            "üóíÔ∏è  Notas:          /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/preprocesamiento_decisiones.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# üîö CIERRE Priscilla ‚Äì VERIFICACI√ìN FINAL Y RESUMEN\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Rutas de salida esperadas\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "paths = {\n",
        "    \"Metadata\": METADATA_FILE,\n",
        "    \"Base documentos JSONL\": os.path.join(OUT_DIR, \"base_documentos.jsonl\"),\n",
        "    \"Base documentos Parquet\": os.path.join(OUT_DIR, \"base_documentos.parquet\"),\n",
        "    \"Segmentaci√≥n A (chunks fijos)\": os.path.join(OUT_DIR, \"seg_a.jsonl\"),\n",
        "    \"Segmentaci√≥n B (encabezados)\": os.path.join(OUT_DIR, \"seg_b.jsonl\"),\n",
        "    \"Notas de preprocesamiento\": os.path.join(OUT_DIR, \"preprocesamiento_decisiones.md\"),\n",
        "}\n",
        "\n",
        "print(\"üìÇ Verificando archivos generados...\\n\")\n",
        "for nombre, ruta in paths.items():\n",
        "    if os.path.exists(ruta):\n",
        "        print(f\"‚úÖ {nombre}: encontrado ({os.path.basename(ruta)})\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {nombre}: NO encontrado -> {ruta}\")\n",
        "\n",
        "# Conteo r√°pido de PDFs y documentos base\n",
        "pdfs = [f for f in os.listdir(PDFS_DIR) if f.lower().endswith(\".pdf\")]\n",
        "pdf_count = len(pdfs)\n",
        "base_path = paths[\"Base documentos JSONL\"]\n",
        "base_count = sum(1 for _ in open(base_path, encoding=\"utf-8\")) if os.path.exists(base_path) else 0\n",
        "\n",
        "print(f\"\\nüìä PDFs reales: {pdf_count}\")\n",
        "print(f\"üìÑ Documentos procesados: {base_count}\")\n",
        "\n",
        "if pdf_count == base_count:\n",
        "    print(\"\\n‚úÖ Todos los documentos fueron procesados correctamente.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Hay diferencias entre PDFs y registros procesados. Revisa nombres o metadatos.\")\n",
        "\n",
        "print(\"\\nüß≠ RESUMEN PARA David:\")\n",
        "print(\"\"\"\n",
        "Los datos est√°n listos para generar embeddings:\n",
        "\n",
        "- dataset/seg_a.jsonl ‚Üí fragmentos con segmentaci√≥n A (chunks fijos)\n",
        "- dataset/seg_b.jsonl ‚Üí fragmentos con segmentaci√≥n B (por encabezados)\n",
        "- MetadataRAW.csv (o metadata.csv) ‚Üí autor, fecha, tema por documento\n",
        "\n",
        "Archivos opcionales:\n",
        "- base_documentos.jsonl/.parquet ‚Üí textos completos normalizados\n",
        "- preprocesamiento_decisiones.md ‚Üí descripci√≥n de limpieza y segmentaci√≥n\n",
        "\n",
        "David debe usar seg_a.jsonl y seg_b.jsonl\n",
        "para crear las dos bases vectoriales y comparar su rendimiento.\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE9pHFW5QZ0P",
        "outputId": "f14ef649-ccc1-4caa-e85d-1a1bd3c1fde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Verificando archivos generados...\n",
            "\n",
            "‚úÖ Metadata: encontrado (MetadataRAW.csv)\n",
            "‚úÖ Base documentos JSONL: encontrado (base_documentos.jsonl)\n",
            "‚úÖ Base documentos Parquet: encontrado (base_documentos.parquet)\n",
            "‚úÖ Segmentaci√≥n A (chunks fijos): encontrado (seg_a.jsonl)\n",
            "‚úÖ Segmentaci√≥n B (encabezados): encontrado (seg_b.jsonl)\n",
            "‚úÖ Notas de preprocesamiento: encontrado (preprocesamiento_decisiones.md)\n",
            "\n",
            "üìä PDFs reales: 46\n",
            "üìÑ Documentos procesados: 46\n",
            "\n",
            "‚úÖ Todos los documentos fueron procesados correctamente.\n",
            "\n",
            "üß≠ RESUMEN PARA David:\n",
            "\n",
            "Los datos est√°n listos para generar embeddings:\n",
            "\n",
            "- dataset/seg_a.jsonl ‚Üí fragmentos con segmentaci√≥n A (chunks fijos)\n",
            "- dataset/seg_b.jsonl ‚Üí fragmentos con segmentaci√≥n B (por encabezados)\n",
            "- MetadataRAW.csv (o metadata.csv) ‚Üí autor, fecha, tema por documento\n",
            "\n",
            "Archivos opcionales:\n",
            "- base_documentos.jsonl/.parquet ‚Üí textos completos normalizados\n",
            "- preprocesamiento_decisiones.md ‚Üí descripci√≥n de limpieza y segmentaci√≥n\n",
            "\n",
            "David debe usar seg_a.jsonl y seg_b.jsonl\n",
            "para crear las dos bases vectoriales y comparar su rendimiento.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Paso 2: Configuraci√≥n y carga de datos segmentados (limpio)\n",
        "# ============================================================\n",
        "\n",
        "from typing import List, Dict\n",
        "import os, json\n",
        "\n",
        "# Rutas de datos (definidas por Compa√±ero 1)\n",
        "OUT_DIR = os.path.join(PROYECTO_DIR, \"dataset\")\n",
        "SEG_A_JSONL = os.path.join(OUT_DIR, \"seg_a.jsonl\")\n",
        "SEG_B_JSONL = os.path.join(OUT_DIR, \"seg_b.jsonl\")\n",
        "\n",
        "# Verificar que los archivos existen\n",
        "if not os.path.exists(SEG_A_JSONL):\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ {SEG_A_JSONL}. Aseg√∫rate de que Compa√±ero 1 complet√≥ su parte.\")\n",
        "if not os.path.exists(SEG_B_JSONL):\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ {SEG_B_JSONL}. Aseg√∫rate de que Compa√±ero 1 complet√≥ su parte.\")\n",
        "\n",
        "print(\"Archivos de segmentaci√≥n encontrados:\")\n",
        "print(f\"- Segmentaci√≥n A: {SEG_A_JSONL}\")\n",
        "print(f\"- Segmentaci√≥n B: {SEG_B_JSONL}\")\n",
        "\n",
        "def load_segmented_data(jsonl_path: str) -> List[Dict]:\n",
        "    \"\"\"Carga los datos segmentados desde un archivo JSONL.\"\"\"\n",
        "    data = []\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Cargar datos\n",
        "seg_a_data = load_segmented_data(SEG_A_JSONL)\n",
        "seg_b_data = load_segmented_data(SEG_B_JSONL)\n",
        "\n",
        "print(\"\\nDatos cargados:\")\n",
        "print(f\"- Segmentaci√≥n A: {len(seg_a_data)} fragmentos\")\n",
        "print(f\"- Segmentaci√≥n B: {len(seg_b_data)} fragmentos\")\n",
        "\n",
        "# Mostrar ejemplo de un fragmento\n",
        "if seg_a_data:\n",
        "    ejemplo = seg_a_data[0]\n",
        "    print(\"\\nEjemplo de fragmento (Segmentaci√≥n A):\")\n",
        "    print(f\"- chunk_id: {ejemplo.get('chunk_id', 'N/A')}\")\n",
        "    print(f\"- id_doc: {ejemplo.get('id_doc', 'N/A')}\")\n",
        "    print(f\"- autor: {ejemplo.get('autor', 'N/A')}\")\n",
        "    print(f\"- texto (primeros 100 chars): {ejemplo.get('texto', '')[:100]}‚Ä¶\")\n"
      ],
      "metadata": {
        "id": "VLJfwo7E--ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c01c7d-8e4f-4f96-9531-67f408bc803a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos de segmentaci√≥n encontrados:\n",
            "- Segmentaci√≥n A: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_a.jsonl\n",
            "- Segmentaci√≥n B: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/seg_b.jsonl\n",
            "\n",
            "Datos cargados:\n",
            "- Segmentaci√≥n A: 227 fragmentos\n",
            "- Segmentaci√≥n B: 349 fragmentos\n",
            "\n",
            "Ejemplo de fragmento (Segmentaci√≥n A):\n",
            "- chunk_id: DOC_033_A_000\n",
            "- id_doc: DOC_033\n",
            "- autor: Mar√≠a Jos√© Chac√≥n Rodr√≠guez\n",
            "- texto (primeros 100 chars): apuntes ia clase 7/10 gianmarco oporta pe'rez ingenier'ƒ±a en computacio'n instituto tecnolo'gico de ‚Ä¶\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Paso 3: Tokenizaci√≥n y generaci√≥n de embeddings (estable)\n",
        "# Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
        "# ============================================================\n",
        "\n",
        "import warnings, sys\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tiktoken\n",
        "\n",
        "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
        "    try:\n",
        "        enc = tiktoken.encoding_for_model(model)\n",
        "        return len(enc.encode(text))\n",
        "    except Exception:\n",
        "        return max(1, len(text) // 4)\n",
        "\n",
        "print(\"Configurando modelo de embeddings...\")\n",
        "print(\"Modelo objetivo: sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(\"Dimensi√≥n esperada: 384\")\n",
        "\n",
        "# Detectar dispositivo\n",
        "try:\n",
        "    import torch\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "except Exception:\n",
        "    device = \"cpu\"\n",
        "print(\"Dispositivo:\", device.upper())\n",
        "\n",
        "embeddings_model = None\n",
        "\n",
        "# 1) Intento: wrapper oficial de LangChain\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    embeddings_model = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={\"device\": device},\n",
        "        encode_kwargs={\"normalize_embeddings\": True}\n",
        "    )\n",
        "    _ = embeddings_model.embed_query(\"probe\")\n",
        "    print(\"‚úÖ Embeddings v√≠a langchain_huggingface OK\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Falla en HuggingFaceEmbeddings:\", repr(e))\n",
        "    print(\"Activando fallback con SentenceTransformer...\")\n",
        "\n",
        "# 2) Fallback: usar SentenceTransformer directo y adaptarlo\n",
        "if embeddings_model is None:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    st_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
        "\n",
        "    class STEmbeddingsAdapter:\n",
        "        def __init__(self, model): self.model = model\n",
        "        def embed_query(self, text: str):\n",
        "            return self.model.encode([text], normalize_embeddings=True)[0].tolist()\n",
        "        def embed_documents(self, texts):\n",
        "            return self.model.encode(list(texts), normalize_embeddings=True).tolist()\n",
        "\n",
        "    embeddings_model = STEmbeddingsAdapter(st_model)\n",
        "    _ = embeddings_model.embed_query(\"probe\")\n",
        "    print(\"‚úÖ Embeddings v√≠a SentenceTransformer (fallback) OK\")\n",
        "\n",
        "print(\"‚úÖ Embeddings listos en\", device.upper())\n",
        "\n",
        "# Tokenizaci√≥n de ejemplo\n",
        "if 'seg_a_data' in globals() and seg_a_data:\n",
        "    ejemplo_texto = seg_a_data[0].get(\"texto\", \"\")\n",
        "    tk = count_tokens(ejemplo_texto)\n",
        "    print(\"\\nTokenizaci√≥n de ejemplo:\")\n",
        "    print(f\"- Longitud: {len(ejemplo_texto)} chars  |  Tokens aprox: {tk}  |  L√≠mite ~8000\")\n",
        "\n",
        "# Prueba de generaci√≥n de embedding\n",
        "print(\"\\nProbando generaci√≥n de embedding...\")\n",
        "test_text = \"Este es un texto de prueba para verificar que los embeddings funcionan correctamente.\"\n",
        "emb = embeddings_model.embed_query(test_text)\n",
        "print(\"‚úÖ Embedding OK | Dimensi√≥n:\", len(emb))\n",
        "print(\"Primeros 5 valores:\", emb[:5])\n"
      ],
      "metadata": {
        "id": "DJI3u-Sp_CBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458e4ae0-cd1b-4bd0-8dcf-38e63613316f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configurando modelo de embeddings...\n",
            "Modelo objetivo: sentence-transformers/all-MiniLM-L6-v2\n",
            "Dimensi√≥n esperada: 384\n",
            "Dispositivo: CPU\n",
            "‚úÖ Embeddings v√≠a langchain_huggingface OK\n",
            "‚úÖ Embeddings listos en CPU\n",
            "\n",
            "Tokenizaci√≥n de ejemplo:\n",
            "- Longitud: 3170 chars  |  Tokens aprox: 822  |  L√≠mite ~8000\n",
            "\n",
            "Probando generaci√≥n de embedding...\n",
            "‚úÖ Embedding OK | Dimensi√≥n: 384\n",
            "Primeros 5 valores: [-0.02066517062485218, 0.02713960036635399, -0.03559556230902672, -0.028662530705332756, -0.03590256720781326]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**modulo conflictivo paso 4** error de incompatibilidad de librerias faiss"
      ],
      "metadata": {
        "id": "zq3O4s3cHlEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Paso 4 (robusto, sin cambiar versiones): FAISS con fallback\n",
        "# Import seguro + salidas limpias (LangChain 1.x)\n",
        "# ============================================================\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "import numpy as np, os, shutil, json\n",
        "\n",
        "# LangChain 1.x: Document viene de langchain_core\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# --- Config de verbosidad (dejar en False para salidas limpias) ---\n",
        "VERBOSE = False\n",
        "TRY_FAISS = False  # dejar en False en este entorno (NumPy 2.x)\n",
        "\n",
        "# ---------- 0) Carga segura de FAISS y wrappers LC (condicional) ----------\n",
        "def _load_faiss(try_faiss: bool):\n",
        "    if not try_faiss:\n",
        "        return None\n",
        "    try:\n",
        "        import faiss as _faiss\n",
        "        _ = _faiss.IndexFlatIP(4)  # sanity check\n",
        "        return _faiss\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "_FAISS = _load_faiss(TRY_FAISS)\n",
        "\n",
        "# Importar wrappers de langchain_community solo si intentaremos FAISS\n",
        "LCFAISS = None\n",
        "InMemoryDocstore = None\n",
        "if _FAISS is not None:\n",
        "    try:\n",
        "        from langchain_community.vectorstores import FAISS as LCFAISS\n",
        "        from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "    except Exception:\n",
        "        LCFAISS = None\n",
        "        InMemoryDocstore = None\n",
        "\n",
        "# ---------- 1) Saneo de documentos ----------\n",
        "def create_documents_from_segments(segments: List[Dict[str, Any]]) -> List[Document]:\n",
        "    docs: List[Document] = []\n",
        "    for seg in segments:\n",
        "        txt = seg.get(\"texto\", \"\") or \"\"\n",
        "        if not isinstance(txt, str): txt = str(txt)\n",
        "        txt = txt.strip()\n",
        "        if not txt: continue\n",
        "        meta = {\n",
        "            \"id_doc\": seg.get(\"id_doc\", \"\"),\n",
        "            \"chunk_id\": seg.get(\"chunk_id\", \"\"),\n",
        "            \"segmentacion\": seg.get(\"segmentacion\", \"\"),\n",
        "            \"idx\": int(seg.get(\"idx\", 0) or 0),\n",
        "            \"autor\": seg.get(\"autor\", \"\"),\n",
        "            \"fecha\": seg.get(\"fecha\", \"\"),\n",
        "            \"tema\": seg.get(\"tema\", \"\"),\n",
        "            \"nombre_archivo\": seg.get(\"nombre_archivo\", \"\"),\n",
        "        }\n",
        "        docs.append(Document(page_content=txt, metadata=meta))\n",
        "    return docs\n",
        "\n",
        "# ---------- 2) Conversi√≥n robusta de embeddings ----------\n",
        "def to_float32_c_contiguous(vecs) -> np.ndarray:\n",
        "    import numpy as _np\n",
        "    try:\n",
        "        import torch as _torch\n",
        "    except Exception:\n",
        "        _torch = None\n",
        "\n",
        "    if vecs is None:\n",
        "        raise ValueError(\"Embedder devolvi√≥ None\")\n",
        "\n",
        "    if _torch is not None and isinstance(vecs, _torch.Tensor):\n",
        "        arr = vecs.detach().cpu().to(_torch.float32).numpy()\n",
        "        arr = _np.ascontiguousarray(arr, dtype=_np.float32)\n",
        "        if not _np.isfinite(arr).all(): raise ValueError(\"NaN/Inf en embeddings\")\n",
        "        return arr\n",
        "\n",
        "    if isinstance(vecs, (list, tuple)):\n",
        "        if len(vecs) == 0: raise ValueError(\"Lista de embeddings vac√≠a\")\n",
        "        rows, d = [], None\n",
        "        for i, v in enumerate(vecs):\n",
        "            if v is None: raise ValueError(f\"Fila {i} es None\")\n",
        "            if _torch is not None and hasattr(v, \"detach\") and hasattr(v, \"cpu\"):\n",
        "                v = v.detach().cpu().numpy()\n",
        "            v = _np.asarray(v, dtype=_np.float32).reshape(-1)\n",
        "            if d is None: d = v.shape[0]\n",
        "            elif v.shape[0] != d: raise ValueError(f\"Dimensiones inconsistentes en fila {i}: {v.shape[0]} vs {d}\")\n",
        "            rows.append(v)\n",
        "        arr = _np.vstack(rows).astype(_np.float32, copy=False)\n",
        "        arr = _np.ascontiguousarray(arr, dtype=_np.float32)\n",
        "        if not _np.isfinite(arr).all(): raise ValueError(\"NaN/Inf en embeddings\")\n",
        "        return arr\n",
        "\n",
        "    arr = _np.asarray(vecs, dtype=_np.float32)\n",
        "    arr = _np.ascontiguousarray(arr, dtype=_np.float32)\n",
        "    if arr.ndim != 2: raise ValueError(f\"Embeddings ndim={arr.ndim}, esperado 2\")\n",
        "    if not _np.isfinite(arr).all(): raise ValueError(\"NaN/Inf en embeddings\")\n",
        "    return arr\n",
        "\n",
        "# ---------- 3) Backend de respaldo (NumPy Cosine) ----------\n",
        "@dataclass\n",
        "class SimpleDoc:\n",
        "    text: str\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "class NumpyCosineVS:\n",
        "    \"\"\"VectorStore m√≠nimo compatible con similarity_search_with_score/save/load.\"\"\"\n",
        "    def __init__(self, embeddings, docs: List[Document], X: np.ndarray):\n",
        "        self.embeddings = embeddings\n",
        "        self.docs = [SimpleDoc(d.page_content, dict(d.metadata)) for d in docs]\n",
        "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1.0\n",
        "        self.X = (X / norms).astype(np.float32, copy=False)\n",
        "\n",
        "    def similarity_search_with_score(self, query: str, k: int = 5):\n",
        "        qv = self.embeddings.embed_query(query)\n",
        "        qv = np.asarray(qv, dtype=np.float32).reshape(1, -1)\n",
        "        qn = qv / max(np.linalg.norm(qv), 1e-12)\n",
        "        scores = (self.X @ qn.T).reshape(-1)\n",
        "        k = max(1, min(k, len(scores)))\n",
        "        idx = np.argpartition(-scores, kth=k-1)[:k]\n",
        "        idx = idx[np.argsort(-scores[idx])]\n",
        "        return [(Document(page_content=self.docs[i].text, metadata=self.docs[i].metadata),\n",
        "                 float(scores[i])) for i in idx]\n",
        "\n",
        "    def save_local(self, dirpath: str):\n",
        "        os.makedirs(dirpath, exist_ok=True)\n",
        "        np.save(os.path.join(dirpath, \"vectors.npy\"), self.X)\n",
        "        with open(os.path.join(dirpath, \"meta.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            for d in self.docs:\n",
        "                f.write(json.dumps({\"text\": d.text, \"metadata\": d.metadata}, ensure_ascii=False) + \"\\n\")\n",
        "        with open(os.path.join(dirpath, \"_backend.txt\"), \"w\") as f:\n",
        "            f.write(\"numpy\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_local(cls, dirpath: str, embeddings):\n",
        "        X = np.load(os.path.join(dirpath, \"vectors.npy\"))\n",
        "        docs = []\n",
        "        with open(os.path.join(dirpath, \"meta.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                docs.append(Document(page_content=obj[\"text\"], metadata=obj[\"metadata\"]))\n",
        "        return cls(embeddings=embeddings, docs=docs, X=X)\n",
        "\n",
        "# ---------- 4) Intento FAISS con fallback silencioso ----------\n",
        "def _try_faiss_index(X: np.ndarray, docs: List[Document], embedder, normalize: bool):\n",
        "    \"\"\"\n",
        "    Devuelve (vs, backend) usando FAISS si _FAISS y LCFAISS est√°n disponibles.\n",
        "    Si no lo est√°n, devuelve (None, None) para activar NumPy.\n",
        "    \"\"\"\n",
        "    if _FAISS is None or LCFAISS is None or InMemoryDocstore is None:\n",
        "        return None, None\n",
        "    try:\n",
        "        index = _FAISS.IndexFlatIP(X.shape[1])\n",
        "        Xreq = np.require(X, dtype=np.float32, requirements=[\"C\", \"A\", \"W\"])\n",
        "        try:\n",
        "            index.add(Xreq)\n",
        "        except Exception:\n",
        "            Xcopy = np.array(Xreq, dtype=np.float32, copy=True, order=\"C\")\n",
        "            index.add(Xcopy)\n",
        "\n",
        "        id_map = {str(i): doc for i, doc in enumerate(docs)}\n",
        "        docstore = InMemoryDocstore(id_map)\n",
        "        index_to_docstore_id = {i: str(i) for i in range(X.shape[0])}\n",
        "        vs = LCFAISS(\n",
        "            embedding_function=embedder,\n",
        "            index=index,\n",
        "            docstore=docstore,\n",
        "            index_to_docstore_id=index_to_docstore_id,\n",
        "            normalize_L2=normalize,\n",
        "        )\n",
        "        return vs, \"faiss\"\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def build_vs_with_faiss_or_fallback(docs: List[Document], embedder, normalize=True):\n",
        "    if not docs:\n",
        "        raise ValueError(\"No hay documentos para indexar.\")\n",
        "    texts = [d.page_content for d in docs]\n",
        "    vecs = embedder.embed_documents(texts)\n",
        "    X = to_float32_c_contiguous(vecs)\n",
        "    if normalize:\n",
        "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1.0\n",
        "        X = (X / norms).astype(np.float32, copy=False)\n",
        "\n",
        "    vs, backend = _try_faiss_index(X, docs, embedder, normalize)\n",
        "    if vs is not None:\n",
        "        return vs, backend\n",
        "    return NumpyCosineVS(embeddings=embedder, docs=docs, X=X), \"numpy\"\n",
        "\n",
        "# ---------- 5) Creaci√≥n y guardado de A/B ----------\n",
        "VECTORSTORE_DIR_A = os.path.join(OUT_DIR, \"vectorstore_a\")\n",
        "VECTORSTORE_DIR_B = os.path.join(OUT_DIR, \"vectorstore_b\")\n",
        "for p in [VECTORSTORE_DIR_A, VECTORSTORE_DIR_B]:\n",
        "    if os.path.exists(p): shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "print(\"Creando bases vectoriales‚Ä¶\")\n",
        "try:\n",
        "    docs_a = create_documents_from_segments(seg_a_data)\n",
        "    vectorstore_a, backend_a = build_vs_with_faiss_or_fallback(docs_a, embeddings_model, normalize=True)\n",
        "    vectorstore_a.save_local(VECTORSTORE_DIR_A)\n",
        "    print(f\"‚úì A creada y guardada ({len(docs_a)} docs) | backend: {backend_a}\")\n",
        "\n",
        "    docs_b = create_documents_from_segments(seg_b_data)\n",
        "    vectorstore_b, backend_b = build_vs_with_faiss_or_fallback(docs_b, embeddings_model, normalize=True)\n",
        "    vectorstore_b.save_local(VECTORSTORE_DIR_B)\n",
        "    print(f\"‚úì B creada y guardada ({len(docs_b)} docs) | backend: {backend_b}\")\n",
        "\n",
        "    print(\"‚úì Ambos vectorstores listos.\")\n",
        "except Exception as e:\n",
        "    print(\"‚úó Error creando vectorstores:\", e)\n",
        "    raise\n",
        "\n",
        "# ---------- 6) Prueba de b√∫squeda (salida breve) ----------\n",
        "test_query = \"inteligencia artificial y aprendizaje autom√°tico\"\n",
        "try:\n",
        "    res_a = vectorstore_a.similarity_search_with_score(test_query, k=3)\n",
        "    res_b = vectorstore_b.similarity_search_with_score(test_query, k=3)\n",
        "\n",
        "    print(f\"\\nConsulta: ‚Äú{test_query}‚Äù\")\n",
        "    print(\"Segmentaci√≥n A:\")\n",
        "    for i, (doc, score) in enumerate(res_a, 1):\n",
        "        autor = doc.metadata.get(\"autor\", \"N/A\")\n",
        "        resumen = (doc.page_content[:100].replace(\"\\n\",\" \") + \"‚Ä¶\") if len(doc.page_content) > 100 else doc.page_content\n",
        "        print(f\"  {i}. score={score:.4f} | autor={autor} | {resumen}\")\n",
        "\n",
        "    print(\"\\nSegmentaci√≥n B:\")\n",
        "    for i, (doc, score) in enumerate(res_b, 1):\n",
        "        autor = doc.metadata.get(\"autor\", \"N/A\")\n",
        "        resumen = (doc.page_content[:100].replace(\"\\n\",\" \") + \"‚Ä¶\") if len(doc.page_content) > 100 else doc.page_content\n",
        "        print(f\"  {i}. score={score:.4f} | autor={autor} | {resumen}\")\n",
        "\n",
        "    print(f\"\\nBackends: A={backend_a}, B={backend_b}\")\n",
        "except Exception as e:\n",
        "    print(\"‚úó Error en b√∫squeda:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irw2GKdzzUa7",
        "outputId": "a7b489ed-372e-483a-de52-37ea25263eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando bases vectoriales‚Ä¶\n",
            "‚úì A creada y guardada (227 docs) | backend: numpy\n",
            "‚úì B creada y guardada (349 docs) | backend: numpy\n",
            "‚úì Ambos vectorstores listos.\n",
            "\n",
            "Consulta: ‚Äúinteligencia artificial y aprendizaje autom√°tico‚Äù\n",
            "Segmentaci√≥n A:\n",
            "  1. score=0.6387 | autor=Andrey Ure√±a Berm√∫dez | de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerz‚Ä¶\n",
            "  2. score=0.6069 | autor=Luis Alfredo Gonz√°lez S√°nchez | notas de clase inteligenciaartificial-12deagosto-semana2 luis alfredo gonza'lez sa'nchez escuela de ‚Ä¶\n",
            "  3. score=0.5983 | autor=Rodolfo David Acu√±a L√≥pez | - sistema mostrando comportamiento \"inteligente\" 1980s - algo \"inteligente\" que resuelva tareas comp‚Ä¶\n",
            "\n",
            "Segmentaci√≥n B:\n",
            "  1. score=0.6504 | autor=Andrey Ure√±a Berm√∫dez | vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosd‚Ä¶\n",
            "  2. score=0.6386 | autor=Rodolfo David Acu√±a L√≥pez | references [1] apuntes de la clase de inteligencia artificial, profesor steven andrey  pachecoportug‚Ä¶\n",
            "  3. score=0.6221 | autor=Priscilla Jim√©nez Salgado | la inteligencia artificialtiene aplicaciones en una gran variedad de a'reas. algunos ejemplos destac‚Ä¶\n",
            "\n",
            "Backends: A=numpy, B=numpy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 5: Crear RAG Tool en LangChain\n",
        "# La herramienta consulta la base vectorial y devuelve:\n",
        "# - fragmento (texto)\n",
        "# - documento de origen (id_doc, nombre_archivo)\n",
        "# - autor\n",
        "# ============================================================\n",
        "\n",
        "from typing import Any, List, Tuple, Dict, Optional\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import inspect\n",
        "from langchain_core.tools import Tool  # Para LangChain 0.2; si usas 0.1, cambia a langchain.tools\n",
        "\n",
        "# ---- Helpers de parsing ------------------------------------------------------\n",
        "\n",
        "def _parse_query_and_filters(raw_query: str) -> tuple[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Extrae filtros de la query y devuelve (texto_libre, filtros_dict).\n",
        "    Formatos admitidos (espacios opcionales):\n",
        "      - autor:\"Alan Turing\"   |  autor: Alan Turing\n",
        "      - fecha>=2024-01-01     |  fecha<=2025-12-31 (ISO YYYY-MM-DD)\n",
        "      - tema:\"redes neuronales\" | tema: redes neuronales\n",
        "      - tags: audio, cnn\n",
        "    \"\"\"\n",
        "    q = raw_query.strip()\n",
        "\n",
        "    # Mapa de alias -> campo real de metadata (ajusta seg√∫n tus metadatos)\n",
        "    keymap = {\n",
        "        \"autor\": \"autor\",\n",
        "        \"author\": \"autor\",\n",
        "        \"fecha\": \"fecha\",\n",
        "        \"date\": \"fecha\",\n",
        "        \"tema\": \"tema\",\n",
        "        \"topic\": \"tema\",\n",
        "        \"tags\": \"tags\",\n",
        "        \"etiquetas\": \"tags\"\n",
        "    }\n",
        "\n",
        "    # Captura patrones key op value (op: :, =, >=, <=, >, <)\n",
        "    # Ej.: autor:\"Alan Turing\", fecha>=2024-01-01, tema: redes neuronales\n",
        "    token_pat = re.compile(\n",
        "        r'(?P<k>\\w+)\\s*(?P<op>:|=|>=|<=|>|<)\\s*(?P<v>\"[^\"]+\"|\\'[^\\']+\\'|[^,;]+)'\n",
        "    )\n",
        "\n",
        "    filters: Dict[str, Any] = {}\n",
        "    consumed_spans = []\n",
        "\n",
        "    for m in token_pat.finditer(q):\n",
        "        k_raw = m.group(\"k\").lower()\n",
        "        op = m.group(\"op\")\n",
        "        v_raw = m.group(\"v\").strip()\n",
        "\n",
        "        # Limpia comillas\n",
        "        if len(v_raw) >= 2 and ((v_raw[0] == '\"' and v_raw[-1] == '\"') or (v_raw[0] == \"'\" and v_raw[-1] == \"'\")):\n",
        "            v = v_raw[1:-1].strip()\n",
        "        else:\n",
        "            v = v_raw.strip()\n",
        "\n",
        "        # Normaliza clave\n",
        "        k = keymap.get(k_raw)\n",
        "        if not k:\n",
        "            continue  # clave desconocida -> la ignoramos\n",
        "\n",
        "        # Guardamos como lista de condiciones por clave (p.ej., m√∫ltiples tags)\n",
        "        if k not in filters:\n",
        "            filters[k] = []\n",
        "\n",
        "        # Procesamos valores especiales\n",
        "        if k == \"tags\":\n",
        "            # Separar por comas y limpiar\n",
        "            tags = [t.strip() for t in re.split(r'[;,]', v) if t.strip()]\n",
        "            filters[k].append((\"in\", tags))\n",
        "        elif k == \"fecha\":\n",
        "            # Intentamos parsear fecha ISO\n",
        "            try:\n",
        "                _ = datetime.fromisoformat(v)\n",
        "            except Exception:\n",
        "                # si no es ISO, lo tratamos como string literal\n",
        "                pass\n",
        "            filters[k].append((op, v))\n",
        "        else:\n",
        "            # texto exacto/contiene (para op \":\" interpretamos \"contiene\")\n",
        "            if op == \":\":\n",
        "                filters[k].append((\"contains\", v))\n",
        "            elif op == \"=\":\n",
        "                filters[k].append((\"eq\", v))\n",
        "            else:\n",
        "                # Para autor/tema normalmente no tiene sentido > <, pero lo admitimos como \"eq\" por compatibilidad\n",
        "                filters[k].append((\"eq\", v))\n",
        "\n",
        "        consumed_spans.append(m.span())\n",
        "\n",
        "    # Remueve las partes ‚Äúconsumidas‚Äù de la query para dejar el texto libre\n",
        "    free_text_parts = []\n",
        "    last_end = 0\n",
        "    for (start, end) in consumed_spans:\n",
        "        # a√±ade el texto entre patrones\n",
        "        free_text_parts.append(q[last_end:start])\n",
        "        last_end = end\n",
        "    free_text_parts.append(q[last_end:])\n",
        "    free_text = \" \".join(p.strip() for p in free_text_parts).strip()\n",
        "    return (free_text, filters)\n",
        "\n",
        "\n",
        "def _meta_match(meta: Dict[str, Any], filters: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Aplica filtros a un diccionario de metadata.\n",
        "    Soporta:\n",
        "      - autor: (\"contains\", \"Turing\") | (\"eq\",\"Alan Turing\")\n",
        "      - tema:  idem\n",
        "      - fecha: (\">=\", \"2024-01-01\") etc. (si ISO; si no, compara string)\n",
        "      - tags:  (\"in\", [\"audio\",\"cnn\"]) => al menos uno debe estar en meta['tags']\n",
        "    \"\"\"\n",
        "    def _as_str(x): return \"\" if x is None else str(x)\n",
        "\n",
        "    for key, conds in filters.items():\n",
        "        val = meta.get(key)\n",
        "        # Normaliza tipo\n",
        "        if key == \"tags\":\n",
        "            # meta puede ser str \"a,b,c\" o lista\n",
        "            if isinstance(val, str):\n",
        "                meta_tags = [t.strip().lower() for t in re.split(r'[;,]', val) if t.strip()]\n",
        "            elif isinstance(val, list):\n",
        "                meta_tags = [str(t).strip().lower() for t in val]\n",
        "            else:\n",
        "                meta_tags = []\n",
        "        elif key == \"fecha\":\n",
        "            # Mantener string original; si es ISO podemos parsear\n",
        "            meta_fecha = _as_str(val)\n",
        "        else:\n",
        "            meta_text = _as_str(val)\n",
        "\n",
        "        for (op, expect) in conds:\n",
        "            if key == \"tags\" and op == \"in\":\n",
        "                wanted = [t.lower() for t in expect]\n",
        "                if not any(w in meta_tags for w in wanted):\n",
        "                    return False\n",
        "            elif key in (\"autor\", \"tema\"):\n",
        "                if op == \"contains\":\n",
        "                    if expect.lower() not in meta_text.lower():\n",
        "                        return False\n",
        "                elif op == \"eq\":\n",
        "                    if meta_text.lower() != expect.lower():\n",
        "                        return False\n",
        "                else:\n",
        "                    # Otros ops no aplican; consideramos fallo\n",
        "                    return False\n",
        "            elif key == \"fecha\":\n",
        "                # intentamos comparaci√≥n temporal si ambas son ISO (YYYY-MM-DD)\n",
        "                lhs, rhs = meta_fecha, str(expect)\n",
        "                try:\n",
        "                    d_lhs = datetime.fromisoformat(lhs)\n",
        "                    d_rhs = datetime.fromisoformat(rhs)\n",
        "                    if op == \">=\" and not (d_lhs >= d_rhs): return False\n",
        "                    if op == \"<=\" and not (d_lhs <= d_rhs): return False\n",
        "                    if op == \">\"  and not (d_lhs >  d_rhs): return False\n",
        "                    if op == \"<\"  and not (d_lhs <  d_rhs): return False\n",
        "                    if op in (\":\", \"=\", \"eq\") and not (d_lhs == d_rhs): return False\n",
        "                except Exception:\n",
        "                    # Si no son ISO, compara como string\n",
        "                    if op in (\":\", \"=\", \"eq\") and not (lhs == rhs): return False\n",
        "                    if op == \">=\" and not (lhs >= rhs): return False\n",
        "                    if op == \"<=\" and not (lhs <= rhs): return False\n",
        "                    if op == \">\"  and not (lhs >  rhs): return False\n",
        "                    if op == \"<\"  and not (lhs <  rhs): return False\n",
        "            else:\n",
        "                # clave no soportada\n",
        "                return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def _format_result(i: int, score: Optional[float], seg: str, doc: Any) -> str:\n",
        "    try:\n",
        "        score_str = f\"{float(score):.4f}\" if score is not None else \"N/A\"\n",
        "    except Exception:\n",
        "        score_str = str(score) if score is not None else \"N/A\"\n",
        "\n",
        "    meta = getattr(doc, \"metadata\", {}) or {}\n",
        "    fragmento = getattr(doc, \"page_content\", \"\") or \"\"\n",
        "    preview = fragmento[:500] + (\"...\" if len(fragmento) > 500 else \"\")\n",
        "\n",
        "    id_doc         = str(meta.get(\"id_doc\", \"N/A\"))\n",
        "    nombre_archivo = str(meta.get(\"nombre_archivo\", \"N/A\"))\n",
        "    autor          = str(meta.get(\"autor\", \"N/A\"))\n",
        "    chunk_id       = str(meta.get(\"chunk_id\", \"N/A\"))\n",
        "\n",
        "    source_line = f\"Documento: {id_doc}\"\n",
        "    if nombre_archivo and nombre_archivo != \"N/A\":\n",
        "        source_line += f\" ¬∑ archivo: {nombre_archivo}\"\n",
        "    source_line += f\" ¬∑ chunk: {chunk_id}\"\n",
        "\n",
        "    return (\n",
        "        f\"[{seg} ¬∑ Resultado {i} ¬∑ score/distancia: {score_str}]\\n\"\n",
        "        f\"Fragmento:\\n{preview}\\n\"\n",
        "        f\"{source_line}\\n\"\n",
        "        f\"Autor: {autor}\"\n",
        "    )\n",
        "\n",
        "# ---- Factory del Tool con filtros -------------------------------------------\n",
        "\n",
        "def create_rag_tool(\n",
        "    vectorstore: Any,\n",
        "    segmentacion_name: str,\n",
        "    default_top_k: int = 5,\n",
        "    name_prefix: str = \"rag_search\"\n",
        ") -> Tool:\n",
        "    \"\"\"\n",
        "    Tool RAG que soporta filtros por metadata embebidos en la query:\n",
        "      - autor:\"Nombre\"\n",
        "      - fecha>=YYYY-MM-DD\n",
        "      - tema: algo\n",
        "      - tags: a, b, c\n",
        "    \"\"\"\n",
        "\n",
        "    def rag_search(user_query: str) -> str:\n",
        "        query_text, filters = _parse_query_and_filters(user_query or \"\")\n",
        "\n",
        "        if not query_text and not filters:\n",
        "            return \"No se proporcion√≥ una consulta.\"\n",
        "\n",
        "        # --- Intento 1: usar filtro nativo si el vectorstore lo admite ---\n",
        "        results: List[Tuple[Any, Optional[float]]] = []\n",
        "        used_native_filter = False\n",
        "        k_request = default_top_k\n",
        "\n",
        "        # ¬øsimilarity_search_with_score acepta 'filter'?\n",
        "        sig = None\n",
        "        try:\n",
        "            sig = inspect.signature(vectorstore.similarity_search_with_score)\n",
        "        except Exception:\n",
        "            sig = None\n",
        "\n",
        "        if sig and \"filter\" in sig.parameters and filters:\n",
        "            # Convertimos nuestros filtros a un dict simple de igualdad/contiene\n",
        "            # Nota: distintos backends aceptan diferentes operadores;\n",
        "            # aqu√≠ probamos un mapeo b√°sico (autor/tema eq|contains, tags in).\n",
        "            backend_filter = {}\n",
        "\n",
        "            # Igualdades simples (preferimos eq a contains si hay solo eq)\n",
        "            for k, conds in filters.items():\n",
        "                if k == \"tags\":\n",
        "                    # algunos backends aceptan {\"tags\": {\"$in\": [...]}}\n",
        "                    # aqu√≠ probamos un formato directo (depende del backend)\n",
        "                    values = []\n",
        "                    for (op, v) in conds:\n",
        "                        if op == \"in\":\n",
        "                            values.extend(v)\n",
        "                    if values:\n",
        "                        backend_filter[k] = values  # puede requerir adaptaci√≥n\n",
        "                elif k in (\"autor\", \"tema\"):\n",
        "                    # prioriza \"eq\" si existe, si no \"contains\"\n",
        "                    eq_val = next((v for (op, v) in conds if op == \"eq\"), None)\n",
        "                    contains_val = next((v for (op, v) in conds if op == \"contains\"), None)\n",
        "                    if eq_val is not None:\n",
        "                        backend_filter[k] = eq_val\n",
        "                    elif contains_val is not None:\n",
        "                        backend_filter[k] = contains_val\n",
        "                elif k == \"fecha\":\n",
        "                    # muchos backends no soportan rangos; lo haremos en fallback\n",
        "                    pass\n",
        "\n",
        "            try:\n",
        "                raw = vectorstore.similarity_search_with_score(\n",
        "                    query_text if query_text else \"\", k=max(k_request, 10),\n",
        "                    filter=backend_filter if backend_filter else None\n",
        "                )\n",
        "                # Normaliza a [(doc, score)]\n",
        "                if raw and isinstance(raw[0], tuple) and len(raw[0]) == 2:\n",
        "                    results = [(doc, score) for doc, score in raw]\n",
        "                elif raw:\n",
        "                    results = [(doc, None) for doc in raw]\n",
        "                used_native_filter = True\n",
        "            except Exception:\n",
        "                used_native_filter = False\n",
        "                results = []\n",
        "\n",
        "        # --- Intento 2: fallback universal (k grande + filtrado en Python) ---\n",
        "        if not results:\n",
        "            try:\n",
        "                raw = vectorstore.similarity_search_with_score(\n",
        "                    query_text if query_text else \"\", k=max(default_top_k * 10, 50)\n",
        "                )\n",
        "                if raw and isinstance(raw[0], tuple) and len(raw[0]) == 2:\n",
        "                    results = [(doc, score) for doc, score in raw]\n",
        "                elif raw:\n",
        "                    results = [(doc, None) for doc in raw]\n",
        "            except Exception:\n",
        "                # fallback extra: similarity_search sin score\n",
        "                try:\n",
        "                    raw = vectorstore.similarity_search(query_text if query_text else \"\", k=max(default_top_k * 10, 50))\n",
        "                    results = [(doc, None) for doc in raw]\n",
        "                except Exception as e:\n",
        "                    return f\"Error en la b√∫squeda RAG: {str(e)}\"\n",
        "\n",
        "            # si tenemos filtros, aplicarlos en memoria\n",
        "            if filters:\n",
        "                filtered = []\n",
        "                for (doc, sc) in results:\n",
        "                    meta = getattr(doc, \"metadata\", {}) or {}\n",
        "                    if _meta_match(meta, filters):\n",
        "                        filtered.append((doc, sc))\n",
        "                results = filtered\n",
        "\n",
        "        if not results:\n",
        "            return \"No se encontraron fragmentos relevantes para la consulta.\"\n",
        "\n",
        "        # Limitar a top-k finales\n",
        "        results = results[:default_top_k]\n",
        "\n",
        "        # Formato de salida\n",
        "        lines = []\n",
        "        for i, (doc, score) in enumerate(results, 1):\n",
        "            lines.append(_format_result(i, score, segmentacion_name, doc))\n",
        "\n",
        "        # Nota: ‚Äúscore‚Äù puede ser distancia: menor = mejor (FAISS).\n",
        "        # No asumimos que mayor sea mejor universalmente.\n",
        "        # Si usaste filtro nativo, parte del trabajo ya se hizo ‚Äúaguas arriba‚Äù.\n",
        "        return \"\\n\\n\".join(lines)\n",
        "\n",
        "    tool_name = f\"{name_prefix}_{segmentacion_name}\"\n",
        "    tool_desc = (\n",
        "        f\"B√∫squeda RAG en segmentaci√≥n {segmentacion_name} con filtros por metadata. \"\n",
        "        f\"Admite en la query: autor:\\\"Nombre\\\", fecha>=YYYY-MM-DD, tema: algo, tags: a,b,c. \"\n",
        "        f\"Devuelve fragmentos + fuente (id_doc, archivo, chunk) + autor.\"\n",
        "    )\n",
        "\n",
        "    return Tool(\n",
        "        name=tool_name,\n",
        "        description=tool_desc,\n",
        "        func=rag_search,\n",
        "        return_direct=False\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "LneHSDcq_G3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Comparaci√≥n RAG A vs B (paper-friendly, sin overlap)\n",
        "# A = chunks fijos | B = encabezados/secciones\n",
        "# Filtros: autor=\"Priscilla\" + (opcional) tema\n",
        "# Produce: m√©tricas concisas, tablas por segmento y CSV separados.\n",
        "# ============================================================\n",
        "\n",
        "import time, re, os\n",
        "from typing import List, Dict, Tuple, Any, Optional\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Crear tools por segmentaci√≥n\n",
        "# -----------------------------\n",
        "print(\"Creando herramientas RAG...\")\n",
        "rag_tool_a = create_rag_tool(vectorstore_a, segmentacion_name=\"A\", default_top_k=5)\n",
        "rag_tool_b = create_rag_tool(vectorstore_b, segmentacion_name=\"B\", default_top_k=5)\n",
        "\n",
        "print(\"Herramientas RAG creadas:\")\n",
        "print(f\"   - {rag_tool_a.name}: {rag_tool_a.description[:200]}...\")\n",
        "print(f\"   - {rag_tool_b.name}: {rag_tool_b.description[:200]}...\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Configuraci√≥n del experimento\n",
        "# -----------------------------\n",
        "AUTOR = \"Priscilla\"      # filtro obligatorio por metadata\n",
        "TEMA  = \"machine learning\"  # <-- pon None si no quieres tema (o cambia el t√©rmino)\n",
        "\n",
        "# Construimos la query (autor + tema opcional)\n",
        "if TEMA and TEMA.strip():\n",
        "    test_query = f'autor:\"{AUTOR}\" tema:\"{TEMA.strip()}\"'\n",
        "    subtitulo  = f'Filtro: autor=\"{AUTOR}\" + tema=\"{TEMA.strip()}\"'\n",
        "else:\n",
        "    test_query = f'autor:\"{AUTOR}\"'\n",
        "    subtitulo  = f'Filtro: autor=\"{AUTOR}\"'\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Helpers de ejecuci√≥n y parsing\n",
        "# -----------------------------\n",
        "def _safe_invoke(tool, query: str) -> Tuple[str, float]:\n",
        "    \"\"\"Invoca el tool (invoke/run) y mide tiempo.\"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    try:\n",
        "        out = tool.invoke(query) if hasattr(tool, \"invoke\") else tool.run(query)\n",
        "    except Exception as e:\n",
        "        out = f\"[ERROR] {e}\"\n",
        "    dt = time.perf_counter() - t0\n",
        "    return out, dt\n",
        "\n",
        "_result_header_pat = re.compile(\n",
        "    r'^\\[(?P<seg>[^|\\]]+?)\\s*¬∑\\s*Resultado\\s+(?P<idx>\\d+)\\s*¬∑\\s*score/distancia:\\s*(?P<score>[^\\]]+)\\]',\n",
        "    re.MULTILINE\n",
        ")\n",
        "_doc_line_pat = re.compile(\n",
        "    r'(?im)^Documento:\\s*(?P<id_doc>[^\\n¬∑]+)'\n",
        "    r'(?:\\s*¬∑\\s*archivo:\\s*(?P<archivo>[^\\n¬∑]+))?'\n",
        "    r'\\s*¬∑\\s*chunk:\\s*(?P<chunk_id>[^\\n]+)'\n",
        ")\n",
        "_author_line_pat = re.compile(r'(?im)^Autor:\\s*(?P<autor>.+)$')\n",
        "\n",
        "def _parse_results(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parsea salida del Tool en √≠tems estructurados:\n",
        "    segmento, resultado_idx, score/distancia, id_doc, archivo, chunk_id, autor, fragmento\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip() or text.strip().startswith(\"[ERROR]\"):\n",
        "        return []\n",
        "    items = []\n",
        "    headers = list(_result_header_pat.finditer(text))\n",
        "    if not headers:\n",
        "        return []\n",
        "    for i, m in enumerate(headers):\n",
        "        start = m.end()\n",
        "        end = headers[i + 1].start() if i + 1 < len(headers) else len(text)\n",
        "        block = text[start:end].strip()\n",
        "\n",
        "        seg   = m.group(\"seg\").strip()\n",
        "        idx   = int(m.group(\"idx\"))\n",
        "        score = m.group(\"score\").strip()\n",
        "\n",
        "        doc_m  = _doc_line_pat.search(block)\n",
        "        auth_m = _author_line_pat.search(block)\n",
        "\n",
        "        id_doc   = doc_m.group(\"id_doc\").strip() if doc_m else \"N/A\"\n",
        "        archivo  = doc_m.group(\"archivo\").strip() if (doc_m and doc_m.group(\"archivo\")) else \"N/A\"\n",
        "        chunk_id = doc_m.group(\"chunk_id\").strip() if doc_m else \"N/A\"\n",
        "        autor    = auth_m.group(\"autor\").strip() if auth_m else \"N/A\"\n",
        "\n",
        "        # fragmento sin las l√≠neas de Documento/Autor\n",
        "        frag = _doc_line_pat.sub(\"\", block)\n",
        "        frag = _author_line_pat.sub(\"\", frag)\n",
        "        frag = frag.strip()\n",
        "\n",
        "        items.append({\n",
        "            \"segmento\": seg,\n",
        "            \"resultado_idx\": idx,\n",
        "            \"score_ou_distancia\": score,\n",
        "            \"id_doc\": id_doc,\n",
        "            \"archivo\": archivo,\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"autor\": autor,\n",
        "            \"chars_fragmento\": len(frag),\n",
        "            \"fragmento\": frag\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def _metrics(items: List[Dict[str, Any]], elapsed_s: float) -> Dict[str, Any]:\n",
        "    \"\"\"M√©tricas concisas para el paper.\"\"\"\n",
        "    chars_total = sum(it.get(\"chars_fragmento\", 0) for it in items)\n",
        "    docs = {(it[\"id_doc\"] or \"N/A\").strip() for it in items}\n",
        "    chunks = {((it[\"id_doc\"] or \"N/A\").strip(), (it[\"chunk_id\"] or \"N/A\").strip()) for it in items}\n",
        "    return {\n",
        "        \"items\": len(items),\n",
        "        \"docs_unicos\": len(docs),\n",
        "        \"chunks_unicos\": len(chunks),\n",
        "        \"chars\": chars_total,\n",
        "        \"tiempo_ms\": round(elapsed_s * 1000.0, 1)\n",
        "    }\n",
        "\n",
        "def _to_df(items: List[Dict[str, Any]]) -> pd.DataFrame:\n",
        "    cols = [\"segmento\",\"resultado_idx\",\"id_doc\",\"chunk_id\",\"archivo\",\"autor\",\"score_ou_distancia\",\"chars_fragmento\",\"fragmento\"]\n",
        "    df = pd.DataFrame(items)\n",
        "    if df.empty:\n",
        "        return pd.DataFrame(columns=cols)\n",
        "    return df[cols].sort_values([\"resultado_idx\"]).reset_index(drop=True)\n",
        "\n",
        "def _print_section_title(title: str):\n",
        "    print(\"\\n\" + \"=\"*72)\n",
        "    print(title)\n",
        "    print(\"=\"*72 + \"\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Ejecutar A y B (por separado)\n",
        "# -----------------------------\n",
        "_print_section_title(\"PRUEBA EMP√çRICA A vs B (sin overlap)\")\n",
        "print(subtitulo)\n",
        "print(\"A: segmentaci√≥n por chunks fijos | B: por encabezados/secciones\\n\")\n",
        "\n",
        "raw_a, t_a = _safe_invoke(rag_tool_a, test_query)\n",
        "raw_b, t_b = _safe_invoke(rag_tool_b, test_query)\n",
        "\n",
        "items_a = _parse_results(raw_a)\n",
        "items_b = _parse_results(raw_b)\n",
        "\n",
        "metrics_a = _metrics(items_a, t_a)\n",
        "metrics_b = _metrics(items_b, t_b)\n",
        "\n",
        "df_a = _to_df(items_a)\n",
        "df_b = _to_df(items_b)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Exportar CSV por segmento\n",
        "# -----------------------------\n",
        "os.makedirs(\"rag_eval\", exist_ok=True)\n",
        "df_a.to_csv(\"rag_eval/segmento_A_resultados.csv\", index=False)\n",
        "df_b.to_csv(\"rag_eval/segmento_B_resultados.csv\", index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Reporte por segmento (paper-friendly)\n",
        "# -----------------------------\n",
        "# A)\n",
        "_print_section_title(\"RESULTADOS ‚Äî Segmento A (chunks fijos)\")\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"M√©tricas A: items={metrics_a['items']} | docs_unicos={metrics_a['docs_unicos']} | \"\n",
        "      f\"chunks_unicos={metrics_a['chunks_unicos']} | chars={metrics_a['chars']} | \"\n",
        "      f\"tiempo={metrics_a['tiempo_ms']} ms\")\n",
        "\n",
        "# Vista tabular resumida (sin mostrar fragmento completo para ser conciso)\n",
        "cols_summary = [\"resultado_idx\",\"id_doc\",\"chunk_id\",\"archivo\",\"autor\",\"score_ou_distancia\",\"chars_fragmento\"]\n",
        "print(\"\\nTabla A (resumen):\")\n",
        "print(df_a[cols_summary].to_string(index=False) if not df_a.empty else \"(sin resultados)\")\n",
        "\n",
        "# Vista corta (primer fragmento) ‚Äî √∫til para el paper (cita breve)\n",
        "if not df_a.empty:\n",
        "    fragA = df_a.loc[0, \"fragmento\"]\n",
        "    previewA = fragA[:700] + (\"...\" if len(fragA) > 700 else \"\")\n",
        "    print(\"\\nVista previa A (Top-1 fragmento, truncado):\")\n",
        "    print(previewA)\n",
        "else:\n",
        "    print(\"\\nVista previa A: (sin resultados)\")\n",
        "\n",
        "# B)\n",
        "_print_section_title(\"RESULTADOS ‚Äî Segmento B (encabezados/secciones)\")\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"M√©tricas B: items={metrics_b['items']} | docs_unicos={metrics_b['docs_unicos']} | \"\n",
        "      f\"chunks_unicos={metrics_b['chunks_unicos']} | chars={metrics_b['chars']} | \"\n",
        "      f\"tiempo={metrics_b['tiempo_ms']} ms\")\n",
        "\n",
        "print(\"\\nTabla B (resumen):\")\n",
        "print(df_b[cols_summary].to_string(index=False) if not df_b.empty else \"(sin resultados)\")\n",
        "\n",
        "if not df_b.empty:\n",
        "    fragB = df_b.loc[0, \"fragmento\"]\n",
        "    previewB = fragB[:700] + (\"...\" if len(fragB) > 700 else \"\")\n",
        "    print(\"\\nVista previa B (Top-1 fragmento, truncado):\")\n",
        "    print(previewB)\n",
        "else:\n",
        "    print(\"\\nVista previa B: (sin resultados)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Nota metodol√≥gica concisa (lista para el paper)\n",
        "# -----------------------------\n",
        "print(\"\\n[Nota metodol√≥gica]\")\n",
        "print(\n",
        "    \"Dise√±o: comparaci√≥n de recuperaci√≥n RAG en dos segmentaciones independientes: \"\n",
        "    \"A (chunks fijos) y B (encabezados/secciones). La consulta restringe por metadata \"\n",
        "    f\"({subtitulo}). Se reportan, por segmento: n√∫mero de items, documentos √∫nicos, \"\n",
        "    \"chunks √∫nicos, caracteres agregados y latencia. Los resultados se presentan por \"\n",
        "    \"separado (sin superposiciones), priorizando claridad y concisi√≥n.\"\n",
        ")\n",
        "\n",
        "print(\"\\n[CSV generados]\")\n",
        "print(\" - rag_eval/segmento_A_resultados.csv\")\n",
        "print(\" - rag_eval/segmento_B_resultados.csv\")\n"
      ],
      "metadata": {
        "id": "Ehh92iT6nybV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482ecf65-9712-4027-a6cc-6db12af24ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creando herramientas RAG...\n",
            "Herramientas RAG creadas:\n",
            "   - rag_search_A: B√∫squeda RAG en segmentaci√≥n A con filtros por metadata. Admite en la query: autor:\"Nombre\", fecha>=YYYY-MM-DD, tema: algo, tags: a,b,c. Devuelve fragmentos + fuente (id_doc, archivo, chunk) + autor....\n",
            "   - rag_search_B: B√∫squeda RAG en segmentaci√≥n B con filtros por metadata. Admite en la query: autor:\"Nombre\", fecha>=YYYY-MM-DD, tema: algo, tags: a,b,c. Devuelve fragmentos + fuente (id_doc, archivo, chunk) + autor....\n",
            "\n",
            "========================================================================\n",
            "PRUEBA EMP√çRICA A vs B (sin overlap)\n",
            "========================================================================\n",
            "\n",
            "Filtro: autor=\"Priscilla\" + tema=\"machine learning\"\n",
            "A: segmentaci√≥n por chunks fijos | B: por encabezados/secciones\n",
            "\n",
            "\n",
            "========================================================================\n",
            "RESULTADOS ‚Äî Segmento A (chunks fijos)\n",
            "========================================================================\n",
            "\n",
            "Query: autor:\"Priscilla\" tema:\"machine learning\"\n",
            "M√©tricas A: items=2 | docs_unicos=1 | chunks_unicos=2 | chars=794 | tiempo=27.3 ms\n",
            "\n",
            "Tabla A (resumen):\n",
            " resultado_idx  id_doc      chunk_id archivo                     autor score_ou_distancia  chars_fragmento\n",
            "             1 DOC_003 DOC_003_A_005     N/A Priscilla Jim√©nez Salgado             0.1345              280\n",
            "             2 DOC_003 DOC_003_A_004     N/A Priscilla Jim√©nez Salgado             0.0459              514\n",
            "\n",
            "Vista previa A (Top-1 fragmento, truncado):\n",
            "Fragmento:\n",
            "ma's eficiente hacia una solucio'n, representando la mejor la pro'xima semana la modalidad sera' virtual. opcio'ncomoelcaminodemenorcosto. hasta el martes 26 de agosto las clases sera'n presencialesyesemarteshabra' quiz acumulativo. fig.11. ejemplodelprocesodebu'squeda\n",
            "\n",
            "========================================================================\n",
            "RESULTADOS ‚Äî Segmento B (encabezados/secciones)\n",
            "========================================================================\n",
            "\n",
            "Query: autor:\"Priscilla\" tema:\"machine learning\"\n",
            "M√©tricas B: items=1 | docs_unicos=1 | chunks_unicos=1 | chars=514 | tiempo=21.9 ms\n",
            "\n",
            "Tabla B (resumen):\n",
            " resultado_idx  id_doc      chunk_id archivo                     autor score_ou_distancia  chars_fragmento\n",
            "             1 DOC_003 DOC_003_B_002     N/A Priscilla Jim√©nez Salgado             0.0664              514\n",
            "\n",
            "Vista previa B (Top-1 fragmento, truncado):\n",
            "Fragmento:\n",
            "ii. machinelearning\n",
            "machine learning. esto incluye experimentar\n",
            "conaprendizajenosupervisado.\n",
            "\n",
            "el concepto de machine learning consiste en\n",
            "disenÀúar ma'quinas capaces de realizar tareas sin estar\n",
            "programadasdeformaexpl'ƒ±cita,extrayendolalo'gica\n",
            "directamentedelosdatos.\n",
            "por ejemplo, no se le indicara' a la computadora\n",
            "que' es un perro ni las reglas que lo definen (cola, fig.4. descripcio'ngeneraldelusodemachinelearningenlaciencia.\n",
            "ojos, raza, etc.); en su lugar, se le proporcionara' una\n",
            "imagen y ret...\n",
            "\n",
            "[Nota metodol√≥gica]\n",
            "Dise√±o: comparaci√≥n de recuperaci√≥n RAG en dos segmentaciones independientes: A (chunks fijos) y B (encabezados/secciones). La consulta restringe por metadata (Filtro: autor=\"Priscilla\" + tema=\"machine learning\"). Se reportan, por segmento: n√∫mero de items, documentos √∫nicos, chunks √∫nicos, caracteres agregados y latencia. Los resultados se presentan por separado (sin superposiciones), priorizando claridad y concisi√≥n.\n",
            "\n",
            "[CSV generados]\n",
            " - rag_eval/segmento_A_resultados.csv\n",
            " - rag_eval/segmento_B_resultados.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 6: Crear WebSearch Tool (DuckDuckGo)\n",
        "# Solo debe usarse cuando el usuario lo pida expl√≠citamente\n",
        "# ============================================================\n",
        "\n",
        "#%pip install -q ddgs  # descomenta si a√∫n no lo tienes\n",
        "\n",
        "from langchain_core.tools import Tool\n",
        "from ddgs import DDGS\n",
        "import re, time\n",
        "\n",
        "def _normalize_query(q: str) -> str:\n",
        "    q = q.strip()\n",
        "    m = re.match(r'^(site:[^\\s]+)\\s+(.+)$', q)\n",
        "    if m:\n",
        "        site, rest = m.group(1), m.group(2).strip()\n",
        "        if not (rest.startswith('\"') and rest.endswith('\"')) and \" \" in rest:\n",
        "            rest = f'\"{rest}\"'\n",
        "        return f\"{site} {rest}\"\n",
        "    return q\n",
        "\n",
        "def _ddg_text_search(ddgs_obj: DDGS, q: str, region: str, max_results: int):\n",
        "    \"\"\"Compatibilidad entre versiones de ddgs.\"\"\"\n",
        "    try:\n",
        "        # Firmas nuevas (query como primer posicional o keyword)\n",
        "        return list(ddgs_obj.text(q, region=region, max_results=max_results))\n",
        "    except TypeError:\n",
        "        # Firmas antiguas (keywords=...)\n",
        "        return list(ddgs_obj.text(keywords=q, region=region, max_results=max_results))\n",
        "\n",
        "def create_web_search_tool() -> Tool:\n",
        "    def web_search_func(query: str, max_results: int = 6, region: str = \"wt-wt\") -> str:\n",
        "        if not isinstance(query, str) or not query.strip():\n",
        "            return \"Consulta inv√°lida para web_search.\"\n",
        "        q = _normalize_query(query)\n",
        "        last_err = None\n",
        "        for t in range(2):  # reintentos suaves\n",
        "            try:\n",
        "                with DDGS() as ddgs_obj:\n",
        "                    results = _ddg_text_search(ddgs_obj, q, region, max_results)\n",
        "                if results:\n",
        "                    lines = []\n",
        "                    for r in results[:max_results]:\n",
        "                        title = r.get(\"title\") or \"(sin t√≠tulo)\"\n",
        "                        href = r.get(\"href\") or \"\"\n",
        "                        snippet = (r.get(\"body\") or \"\").replace(\"\\n\", \" \")\n",
        "                        if len(snippet) > 150:\n",
        "                            snippet = snippet[:150] + \"‚Ä¶\"\n",
        "                        lines.append(f\"- {title}\\n  {href}\\n  {snippet}\")\n",
        "                    return \"\\n\".join(lines)\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "            time.sleep(0.6 + 0.4 * t)\n",
        "        return (\"Sin resultados (o bloqueado por el buscador).\"\n",
        "                if last_err is None else f\"Error en b√∫squeda: {type(last_err).__name__}: {last_err}\")\n",
        "    return Tool(\n",
        "        name=\"web_search\",\n",
        "        description=(\"B√∫squeda web (DuckDuckGo v√≠a ddgs). √ösala s√≥lo cuando el usuario lo pida \"\n",
        "                     \"o si se necesita informaci√≥n externa.\"),\n",
        "        func=web_search_func,\n",
        "    )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "knyTiCAV_LJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "## Prueba paso 6 Web Search\n",
        "web_search_tool = create_web_search_tool()\n",
        "print(\"WebSearch Tool:\", web_search_tool.description)\n",
        "\n",
        "web = create_web_search_tool()\n",
        "print(web.invoke(\"site:wikipedia.org aprendizaje por refuerzo\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "X69YgP_p4Anq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bb1b88-8f53-4b77-86a5-9bc4e43dbaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WebSearch Tool: B√∫squeda web (DuckDuckGo v√≠a ddgs). √ösala s√≥lo cuando el usuario lo pida o si se necesita informaci√≥n externa.\n",
            "- Aprendizaje por refuerzo - Wikipedia, la enciclopedia libre\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo\n",
            "  El aprendizaje por refuerzo o aprendizaje reforzado (en ingl√©s: reinforcement learning) es un √°rea del aprendizaje autom√°tico (AA) inspirada en la psi‚Ä¶\n",
            "- Aprendizaje por Refuerzo Cu√°ntico - Wikipedia, la enciclopedia libre\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_por_Refuerzo_Cu√°ntico\n",
            "  El Aprendizaje por Refuerzo Cu√°ntico es un √°rea de investigaci√≥n, en la que se aprovechan las propiedades de la mec√°nica cu√°ntica de cara a optimizar ‚Ä¶\n",
            "- File:Markov diagram v2.svg - Wikipedia\n",
            "  https://en.wikipedia.org/wiki/File:Markov_diagram_v2.svg\n",
            "  Diagrama de un proceso de decisi√≥n de Markov en aprendizaje por refuerzo , basado en una figura del libro \"Reinforcement Learning An Introduction\" (se‚Ä¶\n",
            "- Aprendizaje por refuerzo a partir de retroalimentaci√≥n humana\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo_a_partir_de_retroalimentaci√≥n_humana\n",
            "  Visi√≥n general del aprendizaje por refuerzo a partir de retroalimentaci√≥n humana La retroalimentaci√≥n humana se suele recoger pidiendo a los humanos q‚Ä¶\n",
            "- Aprendizaje de refuerzo profundo - Wikipedia, la enciclopedia libre\n",
            "  https://es.wikipedia.org/wiki/Aprendizaje_de_refuerzo_profundo\n",
            "  Con esta capa de abstracci√≥n, los algoritmos de aprendizaje por refuerzo profundo pueden dise√±arse de forma que se generalicen y el mismo modelo pueda‚Ä¶\n",
            "- Q-learning - Wikipedia, la enciclopedia libre\n",
            "  https://es.wikipedia.org/wiki/Q-learning\n",
            "  Q-learning doble 20 es un algoritmo de aprendizaje por refuerzo sin pol√≠tica, donde una pol√≠tica diferente se utiliza para la evaluaci√≥n del valor que‚Ä¶\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar la tool web\n",
        "web_search_tool = create_web_search_tool()\n"
      ],
      "metadata": {
        "id": "eBQS4WCfkkCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 7: Resumen y verificaci√≥n final\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RESUMEN DAVID\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nDatos procesados:\")\n",
        "print(f\" - Fragmentos Segmentaci√≥n A: {len(seg_a_data)}\")\n",
        "print(f\" - Fragmentos Segmentaci√≥n B: {len(seg_b_data)}\")\n",
        "\n",
        "print(\"\\n Bases vectoriales creadas:\")\n",
        "print(f\" - Vectorstore A: {VECTORSTORE_DIR_A}\")\n",
        "print(f\" - Vectorstore B: {VECTORSTORE_DIR_B}\")\n",
        "\n",
        "print(\"\\n Herramientas disponibles:\")\n",
        "print(f\" 1. {rag_tool_a.name}: B√∫squeda RAG con segmentaci√≥n A (chunks fijos)\")\n",
        "print(f\" 2. {rag_tool_b.name}: B√∫squeda RAG con segmentaci√≥n B (encabezados)\")\n",
        "print(f\" 3. {web_search_tool.name}: B√∫squeda web (solo cuando se solicite expl√≠citamente)\")\n",
        "\n",
        "print(\"\\nüìù Flujo implementado:\")\n",
        "print(\" 1. Tokenizaci√≥n: Verificaci√≥n de tokens con tiktoken\")\n",
        "print(\" 2. Embeddings: Generaci√≥n con sentence-transformers/all-MiniLM-L6-v2 (modelo local, gratuito)\")\n",
        "print(\" 3. Almacenamiento: Dos vectorstores FAISS (uno por segmentaci√≥n)\")\n",
        "print(\" 4. Consulta: Herramientas RAG que retornan fragmento, documento y autor\")\n",
        "print(\" 5. WebSearch: Herramienta disponible con restricci√≥n de uso\")\n"
      ],
      "metadata": {
        "id": "xHP7IO-2_OUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6407e4-c4ea-47b9-b6b6-079c6cb22c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "RESUMEN DAVID\n",
            "======================================================================\n",
            "\n",
            "Datos procesados:\n",
            " - Fragmentos Segmentaci√≥n A: 227\n",
            " - Fragmentos Segmentaci√≥n B: 349\n",
            "\n",
            " Bases vectoriales creadas:\n",
            " - Vectorstore A: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/vectorstore_a\n",
            " - Vectorstore B: /content/drive/MyDrive/Colab Notebooks/Tarea3-IA/dataset/vectorstore_b\n",
            "\n",
            " Herramientas disponibles:\n",
            " 1. rag_search_a: B√∫squeda RAG con segmentaci√≥n A (chunks fijos)\n",
            " 2. rag_search_b: B√∫squeda RAG con segmentaci√≥n B (encabezados)\n",
            " 3. web_search: B√∫squeda web (solo cuando se solicite expl√≠citamente)\n",
            "\n",
            "üìù Flujo implementado:\n",
            " 1. Tokenizaci√≥n: Verificaci√≥n de tokens con tiktoken\n",
            " 2. Embeddings: Generaci√≥n con sentence-transformers/all-MiniLM-L6-v2 (modelo local, gratuito)\n",
            " 3. Almacenamiento: Dos vectorstores FAISS (uno por segmentaci√≥n)\n",
            " 4. Consulta: Herramientas RAG que retornan fragmento, documento y autor\n",
            " 5. WebSearch: Herramienta disponible con restricci√≥n de uso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# COMPA√ëERO 3 ‚Äì AGENTE, ORQUESTACI√ìN, MEMORIA Y APP\n",
        "# ============================================================\n",
        "# Paso 1: Instalaci√≥n de dependencias necesarias (versi√≥n estable Gemini)\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "print(\"üîß Configurando entorno estable para LangChain + Gemini...\")\n",
        "\n",
        "# 1Ô∏è‚É£ Eliminar versiones conflictivas (1.x)\n",
        "!pip uninstall -y -q langchain langchain-core langchain-community langchain-openai langchain-text-splitters langchain-classic langgraph langgraph-prebuilt\n",
        "\n",
        "# 2Ô∏è‚É£ Instalar versiones compatibles con Gemini (familia 0.2.x)\n",
        "!pip install -q \"langchain==0.2.*\" \"langchain-core==0.2.*\" \"langchain-community==0.2.*\" \"langchain-text-splitters==0.2.*\" \"langchain-google-genai==1.0.*\"\n",
        "\n",
        "# 3Ô∏è‚É£ Instalar librer√≠as adicionales para interfaz web\n",
        "!pip install -q streamlit streamlit-chat\n",
        "\n",
        "print(\"\\n‚úÖ Dependencias instaladas correctamente y coherentes con Gemini\")\n",
        "print(\"üì¶ LangChain 0.2.x + langchain-google-genai 1.x\")\n",
        "print(\"üì¶ Streamlit y streamlit-chat listos para la app\")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# (Opcional) Si vas a usar Vertex AI en vez de AI Studio:\n",
        "# %pip install -U \"google-cloud-aiplatform>=1.66.0\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JlDBm2Il_S8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "af1c68a4-e0d4-4cd7-d4b2-8620f0a90c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"üîß Configurando entorno estable para LangChain + Gemini...\")\\n\\n# 1Ô∏è‚É£ Eliminar versiones conflictivas (1.x)\\n!pip uninstall -y -q langchain langchain-core langchain-community langchain-openai langchain-text-splitters langchain-classic langgraph langgraph-prebuilt\\n\\n# 2Ô∏è‚É£ Instalar versiones compatibles con Gemini (familia 0.2.x)\\n!pip install -q \"langchain==0.2.*\" \"langchain-core==0.2.*\" \"langchain-community==0.2.*\" \"langchain-text-splitters==0.2.*\" \"langchain-google-genai==1.0.*\"\\n\\n# 3Ô∏è‚É£ Instalar librer√≠as adicionales para interfaz web\\n!pip install -q streamlit streamlit-chat\\n\\nprint(\"\\n‚úÖ Dependencias instaladas correctamente y coherentes con Gemini\")\\nprint(\"üì¶ LangChain 0.2.x + langchain-google-genai 1.x\")\\nprint(\"üì¶ Streamlit y streamlit-chat listos para la app\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 2: Configuraci√≥n del modelo Gemini y definici√≥n de prompts\n",
        "# (dos variantes A/B para pruebas emp√≠ricas)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from langchain.agents import AgentExecutor, create_react_agent\n",
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.memory import ConversationBufferWindowMemory\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configurar API Key de Google (Gemini)\n",
        "# IMPORTANTE: Configura GOOGLE_API_KEY en Colab Secrets o como variable de entorno\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  GOOGLE_API_KEY no configurada.\")\n",
        "    print(\"   Config√∫rala en Colab Secrets o como variable de entorno:\")\n",
        "    print(\"   - Ve a Colab Secrets (icono de candado en la barra lateral)\")\n",
        "    print(\"   - Agrega una nueva clave: GOOGLE_API_KEY\")\n",
        "    print(\"   - O usa: os.environ['GOOGLE_API_KEY'] = 'tu-api-key'\")\n",
        "else:\n",
        "    # üëâ Exportar al entorno para que los clientes la detecten autom√°ticamente\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    print(\"‚úÖ Google API Key configurada\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# PROMPT VARIANTE A (prioriza rag_search_A; web_search solo si el usuario lo pide expl√≠citamente)\n",
        "# ------------------------------------------------------------\n",
        "AGENT_PROMPT_A = \"\"\"Eres un asistente acad√©mico especializado en el curso de Inteligencia Artificial.\n",
        "Tu nombre es AsistenteIA y tu rol es ayudar a los estudiantes a encontrar informaci√≥n en los apuntes del curso.\n",
        "\n",
        "**INSTRUCCIONES IMPORTANTES (VARIANTE A):**\n",
        "1. Antes de responder, consulta SIEMPRE los apuntes usando EXCLUSIVAMENTE la herramienta `rag_search_A`.\n",
        "2. Cita SIEMPRE el documento de origen y el autor cuando uses informaci√≥n de los apuntes.\n",
        "3. Solo usa la b√∫squeda en la web (`web_search`) si el usuario lo solicita expl√≠citamente (por ejemplo: \"busca en la web\", \"revisa en internet\", \"fuentes externas\") **y solo si la herramienta `web_search` est√° disponible en esta ejecuci√≥n**. Si no est√° disponible, ind√≠calo claramente y contin√∫a con `rag_search_A`.\n",
        "\n",
        "**ESTILO DE RESPUESTA:**\n",
        "- S√© claro, conciso y educativo.\n",
        "- Explica conceptos de manera accesible.\n",
        "- Usa ejemplos cuando sea √∫til.\n",
        "- Cita siempre tus fuentes: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "**HERRAMIENTAS DISPONIBLES EN ESTA VARIANTE:**\n",
        "- rag_search_A: Busca en apuntes usando segmentaci√≥n por chunks fijos (m√°s preciso para fragmentos espec√≠ficos).\n",
        "- (Opcional en tiempo de ejecuci√≥n) web_search: solo si el usuario lo pide expl√≠citamente y la herramienta est√° habilitada.\n",
        "\n",
        "**EJEMPLO DE USO:**\n",
        "Usuario: \"¬øQu√© es el aprendizaje supervisado?\"\n",
        "1. Usa `rag_search_A` para buscar en los apuntes.\n",
        "2. Responde bas√°ndote en los resultados encontrados.\n",
        "3. Cita: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "{history}\n",
        "\n",
        "Pregunta: {input}\n",
        "Piensa paso a paso y decide qu√© informaci√≥n recuperar con `rag_search_A`. Solo usa `web_search` si el usuario lo pide expl√≠citamente y la herramienta est√° disponible.\n",
        "\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# PROMPT VARIANTE B (prioriza rag_search_B; web_search solo si el usuario lo pide expl√≠citamente)\n",
        "# ------------------------------------------------------------\n",
        "AGENT_PROMPT_B = \"\"\"Eres un asistente acad√©mico especializado en el curso de Inteligencia Artificial.\n",
        "Tu nombre es AsistenteIA y tu rol es ayudar a los estudiantes a encontrar informaci√≥n en los apuntes del curso.\n",
        "\n",
        "**INSTRUCCIONES IMPORTANTES (VARIANTE B):**\n",
        "1. Antes de responder, consulta SIEMPRE los apuntes usando EXCLUSIVAMENTE la herramienta `rag_search_B`.\n",
        "2. Cita SIEMPRE el documento de origen y el autor cuando uses informaci√≥n de los apuntes.\n",
        "3. Solo usa la b√∫squeda en la web (`web_search`) si el usuario lo solicita expl√≠citamente (por ejemplo: \"busca en la web\", \"revisa en internet\", \"fuentes externas\") **y solo si la herramienta `web_search` est√° disponible en esta ejecuci√≥n**. Si no est√° disponible, ind√≠calo claramente y contin√∫a con `rag_search_B`.\n",
        "\n",
        "**ESTILO DE RESPUENSA:**\n",
        "- S√© claro, conciso y educativo.\n",
        "- Explica conceptos de manera accesible.\n",
        "- Usa ejemplos cuando sea √∫til.\n",
        "- Cita siempre tus fuentes: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "**HERRAMIENTAS DISPONIBLES EN ESTA VARIANTE:**\n",
        "- rag_search_B: Busca en apuntes usando segmentaci√≥n por encabezados (mejor para temas completos).\n",
        "- (Opcional en tiempo de ejecuci√≥n) web_search: solo si el usuario lo pide expl√≠citamente y la herramienta est√° habilitada.\n",
        "\n",
        "**EJEMPLO DE USO:**\n",
        "Usuario: \"¬øQu√© es el aprendizaje supervisado?\"\n",
        "1. Usa `rag_search_B` para buscar en los apuntes.\n",
        "2. Responde bas√°ndote en los resultados encontrados.\n",
        "3. Cita: \"Seg√∫n [Autor] en [Documento]...\"\n",
        "\n",
        "{history}\n",
        "\n",
        "Pregunta: {input}\n",
        "Piensa paso a paso y decide qu√© informaci√≥n recuperar con `rag_search_B`. Solo usa `web_search` si el usuario lo pide expl√≠citamente y la herramienta est√° disponible.\n",
        "\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "print(\"‚úÖ Prompts definidos para pruebas emp√≠ricas A/B\")\n",
        "print(\"   - AGENT_PROMPT_A ‚Üí Prioriza SOLO rag_search_A (chunks fijos). web_search: solo si el usuario lo pide y est√° disponible.\")\n",
        "print(\"   - AGENT_PROMPT_B ‚Üí Prioriza SOLO rag_search_B (encabezados). web_search: solo si el usuario lo pide y est√° disponible.\")\n",
        "print(\"\\nüìå Recuerda: al crear los agentes, pasa √∫nicamente su tool de RAG correspondiente; a√±ade `web_search` SOLO si planeas permitirlo en esa ejecuci√≥n.\")\n"
      ],
      "metadata": {
        "id": "whA2LHY9_T8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6fa0351-4e55-4e2d-c989-6268b694c669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Google API Key configurada\n",
            "‚úÖ Prompts definidos para pruebas emp√≠ricas A/B\n",
            "   - AGENT_PROMPT_A ‚Üí Prioriza SOLO rag_search_A (chunks fijos). web_search: solo si el usuario lo pide y est√° disponible.\n",
            "   - AGENT_PROMPT_B ‚Üí Prioriza SOLO rag_search_B (encabezados). web_search: solo si el usuario lo pide y est√° disponible.\n",
            "\n",
            "üìå Recuerda: al crear los agentes, pasa √∫nicamente su tool de RAG correspondiente; a√±ade `web_search` SOLO si planeas permitirlo en esa ejecuci√≥n.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 3 (A/B): Configurar Gemini y crear agentes A/B (tool-calling robusto)\n",
        "# - Normaliza nombres de tools\n",
        "# - DEBUG opcional con impresi√≥n de tool_calls\n",
        "# - Fallback: si no hay tool_calls ni texto, fuerza 1 llamada RAG\n",
        "# - Fix: ToolMessage siempre con tool_call_id string (LangChain 1.x)\n",
        "# - Hotfix: normalizaci√≥n de tool_calls (soporta esquemas OpenAI-like y Gemini-like)\n",
        "# - Mejora: prioridad web cuando el usuario lo pide\n",
        "# - Mejora: manejo 429 proactivo + s√≠ntesis inmediata (sin 2¬™ vuelta del LLM)\n",
        "# ============================================================\n",
        "\n",
        "import os, json, textwrap\n",
        "from uuid import uuid4\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- LLM Gemini ---\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# --- Mensajes / memoria ---\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "DEBUG = False  # pon True para ver trazas de tool_calls\n",
        "\n",
        "# ============================================================\n",
        "# 0) API Key\n",
        "# ============================================================\n",
        "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\") or userdata.get(\"GOOGLE_API_KEY\")\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  No se puede continuar sin GOOGLE_API_KEY.\")\n",
        "    llm = None\n",
        "else:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "# ============================================================\n",
        "# 3A) Configurar LLM con fallback ordenado\n",
        "# ============================================================\n",
        "def configurar_llm(preferidos=None, temperature=0.1):\n",
        "    if preferidos is None:\n",
        "        preferidos = [\n",
        "            \"gemini-2.0-flash\",      # ‚Üê prueba este primero (suele respirar mejor)\n",
        "            \"gemini-2.5-flash\",\n",
        "            \"gemini-flash-latest\",\n",
        "            \"gemini-2.5-pro\",\n",
        "            \"gemini-pro-latest\",\n",
        "        ]\n",
        "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "        raise RuntimeError(\"GOOGLE_API_KEY no est√° disponible.\")\n",
        "\n",
        "    print(\"üîß Configurando modelo Gemini (con fallbacks)...\")\n",
        "    last_err = None\n",
        "    for name in preferidos:\n",
        "        try:\n",
        "            _llm = ChatGoogleGenerativeAI(\n",
        "                model=name,\n",
        "                google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            # ping m√≠nimo\n",
        "            _ = _llm.invoke(\"ping\")\n",
        "            print(f\"‚úÖ Modelo configurado: {name}\")\n",
        "            return _llm\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Fall√≥ {name}: {e}\")\n",
        "            last_err = e\n",
        "    raise RuntimeError(f\"No se pudo configurar ning√∫n modelo. √öltimo error: {last_err}\")\n",
        "\n",
        "llm = configurar_llm() if GOOGLE_API_KEY else None\n",
        "\n",
        "# ============================================================\n",
        "# 3B) Agentes A/B con bucle de tool-calling robustecido\n",
        "# ============================================================\n",
        "\n",
        "_SESSION_STORES = {}\n",
        "_SYSTEM_SET = set()\n",
        "\n",
        "def get_history(session_id: str) -> ChatMessageHistory:\n",
        "    hist = _SESSION_STORES.get(session_id)\n",
        "    if hist is None:\n",
        "        hist = ChatMessageHistory()\n",
        "        _SESSION_STORES[session_id] = hist\n",
        "    return hist\n",
        "\n",
        "def ensure_system_prompt(session_id: str, system_text: str):\n",
        "    if session_id not in _SYSTEM_SET:\n",
        "        get_history(session_id).add_message(SystemMessage(content=system_text))\n",
        "        _SYSTEM_SET.add(session_id)\n",
        "\n",
        "def _usuario_pide_web(texto: str) -> bool:\n",
        "    if not isinstance(texto, str):\n",
        "        return False\n",
        "    t = texto.lower()\n",
        "    gatillos = [\n",
        "        \"busca en la web\", \"buscar en la web\", \"revisa en la web\",\n",
        "        \"en internet\", \"busca en internet\", \"buscar en internet\",\n",
        "        \"web_search\", \"fuentes externas\", \"googlealo\", \"googlea esto\",\n",
        "        \"consulta la web\", \"haz una b√∫squeda web\",\n",
        "    ]\n",
        "    return any(g in t for g in gatillos)\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    return (s or \"\").strip().lower()\n",
        "\n",
        "def _map_tools_por_nombre(tools: list):\n",
        "    return {_norm(getattr(t, \"name\", \"\")): t for t in tools if getattr(t, \"name\", \"\")}\n",
        "\n",
        "# ============================================================\n",
        "# HOTFIX tool_calls: normalizador y extractor robustos\n",
        "#  - Soporta:\n",
        "#    A) {\"function\": {\"name\": \"...\", \"arguments\": \"...|dict\"}, \"id\": \"...\"}   (OpenAI-like)\n",
        "#    B) {\"name\": \"...\", \"args\": {...}, \"id\": \"...\"}                            (Gemini via LC 1.x)\n",
        "#    C) {\"name\": \"...\", \"arguments\": {...}, \"id\": \"...\"}                       (variantes planas)\n",
        "# ============================================================\n",
        "def _normalize_call_dict(call):\n",
        "    \"\"\"Devuelve un dict unificado: {\"name\": str|None, \"arguments\": dict|str|None, \"id\": str|None}\"\"\"\n",
        "    if not isinstance(call, dict):\n",
        "        return {\"name\": None, \"arguments\": None, \"id\": None}\n",
        "    cid = call.get(\"id\") or call.get(\"tool_call_id\") or f\"auto-{uuid4().hex[:8]}\"\n",
        "\n",
        "    # Caso A (OpenAI-like)\n",
        "    fn = call.get(\"function\")\n",
        "    if isinstance(fn, dict) and (\"name\" in fn or \"arguments\" in fn):\n",
        "        nm = fn.get(\"name\")\n",
        "        args = fn.get(\"arguments\")\n",
        "        return {\"name\": nm, \"arguments\": args, \"id\": cid}\n",
        "\n",
        "    # Caso B (Gemini via LangChain)\n",
        "    if \"name\" in call and (\"args\" in call or \"arguments\" in call):\n",
        "        nm = call.get(\"name\")\n",
        "        args = call.get(\"args\", call.get(\"arguments\"))\n",
        "        return {\"name\": nm, \"arguments\": args, \"id\": cid}\n",
        "\n",
        "    # √öltimo intento: llaves planas\n",
        "    nm = call.get(\"name\")\n",
        "    args = call.get(\"arguments\") or call.get(\"args\")\n",
        "    return {\"name\": nm, \"arguments\": args, \"id\": cid}\n",
        "\n",
        "def _extraer_tool_calls(ai_msg: AIMessage):\n",
        "    calls = getattr(ai_msg, \"tool_calls\", None)\n",
        "    kw = getattr(ai_msg, \"additional_kwargs\", {}) or {}\n",
        "    if not calls:\n",
        "        calls = kw.get(\"tool_calls\") or kw.get(\"tool_calls_json\") or []\n",
        "    if DEBUG:\n",
        "        print(\"üß© tool_calls crudos:\", calls if calls else \"(ninguno)\")\n",
        "        if kw:\n",
        "            snippet = textwrap.shorten(str(kw), width=400, placeholder=\" ‚Ä¶\")\n",
        "            print(\"üß© additional_kwargs:\", snippet)\n",
        "    norm = []\n",
        "    for c in (calls or []):\n",
        "        norm.append(_normalize_call_dict(c))\n",
        "    return norm\n",
        "\n",
        "def _ejecutar_tool_call(tool, call_norm):\n",
        "    # call_norm viene normalizado por _extraer_tool_calls\n",
        "    fn = getattr(tool, \"invoke\", None) or getattr(tool, \"run\", None) or getattr(tool, \"func\", None)\n",
        "    if fn is None:\n",
        "        return \"[Error: herramienta no es invocable]\"\n",
        "    args = call_norm.get(\"arguments\", {})\n",
        "    # Decodifica si vino como JSON string\n",
        "    if isinstance(args, str):\n",
        "        try:\n",
        "            args = json.loads(args or \"{}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        if isinstance(args, dict):\n",
        "            if \"query\" in args:\n",
        "                try:\n",
        "                    return fn(args[\"query\"])\n",
        "                except TypeError:\n",
        "                    return fn(args)\n",
        "            return fn(args)\n",
        "        else:\n",
        "            return fn(args)\n",
        "    except Exception as e:\n",
        "        return f\"[Error ejecutando tool: {e}]\"\n",
        "\n",
        "# --- Helpers para ToolMessage con ID v√°lido (requisito en LC 1.x) ---\n",
        "def _mk_tool_msg(name: str, content: str, tool_call_id: str | None):\n",
        "    tid = tool_call_id if isinstance(tool_call_id, str) and tool_call_id else f\"auto-{_norm(name)}-{uuid4().hex[:8]}\"\n",
        "    nm = name if (name and str(name).strip()) else \"tool\"\n",
        "    return ToolMessage(content=str(content), name=nm, tool_call_id=tid)\n",
        "\n",
        "def _patch_calls_into_history(history, raw_name, tool_out, t_id):\n",
        "    history.add_message(_mk_tool_msg(raw_name or \"tool\", tool_out, t_id))\n",
        "\n",
        "def _patch_forced_rag_into_history(history, principal, tool_out):\n",
        "    history.add_message(_mk_tool_msg(getattr(principal, \"name\", \"rag\"), tool_out, f\"forced-{uuid4().hex[:8]}\"))\n",
        "\n",
        "# ====== S√çNTESIS INMEDIATA + 429 PROACTIVO ======\n",
        "ALWAYS_SYNTHESIZE_AFTER_TOOLS = True\n",
        "MAX_TOOL_MSGS_FOR_SYNTHESIS = 2\n",
        "LOG_429_BANNER = True\n",
        "\n",
        "def _sintetizar_desde_tools(history, max_msgs: int = MAX_TOOL_MSGS_FOR_SYNTHESIS) -> str:\n",
        "    tmsgs = [m for m in history.messages if isinstance(m, ToolMessage)]\n",
        "    if not tmsgs:\n",
        "        return \"\"\n",
        "    rec = tmsgs[-max_msgs:]\n",
        "    partes = []\n",
        "    for m in rec:\n",
        "        nombre = getattr(m, \"name\", \"tool\")\n",
        "        contenido = (m.content or \"\").strip()\n",
        "        if not contenido:\n",
        "            continue\n",
        "        if len(contenido) > 1200:\n",
        "            contenido = contenido[:1200] + \" ...\"\n",
        "        partes.append(f\"‚Ä¢ {nombre}\\n{contenido}\")\n",
        "    if not partes:\n",
        "        return \"\"\n",
        "    return \"S√≠ntesis basada en herramientas:\\n\\n\" + \"\\n\\n\".join(partes)\n",
        "\n",
        "class SimpleAgentExecutor:\n",
        "    def __init__(self, llm_base, system_text: str, tools: list, variante_label: str, max_iters: int = 2):\n",
        "        # ‚Üì max_iters=2 para no golpear la cuota\n",
        "        self.llm = llm_base.bind_tools(tools) if tools else llm_base\n",
        "        self.system_text = system_text\n",
        "        self.tools = tools\n",
        "        self.tools_by_name = _map_tools_por_nombre(tools)  # normalizados\n",
        "        self.variante_label = variante_label\n",
        "        self.max_iters = max_iters\n",
        "        if DEBUG:\n",
        "            print(f\"[DEBUG] Tools registradas ({variante_label}):\", list(self.tools_by_name.keys()))\n",
        "\n",
        "    def invoke(self, payload: dict, config: dict = None):\n",
        "        cfg = config or {}\n",
        "        sid = payload.get(\"session_id\") or (cfg.get(\"configurable\", {}) or {}).get(\"session_id\") or \"default\"\n",
        "\n",
        "        ensure_system_prompt(sid, self.system_text)\n",
        "        history = get_history(sid)\n",
        "\n",
        "        user_input = payload.get(\"input\") or \"\"\n",
        "        history.add_message(HumanMessage(content=user_input))\n",
        "        mensajes = history.messages[:]\n",
        "        explicito_web = _usuario_pide_web(user_input)\n",
        "\n",
        "        forced_rag_used = False\n",
        "        forced_web_used = False\n",
        "\n",
        "        for it in range(1, self.max_iters + 1):\n",
        "            try:\n",
        "                ai_msg = self.llm.invoke(mensajes)\n",
        "            except Exception as e:\n",
        "                msg = str(e)\n",
        "\n",
        "                # === 429 PROACTIVO: si el LLM falla por cuota ANTES de tool_calls ===\n",
        "                if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
        "                    if LOG_429_BANNER:\n",
        "                        print(\"‚ö†Ô∏è [MODO 429 PROACTIVO] LLM fall√≥ por cuota; forzando tools y sintetizando‚Ä¶\")\n",
        "                    did_tool = False\n",
        "\n",
        "                    # 1) Si el usuario pidi√≥ web y existe la tool, forzar web primero\n",
        "                    if explicito_web and \"web_search\" in self.tools_by_name:\n",
        "                        web_tool = self.tools_by_name[\"web_search\"]\n",
        "                        forced_call = {\"name\": web_tool.name,\n",
        "                                       \"arguments\": {\"query\": user_input},\n",
        "                                       \"id\": f\"forced-web-{uuid4().hex[:8]}\"}\n",
        "                        tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                        _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "                        did_tool = True\n",
        "\n",
        "                    # 2) Si no hicimos web, forzar RAG principal de la variante\n",
        "                    if not did_tool:\n",
        "                        principal = None\n",
        "                        for key in [\"rag_search_a\", \"rag_search_b\"]:\n",
        "                            if key in self.tools_by_name:\n",
        "                                principal = self.tools_by_name[key]; break\n",
        "                        if principal is not None:\n",
        "                            forced_call = {\"name\": principal.name,\n",
        "                                           \"arguments\": {\"query\": user_input},\n",
        "                                           \"id\": f\"forced-{uuid4().hex[:8]}\"}\n",
        "                            tool_out = _ejecutar_tool_call(principal, forced_call)\n",
        "                            _patch_forced_rag_into_history(history, principal, tool_out)\n",
        "                            did_tool = True\n",
        "\n",
        "                    # 3) Sintetizar con lo que haya\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                    # 4) No hubo tool posible\n",
        "                    return {\n",
        "                        \"output\": (\"Estoy temporalmente limitado por cuota (429) y no logr√© ejecutar \"\n",
        "                                   \"una herramienta de respaldo. Intenta de nuevo o cambia de modelo.\"),\n",
        "                        \"iterations\": it\n",
        "                    }\n",
        "\n",
        "                # Otros errores\n",
        "                return {\"output\": f\"[Error del modelo: {e}]\", \"iterations\": it}\n",
        "\n",
        "            history.add_message(ai_msg)\n",
        "            calls = _extraer_tool_calls(ai_msg)\n",
        "\n",
        "            if DEBUG:\n",
        "                clen = len(ai_msg.content or \"\")\n",
        "                print(f\"[DEBUG] iter {it} | content len={clen} | tool_calls={bool(calls)}\")\n",
        "                if calls:\n",
        "                    for c in calls:\n",
        "                        print(\"   call(norm):\", c.get(\"name\"), c.get(\"arguments\"))\n",
        "\n",
        "            # === Cuando hay tool_calls, las ejecutamos ===\n",
        "            if calls:\n",
        "                # PRIORIDAD WEB: si el usuario pidi√≥ web y el LLM NO la incluy√≥, la forzamos primero\n",
        "                if explicito_web and \"web_search\" in self.tools_by_name:\n",
        "                    has_web = False\n",
        "                    for c in calls:\n",
        "                        nm = (c.get(\"name\") or \"\").strip().lower()\n",
        "                        if nm == \"web_search\":\n",
        "                            has_web = True\n",
        "                            break\n",
        "                    if not has_web:\n",
        "                        web_tool = self.tools_by_name[\"web_search\"]\n",
        "                        forced_call = {\n",
        "                            \"name\": web_tool.name,\n",
        "                            \"arguments\": {\"query\": user_input},\n",
        "                            \"id\": f\"forced-web-{uuid4().hex[:8]}\",\n",
        "                        }\n",
        "                        tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                        _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "\n",
        "                # Ejecutar los calls (respetando la pol√≠tica web)\n",
        "                for call_norm in calls:\n",
        "                    raw_name = (call_norm.get(\"name\") or \"\").strip()\n",
        "                    t_name = _norm(raw_name)\n",
        "                    t_id = call_norm.get(\"id\", \"\")\n",
        "\n",
        "                    if t_name == \"web_search\" and not explicito_web:\n",
        "                        tool_out = (\n",
        "                            \"Solicitud de web detectada, pero esta variante solo usa web_search si el usuario \"\n",
        "                            \"lo pide expl√≠citamente. Contin√∫o con los apuntes.\"\n",
        "                        )\n",
        "                    else:\n",
        "                        tool = self.tools_by_name.get(t_name)\n",
        "                        tool_out = (f\"[Tool '{raw_name}' no disponible en esta variante]\"\n",
        "                                    if tool is None else _ejecutar_tool_call(tool, call_norm))\n",
        "\n",
        "                    _patch_calls_into_history(history, raw_name or \"tool\", tool_out, t_id)\n",
        "\n",
        "                # S√çNTESIS INMEDIATA\n",
        "                if ALWAYS_SYNTHESIZE_AFTER_TOOLS:\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                mensajes = history.messages[:]\n",
        "                continue  # siguiente iteraci√≥n tras observar las tools\n",
        "\n",
        "            # === Si NO hay tool_calls: intentar forzar lo necesario ===\n",
        "            txt = (ai_msg.content or \"\").strip()\n",
        "\n",
        "            # (A) Si el usuario pidi√≥ WEB expl√≠cita y no hubo tool_calls, forzar web una vez\n",
        "            if explicito_web and not forced_web_used and \"web_search\" in self.tools_by_name:\n",
        "                web_tool = self.tools_by_name[\"web_search\"]\n",
        "                forced_call = {\"name\": getattr(web_tool, \"name\", \"web_search\"),\n",
        "                               \"arguments\": {\"query\": user_input},\n",
        "                               \"id\": f\"forced-web-{uuid4().hex[:8]}\"}\n",
        "                tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "\n",
        "                # S√≠ntesis inmediata tras forzar web\n",
        "                if ALWAYS_SYNTHESIZE_AFTER_TOOLS:\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                mensajes = history.messages[:]\n",
        "                forced_web_used = True\n",
        "                continue\n",
        "\n",
        "            # (B) Si no pidi√≥ web, forzar el RAG principal una vez\n",
        "            if not forced_rag_used:\n",
        "                principal = None\n",
        "                for key in [\"rag_search_a\", \"rag_search_b\"]:\n",
        "                    if key in self.tools_by_name:\n",
        "                        principal = self.tools_by_name[key]; break\n",
        "                if principal is not None:\n",
        "                    forced_call = {\"name\": principal.name,\n",
        "                                   \"arguments\": {\"query\": user_input},\n",
        "                                   \"id\": f\"forced-{uuid4().hex[:8]}\"}\n",
        "                    tool_out = _ejecutar_tool_call(principal, forced_call)\n",
        "                    _patch_forced_rag_into_history(history, principal, tool_out)\n",
        "\n",
        "                    # S√≠ntesis inmediata tras forzar RAG\n",
        "                    if ALWAYS_SYNTHESIZE_AFTER_TOOLS:\n",
        "                        sintetico = _sintetizar_desde_tools(history)\n",
        "                        if sintetico:\n",
        "                            return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                    mensajes = history.messages[:]\n",
        "                    forced_rag_used = True\n",
        "                    continue\n",
        "\n",
        "            # (C) Si el modelo s√≠ dio texto, devu√©lvelo\n",
        "            if txt:\n",
        "                return {\"output\": txt, \"iterations\": it}\n",
        "\n",
        "            # (D) √öltimo recurso: s√≠ntesis si hay herramientas en historial\n",
        "            sintetico = _sintetizar_desde_tools(history)\n",
        "            if sintetico:\n",
        "                return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "            return {\"output\": \"No fue posible recuperar informaci√≥n en este momento.\", \"iterations\": it}\n",
        "\n",
        "        # max_iters alcanzado: intenta s√≠ntesis antes de rendirte\n",
        "        sintetico = _sintetizar_desde_tools(history)\n",
        "        if sintetico:\n",
        "            return {\"output\": sintetico, \"iterations\": self.max_iters}\n",
        "        return {\"output\": \"Se alcanz√≥ el m√°ximo de iteraciones sin respuesta final.\", \"iterations\": self.max_iters}\n",
        "\n",
        "def build_agent_executor(llm_base, system_text: str, tools: list, variante_label: str, max_iters: int = 2):\n",
        "    print(f\"\\nü§ñ Creando agente {variante_label} ...\")\n",
        "    return SimpleAgentExecutor(\n",
        "        llm_base=llm_base,\n",
        "        system_text=system_text,\n",
        "        tools=tools,\n",
        "        variante_label=variante_label,\n",
        "        max_iters=max_iters,\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# 3B-bis) Construcci√≥n concreta de A y B (usa tus prompts del Paso 2)\n",
        "# ============================================================\n",
        "\n",
        "agent_executor_A = None\n",
        "agent_executor_B = None\n",
        "\n",
        "if llm is not None:\n",
        "    tiene_a = \"rag_tool_a\" in globals()\n",
        "    tiene_b = \"rag_tool_b\" in globals()\n",
        "    tiene_web = \"web_search_tool\" in globals()\n",
        "    tiene_promptA = \"AGENT_PROMPT_A\" in globals()\n",
        "    tiene_promptB = \"AGENT_PROMPT_B\" in globals()\n",
        "\n",
        "    print(\"\\nüîß Estado de herramientas/prompts detectadas:\")\n",
        "    print(f\"   - rag_tool_a: {'s√≠' if tiene_a else 'no'}\")\n",
        "    print(f\"   - rag_tool_b: {'s√≠' if tiene_b else 'no'}\")\n",
        "    print(f\"   - web_search_tool (opcional): {'s√≠' if tiene_web else 'no'}\")\n",
        "    print(f\"   - AGENT_PROMPT_A: {'s√≠' if tiene_promptA else 'no'}\")\n",
        "    print(f\"   - AGENT_PROMPT_B: {'s√≠' if tiene_promptB else 'no'}\")\n",
        "\n",
        "    if tiene_a and tiene_promptA:\n",
        "        tools_A = [rag_tool_a] + ([web_search_tool] if tiene_web else [])\n",
        "        agent_executor_A = build_agent_executor(\n",
        "            llm_base=llm,\n",
        "            system_text=AGENT_PROMPT_A,\n",
        "            tools=tools_A,\n",
        "            variante_label=\"A (rag_search_A)\",\n",
        "            max_iters=2,\n",
        "        )\n",
        "        print(\"‚úÖ Agente A listo (rag_search_A + web opcional si el usuario lo pide)\")\n",
        "    else:\n",
        "        print(\"‚è≥ Agente A pendiente (falta rag_tool_a o AGENT_PROMPT_A)\")\n",
        "\n",
        "    if tiene_b and tiene_promptB:\n",
        "        tools_B = [rag_tool_b] + ([web_search_tool] if tiene_web else [])\n",
        "        agent_executor_B = build_agent_executor(\n",
        "            llm_base=llm,\n",
        "            system_text=AGENT_PROMPT_B,\n",
        "            tools=tools_B,\n",
        "            variante_label=\"B (rag_search_B)\",\n",
        "            max_iters=2,\n",
        "        )\n",
        "        print(\"‚úÖ Agente B listo (rag_search_B + web opcional si el usuario lo pide)\")\n",
        "    else:\n",
        "        print(\"‚è≥ Agente B pendiente (falta rag_tool_b o AGENT_PROMPT_B)\")\n",
        "\n",
        "print(\"\\nüìå Uso:\")\n",
        "print(\"  respA = agent_executor_A.invoke({'input': 'tu pregunta', 'session_id': 'A'})\")\n",
        "print(\"  respB = agent_executor_B.invoke({'input': 'tu pregunta', 'session_id': 'B'})\")\n",
        "print(\"  # Activa DEBUG=True (arriba) para ver tool_calls y args en la consola.\")\n"
      ],
      "metadata": {
        "id": "jZbx8hyT_WRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4914b7e7-7311-4e9b-bcf1-19296d6f2d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Configurando modelo Gemini (con fallbacks)...\n",
            "‚úÖ Modelo configurado: gemini-2.0-flash\n",
            "\n",
            "üîß Estado de herramientas/prompts detectadas:\n",
            "   - rag_tool_a: s√≠\n",
            "   - rag_tool_b: s√≠\n",
            "   - web_search_tool (opcional): s√≠\n",
            "   - AGENT_PROMPT_A: s√≠\n",
            "   - AGENT_PROMPT_B: s√≠\n",
            "\n",
            "ü§ñ Creando agente A (rag_search_A) ...\n",
            "‚úÖ Agente A listo (rag_search_A + web opcional si el usuario lo pide)\n",
            "\n",
            "ü§ñ Creando agente B (rag_search_B) ...\n",
            "‚úÖ Agente B listo (rag_search_B + web opcional si el usuario lo pide)\n",
            "\n",
            "üìå Uso:\n",
            "  respA = agent_executor_A.invoke({'input': 'tu pregunta', 'session_id': 'A'})\n",
            "  respB = agent_executor_B.invoke({'input': 'tu pregunta', 'session_id': 'B'})\n",
            "  # Activa DEBUG=True (arriba) para ver tool_calls y args en la consola.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üîß Parche: s√≠ntesis inmediata + manejo 429 proactivo + menos iteraciones\n",
        "import os, json\n",
        "from uuid import uuid4\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "# === 1) Sintetizar SIEMPRE tras usar tools (sin esperar otra vuelta del LLM) ===\n",
        "ALWAYS_SYNTHESIZE_AFTER_TOOLS = True   # evita depender del 2¬∫ turno del modelo\n",
        "MAX_TOOL_MSGS_FOR_SYNTHESIS = 2        # usa las √∫ltimas N salidas de herramientas\n",
        "LOG_429_BANNER = True                  # imprime un banner cuando entra al modo 429 proactivo\n",
        "\n",
        "def _sintetizar_desde_tools(history, max_msgs: int = MAX_TOOL_MSGS_FOR_SYNTHESIS) -> str:\n",
        "    from langchain_core.messages import ToolMessage\n",
        "    tmsgs = [m for m in history.messages if isinstance(m, ToolMessage)]\n",
        "    if not tmsgs:\n",
        "        return \"\"\n",
        "    rec = tmsgs[-max_msgs:]\n",
        "    partes = []\n",
        "    for m in rec:\n",
        "        nombre = getattr(m, \"name\", \"tool\")\n",
        "        contenido = (m.content or \"\").strip()\n",
        "        if not contenido:\n",
        "            continue\n",
        "        if len(contenido) > 1200:\n",
        "            contenido = contenido[:1200] + \" ...\"\n",
        "        partes.append(f\"‚Ä¢ {nombre}\\n{contenido}\")\n",
        "    if not partes:\n",
        "        return \"\"\n",
        "    return \"S√≠ntesis basada en herramientas:\\n\\n\" + \"\\n\\n\".join(partes)\n",
        "\n",
        "# === 2) Reemplazo del ejecutor: menos iteraciones, s√≠ntesis inmediata y 429 proactivo ===\n",
        "class SimpleAgentExecutor:\n",
        "    def __init__(self, llm_base, system_text: str, tools: list, variante_label: str, max_iters: int = 2):\n",
        "        # ‚Üì max_iters bajo para no quemar cuota\n",
        "        self.llm = llm_base.bind_tools(tools) if tools else llm_base\n",
        "        self.system_text = system_text\n",
        "        self.tools = tools\n",
        "        self.tools_by_name = _map_tools_por_nombre(tools)\n",
        "        self.variante_label = variante_label\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def invoke(self, payload: dict, config: dict = None):\n",
        "        cfg = config or {}\n",
        "        sid = payload.get(\"session_id\") or (cfg.get(\"configurable\", {}) or {}).get(\"session_id\") or \"default\"\n",
        "\n",
        "        ensure_system_prompt(sid, self.system_text)\n",
        "        history = get_history(sid)\n",
        "\n",
        "        user_input = payload.get(\"input\") or \"\"\n",
        "        history.add_message(HumanMessage(content=user_input))\n",
        "        mensajes = history.messages[:]\n",
        "        explicito_web = _usuario_pide_web(user_input)\n",
        "\n",
        "        forced_rag_used = False\n",
        "        forced_web_used = False\n",
        "\n",
        "        for it in range(1, self.max_iters + 1):\n",
        "            try:\n",
        "                ai_msg = self.llm.invoke(mensajes)\n",
        "            except Exception as e:\n",
        "                msg = str(e)\n",
        "\n",
        "                # ‚Äî‚Äî‚Äî PARCHE 429 PROACTIVO ‚Äî‚Äî‚Äî\n",
        "                if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
        "                    if LOG_429_BANNER:\n",
        "                        print(\"‚ö†Ô∏è [MODO 429 PROACTIVO] LLM fall√≥ por cuota; forzando tools y sintetizando‚Ä¶\")\n",
        "                    did_tool = False\n",
        "\n",
        "                    # 1) Si el usuario pidi√≥ web y existe la tool, la forzamos\n",
        "                    if explicito_web and \"web_search\" in self.tools_by_name:\n",
        "                        web_tool = self.tools_by_name[\"web_search\"]\n",
        "                        forced_call = {\"name\": web_tool.name,\n",
        "                                       \"arguments\": {\"query\": user_input},\n",
        "                                       \"id\": f\"forced-web-{uuid4().hex[:8]}\"}\n",
        "                        tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                        _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "                        did_tool = True\n",
        "\n",
        "                    # 2) Si no hicimos web, forzamos el RAG principal de la variante\n",
        "                    if not did_tool:\n",
        "                        principal = None\n",
        "                        for key in [\"rag_search_a\", \"rag_search_b\"]:\n",
        "                            if key in self.tools_by_name:\n",
        "                                principal = self.tools_by_name[key]; break\n",
        "                        if principal is not None:\n",
        "                            forced_call = {\"name\": principal.name,\n",
        "                                           \"arguments\": {\"query\": user_input},\n",
        "                                           \"id\": f\"forced-{uuid4().hex[:8]}\"}\n",
        "                            tool_out = _ejecutar_tool_call(principal, forced_call)\n",
        "                            _patch_forced_rag_into_history(history, principal, tool_out)\n",
        "                            did_tool = True\n",
        "\n",
        "                    # 3) Sintetizamos con lo que haya salido de las tools forzadas\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                    # 4) Si por alguna raz√≥n no hubo tool posible, mensaje claro\n",
        "                    return {\n",
        "                        \"output\": (\"Estoy temporalmente limitado por cuota (429) y no logr√© ejecutar \"\n",
        "                                   \"una herramienta de respaldo. Intenta de nuevo o cambia de modelo.\"),\n",
        "                        \"iterations\": it\n",
        "                    }\n",
        "                # ‚Äî‚Äî‚Äî FIN PARCHE 429 PROACTIVO ‚Äî‚Äî‚Äî\n",
        "\n",
        "                # Otros errores no relacionados con cuota\n",
        "                return {\"output\": f\"[Error del modelo: {e}]\", \"iterations\": it}\n",
        "\n",
        "            history.add_message(ai_msg)\n",
        "            calls = _extraer_tool_calls(ai_msg)\n",
        "\n",
        "            # === Cuando hay tool_calls, las ejecutamos ===\n",
        "            if calls:\n",
        "                for call in calls:\n",
        "                    raw_name = (call.get(\"name\") or \"\").strip()\n",
        "                    t_name = _norm(raw_name)\n",
        "                    t_id = call.get(\"id\", \"\")\n",
        "                    if t_name == \"web_search\" and not explicito_web:\n",
        "                        tool_out = (\"Solicitud de web detectada, pero esta variante solo usa web_search si el usuario \"\n",
        "                                    \"lo pide expl√≠citamente. Contin√∫o con los apuntes.\")\n",
        "                    else:\n",
        "                        tool = self.tools_by_name.get(t_name)\n",
        "                        tool_out = (f\"[Tool '{raw_name}' no disponible en esta variante]\"\n",
        "                                    if tool is None else _ejecutar_tool_call(tool, call))\n",
        "                    _patch_calls_into_history(history, raw_name, tool_out, t_id)\n",
        "\n",
        "                # ‚ö†Ô∏è S√çNTESIS INMEDIATA: evita requerir 2¬∫ turno del LLM\n",
        "                if ALWAYS_SYNTHESIZE_AFTER_TOOLS:\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "                # Si quisieras dar oportunidad al LLM, comenta el return de arriba\n",
        "                mensajes = history.messages[:]\n",
        "                continue\n",
        "\n",
        "            # === Sin tool_calls: intentamos forzar lo necesario ===\n",
        "            txt = (ai_msg.content or \"\").strip()\n",
        "\n",
        "            # Forzar web si el usuario lo pidi√≥ expl√≠citamente\n",
        "            if explicito_web and not forced_web_used and \"web_search\" in self.tools_by_name:\n",
        "                web_tool = self.tools_by_name[\"web_search\"]\n",
        "                forced_call = {\"name\": web_tool.name, \"arguments\": {\"query\": user_input}, \"id\": f\"forced-web-{uuid4().hex[:8]}\"}\n",
        "                tool_out = _ejecutar_tool_call(web_tool, forced_call)\n",
        "                _patch_calls_into_history(history, web_tool.name, tool_out, forced_call[\"id\"])\n",
        "                # S√≠ntesis inmediata tras forzar web\n",
        "                sintetico = _sintetizar_desde_tools(history)\n",
        "                if sintetico:\n",
        "                    return {\"output\": sintetico, \"iterations\": it}\n",
        "                mensajes = history.messages[:]\n",
        "                forced_web_used = True\n",
        "                continue\n",
        "\n",
        "            # Forzar RAG una vez\n",
        "            if not forced_rag_used:\n",
        "                principal = None\n",
        "                for key in [\"rag_search_a\", \"rag_search_b\"]:\n",
        "                    if key in self.tools_by_name:\n",
        "                        principal = self.tools_by_name[key]; break\n",
        "                if principal is not None:\n",
        "                    forced_call = {\"name\": principal.name, \"arguments\": {\"query\": user_input}, \"id\": f\"forced-{uuid4().hex[:8]}\"}\n",
        "                    tool_out = _ejecutar_tool_call(principal, forced_call)\n",
        "                    _patch_forced_rag_into_history(history, principal, tool_out)\n",
        "                    # S√≠ntesis inmediata tras forzar RAG\n",
        "                    sintetico = _sintetizar_desde_tools(history)\n",
        "                    if sintetico:\n",
        "                        return {\"output\": sintetico, \"iterations\": it}\n",
        "                    mensajes = history.messages[:]\n",
        "                    forced_rag_used = True\n",
        "                    continue\n",
        "\n",
        "            # Si el modelo dio texto, devu√©lvelo\n",
        "            if txt:\n",
        "                return {\"output\": txt, \"iterations\": it}\n",
        "\n",
        "            # √öltimo recurso: s√≠ntesis si hay herramientas\n",
        "            sintetico = _sintetizar_desde_tools(history)\n",
        "            if sintetico:\n",
        "                return {\"output\": sintetico, \"iterations\": it}\n",
        "\n",
        "            return {\"output\": \"No fue posible recuperar informaci√≥n en este momento.\", \"iterations\": it}\n",
        "\n",
        "        # max_iters alcanzado: intenta s√≠ntesis antes de rendirte\n",
        "        sintetico = _sintetizar_desde_tools(history)\n",
        "        if sintetico:\n",
        "            return {\"output\": sintetico, \"iterations\": self.max_iters}\n",
        "        return {\"output\": \"Se alcanz√≥ el m√°ximo de iteraciones sin respuesta final.\", \"iterations\": self.max_iters}\n"
      ],
      "metadata": {
        "id": "5kVqDH1VI-6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_SESSION_STORES.clear(); _SYSTEM_SET.clear()\n",
        "tools_A = [rag_tool_a, web_search_tool]\n",
        "tools_B = [rag_tool_b, web_search_tool]\n",
        "agent_executor_A = build_agent_executor(llm, AGENT_PROMPT_A, tools_A, \"A (rag_search_a)\", max_iters=2)\n",
        "agent_executor_B = build_agent_executor(llm, AGENT_PROMPT_B, tools_B, \"B (rag_search_b)\", max_iters=2)\n",
        "print(\"A keys:\", list(agent_executor_A.tools_by_name.keys()))\n",
        "print(\"B keys:\", list(agent_executor_B.tools_by_name.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVcJeGK2sO6-",
        "outputId": "d8f737be-d52c-4d35-9386-0726cb0735c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü§ñ Creando agente A (rag_search_a) ...\n",
            "\n",
            "ü§ñ Creando agente B (rag_search_b) ...\n",
            "A keys: ['rag_search_a', 'web_search']\n",
            "B keys: ['rag_search_b', 'web_search']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üî¨ Tests concisos A/B (mismo prompt) + Web\n",
        "import uuid\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "def _sid(tag):\n",
        "    return f\"{tag}-{uuid.uuid4().hex[:6]}\"\n",
        "\n",
        "def _tool_names_for(session_id):\n",
        "    hist = _SESSION_STORES.get(session_id)\n",
        "    if not hist:\n",
        "        return []\n",
        "    return [m.name for m in hist.messages if isinstance(m, ToolMessage)]\n",
        "\n",
        "def _run(agent, tag, prompt):\n",
        "    sid = _sid(tag)\n",
        "    resp = agent.invoke({\"input\": prompt, \"session_id\": sid})\n",
        "    out = (resp.get(\"output\",\"\") or \"\")[:400].replace(\"\\n\", \" \")\n",
        "    tools = _tool_names_for(sid)\n",
        "    used = \", \".join(tools) if tools else \"(ninguna)\"\n",
        "    print(f\"\\n[{tag}] iters={resp.get('iterations')} ¬∑ salida:\\n  {out}\")\n",
        "    print(f\"   tools usadas: {used}\")\n",
        "    return sid, tools\n",
        "\n",
        "def _ok(msg, cond):\n",
        "    print((\"‚úÖ \" if cond else \"‚ùå \") + msg)\n",
        "\n",
        "assert agent_executor_A and agent_executor_B, \"A/B no est√°n construidos.\"\n",
        "\n",
        "# ----------------- 1) MISMO PROMPT: RAG A vs RAG B -----------------\n",
        "base_prompt = \"¬øQu√© es inteligencia artificial? Cita autor y documento de los apuntes.\"\n",
        "\n",
        "sidA_rag, toolsA_rag = _run(agent_executor_A, \"A¬∑RAG(mismo prompt)\", base_prompt)\n",
        "sidB_rag, toolsB_rag = _run(agent_executor_B, \"B¬∑RAG(mismo prompt)\", base_prompt)\n",
        "\n",
        "setA = set(toolsA_rag)\n",
        "setB = set(toolsB_rag)\n",
        "\n",
        "# Exclusividad: solo su RAG (permitimos llamadas repetidas, por eso usamos set)\n",
        "_ok(\"A usa SOLO rag_search_a (sin web_search ni rag_search_b)\",\n",
        "    setA == {\"rag_search_a\"})\n",
        "_ok(\"B usa SOLO rag_search_b (sin web_search ni rag_search_a)\",\n",
        "    setB == {\"rag_search_b\"})\n",
        "\n",
        "# ----------------- 2) MISMO PROMPT + WEB EXPL√çCITO -----------------\n",
        "web_prompt = base_prompt + \" Por favor, busca en la web internet 2 novedades recientes y resume en 2 l√≠neas.\"\n",
        "\n",
        "sidA_web, toolsA_web = _run(agent_executor_A, \"A¬∑WEB(mismo prompt)\", web_prompt)\n",
        "sidB_web, toolsB_web = _run(agent_executor_B, \"B¬∑WEB(mismo prompt)\", web_prompt)\n",
        "\n",
        "setA_web = set(toolsA_web)\n",
        "setB_web = set(toolsB_web)\n",
        "\n",
        "# Debe invocar web_search (si adem√°s cae a RAG, no es problema)\n",
        "_ok(\"A¬∑WEB invoca web_search\", \"web_search\" in setA_web)\n",
        "_ok(\"B¬∑WEB invoca web_search\", \"web_search\" in setB_web)\n",
        "\n",
        "# ----------------- RESUMEN -----------------\n",
        "print(\"\\n‚Äî RESUMEN ‚Äî\")\n",
        "print(f\"A¬∑RAG -> {'OK' if setA == {'rag_search_a'} else 'FAIL'}   | tools: {toolsA_rag}\")\n",
        "print(f\"B¬∑RAG -> {'OK' if setB == {'rag_search_b'} else 'FAIL'}   | tools: {toolsB_rag}\")\n",
        "print(f\"A¬∑WEB -> {'OK' if 'web_search' in setA_web else 'FAIL'} | tools: {toolsA_web}\")\n",
        "print(f\"B¬∑WEB -> {'OK' if 'web_search' in setB_web else 'FAIL'} | tools: {toolsB_web}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XauI6YzAzmPv",
        "outputId": "321368b7-28fc-4bef-f32c-55e21a41aed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[A¬∑RAG(mismo prompt)] iters=1 ¬∑ salida:\n",
            "  S√≠ntesis basada en herramientas:  ‚Ä¢ rag_search_a [A ¬∑ Resultado 1 ¬∑ score/distancia: 0.5599] Fragmento: de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligen\n",
            "   tools usadas: rag_search_a\n",
            "\n",
            "[B¬∑RAG(mismo prompt)] iters=1 ¬∑ salida:\n",
            "  S√≠ntesis basada en herramientas:  ‚Ä¢ rag_search_b [B ¬∑ Resultado 1 ¬∑ score/distancia: 0.7283] Fragmento: references [1] apuntes de la clase de inteligencia artificial, profesor steven andrey  pachecoportuguez,institutotecnolo'gicodecostarica,2025. Documento: DOC_001 ¬∑ chunk: DOC_001_B_006 Autor: Rodolfo David Acu√±a L√≥pez  [B ¬∑ Resultado 2 ¬∑ score/distancia: 0.6157] Fragmento: vii. conclusio'n los t\n",
            "   tools usadas: rag_search_b\n",
            "‚úÖ A usa SOLO rag_search_a (sin web_search ni rag_search_b)\n",
            "‚úÖ B usa SOLO rag_search_b (sin web_search ni rag_search_a)\n",
            "\n",
            "[A¬∑WEB(mismo prompt)] iters=1 ¬∑ salida:\n",
            "  S√≠ntesis basada en herramientas:  ‚Ä¢ rag_search_a [A ¬∑ Resultado 1 ¬∑ score/distancia: 0.5599] Fragmento: de transparencia y responsabilidad. vii. conclusio'n los temas revisados durante esta semana refuerzan la comprensio'ndeco'molosmodelosdelenguajemodernosprocesan informacio'n y co'mo se esta'n extendiendo hacia arquitecturas ma's complejas y u'tiles, como los sistemas rag y los agentes inteligen\n",
            "   tools usadas: rag_search_a\n",
            "\n",
            "[B¬∑WEB(mismo prompt)] iters=1 ¬∑ salida:\n",
            "  S√≠ntesis basada en herramientas:  ‚Ä¢ rag_search_b [B ¬∑ Resultado 1 ¬∑ score/distancia: 0.7283] Fragmento: references [1] apuntes de la clase de inteligencia artificial, profesor steven andrey  pachecoportuguez,institutotecnolo'gicodecostarica,2025. Documento: DOC_001 ¬∑ chunk: DOC_001_B_006 Autor: Rodolfo David Acu√±a L√≥pez  [B ¬∑ Resultado 2 ¬∑ score/distancia: 0.6157] Fragmento: vii. conclusio'n los t\n",
            "   tools usadas: rag_search_b\n",
            "‚ùå A¬∑WEB invoca web_search\n",
            "‚ùå B¬∑WEB invoca web_search\n",
            "\n",
            "‚Äî RESUMEN ‚Äî\n",
            "A¬∑RAG -> OK   | tools: ['rag_search_a']\n",
            "B¬∑RAG -> OK   | tools: ['rag_search_b']\n",
            "A¬∑WEB -> FAIL | tools: ['rag_search_a']\n",
            "B¬∑WEB -> FAIL | tools: ['rag_search_b']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aseg√∫rate de tener la instancia creada en una celda previa:\n",
        "# web_search_tool = create_web_search_tool()\n",
        "\n",
        "# 1) Limpiar sesiones/historial para que no arrastren estado viejo\n",
        "_SESSION_STORES.clear()\n",
        "_SYSTEM_SET.clear()\n",
        "\n",
        "# 2) Reconstruir ambos agentes *incluyendo* la tool web\n",
        "tools_A = [rag_tool_a, web_search_tool]\n",
        "tools_B = [rag_tool_b, web_search_tool]\n",
        "\n",
        "agent_executor_A = build_agent_executor(llm, AGENT_PROMPT_A, tools_A, \"A (rag_search_a)\", max_iters=2)\n",
        "agent_executor_B = build_agent_executor(llm, AGENT_PROMPT_B, tools_B, \"B (rag_search_b)\", max_iters=2)\n",
        "\n",
        "# 3) Verificar que ambos ven 'web_search'\n",
        "print(\"A keys:\", list(agent_executor_A.tools_by_name.keys()))\n",
        "print(\"B keys:\", list(agent_executor_B.tools_by_name.keys()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCPYcKRNly1P",
        "outputId": "f272f94e-636c-472a-98f3-9698c2986db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü§ñ Creando agente A (rag_search_a) ...\n",
            "\n",
            "ü§ñ Creando agente B (rag_search_b) ...\n",
            "A keys: ['rag_search_a', 'web_search']\n",
            "B keys: ['rag_search_b', 'web_search']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 4: Probar el agente con ejemplos\n",
        "# ============================================================\n",
        "\n",
        "if agent_executor:\n",
        "    print(\"üß™ Probando el agente con ejemplos...\\n\")\n",
        "\n",
        "    # Si te molestan los logs ruidosos, puedes desactivar verbose en tiempo de ejecuci√≥n:\n",
        "    agent_executor.verbose = False   # ‚Üê opcional\n",
        "\n",
        "    def safe_tools_used(result):\n",
        "        steps = result.get(\"intermediate_steps\", []) or []\n",
        "        tools = []\n",
        "        for step in steps:\n",
        "            try:\n",
        "                action = step[0]  # (AgentAction, observation)\n",
        "                tools.append(getattr(action, \"tool\", \"_Unknown\"))\n",
        "            except Exception:\n",
        "                tools.append(\"_Exception\")\n",
        "        return tools\n",
        "\n",
        "    # ---------- Ejemplo 1 ----------\n",
        "    print(\"=\"*70)\n",
        "    print(\"Ejemplo 1: Consulta sobre los apuntes\")\n",
        "    print(\"=\"*70)\n",
        "    test_query_1 = \"¬øQu√© es la inteligencia artificial seg√∫n los apuntes del curso?\"\n",
        "    print(f\"\\n‚ùì Pregunta: {test_query_1}\\n\")\n",
        "\n",
        "    try:\n",
        "        result_1 = agent_executor.invoke({\"input\": test_query_1})\n",
        "        output_1 = result_1.get(\"output\", \"\")\n",
        "        print(\"\\n‚úÖ Respuesta del agente:\")\n",
        "        print(output_1[:500] + \"...\" if len(output_1) > 500 else output_1)\n",
        "        print(f\"\\nüîß Herramientas usadas: {safe_tools_used(result_1)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "    # ---------- Ejemplo 2 ----------\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Ejemplo 2: Consulta espec√≠fica\")\n",
        "    print(\"=\"*70)\n",
        "    test_query_2 = \"Expl√≠came sobre aprendizaje supervisado\"\n",
        "    print(f\"\\n‚ùì Pregunta: {test_query_2}\\n\")\n",
        "\n",
        "    try:\n",
        "        result_2 = agent_executor.invoke({\"input\": test_query_2})\n",
        "        output_2 = result_2.get(\"output\", \"\")\n",
        "        print(\"\\n‚úÖ Respuesta del agente:\")\n",
        "        print(output_2[:500] + \"...\" if len(output_2) > 500 else output_2)\n",
        "        print(f\"\\nüîß Herramientas usadas: {safe_tools_used(result_2)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  El agente no est√° configurado. Configura GOOGLE_API_KEY primero.\")\n"
      ],
      "metadata": {
        "id": "sSjoa9ko_ag7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "98841f82-25fd-4b38-d6d1-da04e12645ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent_executor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3420841964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ============================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0magent_executor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üß™ Probando el agente con ejemplos...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent_executor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 5: App Streamlit con ‚ÄúWeb solo si el usuario lo pide‚Äù\n",
        "# ============================================================\n",
        "\n",
        "import os, pickle, re\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.tools.render import render_text_description\n",
        "from langchain.tools import Tool\n",
        "\n",
        "# --------- guardar config para la app ----------\n",
        "STREAMLIT_DATA_PATH = os.path.join(OUT_DIR, \"streamlit_data.pkl\")\n",
        "with open(STREAMLIT_DATA_PATH, \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"agent_prompt\": AGENT_PROMPT,\n",
        "        \"vectorstore_a_path\": VECTORSTORE_DIR_A,\n",
        "        \"vectorstore_b_path\": VECTORSTORE_DIR_B,\n",
        "        \"proyecto_dir\": PROYECTO_DIR,\n",
        "    }, f)\n",
        "print(\"‚úÖ Configuraci√≥n guardada para Streamlit:\", STREAMLIT_DATA_PATH)\n",
        "\n",
        "# --------- archivo de la app ----------\n",
        "STREAMLIT_APP_CODE = f'''import streamlit as st\n",
        "import os, re, pickle\n",
        "from typing import List\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.tools import Tool\n",
        "from langchain.tools.render import render_text_description\n",
        "\n",
        "# DuckDuckGo (opcional)\n",
        "try:\n",
        "    from langchain_community.tools import DuckDuckGoSearchRun\n",
        "    _HAS_DDG = True\n",
        "except Exception:\n",
        "    _HAS_DDG = False\n",
        "\n",
        "st.set_page_config(page_title=\"AsistenteIA - Curso de IA\", page_icon=\"ü§ñ\", layout=\"wide\")\n",
        "st.title(\"ü§ñ AsistenteIA - Curso de Inteligencia Artificial\")\n",
        "st.caption(\"Prioriza apuntes (RAG). Usa la web **solo** si lo pides expl√≠citamente.\")\n",
        "\n",
        "BASE_DIR = \"{PROYECTO_DIR}\"\n",
        "OUT_DIR = os.path.join(BASE_DIR, \"dataset\")\n",
        "CFG_PATH = os.path.join(OUT_DIR, \"streamlit_data.pkl\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_cfg():\n",
        "    with open(CFG_PATH, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_embeddings():\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={{\"device\": \"cpu\"}},\n",
        "        encode_kwargs={{\"normalize_embeddings\": True}}\n",
        "    )\n",
        "\n",
        "@st.cache_resource\n",
        "def load_vectorstores():\n",
        "    cfg = load_cfg()\n",
        "    emb = load_embeddings()\n",
        "    vs_a = FAISS.load_local(cfg[\"vectorstore_a_path\"], emb, allow_dangerous_deserialization=True)\n",
        "    vs_b = FAISS.load_local(cfg[\"vectorstore_b_path\"], emb, allow_dangerous_deserialization=True)\n",
        "    return vs_a, vs_b\n",
        "\n",
        "def create_rag_tool(vs, name: str) -> Tool:\n",
        "    def rag_search(query: str, k: int = 5) -> str:\n",
        "        try:\n",
        "            results = vs.similarity_search_with_score(query, k=k)\n",
        "            if not results:\n",
        "                return \"No se encontraron fragmentos relevantes.\"\n",
        "            out = []\n",
        "            for i, (doc, score) in enumerate(results, 1):\n",
        "                autor = doc.metadata.get(\"autor\", \"N/A\")\n",
        "                id_doc = doc.metadata.get(\"id_doc\", \"N/A\")\n",
        "                chunk_id = doc.metadata.get(\"chunk_id\", \"N/A\")\n",
        "                frag = (doc.page_content or \"\")[:700].replace(\"\\\\n\",\" \")\n",
        "                out.append(\n",
        "                    f\"[Resultado {{i}} ¬∑ Score: {{score:.4f}}]\\\\n\"\n",
        "                    f\"Fragmento: {{frag}}...\\\\n\"\n",
        "                    f\"Seg√∫n [{{autor}}] en [{{id_doc}}] (chunk {{chunk_id}}).\"\n",
        "                )\n",
        "            return \"\\\\n\\\\n\".join(out)\n",
        "        except Exception as e:\n",
        "            return f\"Error en RAG {{name}}: {{e}}\"\n",
        "    return Tool(\n",
        "        name=f\"rag_search_{{name}}\",\n",
        "        description=f\"Busca en apuntes (segmentaci√≥n {{name}}). Devuelve fragmentos con cita.\",\n",
        "        func=rag_search\n",
        "    )\n",
        "\n",
        "def create_web_tool() -> Tool:\n",
        "    if not _HAS_DDG:\n",
        "        return Tool(\n",
        "            name=\"web_search\",\n",
        "            description=\"B√∫squeda web (stub). √ösala solo si la pides expl√≠citamente.\",\n",
        "            func=lambda q: \"WebSearch no disponible (falta DuckDuckGoSearchRun).\"\n",
        "        )\n",
        "    search = DuckDuckGoSearchRun()\n",
        "    def _web(q: str) -> str:\n",
        "        try:\n",
        "            r = search.run(q)\n",
        "            return f\"Resultados web para '{{q}}'\\\\n\\\\n{{r}}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error en b√∫squeda web: {{e}}\"\n",
        "    return Tool(\n",
        "        name=\"web_search\",\n",
        "        description=\"B√∫squeda en internet (DuckDuckGo). Solo cuando lo pidas expl√≠citamente.\",\n",
        "        func=_web\n",
        "    )\n",
        "\n",
        "# --------- Estado ----------\n",
        "if \"messages\" not in st.session_state: st.session_state.messages = []\n",
        "if \"agent_rag\" not in st.session_state: st.session_state.agent_rag = None\n",
        "if \"agent_rag_web\" not in st.session_state: st.session_state.agent_rag_web = None\n",
        "\n",
        "# --------- Sidebar ----------\n",
        "with st.sidebar:\n",
        "    st.header(\"‚öôÔ∏è Configuraci√≥n\")\n",
        "    gkey = st.text_input(\"Google API Key (Gemini)\", type=\"password\", value=os.getenv(\"GOOGLE_API_KEY\",\"\"))\n",
        "    if gkey: os.environ[\"GOOGLE_API_KEY\"] = gkey\n",
        "\n",
        "    if gkey and (st.session_state.agent_rag is None or st.session_state.agent_rag_web is None):\n",
        "        with st.spinner(\"Inicializando agentes...\"):\n",
        "            cfg = load_cfg()\n",
        "            vs_a, vs_b = load_vectorstores()\n",
        "\n",
        "            ragA = create_rag_tool(vs_a, \"A\")\n",
        "            ragB = create_rag_tool(vs_b, \"B\")\n",
        "            web  = create_web_tool()\n",
        "\n",
        "            def make_agent(tools):\n",
        "                llm = ChatGoogleGenerativeAI(\n",
        "                    model=\"gemini-2.5-flash\",\n",
        "                    google_api_key=gkey,\n",
        "                    temperature=0.1\n",
        "                )\n",
        "                tool_str = render_text_description(tools)\n",
        "                tool_names = \", \".join([t.name for t in tools])\n",
        "\n",
        "                # ‚úÖ Incluimos {{history}} y {{agent_scratchpad}} en el template\n",
        "                prompt = PromptTemplate(\n",
        "                    template=(\n",
        "                        \"{{agent_profile}}\\\\n\\\\n\"\n",
        "                        \"Tienes acceso a estas herramientas:\\\\n{{tools}}\\\\n\\\\n\"\n",
        "                        \"Sigue este formato EXACTO (sin bloques de c√≥digo):\\\\n\\\\n\"\n",
        "                        \"Question: {{input}}\\\\n\"\n",
        "                        \"Thought: razona brevemente el siguiente paso\\\\n\"\n",
        "                        \"Action: una de [{{tool_names}}]\\\\n\"\n",
        "                        \"Action Input: el input para la acci√≥n\\\\n\"\n",
        "                        \"Observation: el resultado de la acci√≥n\\\\n\"\n",
        "                        \"... (repite Thought/Action/Action Input/Observation si hace falta) ...\\\\n\"\n",
        "                        \"Thought: I now know the final answer\\\\n\"\n",
        "                        \"Final Answer: tu respuesta final en espa√±ol, citando autor y documento si aplica\\\\n\\\\n\"\n",
        "                        \"Historial:\\\\n{{history}}\\\\n\\\\n\"\n",
        "                        \"Razonamiento y pasos previos:\\\\n{{agent_scratchpad}}\\\\n\"\n",
        "                    ),\n",
        "                    input_variables=[\"history\",\"input\",\"agent_scratchpad\",\"tools\",\"tool_names\"]\n",
        "                ).partial(\n",
        "                    agent_profile=cfg[\"agent_prompt\"],\n",
        "                    tools=tool_str,\n",
        "                    tool_names=tool_names\n",
        "                )\n",
        "\n",
        "                memory = ConversationBufferWindowMemory(\n",
        "                    k=5,\n",
        "                    memory_key=\"history\",\n",
        "                    return_messages=True,\n",
        "                    output_key=\"output\"\n",
        "                )\n",
        "\n",
        "                agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
        "                return AgentExecutor(\n",
        "                    agent=agent,\n",
        "                    tools=tools,\n",
        "                    memory=memory,\n",
        "                    verbose=True,\n",
        "                    handle_parsing_errors=True,\n",
        "                    max_iterations=8,\n",
        "                    return_intermediate_steps=True\n",
        "                )\n",
        "\n",
        "            # agente SOLO RAG\n",
        "            st.session_state.agent_rag = make_agent([ragA, ragB])\n",
        "            # agente RAG + Web\n",
        "            st.session_state.agent_rag_web = make_agent([ragA, ragB, web])\n",
        "\n",
        "            st.success(\"‚úÖ Agentes listos: RAG y RAG+Web\")\n",
        "\n",
        "# --------- Render previo ----------\n",
        "for m in st.session_state.messages:\n",
        "    with st.chat_message(m[\"role\"]):\n",
        "        st.markdown(m[\"content\"])\n",
        "\n",
        "# --------- Chat ----------\n",
        "if user_prompt := st.chat_input(\"Pregunta algo sobre los apuntes de IA...\"):\n",
        "    st.session_state.messages.append({{\"role\":\"user\",\"content\":user_prompt}})\n",
        "    with st.chat_message(\"user\"): st.markdown(user_prompt)\n",
        "\n",
        "    # ¬øEl usuario pidi√≥ expl√≠citamente web?\n",
        "    wants_web = re.search(r\"\\\\b(web|internet|google|websearch|buscar en (la )?web)\\\\b\", user_prompt, re.I)\n",
        "\n",
        "    executor = st.session_state.agent_rag_web if wants_web else st.session_state.agent_rag\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        if executor is None:\n",
        "            st.warning(\"Configura la API key en el sidebar.\")\n",
        "        else:\n",
        "            with st.spinner(\"Pensando...\"):\n",
        "                try:\n",
        "                    result = executor.invoke({{\"input\": user_prompt}})\n",
        "                    resp = result.get(\"output\",\"\")\n",
        "                    st.markdown(resp)\n",
        "                    st.session_state.messages.append({{\"role\":\"assistant\",\"content\":resp}})\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error: {{e}}\")\n",
        "'''\n",
        "\n",
        "STREAMLIT_APP_PATH = os.path.join(PROYECTO_DIR, \"streamlit_app.py\")\n",
        "with open(STREAMLIT_APP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(STREAMLIT_APP_CODE)\n",
        "\n",
        "print(\"‚úÖ App Streamlit escrita:\")\n",
        "print(\"   \", STREAMLIT_APP_PATH)\n",
        "print(\"üöÄ Ejecuta:  streamlit run streamlit_app.py  (o tu bloque con ngrok)\")\n"
      ],
      "metadata": {
        "id": "VV3it8BI_c_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================\n",
        "# Paso 6: Resumen final y verificaci√≥n de Compa√±ero 3 (actualizado)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Intentar recuperar rutas ya definidas; si no existen, usar valores por defecto seguros\n",
        "try:\n",
        "    app_path = STREAMLIT_APP_PATH\n",
        "except NameError:\n",
        "    try:\n",
        "        app_path = os.path.join(PROYECTO_DIR, \"streamlit_app.py\")\n",
        "    except NameError:\n",
        "        app_path = \"streamlit_app.py\"\n",
        "\n",
        "try:\n",
        "    proyecto_dir = PROYECTO_DIR\n",
        "except NameError:\n",
        "    proyecto_dir = \".\"\n",
        "\n",
        "try:\n",
        "    streamlit_data_path = STREAMLIT_DATA_PATH\n",
        "except NameError:\n",
        "    try:\n",
        "        streamlit_data_path = os.path.join(os.path.join(proyecto_dir, \"dataset\"), \"streamlit_data.pkl\")\n",
        "    except Exception:\n",
        "        streamlit_data_path = \"dataset/streamlit_data.pkl\"\n",
        "\n",
        "try:\n",
        "    vectorstore_a_path = VECTORSTORE_DIR_A\n",
        "except NameError:\n",
        "    vectorstore_a_path = \"<VECTORSTORE_DIR_A>\"\n",
        "\n",
        "try:\n",
        "    vectorstore_b_path = VECTORSTORE_DIR_B\n",
        "except NameError:\n",
        "    vectorstore_b_path = \"<VECTORSTORE_DIR_B>\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"‚úÖ RESUMEN DE LA PARTE DEL COMPA√ëERO 3\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìã Componentes implementados:\")\n",
        "\n",
        "print(\"\\n1. ‚úÖ Prompt base/perfil del agente:\")\n",
        "print(\"   - Nombre: AsistenteIA\")\n",
        "print(\"   - Rol: Asistente acad√©mico especializado en IA\")\n",
        "print(\"   - Estilo: Claro, educativo, con citas\")\n",
        "print(\"   - Restricci√≥n: Primero apuntes (RAG), luego web (solo si el usuario lo solicita o no hay cobertura)\")\n",
        "\n",
        "print(\"\\n2. ‚úÖ Agente orquestador:\")\n",
        "print(\"   - Modelos preferidos (fallback en cascada):\")\n",
        "print(\"       gemini-2.5-flash ‚Üí gemini-flash-latest ‚Üí gemini-2.5-pro ‚Üí gemini-pro-latest ‚Üí gemini-2.0-flash ‚Üí gemini-1.5-flash\")\n",
        "print(\"   - Par√°metros: temperature=0.1, max_output_tokens=1024\")\n",
        "print(\"   - Patr√≥n: ReAct ESTRICTO (Reasoning + Acting) con formato impuesto\")\n",
        "print(\"   - Post-procesado: verificaci√≥n de cita con ensure_citation\")\n",
        "print(\"   - Decisi√≥n: Entre rag_search_A, rag_search_B, web_search (stub) o cierre directo\")\n",
        "print(\"   - Max iteraciones: 8\")\n",
        "print(\"   - early_stopping_method: generate\")\n",
        "print(\"   - return_intermediate_steps: True\")\n",
        "\n",
        "print(\"\\n3. ‚úÖ Memoria conversacional:\")\n",
        "print(\"   - Tipo: ConversationBufferWindowMemory\")\n",
        "print(\"   - Ventana: √öltimas 5 interacciones (k=5)\")\n",
        "print(\"   - Caracter√≠stica: No guarda historial permanente\")\n",
        "\n",
        "print(\"\\n4. ‚úÖ Interfaz Streamlit:\")\n",
        "print(\"   - Aplicaci√≥n web completa con chat en tiempo real\")\n",
        "print(\"   - Indica herramientas usadas por turno\")\n",
        "print(\"   - Sidebar para configurar GOOGLE_API_KEY y ver √∫ltimas herramientas\")\n",
        "print(f\"   - Archivo de aplicaci√≥n: {app_path}\")\n",
        "print(f\"   - Configuraci√≥n guardada: {streamlit_data_path}\")\n",
        "\n",
        "print(\"\\n5. ‚úÖ Integraci√≥n completa:\")\n",
        "print(\"   - Agente + Tools + Memoria + Interfaz integrados\")\n",
        "print(\"   - Listo para demostraci√≥n en vivo con tus vectorstores\")\n",
        "\n",
        "print(\"\\nüìä Herramientas disponibles para el agente:\")\n",
        "print(\"   1) rag_search_A: B√∫squeda en apuntes (segmentaci√≥n A - chunks fijos) con cita obligatoria\")\n",
        "print(\"   2) rag_search_B: B√∫squeda en apuntes (segmentaci√≥n B - encabezados) con cita obligatoria\")\n",
        "print(\"   3) web_search: B√∫squeda web (stub en esta demo; usar solo si se solicita)\")\n",
        "\n",
        "print(\"\\nüìÅ Rutas de vectorstores (FAISS):\")\n",
        "print(f\"   - VECTORSTORE_DIR_A: {vectorstore_a_path}\")\n",
        "print(f\"   - VECTORSTORE_DIR_B: {vectorstore_b_path}\")\n",
        "\n",
        "print(\"\\nüöÄ Para ejecutar la aplicaci√≥n:\")\n",
        "print(\"   1) Asegura que los vectorstores (A y B) existan en las rutas anteriores.\")\n",
        "print(\"   2) Configura la variable de entorno GOOGLE_API_KEY (o en el sidebar de la app).\")\n",
        "print(f\"   3) cd {proyecto_dir}\")\n",
        "print(f\"   4) streamlit run {os.path.basename(app_path)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ COMPA√ëERO 3 - TAREA COMPLETADA\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüéØ El sistema est√° listo para:\")\n",
        "print(\"   - Comparar el comportamiento de ambas segmentaciones (A vs B)\")\n",
        "print(\"   - Observar herramientas usadas, trazas ReAct e impacto de la memoria\")\n",
        "print(\"   - Demostraci√≥n presencial con consultas reales del curso\")\n",
        "print(\"=\"*70)\n",
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "id": "AuG_0BzK_f-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install -q \"langchain_community==0.2.*\" duckduckgo-search"
      ],
      "metadata": {
        "id": "E-6Z4ZGXv-V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip -q install pyngrok streamlit requests\n",
        "\n",
        "import os, time, shlex, subprocess, requests\n",
        "from pyngrok import ngrok\n",
        "\n",
        "APP_PATH = \"/content/drive/MyDrive/Colab Notebooks/Tarea3-IA/streamlit_app.py\"\n",
        "PORT = 8502\n",
        "\n",
        "# 1) Arranca Streamlit\n",
        "!pkill -f \"streamlit run\" || true\n",
        "cmd = f'streamlit run \"{APP_PATH}\" --server.port {PORT} --server.address 0.0.0.0 --server.headless true --browser.gatherUsageStats false'\n",
        "sp = subprocess.Popen(shlex.split(cmd))\n",
        "\n",
        "# 2) Espera a que est√© saludable\n",
        "import time\n",
        "ok = False\n",
        "for _ in range(40):\n",
        "    time.sleep(0.5)\n",
        "    try:\n",
        "        r = requests.get(f\"http://localhost:{PORT}/_stcore/health\", timeout=1)\n",
        "        if r.ok:\n",
        "            ok = True\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "if not ok:\n",
        "    raise SystemExit(\"Streamlit no respondi√≥; revisa logs con: !pkill -f 'streamlit run'; !tail -n 120 /tmp/streamlit.log\")\n",
        "\n",
        "# 3) Configura authtoken (desde Secrets o pegado)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    token = userdata.get(\"NGROK_AUTHTOKEN\")\n",
        "except Exception:\n",
        "    token = None\n",
        "# token = \"PEGAR_AQUI_TU_TOKEN\"  # <-- alternativa manual\n",
        "if not token:\n",
        "    raise SystemExit(\"Falta NGROK_AUTHTOKEN (ponlo en Colab Secrets o p√©galo en la variable 'token').\")\n",
        "\n",
        "ngrok.set_auth_token(token)\n",
        "\n",
        "# 4) Cierra t√∫neles previos y abre uno nuevo\n",
        "for t in ngrok.get_tunnels():\n",
        "    ngrok.disconnect(t.public_url)\n",
        "\n",
        "public_url = ngrok.connect(PORT, \"http\").public_url\n",
        "print(\"üåê URL p√∫blica:\", public_url)\n"
      ],
      "metadata": {
        "id": "L5GWdnK3yW3p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}